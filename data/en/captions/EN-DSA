welcome. i'm bob sedgewick, professor of computer science at princeton. this is our online course algorithms developed by myself and kevin wayne here at princeton. we're gonna start with an overview discussion of why you might want to study algorithms and a little bit of discussion about the resources that you need to take this course. so, what is this course? it's an intermediate level survey course on algorithms. we're going to concentrate on programming and problem solving in the context of real applications, and our focus is going to be on two things, algorithms which are methods for solving problems and data structures which store the information associated in problem, with a problem and go hand in hand with algorithms. these are the basic topics that we'll cover in part one and part two of the course. the first part is data type sorting and searching. we'll consider a number of data structures and algorithms that are basic to all the methods we consider including stacks, queues, bags and priority queues. then we'll consider classic algorithms for sorting, putting things in order. that's quicksort, mergesort, heapsort and radix sorts. and we'll consider classic methods for searching. including binary search trees, red-black binary search trees and hash tables. the second part of the course is for more advanced algorithms including graph algorithms, classic graph searching algorithms, minimum spanning tree and shortest path algorithms, algorithms for processing strings including regular expressions and data compression. and then some advanced algorithms that make use of the basic algorithms that we developed earlier in the course. so, why should one study algorithms? well, their input, impact is very broad and far-reaching. from the internet to biology to, commercial computing, computer graphics, security, multimedia, social networks, and scientific applications, algorithms are all around us. they're used for movies and video games, for particle collision simulation, they're used to study the genome, and all manner of other applications. so, that's one important reason to study algorithms, their impact is broad and far-reaching. algorithms are also interesting to study, because they, they have ancient roots. now the first algorithm we studied goes back to 300 b.c., dating at least to euclid. the concept of an algorithm was formalized actually here at princeton, by church and turing, in the 1930s. but most algorithms that we consider, were discovered in recent decades. in fact, some were discovered by undergraduates in a course, course like this. and there's plenty of other algorithms waiting to be discovered by students like you. the main reason that people study algorithms, is to be able to solve problems that it could not otherwise be addressed. for example, in the first lecture, we're going to talk about the network connectivity problem, where the problem is, given a large set of items that are connected together pairwise is there a way to get from one to another with a path through the connections. as you can see from this example, it's not clear whether or not there's such a path, we need a computer program to do it, in fact, we need an efficient algorithm to do it. in this case the answer is that there is such a path. another reason to study algorithms is for intellectual stimulation. algorithms are very interesting objects to study. don knuth who wrote several books on, on algorithms and was a pioneer in the field said that, "an algorithm must be seen to be believed." you can't just think about an algorithm you have to work with it. another quote from francis sullivan, says, "the great algorithms are the poetry of computation." just like verse, they can be terse, elusive, dense, and even mysterious. but once unlocked, they cast a brilliant new light on some aspect of computing. algorithms are interesting for intellectual stimulation. another reason many people study algorithms and i suspect many of you, is it's necessary to understand good algorithms, efficient algorithms, a good data structures in order to be a proficient programmer. linus torvalds, who created lin, linux, says that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. bad programmers worry about the code, good programmers worry about data structures, and their relationships. and, i might add, the algorithms that process them. niklaus wirth, another pioneer in computer science, wrote a famous book called algorithms + data structures = programs. [cough]. another reason nowadays to study algorithms is that, they have become a common language for understanding, nature. algorithms are computational models, and algorithmic models are replacing mathematical models in scientific inquiry. in the twentieth century, math, scientists developed mathematical models to try to understand natural phenomenon. it soon became clear that those mathematical models were difficult to solve. it was difficult to create solutions, to be able to test hypotheses against natural phenomenon. so, more and more and more now a days people are developing computational models, where they attempt to simulate what might be happening in nature in order to try to better understand it. algorithms play an extremely important role in this process. and we'll see some examples of this in this course. another important reason is that if you know effect, how to effectively use algorithms and data structures you're going to have a much better chance at interviewing for a job in the technology industry then if you don't. so, here's a bunch of reasons that i just went through for studying algorithms. their impact's broad and far-reaching, they have old roots and present new opportunities, they allow us to solve problems that could not otherwise be addressed, you can use them for intellectual stimulation to become a proficient programmer. they might unlock the secrets of life in the universe, and they're good for fun and profit. in fact, a pr ogrammer might ask, why study anything else? well, there's plenty of good reasons to study other things, but i'll submit there's no good reason not to study algorithims. [cough] so, for this course we have two resources that i want to talk about and make sure that people are familiar with before entering into the content. this is a publishing model that kevin wayne and i developed and have been using for many years, and we think it's a very effective way to support the, kinds of lectures that we're going to be giving in this course. down at the bottom, and it's optional for this course, we have a text book. it's a traditional, text book that extensively covers the topics in the course, in fact many more topics than we can present in lecture. and then supporting that textbook, is free online material that we call the book site. you can go to books, the book site to see the lecture slides. but more important, there's code, there's exercises, tere's a great deal of information there. in fact, maybe ten times what's in the book, including a summary of the content. so, during this course you'll be referring to the book site frequently while working online. people often ask about prerequisites. we're assuming that people who take this course know how to program, and know the basics of loops, arrays, functions. they have some exposure to object oriented programming and recursion. we use the java language, but we don't dwell on details of java, we mostly use it as an expository language. we do some math, but not advanced math. if you want to review the material that we think is prerequisite for the material in this course, you can do a quick review by looking at sections 1.1 and 1.2 of the book. either at the book site or in the text book. if you want an in depth review, we have a full text book called, an introduction to programming in java: an interdisciplinary approach. there is a book site and text book as well. but, the bottom line is, you should be able t o program, and the quick exercise to get ready is, to write a java program on your computer perhaps using a programming model, as described on the book site. we will provide much more detail information on that as we get into the assignments. you can use your own programming environment if your comfortable with one or you download ours. we have instructions on the web on how to do that. 
welcome back to algorithms. today, we're going to talk about the union find problem. a set of algorithms for solving the so-called dynamic connectivity problem. we'll look at two classic algorithms. quick find and quick union, and some applications and improvements of those algorithms. the subtext of today's lecture really is to go through the steps that we'll follow over and over again to develop a useful algorithm. the first step is to model the problem. try to understand, basically, what are the main elements of the problem that need to be solved. then we'll find some algorithm to solve the problem. in many cases, the first algorithm we come up with would be fast enough and maybe it fits in memory and, we'll go ahead and use it, and be off and running. but in many other cases maybe it's not fast enough, or there's not enough memory. so, what we do is try to figure out why, find a way to address whatever's causing that problem, find a new algorithm and iterate until we're satisfied. this is the scientific approach to designing and analyzing algorithms, where we build mathematical models to try and understand what's going on, and then we do experiments to validate those models and help us improve things. so, first we'll talk about the dynamic connectivity problem, the model of the problem for union find. so, here's the idea. they're going to have a set of n objects. doesn't really matter what they are. we're going to use the numbers, zero through n to model our objects. and then, we have the idea of a connection between two objects. and, we'll, postulate that there's going to be a command that says, connect two objects. given two objects, provide a connection between them. and then key part of the problem is find query or the connected query, which just asks, is there a path connecting the two objects. so for example, in this set of ten objects, we performed already, a bunch of union commands, connecting four and three, three and eight, six and five, nine and four, two and one. and now we might have a connected query that says, is zero connect ed to seven? well, in this case, there is no connection, so we say no. but if we ask is eight connected to nine? we are going to say yes, even no we don't have a direct connection between eight and nine. there is a path from eight to three to four to nine. so, that's our problem, to be able to officially support these two commands for given set of objects. now, let's say we add a union five, zero. so, that creates a connection between five and zero. seven and two creates a connection between seven and two. and six and one, between six and one. so, now if we ask our zero connected to seven, well one and zero we can do that too. and that's a redundant connection. and now, if we ask is zero connected to seven we're going to answer yes. so that's our problem, intermix union, commands and connected queries and we need to be able to officially support those commands for a large number of objects. so, here's a much bigger example. and you can see that we're going to need efficient algorithms for this. first of all, you can see we're going to need a computer for this. it would take quite, quite some time for a human to figure out whether there's a connection. in this case there is a connection. now, the algorithms that we're looking at today are not going to actually give the path connecting the two objects. it's just going to be able to answer the question, is there a path? in part two of the course, we'll consider algorithms that explicitly find paths. they're not as efficient as union find because they have more work to do. now, applications of these, these algorithms involve objects of all types. these are used for digital photos, where the objects are pixels they're used for networks, where the objects are computers, social networks, where it's people, or computer chips, where it's circuit elements or abstract things like variable names in a program, or elements in a mathematical set, or physical things like metallic sites in a composite system. so, all different types of objects for, but for programming we're going to associate each object with a name and we'll just name the objects with a number, integers from zero to n-1. that's a very convenient initial starting point for our programs because we can use integers as an index into an array then, and then quickly access information relevant to each object. and it also just supresses a lot of details that are not relevant to union find. in fact, to make this mapping from an object name to the integer zero through n - one is to find application of a symbol table or a searching algorithm, which is one of the things that we'll be studying later in this course algorithms and data structures for solving that problem. now, the connections, well, we need, a few abstract properties that these connections have to satisfy. and they're all quite natural and intuitive. so we assume that is connected to is an equivalence relation. that is, every object's connected to itself, it's symmetric. if p's connected to q, then q's connected to p, and it's transitive. if p's connected to q, and q's connected to r, then p's connected to r. now these properties are very intuitive. but it's worthwhile to state them explicitly and make sure that our algorithms maintain them. when we have an equivalence relation a set of objects and connections divide into subsets called connected components. a connected component is a maximal set of objects that's mutually connected. for example in this small example here, there's three connected components. one consisting of just object zero, second one objects one, four and five. and third one the other four objects. and these components have the property that if any two objects in them are connected and there is no object outside that is connected to those objects, that's connected components. our algorithms will gain efficiency by maintaining connected components and using that knowledge to efficiently answer the query that's, that they're presented with. okay, so to implement the operations, we have to find query and the union command. and so we're going to mai ntain the connected components. the find is going to have to check if two objects are in the same component and the union command is going to have to replace components containing two objects with their union. so, for example, if we have these components, and we get the command to union connect, two and five. essentially, we need to merge the connected components containing the one containing two or the one containing five to get a big connected components and now we have only two connected components. all of that leads up to, in a programming world to specifying, a data type which is simply specification of the methods that we are want to going to implement in order to solve this problem. so you know, typical java model, what we will do is create a class called uf that contains two methods, one to implement union, the other one to implement connected, which returns a boolean. the constructor, takes sr unit, the number of objects, so that it can build data structure based on the number of objects. so, and we have to, bear in mind, as we're building our logarithms, that both the number of objects can be huge, but also, the number of operations. we can have a, a very large number, of union and connected, operations and our algorithms are going to have to be efficient, under those conditions. one of the practices that will follow often in this course is to check our api design before getting too far into dealing with the problem, by building a client that is going to use the data type that we develop. so, for this example, we've got a client that, will read information from standard input. first, an integer which is the number of objects that are going to be processed. and then a series of pairs of object names. and what the client does is it, it'll, first it'll read the integer from standard input, and create a, a uf object. and then as long as standard input is not empty, it's going to read two integers from the input. and if they're not connected, then it'll connect them and print them out. if they are connected it'll ignore. so, that's our test client and that's a fine test client to make sure that any implementation does what we expect that it will. so, that's the setup. we've described the operations we want to implement all the way down to code and we have client code that we're going to have to be able to service with our 
now we'll look at our first implementation of an algorithm for solving the dynamic connectivity problem, called quick-find. this is a so called eager algorithm, for solving kind activity problem. the data structure that we're going to use to support the algorithm is simply an integer array indexed by object. the interpretation is the two objects, p and q are connected if and only if, their entries in the array are the same. so for example in this example with our ten objects the idea array that describes the situation after seven connections is illustrated in the middle of the slide. so that, after the, at this point zero, five, and six are all in the same connected component, because they have the same array entry, zero. one, two, and seven all have entry one. and three, four, eight, and nine all have entry eight. so that representation is, shows that they're connected. and clearly, that's going to support a quick implementation of the find operation. we just check the array entries to see if they're equal. check if p and q have the same id. so, six and one have different ids. one has id one, six has id zero. they're not in the same connected component. union is more difficult in order to merge the components, containing two given objects. we have to change all the entries, whose id is equal to one of them to the other one. and arbitrarily we choose to change the ones that are the same as p to the ones that are same as q. so if we're going to union six and one, then we have to change entries zero, five, and six. everybody in the same connected component as six. from zero to one. and this is, as we'll see, this is a bit of a problem when we have a huge number of objects, because there's a lot of values that can change. but still, it's easy to implement, so that'll be our starting point. so we'll start with a, a demo of how this works. so, initially, we set up the id array, with each entry, equal to its index. and so all that says is that all the objects are independent. they're in their own connected component. now, when we get a union operation. so, say, four is supposed to be unio n with three. then we're going to change, all entries, whose id is equal to the first id to the second one. so in this case, we'll change the, connect three and four means that we need to change the four to a three. and we'll continue to do a few more so you'll get an idea of how it works. so three and eight now so to connect three and eight now three and four have to be connected to eight. so both of those entries have to change to eight. okay? so now, what about six and five? so again, we change the first one to match the second one. so to connect six and five, we change the six to a five. what about nine and four? so, now we have to change the, to connect nine and four, we have to change, 9's entry to be the same as 4's. so now we have three, four, eight, and nine. all have entries eight. they're all on the same connected component. two and one means that we connect two and one by changing the 2201. eight and nine are already connected. they have the same, entries in the idea array. so, that connected query, that find says, true, they're already connected. and five and zero have different entries. they're not connected, so we'd return false, in that case, not connected. and then, if we want to connect five and zero. then, as usual we'll connect, the entry corresponding to both five and six to zero. seven and two, union seven and two. that's an easy one. and union, six and one so there is three entries that have to get changed. all those zeros have to get changed to ones. so, that's a quick demo of quick-find. now next we'll look at the code for implementating that. okay, with this concrete demo in mind then moving to coding up this algorithim is pretty straight forward. although it's an interesting programming exercise that a lot of us would get wrong the first time. so let's start with the constructor, well we have a, a private integer array. that's our id array. that's the data structure that's going to support this implementation. the constructor has to create the array and then go through and set the value corresponding to each index i to i. that's straight forward. the find operation, or connected operation. that's the easy one . this is the quick-find algorithm. so it simply takes its two arguments, p and q, and checks whether their id entries are equal, and returns that value. if they're equal, it returns true. if they're not equal, it returns false. the more complicated operation implement is a union. and there, we find first the id corresponding with the first argument, and then the id corresponding to the second argument. and then we go through the whole array, and looking for the entries whose ids are equal to the id of the first argument, and set those to the id of the second argument. that's a pretty straightforward implementation. and i mentioned that a lot of us would get us wrong. the mistake we might make is to put id of p here rather than first picking out, that value. and you can think about the implications of that. that's an insidious bug. so, that's a fine implementation of quickfind so the next thing to decide is how effective or efficient that algorithm is going to be and we'll talk in some detail about how to do that but for this it's sufficient to just think about the number of times the code has to access the array. as we saw when doing the implementation, both the initialized and union operations involved the for-loop that go through the entire array. so they have to touch in a constant proportional to n times after touching array entry. find operation is quick, it's just to a constant number of times check array entries. and this is problematic because the union operation is too expensive. in particular if you just have n union commands on n objects which is not unreasonable. they're either connected or not then that will take quadratic time in squared time. and one of the themes that we'll go through over and over in this course is that quadratic time is much to slow. and we can't accept quadratic time algorithms for large problems. the reason is they don't scale. as computers get faster and bigger, quadratic algorithms actually get slower. now, let's just talk roughly about what i mean by that. a very rough standard, say for now, is that people have computers that can run billions of operations per second, and they have billions of entries in main memory. so, that means that you could touch everything in the main memory in about a second. that's kind of an amazing fact that this rough standard is really held for 50 or 60 years. the computers get bigger but they get faster so to touch everything in the memory is going to take a few seconds. now it's true when computers only have a few thousand words of memory and it's true now that they have billions or more. so let's accept that as what computers are like. now, that means is that, with that huge memory, we can address huge problems. so we could have, billions of objects, and hope to do billions of union commands on them. and, but the problem with that quick find algorithm is that, that would take ten^18th operations, or, say array axises or touching memory. and if you do the math, that works out to 30 some years of computer time. obviously, not practical to address such a problem on today's computer. and, and the reason is, and the problem is that quadratic algorithms don't scale with technology. you might have a new computer that's ten times as fast but you could address a problem that's ten times as big. and with a quadratic algorithm when you do that. it's going to be ten times as slow. that's the kind of situation we're going to try to avoid by developing more efficient algorithms for solving problems like this. 
all right so quickfind is too slow for huge problems. so, how are we going to do better? our first attempt is an alternative called, quick-union. this is so called lazy approach to algorithm design where we try to avoid doing work until we have to. it uses the same data structure or array id with size m but now it has a different interpretation. we are going to think of that array as representing a set of trees that's called a forest as depicted at right. so, each entry in the array is going to contain a reference to its parent in the tree. so, for example, 3's parent is four, 4's parent is nine. so 3's entry is four and 4's entry is nine in the array. now each entry in the array has associated with it a root. that's the root of its tree. elements that are all by themselves in just, in their own connected component, point to themselves, so one points to itself but also nine points to itself. it's the root of the tree, containing two, four and three. so, from this data structure we can associate with each item a root, which is representative, say, of it's connected component. so that's the root of three is nine, going up that root. now, once we can calculate these roots, then we can implement the find operation just by checking whether the two items that we're supposed to check with are connective where they have the same root. that's equivalent to saying, are they in the same connective component? so that's some work, going to find the roots of each item but the union operation is very easy. to merge components containing two different items. two items that are in different components. all we do is set the id of p's route to the id of q's route. let's make p's tree point to q. so in this case, we would change the entry of nine to be six to merge three and five. the components containing three and five. and with just changing one value in the array we get the two large components emerged together. that's the quick-union algorithm. because a union operation only involves changing one entry in the array. find operation requires a little more work. so let's look at the implementation, a demo of that one in operation first. so again we, we start out the same way but now the idea array entry really means that every one of these things is a little tree where the one node each everyone pointing to itself. it's the root of it's own tree so now if we have to put four and three in the same component, then all we do is we take the root, of the component containing the first item and make that a child of the root of the component, component containing the second item. in this case we just make four as parent three. so now three and eight. so again, we take the first item and make it a child of the root of the tree containing the second item. so now three, four, and eight are in the same component. six and five six goes below five. nine and four, so now four is the root of the tree containing four is eight. and the root of tree containing nine is nine. and so we make nine a child of eight. two and one, that's an easy one. now if we get our, our eight and nine connected, we just checked that they have the same root and they both have the same root eight and so they're connected. five and four 4's root is eight. 5's root is five. they're different. they're not connected. five and zero. five goes to be a child of zero. seven and two seven goes to be a child of 2's root which is one. six and one. 6's route is zero 1's its own route, so zero becomes a child of one. each one of these union operations just involves changing one entry in the array. and finally, seven and three. so seven's root is one, three's root is eight, one becomes a child of eight. okay and now we have one connected component with all the items together. alright, so now let's look at the code for implementing quick-union. the constructor is the same as the other one. we create the array and then set each element to be it's own root. now we have a private method that implements this process of finding the root by chasing parent pointers until we get to the point where i is equal to id of i, and if it's not equal, we just move i up one level in the tree, set i equals id of i and return it. so starting at any node, you just follow id equals id of i until they're equal and then you're at a root and that's a private method that we can use to implement the find operation or the connected operation. you just find the root of p and the root of q and if you check if they're equal. and then the union operation is simply find the two roots i and then set the idea the first one could be the second one. actually less code than for quick find, no fore loops. there's this one wild loop that we have to worry about a little bit. but that's a quick and elegant implementation of code to solve the dynamic connectivity problem called quick-union. so now we're going to have to look at can this code be effective for large problems? well unfortunately quick-union is faster but it's also too slow. and it's a little different kind of too slow then for quick find, there's times when it could be fast, but there's also times when it could be too slow. and the defect for quick-union is that the trees can get too tall. which would mean that the find operation would be too expensive. you could wind up with a long skinny tree. of each object just pointing to next and then to do a find operation for object at the bottom would involve going all the way through the tree. costing involving in the ray axises just to do the find operation and that's going to be too slow if you have a lot of operations. 
okay. so, we've looked at the quick union and quick find algorithms. both of which are easy to implement. but simply can't support a huge dynamic connectivity problems. so, how are we going to do better? that's what we'll look at next. a very effective improvement, it's called weighting. and it might have occurred to you while we are looking at these algorithms. the idea is to when implementing the quick union algorithm take steps to avoid having tall trees. if you've got a large tree and a small tree to combine together what you want to try to do is avoid putting the large tree lower, that's going to lead to long tall trees. and there's a relatively easy way to do that. what we'll do is we'll keep track of the number of objects in each tree and then, we'll maintain balance by always making sure that we link the root of the smaller tree to the root of the larger tree. so, we, we avoid this first situation here where we put the larger tree lower. in the weighted algorithm, we always put the smaller tree lower. how we, let's see how we implement that. let's see a demo first. okay, so again start out in our normal starting position, where everybody's in their own tree. and for when there's only two items to link it, it works, works the same way as before. but now, when we have eight to merge with four and three, we put the eight as the child, no matter which order their arguments came, because it's the smaller tree. so, six and five doesn't matter, whichever one goes down doesn't matter. nine and four, so now, nine is the small one four is the big one. so, nine is going to be the one that goes down below. two and one, five and zero. so now, five and zero five is in the bigger tree so zero goes below. seven and two, two is in the bigger tree so seven goes below. six and one they're in equal size trees. and seven and three, three is in the smaller tree so it goes below. so, the weighted algorithm always makes sure that the smaller tree goes below. and again, we wind up with a single tree representing all the objects. but this time, we h ave some guarantee that no item is too far from the root and we'll talk about that explicitly in a second. so, here's an example that shows the effect of doing the weighted quick union where we always put the smaller tree down below for the same set of union commands. this is with a hundred sites and 88 union operations. you can see in the top the big tree has some trees, some nodes, a fair distance from the root. in the bottom, for the weighted algorithm all the nodes are within distance four from the root. the average distance to the root is much, much lower. let's look at the java implementation and then we'll look in more detail at, at that quantitative information. so, we used the same data structure except, now we need an extra array, that for each item, gives the number of objects in the tree routed at that item. that will maintain in the union operation. find implementation is identical to for quick union, you're just checking whether the roots are equal. for the union implementation, we're going to modify the code to check the sizes. and link the root of the smaller tree to the root of the larger tree in each case. and then after changing the id link, we also change the size array. if we make id, i a child of j, then we have to increment the size of j's tree by the size of i's tree. or if we do the other way around, then we have to increment the size of i's tree by the size of j's tree. so, that's the full code in white for implementing quick union. so, not very much code but much, much better performance. in fact we can analyze the running time mathematically and show that defined operation, it takes time proportional to how far down the trees are in the node in the tree, the nodes are in the tree, but we can show that it's guaranteed that the depth of any node in the tree is at most the logarithm to the base two of n. we use the notation lg always for logarithm to the base two. and, and, so for, if n is a thousand, that's going to be ten, if n is a million that's twenty, if n is a billion that's 30. it's a very small number compared to n. so, let's look at the proof of that. we do some mathematical proofs in, in this course when they're critical such as this one. and why is it true that the depth of any node x is, at most, log base two of n? well, the key to understanding that is to, take a look at exactly when does the depth of any node increase? when does it go down further in the tree? well. the x's depth will increase by one, when its tree, t1 in this diagram, is merged into some other tree, t2 in this diagram. well, at that point we said we only do that if the size of t2 was bigger than the or equal to size of t1. so, when the depth of x increases, the size of its tree at least doubles. so, that's the key because that means that the size of the tree containing x can double at most log n times because if you start with one and double log n times, you get n and there's only n nodes in the tree. so, that's a sketch of a proof that the depth of any node x is at most log base two of n. and that has profound impact on the performance of this algorithm. now instead of the initialization always takes time proportional to n. but now, both the union and the connected or find operation takes time proportional to log base two of n. and that is an algorithm that scales. if n grows from a million to a billion, that cost goes from twenty to 30, which is quite not acceptable. now, this was very easy to implement and, and we could stop but usually, what happens in the design of algorithms is now that we understand what it is that gains performance, we take a look and see, well, could we improve it even further. and in this case, it's very easy to improve it much, much more. and that's the idea of path compression. and this idea is that, well, when we're trying to find the root of the tree containing a, a given node. we're touching all the nodes on the path from that node to the root. while we're doi ng that we might as well make each one of those just point to the root. there's no reason not to. so when we're looking, we're trying to find the root of, of p. after we find it, we might as well just go back and make every node on that path just point to the root. that's going to be a constant extra cost. we went up the path once to find the root. now, we'll go up again to just flatten the tree out. and the reason would be, no reason not to do that. we had one line of code to flatten the tree, amazingly. actually to make a one liner code, we use a, a simple variant where we make every other node in the path point to its grandparent on the way up the tree. now, that's not quite as good as totally flattening actually in practice that it actually is just about as good. so, with one line of code, we can keep the trees almost completely flat. now, this algorithm people discovered rather early on after figuring out the weighting and it turns out to be fascinating to analyze quite beyond our scope. but we mentioned this example to illustrate how even a simple algorithmah, can have interesting and complex analysis. and what was proved by hopcroft ulman and tarjan was that if you have n objects, any sequence of m union and find operations will touch the array at most a c (n + m lg star n) times. and now, lg n is kind of a funny function. it's the number of times you have to take the log of n to get one. and the way to think, it's called the iterated log function. and in the real world, it's best to think of that as a number less than five because lg two^ 65536 is five. so, that means that the running time of weighted quick union with path compression is going be linear in the real world and actually could be improved to even a more interesting function called the ackermann function, which is even more slowly growing than lg<i>. and another point about this is it< /i> seems that this is</i> so close to being linear that is t ime proportional to n instead of time proportional to n times the slowly growing function in n. is there a simple algorithm that is linear? and people, looked for a long time for that, and actually it works out to be the case that we can prove that there is no such algorithm. so, there's a lot of theory that goes behind the algorithms that we use. and it's important for us to know that theory and that will help us decide how to choose which algorithms we're going to use in practice, and where to concentrate our effort in trying to find better algorithms. it's amazing fact that was eventually proved by friedman and sachs, that there is no linear time algorithm for the union find problem. but weighted quick union with path compression in practice is, is close enough that it's going to enable the solution of huge problems. so, that's our summary for algorithms for solving the dynamic connectivity problem. with using weighted quick union and with path compression, we can solve problems that could not otherwise be addressed. for example, if you have a billion operations and a billion objects i said before it might take thirty years. we can do it in six seconds. now, and what's most important to recognize about this is that its the algorithm design that enables the solution to the problem. a faster computer wouldn't help much. you could spend millions on a super computer, and maybe you could get it done in six years instead of 30, or in two months but with a fast logarithm, you can do it in seconds, in seconds on your own pc. 
alright. now that we've seen efficient implementations of algorithms that can solve the unifying problem for huge problem instances let's look to see how that might be applied. there's a huge number of applications of union-find. we talked about dynamic connectivity in networks there's many other examples in our computational infrastructure. down at the bottom is one of those important one is in image processing for understanding how to label areas in images. we'll see later kruskal's minimum spanning tree algorithm, which is a graph processing algorithm which uses union-find as a subroutine. there's algorithms in physics for understanding physical phenomenon that we'll look at an example and many others on this list. so, the one we're going to talk about now is called percolation. that's a model for many physical systems i'll give an abstract model and then just talk briefly about how it applies to physical systems. so let's think of an n by n grid of squares that we call sites. and we'll say that each site is open. that's white in the diagram with probably p or blocked, that's black of the diagram with probability one - p and we define a system to, we say that a system is percolated if the top and the bottom are connected by open sites. so the system at the left, you can find a way to get from the top to the bottom through white squares, but the system to the right does not percolate, there's no way to get from the top to the bottom through white squares. so, that's a model for many systems. you can think of for electricity. you could think of a vacant site as being a conductor and, and a block site as being insulated. and so if there's a conductor from top to bottom then the thing conducts electricity. or, you could think of it as, as water flowing through a porous substance of some kind. where a vacant side is just empty and a block side has got some material, and either the water flows through from top to bottom, or not. or you could think of a social network where it's people connected and either there's a c onnection between two people or not and these are a way not to get from one group of people to another communicating through that social network. that's just a few examples of the percolation model. so if we, we are talking abouta randomized model where the sites are vacant with the given probability. and so it's pretty clear that if it's. probability that a site is vacant is low as on the left, two examples on the left in this diagram, it's not going to percolate. there's not enough open site for there to be a connection from the top to the bottom. if the probability is high and there is a lot of open sides, it definitely is going to percolate. there would be lots of ways to get from the top to the bottom. but in the middle, when it's medium, it's questionable whether it percolates or not. so the scientific question, or the, mathematical question from this model is, how do we know, whether it's going to percolate or not? in this problem and in many similar problems, there's what's called a phase transition. which says that, you know, when it's low, it's not going to percolate. when it's high, it is going to percolate. and actually, the threshold between when it percolates and when it doesn't percolate is very sharp. and actually there is a value as n gets large that if you're less than that value it almost certainly will not percolate, if you're greater it almost certainly will. the question is what is that value. this is an example of a mathematical model where the problem is, is very well articulated. what's that threshold value but, nobody knows the solution to that mathematical problem. the only solution we have comes from a computational model, where we run simulations to try and determine the value of that probability. and those simulations are only enable by fast union find algorithms, that's our motivating example for why we might need fast union find algorithms, so let's look at that. so what we're going to run is called a so called monte carlo simulation. where we initialize the whole grid to be block ed all black and then we randomly fill in open sites. and we keep going. and every time we add an open site, we check to see if it makes the system percolate. and we keep going until we get to a point where the system percolates. and we can show that the vacancy percentage at the time that it percolates is an estimate of this threshold value. so what we want to do is run this experiment millions of times, which we can do in a computer, as long as we can, efficiently do the calculation of does it percolate or not. that's a monte carlo simulation, a computational problem that gives us a solution to this, scientifc problem where, mathematical problems nobody knows how to solve yet. so, let's, look in a little bit more detail of how we're going to use our dynam-, dynamic connectivity model to do this. so, it's clear that, we'll create an object corresponding to each site. and we'll give'em a name, from zero to n^2-1 as indicated here. and then we'll connect them together. if they're connected by open sites. so the percolation model on the left corresponds to the, connection model on the right, according to what we've been doing. now, you might say, well, what we want to do is, connect, check whether any site in the bottom row is connected to any site in the top row, and use union find for that. problem with that is, that would be a brute force algorithm. would be quadratic, right on the face of it. because it would have n^2, calls to find, to check whether they're connected. for each site on the top, i'd check each site on the bottom. much too slow. instead, what we do is create a virtual site on the top and on the bottom. and then, when we want to know whether this system percolates, we just check whether the virtual top site is connected to the virtual bottom site. so how do we model opening a new site? well to open a site we just connect it to all it's adjacent open sites. so that's a few calls to union but that's easy to implement. and then with that, simple, relationship we can use the exactly the code that we developed to go ahead and run a simulation for this connectivity problem. and that's where we get the result that, by running enough simulations for a big-enough n, that this, percolation threshold is about .592746. with this fast algorithm we can get an accurate answer to the scientific question. if we use a slow union-find algorithm we won't be able to run it for very big problems and we won't get a very accurate answer. so in summary, we took an important problem. the, the dynamic connectivity problem. we modeled the problem to try to understand precisely what kinds of data structures and algorithms we'd need to solve it. we saw a few easy algorithms for solving the problem, and quickly saw that they were inadequate for addressing huge problems. but then we saw how to improve them to get efficient algorithms. and then left us with, applications that, could not be solved without these efficient algorithms. all of this involves the scientific method. for algorithm design where we try to develop mathematical models that help us understand the properties of the algorithms that we're developing. and then we test those models through experimentation enabling us to improve algorithms iterating, developing better algorithms and more refined models until we get what we need to solve the practical problems that we have of interest. that's going to be the overall architecture for studying algorithms that we're going to use throughout the course. 
welcome back. today we're going to do some math and some science. not a lot, but we need to have a scientific basis for understanding the performance of our algorithms to properly deploy them in practise. so today we're going to talk, about how to, observe performance characteristics of algorithms. we're going to look at how to make mathematical models and how to classify algorithms according to the order of growth of their running time. we'll talk a bit about the theory of algorithms and also how to analyze memory usage. so to put this all in perspective, we're going to think about these issues from the point of view of different types of characters. so the first one is the programmer who needs to solve a problem and get it working and get it deployed. second one is the client who wants to use the whatever program did to get the job done. third one is the theoretician, that's somebody who really wants to understand what's going on. and, and the last one is kind of a team, this basic blocking and tackling sometimes necessary to get, you know, all these things done. so, there's a little bit of each one of these in today's lecture. and actually when you're a student you have to think that you might be playing any or all of these roles some day. so, it's pretty important to understand the different points of view. so, the key that we'll focus on is running time. and actually the idea of understanding the running time of a computation goes way back even to babbage and probably before. and here's a quote from babbage, "as soon as an analytical engine exists, it will necessarily guide the future course of the science. whenever any result is sought by its aid, the question will arise by what course of calculation can these results be arrived at by the machine in the shortest time". if you look at babbage's machine called the analytic engine, it's got a crank on it. and literally the concern that babbage had in knowing how long a computation would take is, how m any times do we have to turn the crank. it's, it's not that different, in today's world. the crank may be something electronic that's happening a billion times a second. but still, we're looking for, how many times does some discreet operation have to be performed in order to get a computation done. so, there are lot of reasons to analyse algorithms. in the context of this course we are mainly interested in performance prediction. and we also want to compare the performance of different algorithms for the same task, and to be able to provide some guarantees on how well they perform. along with this, is understanding some theoretical basis for how algorithms perform. but primarily, the practical reason that we want to be analyzing algorithms and understanding them is to avoid performance bugs. we want to have some confidence that our algorithms going to complete the job in the amount of time, that, that we think it will. and it's very, very frequent to see, in today's computational infrastructure, a situation where the client gets bad performance, because the programmer did not understand the performance characteristics of the algorithm. and today's lecture is about trying to avoid that. now, we're going to focus on performance and comparing algorithms in this course. there's later courses in typical computer science curricula that have more information about the theoretical basis of algorithms and i'll mention a little bit about that later on. but our focus is on being able to predict performance and comparing algorithms. now there's a long list of success stories in designing algorithm with better performance in, in enabling the solution of problems that would otherwise not be solved. and i'll just give a couple of examples. one of the first and most famous is the so called fft algorithm. that's an algorithm for breaking down the wave form of n samples of a signal into periodic components. and that's at the basis for dvds and jpegs and, and many other appl ications. there's an easy way to do it that takes time proportional to n^2. but the fft algorithm, takes only n log n steps. and the difference between n log n and n^2 is, is the difference between being able to solve a large problem and not being able to solve it. a lot of the digital technology, digital media technology that we have today is enabled by that fast algorithm. another example was actually developed by andrew appel, who's now the chair of computer science here at princeton. and it was developed when he was an undergraduate for his senior thesis. it's a fast algorithm for the n body simulation problem. the easy algorithm takes time proportional to n^2, but appel's algorithm was an n log n algorithm that again, meant that scientists can do n body simulation for huge values of n. and that enables new research. s0, o the challenge is that we usually face is, will my program be able to solve a large practical input? and, and actually, the working programmer is actually faced with that all the time. why, why is my program running so slowly? why does it run out of memory? and that's faced programmers for a really long time and the insight to address this. deuter kanoof, in the 1970s, was that, we really can use the scientific method to understand the performance of algorithms in operation. maybe we're not unlocking new secrets of the universe but, we can use the, scientific method, and treat the computer, as something to be studied in that way and come to an understanding of how our program are going to perform. and let's take a look at that in more detail. so this just a quick summary of what we mean by the scientific method, which has, been successful for a couple of centuries now. so, what we're going to do is, observe from some feature of the natural world. in this case, it's going to be the running time of our program on a computer. then we're going to develop hypothesis some model that's consistent with the observations, and we're going to hope that, that hypothesis is good enough that it'll allow us to predict something. usually predict a running time for larger problem size, or on a different computer. and then we'll verify the predictions by making more observations, and validate until we're comfortable that our model hypothesis and observations all agree. that's a way to get comfort that we understand the performance of our programs. now, the within the scientific method, there's some basic principles and the, the first is that if you're going to run experiments, you should expect that somebody else should be able to run experiments and get the same result. and also the hypotheses have to have a specific property that the experiment can show the hypothesis to be wrong. so, it has to be carefully crafted, and we'll be sure to try to do that. so, and again the future of the natural world that we're studying is some particular computer that exists in the natural world. it changes the algorithm from an abstraction to a, some, some kind of actual physical thing happening like electrons racing around inside the computer. 
okay, so the first step is to be able to make some observations about the running time of the programs. and for analysis of algorithms that's easier than in a lot of scientific disciplines, as we'll see. for a running example we're going to use the so-called 3-sum problem. and it's an easy to state problem. if you've got n distinct integers, how many triple sum to exactly zero? for example in this file 8ints.text. text which has eight integers in it. there's four triples that sum to zero. 30 - 40, ten. 30 - twenty - ten and so forth and so our goal is to write a program that can compute this quantity for any input file, any set of n integers. this is actually a, an extremely important computation that's deeply related to many problems in computational geometry which is a branch of computer science that covers the algorithms and underlying science related to graphics and movies and geometric models of all sort. so this is a actually an important practical problem. but it's a simple one to write code for in a view you could write down this program without much effort. it's a, got a static method count that is going to go ahead and take a integer array as an argument. and, is that, that's a number of integers, that's the length of the array. we will start with a variable count equals zero, and then a triple for loop, that checks each triple i j k, we go i from one and j from i+1 to n, and k from j+1 to n, so that we get each triple just once. and then if i+j, ai + aj + ak = zero, we increment the count. alright. and after that triple four loop, we return the count. and then the main method, in this simple class just reads in, all the integers, and prints out the count. so that's a brute force algorithm that is a fine method for solving the three sum problem, now what we're interested in is how much time does this take as a function of' n? well, one to time our program is to is just look at the watch. if you have a stopwatch, or look at the clock or your phone, or whatever you might need you can just go ahead and time it if you want or we have, java has this part of it's standard library, a stopwatch class that will go ahead and compute a lapse time. so, in order, anytime you run a program, if it is setup to easily take input of different sizes, a natural thing to do, is just run it for bigger sizes. so for eight ints this program takes not too much time, for 1000 ints it takes half a second. for 2,000. takes more time. that's 3.7 seconds run it again, still takes 3.7 seconds for 4,000, so each time we're doubling the size of the input and it's definitely taking more time each time. and actually as we'll see if programmers who get in the habit of testing or any time on their program in this way can get so that you can actually pretty easily and quickly evaluate when it's going to finish. in fact. while you're waiting for it to finish you can often figure it out. so that one took 30 seconds for 4k and definitely we could figure it out how long it's going to take for 8k before it finishes, and you'll see how in just a second. i'm not going to wait right now. you can think about what you think. okay so [cough] that's empirical analysis, analysis. run it for various input sizes and measure their running time. now if this were some scientific problem where we were counting something that happen in the natural world. the number of ants in an ant hill or whatever then we'd have only a few data points and we would try to understand whats was going on by doing a plot of or running time with quite interested in on the y axis and problem size with the x axis. hit a curve like this and actually whats science usually do because of some many problems fall into out of this class is do the plot as a lg, lg plot. if you do it as a lg, lg plot very often you'll get a straight line. and the slope of the straight line is the key to what's going on. in this case, the slope of the straight line is three and so you can run what's called a regression to fit a late, the straight line through the data points. and then, it's not difficult to show to do the math to show that if you get a straight line and the slope is b, then your function is proportional to a, n^b. that's called the power law. and that's true of many, many scientific problems including most algorithms. so here's a little bit of the math for that. so the straight line means that since we did a lg, lg plot with powers of two, that lg(t(n) = b lg n + c. and we have our empirical values of b and c and then if you raise both sides of that equation to two to that power then you get t(n) = a constant times n^b. so right away just from observation we have a pretty good model for the running time for our program, we can figure and do the math and figure out that it seems as though the running time is about ten^-10 n^3 seconds. we can use that hypothesis to go ahead and make predictions. just plug in for different values of n and it says it will take us 400 seconds for 16,000. 400 seconds is plenty of time but now we can go ahead and invest and run that experiment and sure enough we're pretty close to that 408 seconds when we run it. and now we can make a prediction for 32,000 or for or for whatever else we might be interested in. the model helps us do predictions without investing the expense to run the experiments. in fact, in this situation if there is a power law, and again in a very great majority of computer algorithm running times is going to be a power law. what we can do is just double the size of the input each time the way we were and take the ratio of the running times for n and 2n. and if you do that, that ratio going to converge to a constant. and in fact the log of the ratio is going to converge to that constant, which is the exponent of n and the running time. and you just need a little math to check that one, but that's a very easy and natural way to go ahead and predict running times. so that's what i said before is, so we have this quick way to estimate b in the power law relationsh ip. how do we estimate a? well we can just run it and solve for a. so once we've decided that, that exponent is three let's run it for some big n and we get pretty close model to the one we had from plotting things. so it's almost identical hypothesis and we just got it by running the program double n each time. okay so there is a lot of effects in trying to understand the running time of a program on, on your machine. [cough] so. key effects are independent of what computer it is. and that's the algorithm you're using and what's the data. and that's going to really determine the exponent in the power law. and then there's a lot of, system dependent effects. what kind of hardware do you have? do you have a fast computer or a slow one? what kind of software? what's going on in your computer? all of those things really determine the constant a in the power law. so. in modern systems it is so much going on in the hardware and software, it's sometimes difficult to get really precise measurements. but on the other hand we don't have to sacrifice animals, or fly to another planet the way they do in other sciences, we can just run a huge number of experiments and usually take care of understanding these kind of effects. 
observing what's happening as we did in the last section it gives us a, a way to predict performance but it really doesn't help us understand what the algorithm's doing. so next, we're going to look at mathematical model. a way to get a better concept of what's really happening. again, this concept was really developed and popularized by don knuth starting in the late 60s. at that time, computer systems were really becoming complicated for the first time. and computer scientists were concerned about whether we really were going to be able to understand what's going on. and knuth was very direct in saying that this is something that we certainly can do. we can calculate the total running time of a program by identifying all the basic operations, figuring out the cost, figuring out the frequency of execution and summing up the cost times frequency for all the operations. you have to analyze the program to determine what set of operations and the cost depends on the machine and the computer in the system is what we talked about before. the frequency leads us to mathematics because it depends on the algorithm and input data. knuth has written a series of books that give very detailed and all exact analyses within a particular computer model for a wide range of algorithms. so, from knuth, we know that in principle, we can get accurate mathematical models for the performance of algorithms or programs and operation. all right. so what, what does this process look like? well you can, if you want run experiments. in, in ancient times, we would actually look at the computer manual and every computer came with a manual that said precisely how long each instruction would take. but nowadays, it's a little more complicated. so, we run experiments and, and you can go ahead and do a billion ads and figure out that maybe on your computer, an ad takes 2.1 nano seconds. or you can do more complicated function s like computer sign or an arc tangent although that's already getting close to the analysis of algorithms. so, there's some way to determine the costs of the basic operations. and so, we'll just in most, most of the cases we'll just postulate that it's some constant and you can figure out what the constant is. although when we're working with a collection of objects, of anobjects there are some things that takes time proportional to n like if you're going to allocate a array of size n it takes time proportional to n because in java the default is that all the elements in the array initialize to zero. in other operations it depends on the system implementation and an important one is string concatenation. if you concatenate two strings the running time is proportional to the length of the string. in many novices programming in java, make a mistake of assuming that's a constant time operation when its not. alright, so that's the cost of each operation. more interesting is the frequency of operation, of execution of the operations. so this is a, a, it's a very simple variant of the three sum problem. that's the one sum problem. that's how many numbers are actually equal to zero? how many single numbers add up to zero? so, that one, it's just one four loop, and we go through, and we tested the number zero and increment or count. and by analyzing that code you can see that i and count have to be declared and then they have to be assigned to zero. there's compares of i against n and there's n + one of them. there's compares of a(i) against zero, there's n of those, n array axises and the number incremented is number of times there's an increment is variable. i has incremented n times, but count could be incremented any number from zero to n times. and so that frequency is dependent on the input data. or we might need a model for describing that or maybe there's other operations that are more e xpensive and we won't need to worry about that. so, let's look at the next more complicated problem is what about the frequency of execution of instructions in this program which is the two sum problem, how many pairs of integers sum to zero? well, in this case, you have to do a little bit of math to see that when we when i goes from zero to n, and j goes from i + a to n the number of compares that we do work, plus array axises that we do is two for each time the if statement is executed for ai and aj and that time is, thing is executed n - one times the first time through the loop and n -two^2 and so forth. it's the sum of the integers from zero up to n - one which is a simple discrete sum one-half n, (n - one) and since, and since we're doing it twice the number of array axises is n, n - one. so, we can go ahead and get these actual exact counts. but already, it's getting a little bit tedious to do that. and as far back as turing who also knew that and as well as babbage did, that we want to have a measure of the amount of work involved in the process. he recognized that you didn't want to necessarily go through and do it in full detail. it's still helpful to have a crude estimate. so, you could count up the number of times that every operation is applied, give it weights and, and count the [inaudible] and so forth. but maybe we should just count the ones that are most expensive that's what turing said in 1947, and realistically that's what we do nowadays. so rather than going in and counting every little detail, we take some basic operation that's maybe the most expensive and or and or the one that's executed the most often. the one that cost and frequency is the highest and use that as a proxy for running time. essentially, making the hypothesis that the running time is, is going to grow like a constant times [inaudible], so, in this case, were going to pick array axises. so, that's the first simplification. and the second simplification is that we're going to ignore low order terms in the formulas that we derive. and there's an easy way to do that. it's called the tilde notation and, and the idea is when n is large in a formula like this the n^3 term is much, much higher than the n term or sixteen. in fact, so much so that we wouldn't even hardly notice these low order terms. so, all of these formulas are tilde one-sixth n^3 and that's a fine representative or approximate, approximation to these quantities. and it greatly simplifies their calculations to for a, through a way to lower, lower to terms like this. so, by focusing on one operation and , throwing away the tildes, the lower the terms and this is the technical definition of tilde. it's just, f(n) tilde g (n) means the limit as fn or gn equals one, and you can check that that's going to hold in these kinds of situations. so, that greatly simplifies the frequency counts. and if we're only picking one thing we're just talking about tilde n^2 and maybe another tilde n^2 for the increment for the two sum problems, okay. so again, when n is large, the terms are negligible and when n is really small, they're not negligible but we don't really care because we're trying to estimate running times for large n and running times for small n are going to be small no matter what. all right, so now, we're using both the cost model and the tilde notation and then we can simply say, that this program uses tilde n^2 squared array axises and have implicit the hypothesis that we think the running time is going to be tilde, a constant, times n squared. okay, we now what about three sums, let's do a, a real problem. so now, we have the triple loop. and then, we have to do a more complicated combinatorial problem in is not that big a deal really we are looking at the distinct number of ways you can chose three things out of n and that 's binomial coefficient. and again, doing the math and using the tilde, it's just tilde one-sixth n^3 three ray axises for each triple so we can say one-half n^3. so we're not computing and summing the costs of all operations that's too much work. we're picking the most expensive in terms of cost times frequency and approximating that and trying to get a good model for the running time. so now most, we're not going to do of a full discrete mathematics in this course but there's some basic things that we'll want to use and are, are not that difficult to understand. so, a lot of times we find out that we need to come up with an estimate of a discrete sum. like we did for one + two up to n. or some of the squares or other things like the three sum triple loop. and so actually if you've had basic calculus, one way to think of it as to just replace the sum with an interval, integral. that usually works or we can do the math and use the so-called eulermaclaurin summation formula to get a true approximation. but if you think of it this way you'll believe us when we say that, that thing is tilde one-half n^2 or sum of one+ one-half + one-third up to one / n. that's like integral from x = one to n1 / x and that's natural log of n. now even the three sum triple loop kind of if you're used to multiple integrals, i will quickly give you the one-sixth n^3. there's many more and other techniques that we could use for this. and we're not going to teach all that, but we'll sometimes refer to results of this type. alright, so in principle, knuth tells us that accurate mathematical models are available in practice, we can get really complicated formulas. we also might need some advance mathematics that the theoretician will revel in. but that maybe people learning algorithms for the first time might not be expected to know. so in the end careful exact models are best, best left for exit, experts. there's really a lot of things that can go on. on the other hand approximate models are definitely worthwhile. and for all the algorithms that we consider we'll try to communicate a reasonable approximate model that can be used to describe the running time. sometimes we'll give the mathematical proofs and other times we'll have to just cite the work of some expert. 
now, fortunately when we analyze algorithms, actually not too many different functions arise and actually that property allows us to really classify algorithms according to their performance as the problem size grows. so that's what we'll talk about next. so the good news is there's only these few functions turn up about the algorithms that we are interested in. we can craft things that have other functions and there are counter examples to this. but really a great number of the algorithms that we consider are described by these few functions and that are plotted here. and [cough] the when we are talking about the order of growth, we are not talking about the leading constant. normally we'll say the running time of the algorithm is proportional to n log n. that means we that we think that our hypothesis is that the running time is tilde c lg n, n lg n, where c is some constant. and in these plots, these are lg, lg plots that not really give a good idea of what's going on. if a order of growth is logarithmic or constant, doesn't matter how big the thing is. it's going to be fast of the running time for is t for say a thousand, and for half a million it will be pretty close to t. if it's linear, if it's auto growth is proportional to n then as the running time, as the size increases the running time increases correspondingly. and the same is true, almost, if it's n log n. so those are the algorithms that we strive for. they scale with the input size. as the input grows, so grows the running time. and that's, a reasonable situation to be in. as we talked about when we talked about union-find. if it's quadratic, the running time grows much faster than the input size. and it's not feasible to use such an algorithm for large inputs. and qubic is even worse. so what we find is for many algorithms our first task is really, simply, make sure it's not quadratic or qubit. and these order of growth classifications actually come from kind of simple patterns in terms of the code that we write. so if our code has no loops in it, then the order of growth is going to be constant. if our code has some kind of loop where the input's divided in half, and so binary search algorithm is an example of that. then our order growth will be logarithmic and we'll take a look at that analysis and but if you do the doubling test, it grows almost linearly, if you have a huge input and you double the size it's, it's still going to be i'm sorry, not linearly, constant just like if it's constant. you'll hardly notice that lg n. if you have a loop where you touch everything in your input. than the running time is linear, proportional to end so a typical example of that would be find the maximum, or to count the number of zeros. our one some problem. a very interesting category is a so-called n lg n algorithms or linear rhythmic algorithms. and those are the ones that arise from a particular algorithms design technique called the divide and conquer. and the mergesort algorithm, which we'll talk about in a couple of weeks, is a prime example of that. and then if you have double four loops like our two sum algorithm, that's going to be time proportional to n^2. as we saw, that's quadratic, or triple four loop like our 3-sum algorithm, that's going to be cubic or time proportional to n^3. for a quadratic algorithm or a cubic algorithm, the doubling factor is four or eight as the input size double for cubic algorithm, the running time goes up by a factor of eight, and that's the kind of calculation that you can do in your head while waiting for a program to finish. there's also a category of algorithms who's running time is exponential and in those algorithms n doesn't get very large at and we'll talk about those at the end part two of the course. so these are some practical implications of, of the order growth. and we really dwell on this too much, except to come back to the point that the algorithms we are really interested in, that can solve huge problems, are the linear and n lg n algorithms. because even now a quadr atic algorithm on a typical fast computer could only solve problems and saying that tens of thousands in a cubic algorithm only in the size of thousands. and nowadays those are just not useful because the amount of data that we have is more like the millions or billions or trillions. that fact is becoming more and more evident as time wears on the ancient times would have some discussion about whether quadratic algorithm might be useful but the situation gets worse as the time goes on, so we need better algorithms. to illustrate the process of developing a mathematical model for describing a performance through an algorithm, we'll look at a familiar algorithm called binary search. it's, the goal is that you have a sorted array of integers, say and you're given a key. and you want to know, is that key in the array? and if it is, what, what's its index? and a fast algorithm for doing this is known as binary search, where we compare the key against the middle entry. in this case, if we're looking for 33, we compare it against 53. if its smaller we know its in the left half of the array, if it's larger we know it's in the right half of the array, if it's equal, we found it. and then we apply the same algorithm recursively. so let's quickly look at a demo. so we're looking for 33 in this array, compare it against the middle entry in the array. 53 and it's less so we go left, so now we can concentrate just on the left half of the array, now we look in the middle of this half, that's 25, 33 is bigger so we go right. and now we concentrate on the right half or the left half and we have a smaller sub array. look at the middle, 33 is less so we go left and now we have only the one element to look at and we found our key 33 in the array and we return that index four. if we're looking for something that's not in the array, we do the same process. so, say, we're looking for 34. it's going to be the same. look in the left half, look in the right half. look to the left of the 43. now, there's only one key to look at. a nd it's not 34, so we say, it's not there. so that's binary search. so here's the code for binary search. actually, binary search although it's a simple algorithm, its notoriously tricky to get every detail right. in fact one paper claimed, that the first bug free binary search wasn't published until 1962, and even in 2006, a bug was found in java's implementation of binary search, just an indication of the care that we have to take in developing algorithms especially for libraries that are going to be used by millions of people. so here's an implementation. it's not recursive although often we can implement this recursively. and it's just reflexing code, what i described in words, we have to find. a key, whether a key's in an array. and we use two pointers, low and high, to, indicate the part of the array we are interested in, as long as low is less and equal to high, we compute the middle. and then we compare our key against the middle, actually its a three way compare, see its less or greater or if its equal, we, we return that mid index. if its less we reset the high pointer, if its greater, we reset the low pointer, and we keep on going until the pointers are equal. if they are equal and we haven't found it then we return -one. and it's easy to persuade ourselves that this program works as advertised by thinking about this invariant, if the keys in the array, then it's between low and high in the array. alright, so that's a program that, you are probably familiar with. lets look at the mathematical analysis of that program. and this a, a theorem that we are going to prove easily. we want to a lot of proofs but this is one worth doing. so its say that binary search uses at most one + lg base two event compares, to complete a search, in a sorted array of size f. so we do that, to setup the problem by defining, a variable t(n), which is the number of compares that binary search needed for its array size and. and then we write down a recurrence relation that is reflex the code. and what the code does is, it divides the problem size in half so that. if the event is less or equal to the event over two plus depending on how you count what the compare is think of it as a two way compare so divided in half by doing one compare and that's true as long as n is bigger than one. if it's equal to one the solution is one. so it's a recurrent relation describing the computation. and so we, we can go ahead and, solve this recurrence by applying the recurrence itself, to the first term on the right. now that's called telescoping. so if this is true and we can apply the same thing to t(n/2). and throw out another one and if that's, this is true, apply the same thing to n over four, and throw out another one and so forth until we get down to just one. in which case we have lg n ones left. now this is a true sketch you might have noticed that, that this proof actually only holds if n is a power of two. because we nearly specify in this recurrence what we mean if n is odd. but it's possible to go ahead and sorry, possible to go ahead and take care of that detail as well and show that binary search running time is logarithmic always. all right, so given that fact we can develop a faster algorithm for a threesome. it's a sorting based algorithm. and so what we're going to do is we're going to take the numbers that we have as input and sort them. we'll talk about sorting algorithms next week. and we get that time in time proportional to n lg n but that's not the main part of the computation. the main part of the computation is to after the numbers are sorted, we'll go through and for each pair of numbers ai and aj. we'll do a binary search for -ai + ij. if we find it then we'll have three numbers that sum to zero. so if we [cough] sort our numbers and then go through for each pair do a binary search to see if it's there, so -40, zero. minus that is 40, we do a binary search that's in there so we have one solution to the 3-sum problem. and do that for all pairs of numbers. then a quick analysis says the order of growth of running time is going to be n^2 lg n. then you need a good sort, well, you could use the elementary insertion sort the first one we talk about but the running time of the binary search for each of the pairs, each of the n^2 pairs or n^2/2 pairs we're going to do the binary search, so we get a n^2 lg n running time. so, a quick example of how we could improve the performance, we could find an imroved algorithm to solve a problem. n^2 lg n is much less than n^3 for large n. and so, we're implicitly making the hypothesis that if we do this, do the sort base thing and use binary search, we're going to have a faster program. and, sure enough we can go ahead and run some experiments and find that whereas it took us 50 seconds to solve the problem for 8,000 numbers before. it's taking less than a second now. in 50 seconds we can solve up to 64,000. so typically we expect that better order of growth means. faster in practice and but when it comes to examining the algorithms in detail we can, we can go ahead and do the tests and figure out which algorithm is faster. and certainly going from n^3 to n^2 lg n we're going to expect that we're going to have a much better algorithm. 
in fact the order of growth classifications are so important they've led to enormous amount of research in recent years and just talk briefly about that now. so there is, life is a little bit more complicated than pointed out in the last example and one problem is that the inputs can cause the performance of the algorithm to vary widely. so often we have to think about different ways of analyzing the algorithm depending on the input. so, the running time is going to be somewhere between the best case and the worst case. best case is the lower bound on cost it. it provides something that the running time is going to be bigger than that always or not less than that and then there's the worst case which is the most difficult input. if we analyze that then we can guarantee that the running time in the algorithms not going to be bigger than that. and then in a lot of situations we might consider our input to be random. well we need to, someway to model, what we mean by random for the problem that we're solving but there is a lot of situations where we can do that and then we have a way to predict performance even when the input might vary widely. so for example for 3-sum, it's kind of always the same. with the tilde notation, the only variability in that algorithm is the number of times the counter is incremented and that's in low order terms so it doesn't need to chew up in our analysis. for binary search it's, you might find the thing right away in which case is constant time and we can show that the average and the worst case are both lg based two(n). there's other, in another examples that be much more variability even. so, we have this different types of analysis depending on the input. and but the question is, what about the actual problem that the client is trying to solve? so we have to understand that two in order to be able to understand performance of the algorithm. and there's two approaches that are, or successful in this. one is to design for the worst case. just to make sure that your algorithm are, always runs quickly and that's definitely ideal. another is to, if you can't do that is to randomize and then depend on some kind of probabilistic guarantee and we'll see examples of both of these as we go through the course. now, those kinds of considerations, you know the idea of order of growth leads to discussion of, what's called, what i call the theory of algorithms. and here our goals are, we have a problem to solve like solve the 3-sum problem and we want to know how difficult it is. we want to find the best algorithm for solving that problem. the approach that the computer scientist use for this is to try to suppress as many details as possible in the analysis. and so just analyze the running time to or within a constant factor. that's what order of growth is getting at and also i want to, not worry about the input model at all. and so we focused on worst case design and we can talk about performance of algorithms just in turn of the order of growth and it's actually possible, it's actually possible to do that in a very rigorous way that it's taught us a lot about the difficulty of solving problems. and our goal is to find an optimal algorithm where we can guarantee to within a constant factor certain performance for any input cuz we discovered the worst case but we also can have approved that didn't know algorithm could provide a better performance guarantee. i'll give a couple of easy examples of this. now in order to do this they're, these commonly used notations called the big theta, big o and big omega notations. so the and those definitions are given here. so big theta notation is just the way to describe the order of growth. theta(n)^2 is kind of short hand for anything n^2. it's bounded above and below by constant time n^2 and that's what we really use to classify algorithms. and then, there is big o notation which is upper bounds on performance. when we say o(n^2), we mean that it's less than some constant time n^2 as n grows. and big omega is used for lower bounds means greater than some constant time n^2 as n grows. so those three notations were able to use to classify algorithms and i'll show them in the following. so, examples from our 1-sum, 2-sum, and 3-sum are easy to articulate so our goals are to establish the difficulty of the problem and to develop an optimal algorithm. so, the 1-sum problem is 00 in the array. well, an upper bound on the difficulty of the problem is some specific algorithm. so, for example, the brute force algorithm that looked, that looks at every array entry is a specific algorithm and it means that and that takes o(n) time. we have to look at every, it's less than a constant time n for some constant. so, the running time of the optimal algorithm has to be o(n) that is that's specific algorithm provides an upper bound on the running time of the optimal algorithm. and but in this case it's also easy to develop a lower bound, that's a proof that no algorithm can do better. well, for 1-sum you have to examine all entries in the array. if you miss one, then that one might be zero so that means that the optimal algorithm has to have a running time at least some constant times n where we say the running time is omega of n. now in this case, the upper bound and the lower bound match. so, doing the constant factor so, that's a proof that the brute force algorithm for 1-sum is optimal. it's running time is theta(n). it's both omega and o(n). that's, for that simple problem it was okay to get the optimal algorithm. for a more complicated problems it's going to be more difficult to get upper balance and lower balance and particularly upper balance and lower balance that match. for example let's look at 3-sum. so, upper bound for 3-sum, say our first brute force algorithm, say that the proof, was a proof that the running time of the optimal algorithm is o(n^3) but we found a better improved algorithm. whose running time is o(n^2) lg n. so, that's a better upper bound. lower bound well, we have to examine all entries cuz again, we might miss one that makes 3-sum = zero and that's a proof that the running time in the optimal algorithm is omega(n) but nobody knows higher or lower bound for 3-sum. so there's a gap between the upper bound and the lower bound and open problems. is there an optimal algorithm for 3-sum? we don't know what it is. we don't even know if there's a algorithm whose running time is < o(n^2) or we don't know higher lower bound and linear. so that's an example of an open problem in the theory of algorithms we don't know how difficult it is to solve the 3-sum problem. now, this point of view has been extremely successful in recent decades. we have a new problem, develop some algorithm, proves some lower bound. if there's a gap, we look for new algorithm that will lower the upper bound or we try to find a way to raise the lower bound. usually it's very difficult to prove non-trivial or lower bounds. trivial or lower bound like look at every input items is not so hard non-trivial lower bounds like for example, the proof that we're talking about for union-find problem are much more difficult. and in the last several decades people have learned about the computational difficulty of problems by examining steadily decreasing upper bounds so the algorithms were better worst case running times for lots and lots of important problems and plenty of optimal algorithms and plenty of gaps still remain. it's a fascinating field of research that many people are engaged in. now there is a couple of caveats on this on the context to this course. and the first one is maybe it's overly pessimistic to be focusing on the worst case. we've got data out there. we've got problems to solve. maybe it's not worst case data and lots of fields of engineering and science. we don't focus on the worst case. the worst case for this course would be lightning to strike and it would be over so we don't plan for that. and since similar it's true for algorithms. maybe we should be focusing on understanding prope rties of the input and finding algorithms that are efficient for that input. and the other thing is in order to really predict performance and compare algorithms we need to do a closer analysis than to within a constant factor. so we talked about the tilde notation in the big theta, big o, and big omega, omega that are used in the theory of algorithms. and really there's so much published research in the theory of algorithms that a lot of people make the mistake of interpreting the big o results that are supposed to give improved upper bounds on the difficulty of the problem as approximate models for the running time and that's really a mistake. so in this course, we're going to focus on approximate models by, you know making sure that we use the tilde notation and we'll try to give specific results for certain quantities of interest and the constant, any unspecified constant in the running time. we'll have to do with properties in the machine and in the system so they will be able to use these results to predict performance and to compare algorithms. 
so far, we've been talking about running time. now we have to talk about the memory requirements over our programs as well. well, the basics are we want to know how many bits the program use or bytes, eight bits at a time. and actually, we'll be talking in terms of millions of bits or billions of bits and actually surprisingly there is a controversy about even these basic definitions. computer scientists think of a million bits is two^20 and a billion is two^30 because that's a number of possible things that you can fit into 30 bits and everything is consistent with our calculations. other scientists stick to one million or one billion for a lots of reasons we'll usually use two^20, i mean, a megabyte. now an old computers we used to for many years, we use a 32-bit machine so that pointers were four bytes. just in recent years we've mostly switched to a model where machines are 64-bits and pointers are eight bytes. that allows us to address much more memory but pointers use much more space and actually this transition caused a lot of problems initially because programs were using way more space than people thought they should. you're not going to have to go through this kind of transition the way that we did because 64 bits is definitely enough to address anything that you might need to address, two^64 is really a huge number. so in terms of bytes we have to start out with typical memory usage. now, again, this is very dependent on machine and implementation but these numbers are reasonable and are found on typical implementations. so a boolean, it will be nice of a boolean just took a bit cuz that's just true or false but actually, usually we have to count for a byte for a boolean. all byte is a byte. character nowadays is two byte, 16-bit characters. not that a long ago we used eight bit for chars. integer regular int is four bytes or 32 bits and a float is also four bytes long int is eight and a double is eight. usually, we use double for floating point and ints for integers in most applications. so, that's for primitive types. and then for arrays there's a certain amount of overhead for making an array and then if there's n items, it's whatever the cost of the primitive type times n so an array of doubles is say 8n + 24. and two-dimensional array then well, we can go ahead and compute the exact thing but now, now, it's time to use, the tilde notation. and then for arrays we could say a double is tilde 8n for one-dimensional. for two-dimensional, two-dimensional array of doubles is tilde 8mn. and there's extra terms for the over head but for large m and n that's going to be pretty accurate. so, that's our basic usage for primitive types and arrays in a typical java implementation. now, a lot of our programs and objects like link list and so forth. so, we have to also factor in object overhead to crossover reference and also there's padding built in, in typical implementations to make it so that each object has used a multiple of eight bytes. so, for example if you have a date object that had three int instance variables then that object would take a total of 32 bytes. each int takes four bytes, object overhead is sixteen bytes. it needs four bytes for padding so it's a total of 32 bytes. so and the other one that often comes up is a string and the string is a little bit more complicated than a than an array but the typical implementation of a string in java has a, a reference out to an array of characters and then, its got int values for offset count in a hash value and then some padding and adding it all together the [cough] cost of the string is about 2n + 64 bytes. so, these are the basics that we need to analyze the memory usage for a typical java program. a h, so for primitive, for data type value, if it's a primitive type it's four for an eight, and eight for a double, and so forth. if it's a reference, it's going to be eight bytes and that's for the pointer takes array 24 bytes plus the memory for each entry in an object sixteen bytes plus the memory for the instance variable plus if there's an inner class , it's another eight bytes as we talked about with nodes for link list. and then there's the padding. so then we have to, to think about who is responsible for referenced objects, you know, in, in some cases. and we'll take care of that when we get to these situations. so, as an example, a simple example of memory use analysis, let's take a look at how much memory are rated quick union uf function from a, a few lectures ago, uses as a function of n. and there's only a couple of memory elements and each one of them are easily analyzed using the basics that we just gave it's an object so the sixteen bytes of object overhead there's two int arrays. each one of them have array overhead of 24 plus and then 4n for the n entries. each and vn entries takes four bytes and there's four bytes for the count and there's four bytes for the padding and if you add it altogether it gets 8n + 88 which is tilde 8n and again, all that's saying is when n is large, all we are going to care about in terms of analyzing the memory is that we've got [cough] 2n integers two arrays of size n each one of which takes four bytes for a grand total of 8n bytes. okay. so, in summary we really can figure out how many times we have to turn the crank on modern computers. we can do it with empirical analysis where we actually execute the program, can do experiments and use [inaudible] power law, formulate hypothesis and make predictions. but we can do more, we can do mathematical analysis where we can identify the most costly operations, analyze the frequency of execution of those operations and using the tilde notation to simplify analysis. we can actually explain the behavior, not just predict it. and this is a fine example of the use of the scientific method to understand the artifacts that we're studying, the algorithms. our mathematical models are usually independent of a particular computer system and even implied to machines that are not yet built. but we always validate our mathematical models by running experiments on real machines so that we can be confident where we're making predictions and analyzing algorithms. 
welcome back. today we're going to talk about algorithms and data structures for implementing some fundamental data types called bags, queues and stacks. you maybe somewhat familiar with these, but today we're going to take a careful and close look at them. the idea is that in many applications, we have collections of objects that we want to maintain. and the operations are very simple. we want to add something to the collection, maybe remove something from the collection and iterate through the objects in the collection performing some operation on them, and of course, test if it's empty. now for most of these, the intent is very clear. the key is when it comes to removing an item, which item do we remove? the two fundamental classic data structures for this, the stack and the queue differ in the way in which the item to be removed is chosen. for the stack, we take out the item that was most recently added for, the terminology that we used is push to insert an item and pop to remove the item most recently added. that's also called the lifo discipline last-in-first-out. for queue, we examine the item least recently added and those operations to distinguish them we call inqueue to insert an item and dequeue to remove an item and that's also called the fifo discipline, first in, first out. so now we're going to take a look today on how to implement these things. our subtext today is all about modular programming. and that's going to be a discipline that we're going to follow up carefully throughout this course. the idea is to completely separate the interface and the implementation. so, when we have these types of data structures and data types that are precisely defined like stacks and queues and so forth, what we want to do is completely separate the details of the implementation from the client. the client has, can have many different implementations from which to choose but the client code should only perform the basic operations. the implementation on the other hand, can't know the details of the client needs, all it's supposed to do is implement those operations. in that way, many clients can reuse the same implementation. so this allows us to create modular reusable libraries of algorithms and data structures that we can use to build more complicated algorithms and data structures. it also allows us to focus on performance when appropriate. again, this is a modular programming style that's enabled by object oriented programming languages such as java and we'll be very disciplined in our use of this style. alright. so to begin, we'll talk about the stacks. [cough] stacks are familiar, many of you probably implemented stacks in an introductory programming course but we'll do a thorough introduction to implementations right now. as a warm up, let's suppose that we have string, a collection of strings. they might be short, they might be long and what we want to have is the ability to save away a collection of strings and remove and return the most recently added string periodically, and also test if it's empty. so that's our api. we have a constructor to create an empty stack, we have for insert and we have a method called push that takes a string as argument. and for remove, we have a method pop that returns to the string most recently added. and we have these empty test which returns a boolean. also in some applications, we would include the size as well. so again, as always, we'll first read a client and then look at implementations and our client, simple client is to take some strings on standard input and some pop commands which are indicated with hyphens. and so, it'll this client reads strings from standard input. if the string is equal to the hyphened character, it'll pop the string at the top of the stack and print it. otherwise, if it's a string that's not equal to the hyphen character, it'll just push it on to the stack. so in the example down below here if we have this file called tobe.txt then what we'll, what the client will do is push to be or not to all in the stack then when it comes to this hyphen it'll pop the most recently inserted item which is two in this case then it'll put b in the top of the stack and then pop the top item on the stack which is now b and then pop the item most recently added, b is gone, two is gone so the next is not and so forth. so, this is a simple test client that we can use to test our implementations. so now, let's look at the code for implementing a stack. now, the first implementation that we'll look at, uses link list. if you're not familiar with the link list you'll need to review that in section 1.3, 1.3 at the book or in our introduction to programming java book. even if you are familiar with link list, it's worth taking a look at this code because it's the style of coding that we'll use throughout the coarse for much more complicated data structures. so the idea is to keep a link list where which is consists of nodes that have strings in them and references to the next item in the link list and to, to implement a stack when we do a, a push operation, we insert a new node at the beginning of the link list and we do a pop operation where we move the first node from the beginning of the link list, that's the most recently added items. so, let's look at what that code looks like. we use to implement link list in all linked data structures through out the course. we use what's called an inner class in java and that's just a way to describe that we're going to be manipulating node objects that consist, each consist of a string and a reference to another node. so, the pop operation for link list is very easy to implement. [cough] first, we, we're going to need to return the first item on the list so we save that away. take first that item and save that in the variable item. a h, then, to get rid on the first node, we just advance our pointer to the first item on the list to point two of the next item and then that first node is ready to be reclaimed by the garbage collector. and then, the last thing we need to do is just return the item that we saved away. okay, so that's the pop operation. what about the push operation? [cough] push operation, we want to add a new node at the beginning of the link list. so, first thing we do is save a way the pointer to the beginning of the list. that's a little first thing first. then we'll create a new node, that's going to be the new node that we put at the beginning of the list, that's first equals new node. and then we said it's instance variables. it's items is the string that we want to put at the beginning of the list, in this case, not. and it's next is the old first item of the list which is now the second item of the list. so, after this operation, we are first pointing to the beginning of the list and we have the items on the list in decreasing order of when they were put on to the stack. so that also is a four liner to implement the stack push operation. so this is a complete link list implementation of all the code to implement a link list for a stack of strings in java. it's, it's a class the constructor doesn't have to do anything, there's no constructor. we have this inner class that we use to build the items in the link list and we make them an inner class so we can directly refer to those instance variables. and then the only instance variable of a stack is a reference to the first node on, on the list and that starts out being null. then it's empty is just testing whether the first node on the list is null and then push is the four lines of code that i gave on the previous slide and pop is the three lines of code that i gave on the slide before that. tha t's a complete implementation for the link list that'll work with as a fine push down stack implementation for any client. so now we can analyze the performance of that so that we can provide clients with information and how well the algorithm data structure will perform. in this case, it's easy to see that every operation takes constant time in the worst case. there is only a few instructions for each one of the operations, there's no loops. so that's obviously a very desirable characteristic. then how about space units, usage? well, that depends very much on the implementation in machines so this a typical java implementation that we'll do the analysis for and contest this out for different types of environments easily in this representative. so, in java, an inter class there's for every object there is sixteen bytes of over head. there are some extra over head, eight bytes because that's an inter class and then there is two references that we built in our, in, in our class node. one to string and another one to a node and those are each eight bytes. so, we have 40 bytes per stack node, if we have a stack of size n, we have about 40 n bytes. that's a little extra first but that's about an overhead for the whole stack but when n is large, 40n is a very close estimate to the amount of space needed. this does not include the space for the strings themselves which are owned by the client. but with that, we can properly asses the research usage of this implementation for different client programs. now it's constant time but there's faster implementations of stack and since stack is used inner loop of some algorithms it's important to think about even faster implementations. and another, natural way to implement a stack is to use an array to store the items on a stack so let's take a look at that. this alternative of choosing between length structures and arrays is fundamental and it's go ing to come up again and again when we consider more complicated data structures in algorithms. so, we want to be sure to analyze it in the simple case for stacks to set the stage for more complicated applications later on. alright, so the use in array we just keep the n items on the stack in the array and the array location within the n is the place the top of the stack where the next item is going to go. so, to push we just add a new item at s(n) into pop we remove the item that's at s(n) - one and decrement n. now there is a fundamental defect in using an array and that is that you have to declare the size of array ahead of time and so the stack has a certain capacity. and if there is more items on the stack than the capacity we have to deal with that problem and that's a fundamental problem that we have to deal with in array implementations in all sorts of algorithms and data structures. so again, considering it for the simple case we'll pay off later on. alright, so here's the full implementation of stack for using an array to represent the stack. so now we have an instance variable which is an array of strings and or variable n which is both the size of the stack and the index of the next position, next open position on the stack. this one has a constructor and the constructor creates the array. now, we are cheating in this implementation to keep it simple and we'll take care of this cheat in a little while by requiring the client to provide the capacity of a stack. in a few applications this might be fine but in many, many applications that's two owners are requirement and client really can't know how big the stack is. client might have a lot of stacks that need to be maintained simultaneously and then maybe they reached their maximum capacities at different times and various other things. so, we need to remove this cheat and will. but the code is nearly trivial. if we have the capacity to check if it's empty we check if n is zero. to push an item we use n to index into the array put the item there and then increment n, that's the short cut in many programming languages nowadays for use the index and then increment it. and to pop we decrement the index and then use it to return the item in the array. so each of the operations is a one liner and this is a fine implementation for some clients. that's array implementation of stack but it breaks the api by requiring the client to provide the capacity. so what are we going to do about that? well, there are a couple of things that we didn't consider. we didn't put in a code to throw an exception if the client pops from an empty stack. probably should do that and for overflow, what happens when the client does too much well, we're going to talk about an approach called resizing that will allow us to avoid overflow for clients. there's another issue about whether clients can insert null items into the data structure. in this case, we do allow the client to insert null items but we do have to worry about in java about a problem called loitering and that is the idea that we have references to an object in our array implementation and the stack array and we are not really using it. so, when we decrement that value in, there's still a pointer to the thing that we took off the stack in that array even though we know we're not using it. the java system doesn't know that. so to avoid that and really allow most efficient use of memory it's best to set that. [cough] removed item entry to null so there's no reference to the old item left there and then the garbage collector can reclaim the memory since there's no outstanding references. so that's a, a detailed but an important one that we have to take care of and or implementations to make sure that we're getting most efficien t use of memory. 
okay, our basic array implementation of stacks had the defect where we required clients to provide us the maximum capacity of the stack ahead of time. now, we're going to look at technique for resolving that problem. how do we, we do not implementing the api. the api says we should just be able to create a stack and it should be able to grow and shrink to any size. so, how do we going to go and shrink the array? well, first thing you might think of is when the client pushes a new item onto the stack increase the size of the array by one and when pops, decrease the array by one. that's easy to code up but not worth it because it's much too expensive to do that. the reason is that you have to create a new array, size one bigger and copy all the items to that new array. so inserting the first n items would take time proportional if the text, stacks is size n - one, it's going to take time n. and when it's two time n - one so the first n items will take the sum of the first n integers which we know is about n^2 / two. quadratic time to insert n items into a stack that kind of performance is unacceptable for large problems as we've seen, as we will see many times. so, the challenge is to do the resizing. but somehow ensured that it happens and frequently. so, the well end technique for doing that called repeated doubling is to when the array fills up, create a new array of twice the size and copy all the items over. then we don't create new arrays all that often so here's the implementation of that. we start with an array of size one. if we have a full stack, which we know by testing n which is the number of items in the stack versus the rail length, then we just re-size the array into one of twice the length before inserting the item. and how do we re-size to a new capacity? we create a new array of that capacity and just go ahead and copy our current stack into that, into the first half of that and then retu rn it. and that will reset our instance variable which is our stack to this new bigger array. so, the idea and the consequence of this is if you insert n items into an array, into a stack with this array representation, the time will be proportional to n not n^2. and the reason is that you only create a new array every time it doubles but by the time that it doubles, you've inserted that many items into the stack so on average, it's just like adding one operation to cost of one to each operation. so, if we just, if we just calculate the cost and inserting the first n items you're going to have, instead of the sum of the integers from one end, you're going to have the sum of the powers of two from one to end and that will give a total cost of about 3n. so, that's an array axises. for the copy, there's two array axis. so, to insert an item, its about three array axises. this plot is another way of looking at it which is the number of array axis its taken as you implement push operations. every time you hit a power of two, you take that many array axises but in the sense you've already paid for them by putting those items on the stack. so that's called amortize analysis, where we consider the total cost averaged overall operations and this is a, a fine example and useful example of amortize analysis to get efficiency in a stack implementation. now we, we have, what about the pop? we have to think about how to shrink the array. so, we might think, well, we doubled it when it was full, when do we cut it in half when it gets to be half full. we don't want to get the array to get two empty. well, that one, one doesn't exactly work because of a, a phenomenon called trashing. if you, if the client happens to do push, pop, push, pop alternating when the array is full then, it's going to be doubling, halving, doubling, halving and creating new arrays on every operation to take time proportional to n for every operation and therefore, quadratic time for everything so i don't want to do that. the efficient solution is to wait until the array gets one quarter full before you have it. and that's very easy to implement. we'll just test if the arrays one quarter full, if it is, we re-size it to half full. and so, then at that point, it's half full and you can either grow by adding stuff or shrink by subtracting stuff but there won't be another resizing array operation until, i guess totally full or half again full. so the invariant of that is the arrays always between 25 percent and a 100 percent full, number one and number two that every time you re-size, you've already paid for it in the amortize sense by inserting pushing or popping. so, here's just a what happens to the array for our small client example and you can see at the beginning, it doubles from one to two to four but once it gets to four, it stays once it gets to eight, it stays to that size for a while even though there's some of the operations it doesn't shrink back to four until after there's only two items in there and then it shrinks and so forth. so, array resizing doesn't happen that often but it's a very effective a way of implementing the stack api with an array where the client does not have to provide this maximum capacity of the stack but still were guaranteed that the amount of memory that we use is always only a constant multiple of the number of items actually on the stack. so the analysis now says that the average running time per operation for whatever the sequence of operations is the average running time is going to be proportional to a constant. now, there is a worst case that is at the point when the stack doubles, it takes time proportional to n so it's not quite as good performance as we might like but it's what we the advantage that we get is ve ry fast pushes and pops just access array and increment it and very efficient for most operations. and for many, many clients that's an effective trade off to make. so what about memory usage? well, this is the analysis of memory usage for stacks and it's actually less memory than for strings the amount used is between 8n and 32n depending on how full the array is and just a quick analysis of the amount of space that arrays take in java. so, again this analysis is just for the stack itself not for the strings which the client wants. so, what are the trade offs between using a re-sizing array versus a link list. there's a two different implementations and the same api and the client can use them interchangeably, which one is better? in many situations, we're going to have multiple implementation of apis and depending on properties of the client program you're going to have to choose which one is the better one to use. so, for link list every operation takes constant time in the worst case that's a guarantee but we have to use a little extra time and space to deal with the links. so, it's going to be slower. resizing array implementation we have a good amortized time so total average over the whole process is good. we have less wasted space and probably faster implementation of each operation. and so, for some clients, maybe that makes a difference perhaps, you wouldn't want to use a re-sizing array implementation at the moment that your plane is coming in for a landing and you wouldn't wanted to all of the sudden, not implement some operations quickly. if you need that kind or maybe in an internet switch where packets are coming through at a great rate, you wouldn't want to be in the situation where you're missing some data because something got slow all of the sudden. so, that's a trade off that the client can make if i want that guaranteed, if i want to be sure that eve ry operation is going to be fast use a link list and if i don't need that guarantee, if i just care about the total amount of time i'll probably use the resizing array because the total will be much less because individual operations are fast. so, even with these simple data structures, we have really important trade offs that actually make a difference in lots of practical situations. 
okay, next, we'll briefly consider queue implementations using the same basic underlying data structures. so, here is the corresponding api for queue of strings. actually you know it's the same api for stacks just the names are different. instead of push we have enqueue instead of pop, we have dequeue. and the semantics is different. for enqueue we add an item say at the end of the queue and for dequeue we remove an item from the beginning. it's as if you're waiting in line to buy a ticket. when you're enqueue you're at the end and when that's been in there the longest is the one that comes off. so let's look at how we implement those first using linked list and then arrays. so, now our representation of a queue with the linked list, we need to maintain two pointers references. one to the first item in the list and the other to the last item in the list. when we insert we're going to add the item at the end of the list instead of the beginning and when we remove we'll do the same and we'll take it off the front. so here's the implementation of dequeue. it's identical to the code for pop for a stack. we save away the item. we delete the first note by advancing the reference and then we return the item, so identical. to add a node or enqueue, add a new node to a linked list, we want to put it at the end so that would be the last one return. so we, to add it at the end so first thing we need to is save a link to the last node. we're going to need that because we need to change its reference from null to point to the new node. then we'll create a new note for the end of the list will populate its fields and then that old link will change that from null to a pointer to the new node. so again just a few lines of code. that's basic linked list processing. actually years ago when we taught courses in algorithms and data structures much of the course would be about this kind of pointer manipulation but nowadays that's restricted to just a few implementations like stack and queue and a few other fundamental data structures. so we don't need so much anymore general programs for manipulating linked list. we encapsulate them in basic data types like these. alright, so let's go back to our full implementation and this is just taking care of collecting a curve from the previous slides but also taking care of special cases when the queue is empty to make sure that if the queue is empty after we remove an item, we're going to last at null and make sure that both first and last always are what we want them to be. so those are details that are easy to check. okay, what about arrays? well, we want to do the details but it's not difficult to implement queues with resizing arrays as well and not difficult but definitely a tricky programming exercise that people are welcome to try. so we'll maintain two pointers. the first item in the queue and the tail which is the position for the next item to appear so for enqueue you add a new item at tail and for dequeue you remove an item for head. and the trick is that once you get past the capacity, you have to reset back to zero and so that's a little extra code and then you have to add the resizing capability as well to implement data structure the same as for stack. and we'll leave that as an exercise. 
next we're going to consider addressing another fundamental defect in the implementations we've considered so far that those implementations are only good for strings. what if we want to have queues and stacks of other types of data? and that brings us to the topic of generics. alright. so, we implemented stack of strings but in applications we have all different types of data that we might want to implement like stack of int say or urls or cars or vans or whatever data that we might be processing. so how are we going to implement stacks and queues for that types of data. well, first thing that we might that we might consider and actually we're forced to consider this one in lots of programming environment, is to implement a separate stack class for each type of data that we're using. that really seems unsatisfactory. we have our carefully crafted code that does array resizing and so forth and we're going to copy that code and change the data type string to the data type van or int to everywhere. and what if we have hundreds of different types of data that we're processing. we have hundreds of different implementations. unfortunately that situation at the beginning of java where we stuck with that and there are plenty of programming languages where basically we're stuck with that so what we want to look at is a modern approach to avoiding having multiple implementations for each type of data. so the a quick hack that is widely used is to use casting to implement to reuse the code for different data types. so, we make our implementation with type object so everything in java is a sub type of object and then the client, when the client comes to use it, will simply cast the result to the corresponding type. i don't want to spend a lot of time with this cuz i think this is a unsatisfactory solution. so, in this example we have two types with two stacks one of apples and one of oranges. and then, it's up to the client when it pops something off the apple stacks to cast at the apple to keep the type checking system happy. the problem with this is that the client code has to do this, this casting and it's kind of an insidious bug if it doesn't quite get it. so, the third attempt that we're going to talk about uses generics. and that way the client code doesn't do casting. we can discover mistakes in typed mismatches at compile-time instead of at run-time. so, in this case, we put, with generics, we can have a type parameter on our class and that include, that's inside angle brackets in this code and then, we can [cough] if we have a stack of apples and we tried to push an orange unto a stack of apples then we're going to get a compile-time error because that's stack was declared to only consist of, of apples. and just the guiding principal in good modular programming is that we should welcome compile-time errors and avoid run-time errors because if we can detect an error at compile-time, then we can ship our product or deploy our implementation our implementation of an api and have some confident that it's going to work for any client whereas, the error is not going to get discovered until run-time it might occur with some client development. now, years after, we have to deploy our software and be extremely difficult on everyone. okay. so actually with a good generic implementation it's not difficult to simply [cough], take every place that we used string and replace it with a generic type name as in this code here. on the left is our implementation of a stack of strings using link list. on the right is a generic implementation. so, every place that we used string type on the left we used the word item on the right. and at the top, the class declaration we declared an angle brackets that item is the generic type that we're going to use. the implementation could hardly be more straightforward and it's an excellent way to solve the problem of handling multiple types of data with one implementation. with arrays, it doesn't quite work and again all programming languages and, you know, many programming languages nowadays have difficulties with this and java's got a particular difficulty. so, what we would like to do is just declare a new array using our generic name item as in the highlighted line here. otherwise it's the same. unfortunately, java does not allow generic array creation. so there's various technical reasons for that and you can read, read extensive debates about this on the web that's going to go beyond our scope. for now, what we need to do is put a cast in to make this work. so, we create an array of objects and then we cast it down to an array of items. now in my view, a good code has zero cast. so, we want to avoid cast as much as possible because it, it, it really is declaring some kind of weakness in what we're doing. but in this case we have to put in this one cast and so what we've heard about that is the ugly cast it doesn't, it doesn't make you feel good about the code. it's not something that you will come up with on your own and that's, and that's an undesirable feature, i think for codes so simple as this. but fortunately, we can get through pretty much everything that we're going to do in this course just knowing about this one of lay cast. so now, when we compile this program we get a, a warning message from java. it says that we're using unchecked or unsafe operations and we should recompile with a minus -xlint equals unchecked for details. so, we can go ahead and do that and it says that you have put in, in your code an unchecked cast and we're warning you about that cuz you shouldn't be putting in unchecked cast. and okay, that's fine and you're going to see that when you do compiles using code like these. i, i think maybe they might have added to this warning statement "we apologize for making you do this". it's not our fault that we had to do that, we had to do that cuz of your requirement about not allowing us to declare generic arrays. so with that note please don't think there's something wrong with your code if you follow our prescriptive and, and get this warning message. okay then, it's one of the detail that java takes care of and that's what about primitive types [cough] so the generic type that we're using is for objects and you know, we're casting down from array of objects. so in order to handle generic types we have to use java's wrapper object types. so integer with the capitalized rapid type for int and so forth and many of you were probably familiar with that. and there's a process called auto-boxing which automatically cast between primitive types and wrappers so all of that handles of the, the problem of dealing with primitive types, kind of behind the scenes. and the bottom line is that we can articulate an api for generic stacks that works for any type of data and we've got two implementations, link list and arrays that, that performed very well for [cough] any type of data using the, the resizing or link list as we've described. 
okay. there's another facility that java provides that leads to very elegant compact client code that's definitely worthwhile to add to our basic data types and that's iteration, that's what we're going to talk about now. so, what we want to do is to allow the client to iterate through the items in the collection. but we don't have the client to know whether we're using an array or link list or whatever internal representation we might have in mind. it's not relevant to the client. and a lot of clients only want to do is just iterate through the stuff in the collection. but java does provide a nice a solution to this called iteration. so what we're going to do is look at how to make our stack, and queue, and other data structures that we consider later on implement the so-called iterable interface and it will work for client code no matter which implementation we used so let's take a look at the details of that. so what's an iterable? well, in java lingo what an iterable is, it's, it's a class that has a method that returns an iterator. and so what's an iterator? well an iterator is something, a class that has methods hasnext() and next(). the java also allows remove(). we think that one is bad news, we don't use it can lead to insidious debug, bug debugging problems. so, it's hasnext() and next() and so to make the data structure iterable, we're going to implement those things. it seems like a lot of baggage to carry around and the reason that we do it, why do we go to the trouble doing it is that we can, if we have a data structure that's iterable we can use a very compact and elegant client code in java, the so called for-each statement. so if we have a stack we can say - (for string s : stack). it means for each string in the stack - print it out. and if we didn't have that we would now, if we're using iterators, we could go ahead and write this longhand code but nobody would ever do that cuz it's equivalent to the shorthand or we might have to write client code that does a lot of unnecessary pushes and pops just to do this iteration. so that's the key is to be able to have client code that is so compact for iterating through items in the data structure so we're going to provide iteration for all our basic data structures and it's not too hard to do definitely worthwhile the effort. so here's what it looks like for link list. so it's got to implement iterable so what does that mean implement iterable? it's got to have a, a method iterator() that returns an iterator. so what's an iterator? so, we're going to use an inner class. in this case, we'll call it listiterator that implements iterator and it's generic. and basically what this thing has to do is implement these methods hasnext() and next(). and the semantics just clear from the names. hasnext() is supposed to if, if we're done is supposed to return false. if we're not done we're supposed to return true and the next() is supposed to give the next item in the iteration. so if the thing is a linked list we're going to start out at first. we have that's the, our first item in the list and we're going to maintain an instance variable current inside this iterator which is the current thing that we're iterating. so, get the next one just like if we want to remove the first. we pull out the current item and then advance the current reference and return item. moving current to the next place. the client is always going to be testing hasnext() as i showed as i showed and that stub code before and so when it gets to null it will return false in the iterational stop. but for our iteration, we just have to worry about implementing next() and hasnext() and perhaps using a local instance variable to get it done. we have to probably to make bullet proof code - throw exceptions if a client tries to call next() with no items there and tries to call remove() at all, we're not going to support remove(). for, and for array, it's even simpler. so now with the iterator we have control over which order we go through the items and so that's going to go along with the semantics and the data structure so probably in a stack you want to get the things in stack order like the order that come out of the stack so that's reverse order in the array so in this case then next() is just decrement and return the next one and our instance variable is an index in the array. and then hasnext() is okay as long as that thing is positive. so a little java [cough] code to provide this iteration facility but actually within this framework not too much to do and you can see how to implement this for your own data type and we'll use this paradigm for every basic data type that we, that involves collections of objects that we'll encounter. alright, and in fact, it leads us to actually for a lot of clients it doesn't really matter what order we get the items. really often what we're doing is just inserting items into a collection and then, later on, iterating through the items that we have. that data structure is called a bag and so let's look at what that api looks like. order doesn't matter so all we want to do is add an item maybe you want to know the size and we want to iterate through all the items in the bag. so this is a simpler, narrower api but still it expresses an important little collection of operations and, and we'll use this one and we've already seen the implementations. you just take stack and remove the pop, or queue and remove the dequeue [cough] and you have fine implementation of a useful data structure. 
okay. those are some basic data structures and implementations and it seem quite elementary and simple but actually right away we can get to some very sophisticated applications of these basic concepts and that's what we're going to consider next. now, first thing to mention is that often the kinds of data types and data structures that we implement or found in a java library. so, that's true in many programming environments. so, for example stacks and queues you can find those words mentioned in the java library so there's a java collection library and the so-called list interface which is displayed here. so java has general api for sequences of items and its got things like a, append at the end, remove from the beginning, and so forth. any uses of the resizing array, so many of the principles that we consider does also a, a link list interface. so, why not just use those? why use our own implementations? well, the problem is often in such library code is kind of designed by committee phenomenon that more and more operations get added and the api becomes too broad or bloated. it's not a good idea to have lots and lots of, you know, operations in the same api. and we'll see example in a second. the problem, the real problem is that when you do that you can't know much about the performance or you can't assume much about the performance. and so you can kind of immediately arrive at that performance even for simple clients. so our best practice that we recommend is so few that these basic data structures that we use and there's so simple is to go ahead and use the implementations that we've just discussed for these fundamental data structures. maybe later, later on, after an experienced programmer who knows what he or she is doing could use some of these library collections effectively. but inexperienced programmers often have trouble with it. here's a war story from students programming assignments not that long ago. so, we have an assignment where you need to generate a random open sites in a percolation system. we have one student who was paying attention to what we're saying and uses an array and can pick the indices into that array at random check whether they're open and, and repeat. and so the array is n by n, it's n^2 things and it takes about n^2 time, which is actually a linear time for this application. but then we have another student who had some java before coming to us and considered himself an expert and said, well, i'm going to use linked list because i could use java's library and i don't have to worry about downloading your stupid code. and so, i'll just use that one and pick an index at random and delete and that program took quadratic time and poor kenny, when trying to run his program for the huge instance that we asked found out that it wasn't finishing. and the reason is that the java linked list implementation takes a linear time to find an item with a given index. not constant time like an array. and that's difficult for kenny to think about and difficult to drive that information from the implementation so program is just too slow. and with the swiss knife implementation with so many operations it's hard to know whether or not the particular set of operations that your client needs is efficiently implemented. so our insistence in this course is that students should not use the library until we've implemented it in class. at least that some indication that you understand the performance characteristics. so now, let's look at some applications then of, of stacks. there's the stacks are really actually fundamental underlying computation because they implement , recursion and so, you use stacks often everyday when you wrote, use the back button in the web browser, the places that you've been are saved on a stack. right now we will look at two examples. one, having to deal with compiling from a programming language or interpreting into an actual computation and then the other one is the postscript language which is widely used for, for printing and publishing. so, so the way the compilers implement functions is using stacks. when there's a function call the whole local environment is pushed and then along with the return address and then the function returned is pop the return address in the local environment. so there is the stack there that contains all that information and whether the function calls itself or not is not relevant. the stack contains the recursion. in fact, you can always use an explicit stack to make a recursive program non-recursive. so, this is so when we have the gcd function, computing the greatest common denominator, greatest common denominator p and q is greatest common denominator of q and p mod q and it just calls itself until q gets to be zero. and as this graphic integrates, it just does it by saving the information on a stack. now a specific example that really shows this off and also will illustrate the utility of being able to process multiple types of data with the same code is this example is dijkstra's two-stack algorithm for arithmetic expression evaluation. so the goal is, you got an arithmetic expression this is just actually like a simple stand in for a program and we'll talk about that in a second but let's say, arithmetic expressions. we have operands and operators and you want to evaluate it. and dijkstra's algorithm is very simple to express. you processes through the expression from left to right. if you see a value, you put it, you maintain two stacks and if you see a value, you put it on the value stack and if you see an operator, you put on the operator stack. left parenthesis you ignore. right parenthesis, you pop the operator and two values and push the result. now that's a lot of words let's look at a demo. so we start out with the empty value stack and operator stack and we're going to move from left to right. so, and those are the a top is summarize the four type of things that we could wind up with and what to do so the left parenthesis we've ignored, a value we put on to the value stack. so, that one goes right in to the value stack. operator, we put on to the operator stack. and plus it goes on the operator stack. left parenthesis you ignore. it seems strange to be ignoring parenthesis and we'll get back to that in a second. value, put in the value stack. operator, put on the operating stack. doesn't seem like we're doing much except putting stuff on stacks and now, when we come to our right parenthesis and that's when it gets interesting. what it says is to you have the top operator and the top two values and that's what you want to do. supply that operator to those values and put the resulting value that you get back on to the operation stack. so, we take off the top two things, we do the operation and then we put the thing that we get onto the value stack. and that's right parenthesis. so now continuing along we put a star on. left parenthesis, we ignore. four on, star. the right goes to the value stack and now we got a lot of stuff on the stacks and we got through right parenthesis and that's going to finish up the computation, take the top two items off the stack and the top operator off the operator stack, perform the operation, put the result back on the value stack. another right parenthesis, take the top two values off. perform the operation. put the value on to the value stack and finally, the last right parenthesis, take the two operators of the value stack, operators of the value stack, and operator of the operator stack, perform the operation, put the result back on the value stack. and we're at the end of the computation and that's the result. the value that arithmetic expression is 101. okay? yup. here's the code that implements dijkstra's two-stack algorithm. we have two different stacks. the operand stack the operator stack is string, it could be characters which is just our operator. then our value stack is doubled so that's the same stack code but with generics, we're using, using two different types of data. and then simply perform dijkstra's algorithm. if we have a left parenthesis... read a new string. if we have a left parenthesis, do nothing. if we have plus or times, push it. if we have a right parenthesis, then go ahead and pop the operator. and if it's plus, add the result of the two values at the top of the value stack and if it's a star, multiply the two values on the top of the stack and, and then push the result. so and then when you're done then simply print out the value on the stack and that's a fine and elegant implementation using stacks for any arithmetic expression. and it's easy to extend that to handle other types of things and so, why does this work? well, when the algorithm encounters an operator, say, in the inside, we got the parenthesis, operand, operator, operand, parenthesis its easy to see that what its going to do inside there is put the at the top of the stack whatever it is, is to put the two and three on the top of the value stack and plus on the top of the operating stack and when it hits that right parenthesis, it's going to perform the operation and it's going to proceed then exactly as if the original input where that, where the value replaced. so, just go in from the inside out for every operation enclosed within parenthesis like that it's just repeat the argument that's exactly as if the original expression were (one + five) twenty and then again, replacing that one, one + 100, 101. that's, that's why dijkstra's algorithm works. actually fairly easy to understand why it works. and you can go ahead and extend this algorithm to add functions like logs and sines or other operators and have precedence among operators, have them associate and multiple operations, and so forth. and actually that's on the road to developing a compiler or a way to translate a, a program from a programming language to a computation, so dijkstra's algorithm that uses stack is one way for entering and understanding of the basis of computation. 
okay, what are the rules that we're going to follow? well, let's start with looking at a typical basic sorting problem. say, university has student records and for every student there is a certain amount of information. maybe there's a class number, there is a grade, there's a phone number maybe an address so we refer to an item and it has a record or the information that we're going to sort. but in particular, there's a piece of a record called a key and what we want to do is put the records into order according to the key. that's the sort problem. re-arrange an array of n items into ascending order according to a defined key which is part of the item. now, our goal is to be able to sort any type of data so let's look at a couple of client programs. first example is to just sort some random real numbers into ascending order. so, here's a client that calls our insertion sort method and all it does is read numbers from standard input than into an array a then calls insertion sort and then prints them out. and you can see on the right that the numbers are printed out in sorted order. this seems like an artificial kind of input but actually we'll look at an application even in this lecture. and then there are many applications where random inputs are fine model. here's maybe a more familiar sort client that sort strings. and in this case it reads the strings from a file using our readstrings() method in our in class that which takes a file as argument. so we take the file name as the first command line argument, read in array of string from that file separated by blanks, call an insertion.sort() method again. so, insertion.sort is a method that takes an array a as its parameter and it, it's the first argument and it rearranges the strings in that array to be in sorted order. so in this case words, words three.text has the certain number of three letter words and this client program will result in those three letter words being rearranged into alphabetical order. here's another client that we could use our sort program for, if we achieved the goal of sorting any type of data. in this one, we're going to sort file, file's name in a given directory. so again we use the file class from java and we use, we go and use the listfiles() method from that class to get an array that contains all the file names in the given directory. that's an array with file names in it and insertion.sort() takes that array as its first argument and again sorts them and then we go ahead and use as, go through them one by one and print them and they come out in order of file name. so that's three different clients, three completely different types of data. and the first rule of the game that we have to think about is, how can we make it so that we can implement one sort program that can be used by these three different clients to implement three different types of data. in the way that, that happens is a mechanism known as a callback. so, that's our basic question, how can sort, now, how to compare data of all those different types without being given any information about the type of an item's key? and the answer is that what is we set up a mechanism known as a callback or reference to executable code where the client, by passing an array of objects to the sort function. in java, there's an implicit mechanism that says that any such array of object is going to have the compareto() method, then the sort function calls back the compareto() method associated with the objects in the array when it ever needs, whenever it needs to compare two items. there's a lot of different ways to implement callbacks and that's programming language specific. different languages have different mechanisms. it's all about the idea of passing functions as arguments to other functions which is the pair and gets into functional programming and thinking all the way back to turing and church. for java, because of the desire to check types at compile time, the use of specific method called an interface and then, we'll look at the details of how to implement callbacks with the java interfaces now. it's a little bit of programming language detailed but it's, it's really worthwhile because it allows us to use the sorts that we developed for any type of data in a type safe manner. so we already looked at some clients. this is the example of the client program that sorts the files in a given directory by file name. so it just calls our sort() method with a, an array some type of object as first argument. now, built in to java is the so-called the comparable interface and all the comparable interface is the specification that a type, data type that implements comparable will have a compareto() method. and it's generic and will be compared to against a certain type of item. now when we implement objects that are to be sorted we'll implement the comparable method. that's up in the top class file, implements comparable file. and since sorting is an operation that's used in so many situations, many of the standard java types that you would expect to involve sorts will implement comparable. and all that means is that, that data type has an instance method that will implement the compareto() method. it'll compare this object against the object given as argument and depending on some complicated tests, it'll return -1, meaning less, +1, meaning greater or 0, meaning equal. now, that compareto() method is really all that the sort implementation needs. first it says that, that it's going to take as argument an array of type comparable. so that means, the objects in the array are going to implement the comparable interface or that it will have a compareto() method. and then the sort code can just use that compareto() method, invoked in a sense of the object like an entry in the array and as argument and another instance in the object like another entry in the array to test whether the first is less than the second as in this example. the key point is that the sort implementation has no dependence on the type of data that's handled by the comparable interface and a different comparable array will be sorted in the same way though eventually, because of the interface mechanism, they call back to the actual compareto() code that goes with a type of object being sorted. now there's a few rules and there's natural rules but they're worth talking about and paying attention to that the compareto() method has to implement in the so called a total order. in all that saying is really that it must be possible to put items in order in a sort. so there's three properties. first one says that if v is less than or equal to w and w is less than or equal to v then the only way for that to be true is if they're equal and then there's transitivity. if v less than w, w is less than x, then v must be less than or equal to x. in totality, is that either v is less than or equal to w or w is less than equal to v or both they are equal. and there's plenty of natural total orders in the types of data that we normally want to consider for sort keys. like the integers or natural numbers or real numbers or alphabetical order for strings, chronological order for dates or times and so forth. the cartoon on the right shows that not all orders are necessarily total orders. so, rock, paper, scissors is intransitive. if you know that v is less that w, w is less than v, you don't know that v is less than or equal to v. i'm sorry, v is less than w, w less than equal to x that you don't necessarily know that v is less than or equal to x. alright. so the comparable api then, by convention in java we always need to implement compareto() such that v that compared to w is a total order. and also by convention, it returns a negative integer for its less zero if it's equal positive its greater. if this object is greater than the object given as argument. if the types are incompatible or if either one is null compareto() should throw an exception. now, again, many of java's standard types for numbers and dates and files and so forth implement compareto() by convention. now if we're going to implement our own type then we have to go ahead and implement the comparable interface according to these rules. and usually that's fairly straightforward. so here's an example. it's a simplified version of the date class that's implemented within java just to show the idea of implementing comparable. so, after the class declaration, we write implements comparable and then we fill in the generic with the same type because we're only going to compare dates to other dates. in this implementation, the date class has three instance variables. the month, the day and the year and the constructor fills those from the arguments as you can see. so now, if you want to compare two different dates then the first thing to do is to check if this year is less than that year, over that is the year given, the date given in the argument. if that's true then it's less return -1 and if it's, the year is greater, return +1. otherwise, the year, years must be equal so we have to look at the months to do the compare and so forth down to do the days. only if they're all equal that we return zero. so, that's an example of an implementation of comparable by implementing the compareto() method to put dates in order as you might expect. so the java language helps us with this comparable mechanism so that we can sort data of any type. when we continue to implement sorting algorithms, we're actually even in a hide that beneath our own implementations. so, that are sorting algorithms actually their actual code can be used to implement sorting in many other languages. the way we do that is to take the two primary operations, compares and exchangers that were that were, were used to refer the data and encapsulate them just the static methods. so, we're going to use a method less() that takes two comparable objects as arguments and it just returns, v.compareto(w) less than zero. and then the other thing that we do when we sort items that are in an array is to, to swap or exchange of the item at a given index i with the one at a given index j. and that's every programmer's first introduction to assignment statements. we save a[i] in a variable swap, put a[j] in a[i], and then put swap back in a[j]. so now our sort methods to refer the data will just use this two static methods. and there's a good reason for that. here's an example. suppose we want to test if an array is sorted. so this is a static method that is supposed to return true if the array is sorted and false if it's not. and all it does is just go through the array from the one to the length of the array and test if each item is less than the one before. if you have an item that's less than one before then it's not sorted you return false. if you get all the way through the array without that happening, then you say the array is true. that's pretty simple code, the question is, if you have a sorting algorithm that passes that test, are you sure that it correctly sorted the array? well the answer to that question is, yes if, yes if you used only the less() and exchange() methods to implement, to refer the data because then you know because you used the exchange() method that the data in the array after the sort is the same data as was in the array before the sort, sort. if you have a sort method that can store any values in an array, it could, for example, store zeros in every array entry that method would pass this test, but it didn't really correctly sort the array because overwrote all the values. so, we use less() and exchange() to be sure that we can test that our, our methods work with the method like this. 
the first elementary sorting method that we're going to take a look at is an easy method known as selection sort. the idea of selection sort, is start out with a unsorted array and we'll use these playing cards as an example. and in the ith iteration, we go through the array to try to find the smallest remaining entry, in this case, the 2 is the smallest from any entry. and then we'll swap that with the first entry in the array and then we know we've got one step done. selection sort is based on iterating that idea. okay. so, the basic selection sort method is to, in the ith iteration, find the smallest remaining entry and to the right of i or bigger index than i and then swap that with i. so, we start out i is at the left end and then the remaining, all the remaining entries to the right. we scan through and the smallest one is the two, three entries from the right so we swap that. so that's the first step. now, that part of the array to the left of i is in it's final order and we simply continue. so now, the smallest is the three. swap that with i, increment i. so now, we have the two and three in order, continuing that way. find the smallest, the four. swap that one with i, increment i. find the smallest, it's five, swap that with i, increment i. find the smallest, swap that with i, increment i.  each time we have to scan through all the remaining entries in order to find the smallest. but then, once we found it, we only have to swap two cards those are both key properties of selection sort. now the eight is the smallest and we swap. and now, we know they're in order but the program doesn't so we have to look and decide that i and n are the same and then it swaps it with itself and does the same thing for the last. and so, after that process, then we know that the entire array is in its final order, all sorted. alright. so let's, one way to understand the way that an algorithm works is to think about invariants . so, for the selection sort, we have a pointer that was our variable i, that scans from left to right. now, it's indicated by a little red arrow in this representation. the invariants are that the entries on onto the left of the arrow are never changed and they're in ascending order. no entry to the right of the arrow is smaller than any entry to the left of it. that's the way that we set it up. and the algorithm maintains those invariants by finding the smallest entry to the right and exchange it with the next one. so the code implements the invariants. so, to move the pointer to the right, we increment i. so, now the invariant might be violated so we have to fix it. it might be violated because you might have an element to the right of the pointer that is smaller than some, the element on the pointer. so, what we have to do is identify the index or that minimum entry and exchange it. then once we've exchanged it, again, we preserved our invariant. after that point, no element to the left of the pointer is going to change and all the element, there's no smaller element to the right. [cough] and that gives us immediately our code for the selection sort implementation. we identify the, the length of the array that's n. then we have a for loop that goes through every element in the array, we keep a variable min in that is the index of the going to be the index of the smallest element to the right of pointer i. we have an inter-for loop that for j, if it finds a smaller one, resets min and then once we've looked at all the elements to the right of i we exchange the smallest one with i. that's a complete implementation of selection sort. now it's easy to develop on mathematical model for the cost of selection sort and here's the proposition that describes that. selections or uses about n^2 / 2 compares and exactly n exchanges. and just looking at this trace of selection sort and operation really is a proof, visual proof of this proposition. in this diagram, the entries in black, are the ones that are examined in order to find the minimum each time with the minimum in red. entries in gray are not touched, they're in their final position. well, you can see that this isn't going to be in general an n by n square and about half of the elements in the square are black or about n^2 / 2 and you can see also the exact formula (n - 1) + (n - 2) and so forth is the total number of compares used. and then on each of the ns values of the variable i there's an exchange so that's the cost in terms of the number of exchanges. now, what's interesting about this proposition about selection sort is that, it doesn't matter what order the input is. selection sort is going to use quadratic time because it always has to go through the whole thing to look for the minimum. and another property is that you can't sort moving less data because selection sort does just a linear number of exchanges. every item is put in to it's final position with just one exchange. let's look at an animation of selection sort in operation. [cough] you can see our pointer moving from right to left every time it finds the smallest element to the right, it exchanges it into position. now, if the array is partially sorted, it doesn't matter to selection sort. still has to go through, even if it's totally sorted, still has to go through to the side where that minimum element is. that selection sort, our first elementary sorting method. 
now we'll look at insertion sort, which is another elementary method that interestingly has quite different performance characteristics than selection sort. let's look at a demo of insertion sort. for insertion sort, what we're going to do is we'll move an index i from left to right as before, but now, in the i'th iteration, we're going to move a[i] into position among the elements to its left. let's look at how that works on our example with cards. so, now we start by initializing i at the first card, and we take the idea that everything from i to its left is going to be sorted, and everything from the right we're not going to look at at all. so everything to the left of i is in ascending order, everything to the right, we haven't seen it all yet. so now when we increment i, well, in this case it's already in order, we don't have anything else to do. in the third case now, when i is at the third entry in the array, now we start a index j, and we move that starting at i to the left. and what we need to do is just exchange the 5 with every element to its left that's greater. so first we exchange it with the 10, it's still not in place, so we exchange it with the 7. now we get to the beginning of the array, and once we've done that or we've hit a smaller element, then we have everybody to the left of i in order. so now we increment that again, and we come to the 3. again, we exchange as long as the card immediately to the left is greater. and once we've done that, then we have everything to the left by in ascending order. now, in this case, we have the 8, and we only have to exchange one, and now it's got the 7 to its left and everything is in order. so we've achieved putting it in order with less work in this case. we don't always have to go all the way back to the beginning. 4, exchange it with everybody to its left that's greater, until we find a smaller element, then it's in ascending order. 2 has to go all the way back to the beginning. but then the very next one, the 9 has to only go back one position, and the 6 has to go about halfway back. and then we have the entire array sorted. again, we can look at insertion sort in terms of invariants. our pointer still scans from left to right, but now the elements to the left of the pointer, including it, are in order, but the elements to the right have not yet been seen at all. so we have to look at the code that's going to maintain that invariant as the pointer increments. move the pointer to the right, it's incremented again. now the invariant's broken because the element on the pointer is not in sorted order. to put it in sorted order, we have to move from right to left, exchanging it with every larger elements to its left, and that's what the code at the bottom does. it starts j at i, and decrements j, exchanging j with the elements to its left, a of j with the element to its left, a of j-1, as long as a of j is less than a of j-1 or j is bigger than 0. and that immediately gives this code for insertion sort, which is similar to our code for selection sort and just as simple. it's got two nested for loops, selection sort had two nested for loops, a test, a comparison, and an exchange inside the for loop. and that's a fine implementation of an elementary sorting method. what about the analysis of insertion sort? it's more complicated. our proposition says that insertion sort, to sort randomly ordered array with distinct keys, it'll use about one quarter n squared compares, and about the same number, one quarter n squared exchanges, on the average. this is more complicated to prove. it depends on the array being randomly ordered. and again, you can get a feeling for where the proposition comes from by looking at this n by n trace. again, the black elements are the ones that we compare, and actually, they're also the exchanges. on the red one is the one that's finally put into place. and you can see that for a large array that's randomly ordered, the element that we put into place is going to go about halfway back on the average. so that means about half the elements below the diagonal are going to be black on the average. there's n squared over 2 below the diagonal, half of that is n squared over 4. the exact analysis is not much more detailed than that. this is a bigger trace that shows, again, about half the elements below the diagonal are involved in the sort. [cough] let's look at an animation. since n squared over 4 versus n squared over 2, insertion sort's going to be about twice as fast as selection sort. so we can do about twice as many items in the trace in the same amount of time. it grabs an element and brings it back into position every time. so that's an animation for randomly ordered items. now, insertion sort does depend on the initial order of the data. let's look at the best case and the worst case, which are certainly outliers. if the array happens to be already sorted, all insertion sort does is really validate that each element has got smaller elements to its left. so it does no exchanges. it gets the sorting job done with just n minus 1 compares. on the other hand, if the array is in descending order and has no duplicates, then every element goes all the way back. it makes n squared over 2 compares and n squared over 2 exchanges. so in the first case, it's much, much faster than selection sort, linear instead of quadratic. in the second case, it's slower than selection sort, because it uses about the same number of compares, but it uses many more exchanges. so let's see that in the animation. so this is when the items come in in reverse order. now, every time it gets a new item, it has to exchange it all the way back to the beginning. same kind of dynamic characteristic as selection sort, except, for every step, it's not just comparing, it's also exchanging, which makes it even slower in practice. so this is a bad case that we wouldn't like to see in a practical application. but there's also a good case that actually we take advantage of in plenty of practical applications. and that has to do with when the array is partially sorted. to talk about this in a quantitative way, we define what's called an inversion. an inversion is just a pair of keys that are out of order in the array. so this array has six inversions, t and r are out of order, because r should go before t. t and p are out of order, and so forth. this array has six inversions. and we define an array to be partially sorted if its number of inversions is linear, if it's less than some constant times n. and partially sorted arrays appear often in practice. for example, if you have a large array with just a few, that's sorted except for just a few unsorted elements appended at the end, it's going to be partially sorted. or in other cases, if you only have a few entries out of place, the array's going to be partially sorted. these types of things arise often in practical applications. and what's interesting about insertion sort is that it runs in linear time for partially sorted arrays. and the proof is, the number of comparisons and the number of exchanges is equal to the number of exchanges equal to the number of inversions, and there's an extra compare for every element except the first. so let's look at how that looks in the animation. here's a partially sorted array, and you can see that insertion sort quickly gets the job done. we're going to take advantage of this a little bit later in this lecture. that's insertion sort, our second elementary sorting method. 
now, we'll look at shellsort which is a bit elementary on the face of it but it's not at all elementary as you'll see. the idea of shellsort is that insertion sort is inefficient because elements really move only one position at the time even when we're kind of know that they have a long way to go. the idea behind shellsort is that we'll move entries several positions at a time and the way we're going to do it, it's called h-sorting the array. so, an h-sorted array is h different inter leaves sorted sub-sequences so in this case with h=4 if we start at l and look at every fourth element - m, p, t - then it's sorted. if we start in the second place at e and look at every fourth element, it's sorted. so this is 4 interleave sequences, that's a 4-sorted array. and what we're going to do is implement a sorting method that h-sort for decreasing sequences of values of h. this is one of the oldest sorting methods invented by shell in 1959. so, in this case, it starts out with the input example shown and then the 13-sort - a few items are moved, 4-sort - a few more are moved, and then finally, a 1-sort. and the idea is that each of the sorts can be implemented with only a few exchanges given that the previous ones happened. so first thing is how do we get an array h-sorted? that's actually pretty easy. we just use insertion sort but instead of going one back every time we come with a new item, we go h back. so for example when we come to this a in the insertion sort, then it's, we look at the array before that and then there was m and e in the positions three back so we exchange the a with the larger one to its left, that's m and then the other larger one to its left, that's e and then put it into position. so the code is the same as insertion, as for insertion sort, except that when we go backwards through the array we skip by h instead of just by one. that's how we h-sort an array. and the idea is we're going to use insertion sort because of two reasons based on our understanding of how insertion sort works. while the first thing is if the increments are big then the size of the sub arrays that we're sorting are pretty small so any sorting method including insertion sort is going to work well. but the other thing is if the increments are small because we've done previous h-sorts for bigger values of h, the array is partially sorted and so insertions sort is going to be fast. you wouldn't work to use shellsort as the basis for h-sorting because that always takes quadratic time no matter what order there is in the array. so let's look at example of shellsort with increment 7, 3, and 1. so, we start with this sort example and then 7-sorting it - just involves doing insertion sort but just reaching back 7 each time. in this case, the 4 subfiles stretched out at seven each only have two elements in them. and then we 3-sort. now, because it's 7-sorted and a 3-sort elements are either already in placed or on a go back a few strides. on this case, it's only the a that goes back two. and then we 1-sort and again because of the fact that it's been 7-sorted and 3-sorted, the arrays are almost in order when it comes time to do the 1-sort and most of the items only go back one or two positions. so we have to do a few extra passes to do the higher sorts but the each element moves only a little bit on each path and that's how shellsort gains its efficiency. so actually once you 1-sort, that's insertion sort so you're going to always get a sorted result. the only difference is how efficient is that. now the intuition behind shellsort and actually the mathematical fact is that if you've got an array that's h-sorted and then you k-sort it for another value k different from h, it's still h-sorted. this is one of those mathematical facts that seems obvious but then if you try to prove that maybe it's a little more subtle than you think. so, if you think of all this is, is, is trivial and easy, go ahead and try to write down a proof that a g-sorted array remains g-sorted even after it's h-sorted. but most people will accept that and it's a fact and that's how shellsort gains efficiency. now there's another problem is what increment sequence should we use for shellsort. one of the first things you might think of is let's try powers of two. actually that one doesn't work at all, very well at all because it winds up not comparing elements in even positions with elements in the odd positions until the 1-sort which means performance can be bad. shell's original idea is to try powers to two minus one and that works okay. knuth when he wrote his books in the 60s proposed the increment sequence 3x + 1. we'll start with the 1, 4, 13, 40, 121, 364 like that and that's good because it's easy to compute. when we're using in shellsort of course, we find the largest increment less than our file size and then do the sorts for decreasing values of that increment. but finding the best increment sequence is a research problem that has confounded people for quite a long time. here's an increment sequence that i found after maybe a year's work and it works well but nobody knows if that's the best one. so here's the implementation in java of shellsort for knuth's 3x + 1 increment sequence. we'll just go ahead and compute the increments that are less than n, n / 3 and then starting at that increment whatever it is and say, we started 364 then next time we need an increment, we'll just divide it by 3, 364 integer divide by 3, 364 integer / 3 it gets 121, 40 and so forth. so, this h = h / 3 gets us to the next increment. and then, the implementation is just insertion sort. we just go through starting at h for i and when we do the insertion, the j loop, we decrement j by h each time, otherwise the code is exactly like insertion sort. so, just adding this extra loop for h-sorting and this extra loop to compute the increments to insertion sort, we get a slightly more complicated piece of code but its much, much more efficient. here's what it looks like for a bigger array. we start with the randomly ordered input and you can see that it gets more and more in order on each time that we h-sort for the decreasing values of h. here's an animation. this animation does the whole h-sort for each subarray. it's a little better feeling for what's going on. and now to do the high ones pretty quickly and now it's doing the 1-sort and again it steps through the array pretty quickly. if it's partially sorted it doesn't make much difference - does the higher sorts a little bit faster. but that's simple to implement and very efficient sorting algorithm. now, the analysis of shellsort is still open. now, there's a few things that we can say. for example we can say that the number of comparison and the worst case is o(n3/2) for the 3x + 1 increments. but actually in practice it's much less than that. the problem is nobody knows an accurate model for describing the number of compares taken by shellsort for any interesting increment sequence. this seems to be with a small value, multiple of n times the number of increments used which is some multiple maybe of n log n but nobody is been able to find an accurate model that proves that for any interesting increment sequence for shellsort. so, why we are interested in this algorithm? well, it's a simple idea that leads to substantial performance gains. it's very useful in practice because it's pretty fast except for very huge arrays. it's going to beat even the classical sophisticated methods for medium sized arrays. and it doesn't take much code. it's often used in embedded systems or in hardware sort type systems because there's so little code involved to implement it. and it just leads to a lot of interesting questions. this gets to the intellectual challenge of developing algorithms. if you think what we've been studying so far is trivial, go ahead and find a better increment sequence. try some technique to discover one and try to say something about the average-case performance of shellsort. people have been trying to do that for 50 years without a whole lot of success. so, the lesson is that we can develop good algorithms or good implementations without much code but there are some out there that are still waiting discovery. it could be that there are some increment sequence out there that make shellsort more efficient than any other method, any of the sorting method that we know for pratical file size, no one can deny that. that's shellsort or first non-trivial sorting method. 
next we're going to look at an easy application of sorting to related problem called shuffling. so, suppose you have a deck of cards. one of the things that you might want to try to do is to simply rearrange those cards into random order, that's called shuffling. here's a way to get shuffling done using a sort and seems like the opposite. the idea is, just generate a random real number for every array entry and then sort using those random numbers as the keys. that's an effective way to get things shuffled. and it's possible to prove that, that produces a uniformly random permutation of the input if there's no duplicate values, assuming that you have a real numbers that are generated uniformly at random. and that's just means that it's well shuffled that every possible way of shuffling the deck appears with the equal probability. that's fine but it requires a sort and a sort seems like a lot of work for this problem and the question is, can we do better? can we have a faster way to shuffle? do we really need to pay the cost of a full sort? the answer to that question is, no. there's actually a very easy way to rearrange an array so that the result is a uniformly random permutation. it only require a linear time to get the job done. let's look at the demo. the idea this to pass through the array from left to right with an index i as we've been doing but now we start with the array in order. and actually, it doesn't matter how we start the array and every time we pick an integer between 0 and i uniformly at random and, and swap a[i] with that integer. so, let's look at the beginning, we don't do anything just swap it with itself. now, with i = 2 or i pointing to the second card we generate a random integer in between 0 and i, in this case it's the one to the left and we swap those. increment i, generate a random integer, this time it's going to be the first one again, swap them. increment i, generate a random integer, swap them. increment i, generate a random integer, swap them. and continue in that way. swap. so for every i, we do exactly one swap. now, card could be involved in more than one swap but that's not an issue. the point is that the cards to the left of i are shuffled there uniform, randomly shuffled. on this case, i and r are the same. there's no swap. increment i, generate a random r, swap them. and at the end we have the deck shuffled. that's a linear time shuffling algorithm making use of randomness. it was proved through actually a long time ago even before computer implementations that if you do that, you get a uniformly random permutation and it only takes linear time. so, that's definitely a way to get a deck shuffled quite easily. easy to implement. now it's key that the uniform random number will be between 0 and i-1. you'll often see programmers thinking that they're implementing a shuffle and they just choose for every entry, they just choose random place in the array to exchange it with and that doesn't really work. you could do the items between i and n-1, the ones that you haven't seen yet and that would also work but doing a whole array doesn't give you a uniformly random result. so, with that one caveat, this code is almost trivial. and it's a method in our standard random class. now if you're going to be using random methods that depend on randomness in real applications, you do have to be careful. so this is just an example about software security. there's a lot of difficult and deep issues to worry about in software security and we're not going to worry about all of them. but one thing that we can do is make sure that our algorithms work as advertised. so, here's an example of an implementation for online poker. here's the code that you can find on the web for how to shuffle a deck of cards and that's pretty similar to our code but it's actually got a few bugs, more than a few bugs. so first one is the way that random works it's actually never gets to 52 which means that the last card just stays it can end up in the last place. so, it's definitely not shuffled because of that. maybe that one's minor but it also is picking a random card from the whole deck as we just pointed out that's not uniform. should be between 1 and i or between i+1 and 52. another problem is in this implementation that the random uses just a 32 bit seed that if you do that, there's not enough possible shuffles. the number of possible shuffles is, is, is much more, if n, if n is 52, it's 52 factorial which is a lot bigger than two to the 32nd. so, it's not close to a random or uniform. and the other thing is that, the seed is just a number of milliseconds since midnight and that cuts down the number of shuffles even more. and in fact, it didn't take that much hacking for someone to realize that after seeing five cards and figuring out what the server clock was doing, you could get all the future cards in a real time in a program. and that's a pretty tough thing to have happen if you're implementing online poker. you might want to make sure that if you're advertising that you're doing a random shuffle, that you go ahead and do so. and the famous quote in this many similar quotes, the generation of random numbers is too important to be left to chance. so, if your business does depend on shuffling people have looked at all sorts of options including using hardware random number generators and these various tests available to make sure that it's random and you'd better use good shuffling code that's our topic but the bottom line is don't think that it's easy to shuffle a deck of cards. so that's shuffling - our first non-trivial sorting application. 
>> now we'll look at an application of sorting from the field of computational geometry for an interesting computation. if you have a set of n points in a plane. there's a geometric object called the convex hull which is the smallest polygon that encloses all the points. there's the convex hull for that set of points. [cough] there's a lot of equivalent definitions of this. some of them very mathematical, that extend the higher dimensions. it's the smallest convex set that contain all the points, the smallest area of convex polygon enclosing the points. it's a convex polygon that encloses the points whose vertices points in the set and those are all equivalent definitions. and what we want to do is given the set of points, we're going to have a program that can give us the convex hull. now, which should the output of such a program, such a method be? well, in order to be able to work with the result, it should be a sequence of vertices that gives us that polygon if we follow it. if we've got some points that are on the boundary but aren't really vertices they shouldn't be included. this points out examples of how difficult computational geometry can sometimes be because degenerate cases like these are difficult to deal with in code. we're not going to spend a lot of time on this, in this lecture. but it's something always to be aware of when trying to [cough] apply simple algorithms in situations like these that turn out to be maybe more sophisticated than we might think. >> [inaudible] the large screen. >> oh yeah, got you. mm-hm. well, there's actually a way to compute the convex hull just mechanically if you put the nails around the points and put a rubber band around it, that gives you the convex hull. now, we're not going to be able to really implement that in a computer program but it's surprising how well we can do. here's an application where people want to compute the convex hull. suppose you have a robot that wants to get from s to t and there's an obstacle that's defined by some polygon. you wanted be able to go around the obstacle and it turns out that the shortest path, either it's a straight line from s to t or it's part of the convex hull and is not hard to see why that might be true. and there's plenty of other applications where people want to be able to compute the convex hull. here's another application. if you want to find the pair of points that are the farthest apart in the set of points in the plane, this is sometimes important in statistical calculation or other applications. they're on the convex hull. if you have the convex hull, this computation is easy. [cough] they're, they're going to be extreme points on the convex hull. so, there's a lot of geometric properties of the convex hull that we can take advantage of to develop an algorithm. in here two properties. now, these are the things that have to be proven and we're not going to get into the details of geometric proof but they're intuitive and certainly have no trouble accepting that these things are true. one thing is, that you can traverse the convex hull by making only counter clockwise turns or left turns if you're looking at the screen here. and the other thing is that, so if we travel from p to point 1 then we make a left turn to go to point 5 or counterclockwise turn and then from there, we go to point 9 and 12 and then we eventually get back to the start point. the other thing is, if you take the point with the lowest y coordinate. and then if you look at the polar angle with respect for every other point with the respect to that one, so the angle you get from of the x-axis through p up to the point, then the vertices appear in increasing order of that angle. and again, that's not, not difficult to see that that's a fact. and the algorithm that we're going to look at, called the graham scan is based on those two facts. it's, the idea is to start with point p, the one with the smallest y coordinate. sort the points by polar angle with p where that is we're just going to consider in that order. and then we'll just throw away the ones that do not create a counterclockwise turn and you'll see how that works when we look at the demo. so we start at point p. sort the points by polar angle with p so that is if we take a, a vertical line and sweep it in a counterclockwise direction, what order that we hit the points? the first thing we hit is 0, 1, and then we sweep counterclockwise, we get the 2 and then 3 and 4 and so forth. so, that's the ordering of those points. and so now we'll just consider those points in order and then take them for the convex hull. at the beginning, 0->1 is a line that's on the convex hull. so, the point with the lowest y coordinates on the convex hull and shows the one that is the smallest polar angle that creates with the x-axis. so now what about this one - 2? is that on the convex hull? well, as far as we know at this point, it could be, it could be that the thing is a triangle and 0 is the last point in which case it would be. but in same with 3. as far as we know, that one could be on the convex hull. but as soon as we go out to 4 that's not a counterclockwise turn. it's going the wrong way and essentially what this means is a point 4 is evidence that point, there is no way the point 3 can be on the convex hull. you can [cough] convince yourself with that quite easily. so we just throw a point 3 out. it's not on the convex hull so, and what about the angle from 1 to 2 to 4? that's not counterclockwise either. it's turning the wrong way and it's turning to the right. so point 2 can't be on the convex hull either. and indeed if you just draw the line from 1 to 4, you can see the 2 inside so there is no way it could be in the convex hull. now that's essentially the proof that you have to have a counterclockwise turn. so now, we go on to 5 - turning the wrong way. so, point 4 can't be on the convex hull. so now we go to 6. as far as we know, it could be, but as soon as we hit 7, we know that it can't be cuz that's a right turn. so 6 is not there. go to 8, nope. 7 can't be on the convex hull. go to 9. 8 can't be on the convex hull. now we go to 10 and 11. as far as we know they could be. if 12 weren't there, they would be. as soon as we hit 12 we see that 11 can't be on the convex hull and 10 can't be on the convex hull and that completes the computation of the convex hull with the graham scan. okay. so, there are number of implementation challenges for the graham scan and we're not going to go into detail on this because this is a lecture on sorting algorithms not computational geometry but it is indicative of how, even if we have a good sort, we might have to do some extra work to actually solve our problem in an application. so, how do we find the point with the smallest y coordinate? well you could, you could sort, you could define an order and compare the points by y coordinate so essentially sorting is the [cough] answer to that question. and we'll look at the next lecture of what it means the divine ordering among objects, little more general than what we do for sorting. how to sort the points by polar angle? well again we need to define what we mean when we're comparing points. and then the next lecture again we'll look at ways to define different orderings among points and graham scan is a perfect example. we don't want to just be able to sort things, we don't want to just be able to sort them by defining and compared to. we're going to be able to sort the same things in different way sometimes and this example is a fine motivation of that. figuring out whether what we have is a counter clockwise turn that's a little exercise in geometry and we'll just talk about that briefly in the next couple of slides. and then wow, what are we getting the sort efficient, done efficiently? well, we could use shellsort but actually in the next couple of lectures and we'll look at classical sorts - mergesort and quicksort - that we could use. the idea though is that this example illustrates that good sorting algorithm gives us a good convex hull algorithm. that's an extremely important principle in designing good algorithms. once we have a good algorithm, if we have another problem we can say to ourselves, well, we've got a good solution to this algorithm, can we use that solution to solve our new problem? convex hull, when we have a good sorting algorithm, it gives us a good convex hull algorithm. because the main, the most work in convex hull is the sort. and then again there's all, all kinds of difficulties in implementing convex hull in real world situations because of various degeneracies. and these things are covered on the book site. so the main part of computation that we haven't really talked about and we'll cover briefly is if we have three points, a, b and c, and you go from a to b to c, are you making a counterclockwise turn or not? so, in the example at the left, a to b to c is counterclockwise. example at the right, a to b to c is not counter clockwise. going from a to b you turn left to get to c in the first case and you go right to get to c in the second case and we want to do a computation that distinguishes this. now, this computation will be pretty easy except for the degeneracies. what do you want to count if they're all on the same line. or if the slope is infinity. so, you have to just be aware that these situations have to be dealt with. so, the code isn't quite as simple as you might come up within the first instance that you try. so, there's degeneracies to deal with and floating point precision but people, researchers in computational geometry have worked this out and actually there's not that much code at all in the end involved. the and this is the slide that, that gives the math and i won't talk through this math. if you're interested in implementing this, you can come back to the slide. and it's essentially based on the idea of computing the slopes of the lines between a and b, between a and c and comparing them to decide whether you're turning counter clockwise or clockwise. and this is the specific math that gets that implemented. so [cough] this is if we implement a point data type for computational geometry, you can have a method ccw() that just with this little math calculation (b.x - a.x)(c.y - a.y) minus (b.y - a.y)(c.x - a.x) and we see that calculation here gives you immediately whether it's counter clockwise, clockwise or co-linear. not much code at all. and that method is the basis for the graham scan. the graham scan uses a sort where we give two different ways to sort the points. and that uses a push down stack for the hull, it puts the points on the hull in it goes ahead and for every point considering i'm in the order of the polar sort it'll compare whether the top two points on the hull and the new point implement a ccw turn or not. and if it's not a ccw turn, it pops and then continues going. very little code to implement the convex hull given that you have a sort and that's our main point for this lecture - there is many natural applications of sorting but also will be able to develop new algorithms that use sort that gain efficiency because of the efficiency of sorting. 
welcome back. today we're going to look at mergesort, which is one of two classic sorting algorithms that are critical components in the world's computational infrastructure. we have a full scientific understanding of the properties of these algorithms, and they've been developed as practical system sorts and application sorts that have been heavily used over the past 50 years. in fact quicksort, which we'll consider next time, was honored as one of the top 10 algorithms of the 20th century in science and engineering. on this lecture we're going to look in mergesort, which is the basic sort in plenty of different programming systems including java. next time we'll look at quicksort which is also used in java for different applications. alright, so basic mergesort algorithm. what's it going to look like? the idea is very simple. what we're going to do is divide an array into two halves. recursively, recursively sort each of the halves. and then merge the result. that's the over-view of mergesort. it was actually one of the first non trivial algorithms i implemented on a computer. john von norman realized that the development of the edvac, his edvac computer, one of the first general purpose computers that is going to need a sorting method and he came up with mergesort. he's widely accredited as being the inventor of mergesort. so the idea of mergesort is, is based on the idea of merging. and so to understand how merging works we'll think about the idea of an abstract in place merge. so, we've got an array a and its first half is sorted and its second half is sorted and the computation we need to perform is to replace that with the sorted array where those two sub-halves are merged together. let's look at a demo. [cough] the method that we're going to use is based on taking an auxiliary array to hold the data. this is a, one of the easiest ways to implement the merge. so the first thing we do is copy everything over to the auxiliary array. now, once that's done, what we'll want to do is copy back to the original array to get it in sorted order. in order to do that, we're going to maintain three indices. i, the current entry in the left half, j, the current entry on the right half and k, the current entry in the sorted result. [cough] so the first thing we do is, take the smaller of the two entries pointed to by i and j, and compare those, and take the smallest one, and move that one to be the next item output. and whichever one is taken, we increment its pointer. now we compare the minimum again, again, the one pointed group by j is smaller, so we move that one to k. increment that pointer j and also increment k. now there's two e's, equal we always take the first. so the one on the left array goes to k's position. and now we increment i and k. and again, it's an e and they're equal. we'll take the first one so we move that one up increment i and k. and now j's e is smaller than g. it's the next thing that has to go in the output. so we move that one up and increment j and k. now the one pointed to my i, the g is smallest so move that and increment i and k. move the m up and increment i and k. now the last element in the left sub array is the one that's going to get moved next. and now that first subarray is exhausted so really all we need to do is take the rest of the elements from the right part and move them back in. actually since we copied, we could optimize by avoiding these moves. that's an abstract in-place merge for taking the two sorted sub-halves of an array using an auxiliary array, move them out, and then put them back in in sorted order. alright, so here's the code for merging, which is quite straightforward from the demo. we first in order to sort an array of comparables in this implementation we pass a link to the auxiliary array, in as well. and we have three arguments lo, mid, and hi. so lo is the first part of the array to be sorted. mid's the midpoint that divides the first part from the second, so our conditions are that from lo to mid is sorted, and from mid plus 1 to hi is sorted. [cough] so the merge implementation then, the first thing it does is copy everything over to the auxiliary array. and then that sets up for this four loop that accomplishes the merge. we start our i pointer at the left heart on the left half. the j pointer on the left part of the right half. that's mid plus one. and we start the k pointer at the beginning lo. and for every value of k what we're most often doing is comparing whether aux of j is less than aux of i. and if it is, we move the element of j over in increment j. if it's greater we move the element i over in increment i. and then in both cases, we increment a, not imple, increment k, and that implements the merge. if the i pointer is exhausted, then we just move over the j, next jth element. if the j pointer is exhausted we move over the next ith element. so every time we're moving a new element into k and that's the code that impelements the abstract in place merge. now with this code, we're also introducing the idea of making assertions just to make it easier to debug our code and to have confidence that it's correct. in this case, this insertion just says we want to be sure that a of lo to mid assorted and that mid plus one to high is sorted before our code and then we want to check that, the whole thing is sorted after our code. and generally programmers, java programmers know that it's a good idea to try to do these assertions. not only does it help detect bugs, but it also documents what the code is supposed to do. and that merge code is a good example of this. if you put at the beginning of the code what you expect in the, in the form of an assertion, which is code itself. and you put at the end of the code what you think it's going to do, again in the form of an assertion. you're both testing that these conditions hold, and also telling someone reading the code, what you're trying to do with it. so java is just an assert statement. it takes it, boolean condition. in this case, we're using that method is sorted that we were before. that returns true if the ported is sorted and false if it's not. and what assert will do is it will throw an exception unless that condition is true. now the thing about assertions in java is that you can enable or disable them at runtime. and that's really important, because it means you can put them into your code to check while developing. but it doesn't incur any extra cost at all in production code. so by default, insertions are disabled. something goes wrong somebody analyzing the situation can enable insertions and they often will help find out where, what the problem is. so, the best practice is to use insertions just as we did in that example with merge and to assume that they're not going to be there in production codes. you shouldn't use them for the things like checking if the input is the way you like it. alright, so with that merge implementation, then the sort implementation is a quite simple, recursive procedure shown here. so we use the merge procedure we just showed, and then our sort procedure. it's recursive so, checks that we have something to do first. then it computes the value of the midpoint same way as we did for a binary search. sort the first half. sort the second half, and then merge them together. and then the actual sort is takes just the one argument of the array creates the auxiliary array and then uses that. now, it's important to not create the auxiliary array in the re in the recursive routine because that could lead to extensive cost of extra array creation. and you'll sometimes see mergesort performing poorly because of that bug. otherwise this is a very straight forward implementation. and it's actually a prototype for algorithm design that we'll see come up again and again. it's called divide and conquer. solve a problem by dividing it into two halves, solving the two halves, and then putting the solutions together to get the appropriate answer. [cough] here's a trace of what mergesort does and if you haven't studied a recursive program before it's worthwhile studying this thing in, in some detail. this gives exactly what happens during each of the calls to merge. we start out with a big problem to solve but we divide it in half, then we divide that one in half, and then we divide that one in half. and the very first thing that we actually do is just compare and exchange if necessary the first two elements. and then we do the same thing for the next two elements. then merge those two together to get the first four done. and then we do the same thing for the next four in the array. so now we have two sorted sub-arrays at size four. and we merge those together to get one of size eight. and then we do the same thing on the right, and eventually we have two eights that we merge together to get the final result. very instructive to study this trace to really understand what this recursive algorithm is doing. so now we can animate and again mergesort's more efficient, so we can do more and more items. you can see it's got the first half sorted, now it's working on the second half. and then once it gets the second half sorted, then it's going to go ahead and merge them right together to get the sorted result. it's got a little extra [cough] dynamics in the animation because of the auxiliary array. let's look at it when it's in reverse order again it gets the first half done now it's working on the second half once it gets the second half done then it goes ahead and merges together the whole thing it's just as fast in reverse order as as in auditory order. so you can run a mergesort on huge problems. it's a very efficient algorithm. and so, for example, what this table shows, if you were to try to use a insertion sort for a huge file, say a file with a billion elements, on your pc it'd take a few centuries to finish. even on a super computer, if you're using insertion sort nowadays it'd maybe take a week or more. but if you have a good algorithm like mergesort, and you're trying to do a billion items, you can do it in just less than half an hour on your pc. and a supercomputer can do it in an instant. and smaller problems only take an instant even on your pc. so you can spend a lot of money or a lot of time, or you can use a good algorithm. and that's one of our main themes in this course. a good algorithm is much more effective than spending money or time wasting money or time using a bad one. so let's look at the analysis of mergesort, that's a bit of math but very instructive because this really shows the power of the divide and conquer method. and allow us to take a problem that was taking us quadratic time with methods like insertion and selection sort, and get it done in, in log n time with mergesort. so that's the proposition mergesort uses at most n lg n compares and 6 n lg n array accesses to sort any array of size n. and the way to prove this proposition is to from examining the code, to write down what's called a recurrence relation. and all that is, it's a mathematical reflection of what's going on in the code. if we're sorting n items then let c of n denote the number of compares that we need to sort the n items. in order to get that done, we're sorting the left half and the right half and this notation ceiling of n over 2 and floor of n over 2 that's the n over 2 round up and n over 2 round down, that's the size of the two sub-arrays, and we're going to call the same routine for that size, so the number of compares you need to. for that is c of n over 2, ceiling of n over 2 for the left and ceiling of, floor of n over 2 for the right. and then for the merge, we need at least, at most n compares. if neither one exhausts, we need exactly n compares. and so and that's true as long as n is bigger than 1. if there's only one thing, we're not doing any compares at all. so this is a mathematical formula that we derive by examining the code but it completely describes mathematically what we an upper bound on the number of compares that are going to be needed. and similarly for the number of array accesses, if you count up the number of times you're accessing an array for a merge you could be at most six in. so these are mathematical formulas and there's techniques for solving them and we won't go into that. this is not a course on discrete mathematics. but what we then do is show how to solve the recurrence when n is a power of 2. and then it turns out that it holds for all n, which we can prove by induction from the recurrence. so if you have this recurrence [cough] which is similar to the ones that we're talking about. it's exactly the same when n is a power of 2 let's, let's look at this one. if d of n is 2d of n over 2 plus n with d of 1 equals 0, then d of n equals n log n. we'll look at three proofs of that, just assuming that n is a power of 2. if n is a power of 2, then n over 2 is also a power of two, so the recurrence makes sense. so this is just a graphical representation if we want to compute d of n we want to compute d of n over 2 twice. so that's 2 and then the extra cost for the merge is n, but if we're going to do this twice then we have 2n over 2. so let's, we have 2n over 2s and then for each one of these we have divided into n over 4s and each one of those 4n over 4s has an extra cross for the merge of n over 4. well 2n over 2 is n, 4n over 4 is n and we keep going down, doing that til we get down to d of 2 and we always for the extra cross for the merge, we have n. and how many stages do we have here? well, it's the number of times you divide n by 2 to get down to 2. that's exactly log base 2 of n, so the grand total of all the costs for the merge, which is where the compares are, is log n times n, n log n. it's kind of a graphical proof or a proof by picture that that recurrence has that solution. here's a little bit more mathematical one: we write the recurrence down, and then we divide both sides by n. so then this is d of n over n equals d of n over 2 over n over 2 plus 1. so it's dividing by n. so now, this is a recurrence that telescopes. the first term on the right hand side is exactly the same as the left hand side so we can apply the same formula. and all it does is divides by 2 again and then throws out another 1. and we keep doing that until we get down to d of 1 which is 0. and when we've done that, we've thrown out lg n 1s. so we get d of n over n equals log n, or d of n equals n log n. that's another proof by expansion. or using either one of those techniques you could just get the idea that d of n is close to log n or you can write a program to expand the recurrence and find that. and then once we have the idea that d of n equals n lg n, we can plug back into the original formula. with the inductive hypothesis that d of n equals n lg n, we want to show that d of 2n equals 2n lg 2n, using the recurrence d of 2n equals 2d of n plus throw out the 2n. plugging in n log n we get the desired result. we use this same idea on our initial recurrences for comparison array accesses to show that the running, the number of comparison array accesses is proportional to n log n for mergesort. so that's the running time mergesort is fast other thing that we usually want to know is memory. and one of mergesort's characteristics is that in practical applications, it uses extra space proportional to n. that is, we need that extra auxiliary array for the last merge. we took two sorted subarrays and we talked about an abstract in place merge but we didn't have an actual in place merge. we were using an extra subarray. so n place is important. a lot of times, we're sorting everything we have. we want to fill up the memory with stuff to sort and then sort it. and search and selection in shellsort are in place, they don't use any extra memory. but mergesort you can only sort really half of what you can fit in memory, because you need that auxiliary array for the other half. if you want, again, if you think that the things we're studying are easy, think about the idea of actually doing an in-place merge. people have come up with methods for getting this done. so it's theoretically possible, but the methods are generally too complex to be useful in practice and their not used. but there could be out there some easy way to doing in place merge. that's another great algorithm waiting to be discovered. now there's a, a number of practical improvements that we can use to make mergesort even more efficient than the simple one that we've looked at and we'll take a look of those because they're examples of techniques that we can use for other algorithms. first thing is that mergesort is too complicated to use for tiny arrays. so say the subarrays are only of two, or three, or four there's too much overhead with the recursive calls and so forth to get that done efficiently. and what's worse is, the recursive nature of the sort definitely means that there's going to be lots of subarrays to be sorted. so, one improvement that we can make is to use insertion sort, and just cut off and use insertion sort which is simple and efficient for small subarrays. so that's adding this one line of code to mergesort will make it quite a bit faster. maybe 20% faster. the second improvement that we can make that'll improve the performance for cases when the array is partly sorted, is to just stop if it's already sorted. and that's going to happen in the case where the biggest element in the first half is less or equal to the smallest item in the second half. that means it's done. so that's easy. we just put a test in the recursive mergesort for that, through this one line of code, to check whether we're done. that way, for example, if you were to call mergesort for an array that's already in order it would just do this test every time and it would be done in linear time. that's pretty helpful although not, not totally helpful but there's a lot of situations where that's helpful. the other thing that's possible to do and it's a little mind bending so recommended only for experts. is to save a little bit of time you don't really have to copy over into the auxiliary array. you can kind of switch the role of the input and the auxiliary array every time you make a recursive call. you still need that array but you can set up the code in this way which [cough] sort, to sort an array, put the result in the other one. to merge an array, put the result back in the first one. so it's recursive argument switchery to get the job done. and it's effective, it means you don't have to actually move items, and that saves a little bit of time. so here's a visualization of what the practical mergesort might look like, and this is with big cutoff to small subfiles. so you got a visual feeling of how this sort gets the job done. so it's the first little chunck and then the next little chunk and then merges those together, and so forth and so on. it's a good visual representation of how mergesort gets its job done. that's the basic mergesort algorithm that we're going to look at different versions of in the next. 
next, we're going to look at a bottom-up version of mergesort. well, mergesort is easy to understand as a recursive program. this bottom-up version that has no recursion, it's also quite simple to understand and to code up. the basic idea is to think of the array as being a little at the begining a set of little sorted sub arrays of size one. and then what this method will do is go through and merge those little subarrays of size one together in pairs to get subarrays of size two. then, the whole array consists of sorted subarrays to size two, and then we make another pass through to get size four, and then size eight, and so forth. so, as you can see in this example we start out by merging the first two sub arrays of size one to make a array of size two - e, m - that's sorted, and then do the same thing for the next two elements and the next two and so forth until eventually instead of sixteen individual elements we have eight sorted subarrays of size two. then on another pass through, we can take the e, m and the g, r and merge them together to make egmr, and the e, s and the o, r merge those together to make eors, and so forth. and we have four subarrays of size four. one more pass makes two subarrays of size eight, and the last pass is just a sorted array. the bottom line in this is sequence of passes through the whole array and there's no recursion needed at all. it's extremely easy to code up as you can see from this code. we use the same merge code as before and we take a nested for loop. the first one is the size of the subarray and this loop gets executed on a log n times because each time we double the size of the subarray until we get to n. and then we pass through picking out from low to low+size-1, and then the next part is low+size+size-1 until we run to the end of the array where we might not have a full subarray of size sz. that is a fully complete industrial strength code for sorting. the only downsize as would regular mergesort is that it uses extra space proportional to the size of the array. but otherwise, that's a fine method for merging. that's a bottom-up mergesort. if you look at this visual trace you can see how it works. the thing is totally unsorted, then it gets sorted until subarrays to size four, then eight, sixteen, and 32. now in this case the second subarray to be sorted is smaller but the merge routine doesn't really care about that so much. you can merge things that are not equal in size. and then we get a final sorted array. whatever the size, bottom of mergesort gets the job done in log n passes. each pass using about n compares for a total cost of about n log n. 
with mergesort is a good opportunity to take a look at the intrinsic difficulty in the sorting problem, now that is called complexiting and we'll look at that next. the idea of complexity is it's a frame work for studying the efficiency of all the algorithms for solving a particular problem. that's called computational complexity. and in order to do this sensibly, we need what's called a model of computation. the operations that the algorithms are allowed to perform. for sorting that's kind of straight forward, what we're going to do is have a cost model where we count the comparisons. now in framing of the difficulty of problems were only two things. one is an, what's called an upper bound which is a cost guarantee that's provided by some algorithm for solving the problem. that's an upper bound and how difficult it is to solve the problem. we have an algorithm that can solve it it's the least that easy. and then we also look for a lower bound which is a limit on the cost guarantee of all algorithms. no algorithm can do better. now, what we seek ideally is what's called an optimal algorithm where we prove that the upper bound and the lower bound are the same. that's an algorithm that's, that we know that has the best possible cost guarantee. that's the idea for solving any problem. so, for sorting, let's look at what each of these are. the model of computation is what's called a decision tree, tree. and what that mans is that all we can use is compare, that's the only way we can access the data. so, our cost model is the number compares. mergesort provides, provides an upper bound, that's an algorithm that's guaranteed to get the sort done in time proportional to n log n. and what we'll look at now is the lower bound. there's a trivial lower bound which says you have to look at all the data, that's n and we'll look at a better lower bound and see that mergesort is optimal. so, here's the basic idea for proving a lower bound for sorting. let's say, we ha ve three different items, a, b and c. whatever algorithm we have is going to, first, do a comparison between two of the items. let's say, there a and b. and then there's two cases. either it's yes or it's not yes, let's, let's say, they're distinct. and there will be some code between the compares but either way then there is going to be a different compare. if it's less than b, maybe the next compare is b against c. and if you find that b is less than c and a is less than b, then you know that they're in the, any algorithm that does that knows that the items are in the order a, b, c. if b less than c goes the other way, then it takes another comparison to determine the order. in this case, if c is less than b and a is less than c then those three compares show that the order has to be a, c, b and if c is less than a, then it's going to be c, a, b, those three compares that c is less than a, c less than b and a is less than b. the only possibility is c, a, b. in continuing on the right perhaps the next compare is a less than c and maybe if c is less than a, then another compare, b less than c. so, in this case, if you go from top to bottom in the tree with three compares at most you can determine the ordering of the three different items. the idea of the lower bound generalizes this argument to figure out a number of compares that you need for a minimum to determine the ordering among n items. now, the height of the tree, as i just mentioned, is the worst case number of compares. out of all the orderings the one that's further stand in the tree that's the worst case and so the algorithm, no matter what the input is, the tree tells us a bound, the number of compares taken by the algorithm. and there's got to be at least one leaf for each possible ordering. if there's some ordering that is not appear in a tree corresponding the particular algorithm then that algorithm hasn't can't sort, can't, can't tell the difference between two different orderings. so, the lower bound as a proposition, that uses the decision tree like that to prove that any compare base sorting algorithm has to use at least log base two (n) factorial compares in the worst case. and by stirling's approximation, we know that log base two(n) factorial is proportional to n log based 2n. and then the proof is generalizes what i talked about on the decision tree on the last side, slide. we assume that the array consist of n distinct values there's a position created that describes the performance of any algorithm to compare sequence done by any algorithm to determine the n factorial different orderings. so, this three has to have at least n factorial leaves and if the three of height h, it has utmost two^h leaves. the only, the, the tree that has the most leaves of height h is totally complete and that one has two^h leaves. and those observations give us the lower bound. two^h has to be greater than or equal to the number of leaves. and the number of leaves has to be greater or equal to n factorial so that implies the height of the tree has to be greater than or equal to log base two(n) factorial which is proportional to n log n by stirling's formula. that's a lower bound on the complexity of sorting. so, we knew that the upper bound was n log, proportional to n log n and we just proved that the lower bound is proportional to n log n and that means that mergesort is an optimal algorithm. that's the first goal of algorithm design is to try and find optimal algorithms for the problems that we need to solve. now, you have to take these results in context. really what we proved is that mergesort is optimal with respect to number of compares but we already know that it's not optimal with respect to space usage. mergesort uses twice as extra space proportional to the size of the array it has to sort. and simple algorithms like insertions or dump, they've they don't use any extra space at all. so , what we want to take from these theoretical results is, is a guide when we're looking at implementations and trying to solve practical problems. in this example what it tells us, what theory tells us is don't try to design a sorting algorithm that guarantees to use substantially for your compares than merge sort. say, one-half n log n compares. is there a method that use one-half n log n compares? the lower bound says, no. and that's a very useful thing because otherwise, we might try to define such an algorithm. on the other hand, maybe there is an algorithm that uses n log n compares and also uses optimal space. it's optimal with respect to both space and time. and that's what we're going to look at soon. the other thing is that the lower bound is for the particular model of computation being studied. in this case, compares. it might not hold if the algorithm has more information about the keys, for example, if it's known that the input is almost ordered, we saw that insertion sort can be linear time for files that are almost ordered. or it's something about the distribution of key values if there are a lot of equal keys we can get sorted, get it sorted faster than, n log n. and maybe the way the keys are represented. we'll look at different methods that take advantage of such properties. so, partially ordered arrays we may not need n log n compares. duplicate keys, we may not need n log n compares, we're going to look at the method that i guess that down in linear time and a lot of situations. and later on, we'll look at digital properties of keys where we can use digital character compares instead of whole key compares and got a faster sort for certain practical applications. computational complexity is very useful way to help us understand properties of algorithm and help guide our design decisions. 
next we'll take a look at comparators which is a java mechanism that helps us sort. the same data on different sort keys, different orders. and you're familiar with this. your music library maybe i, at one point, you sort it by the artist's name. in this case we're looking at the b's. but in another situation, you might want to sort it by song names to look through it by song names. that's the same data using different sort keys. how do we arrange to do something is natural as this in our java sorts? now, we use the fourth in order to be able to implement sorts that can sort any type of data, we use java's comparable interface. and that concept is that there's some natural ordering of the data that you'll want to use most of the time, that's what the comparable interface is all about. but there's a different interface called the comparator interface which is a way to help a sort, using some alternate order or many different orders on the same data. and the comparator interface again just says that it's going to implement a method compare() that compares two different keys of the given type, of the generic type. again it has to be a total order and this is very familiar for example with strings. there's many different ways that we might want to sort strings. we might want to use the natural alphabetic order or we might want to make it case insensitive or maybe there is just different languages that have different rules of the ordering. we're sorting strings but we're implementing a different ordering, various different orderings on that same data. that's what the comparator interface is for. so the java system sort will have a different. [cough] method to implement comparators. the idea is that you create a comparator object and then pass that as a second argument to java's sort routine and we can do the same thing for our sorts. the idea is when a decouple, the definition of the data type from the definition of what it means to compare to items of that type. with the natural order, we had to put the definition compared to within the data type. with comparators, we can do that outside of the data type even at some later time. strings were defined and as part of the java system but we can define our own ordering on strings with the comparator. so in our sort implementations we can change them as shown in this example to support comparators. to support comparators in our sort implementations we'll pass an array of objects and instead of an array of comparable and then, there's a second argument passed a comparator. then, the less method will take that comparator as an argument and this is the one that actually invokes the method compare two different keys. this is a straightforward modification to our sorts. and then exchange of course rather doing comparable has to use object. so with these straightforward changes at the comparator as argument to the sort and to less and make array to be sorted array of objects, it's easy to convert any of our implementations to support comparators. to implement a comparator you can use this code as a model. i won't go through it all in detail just to point out that this implements two different comparators as nested classes. say, for this fictional class student, that's got two instance variables - name and section. and the first one called by name implements a comparator for students and when you compare two students by name, it's going to use the string comparative method. if you're going to implement it compared to students by section, then it'll return just the difference of the sections which is my minus if less zero if equal then plus if greater. and this code is straight forward way to implement comparators that you can use as a model. if you need to be able to sort data on two different keys. so [cough] here is just an example of what happens if would those implemented comparators for that class student using the java system sort, if you call array that sort with your a rray of students and you give it this by name comparator, it will put them in order alphabetical order by the name field. and if you give it to by section comparator, it will them in order by the second field very convenient for all kinds of data processing applications. and we came up with that before when we're talking about using a sort for the graham scan. we needed to have a comparison for points that orders them by the polar angle they make, make with the given point p. that's what we needed for the graham scan algorithm for the convex hull. points are defined data type for geometric objects and so what we need is code that will compute the polar angle and use that as the basis for comparison. there's an easy way to do this based on ccw that is described here in this text. most of the time all you need to do is do the ccw of the two points. you either have to check whether [cough] the, one of the points is above p and the other one is below. but otherwise, usually it's a ccw call in this code which again i won't go through in detail as an implementation of a comparator for two d points. it implements the compare method that takes two points as argument and with just a little bit of calculation is able to do the compare. so this code is the basis for applying the sort, system sort method or any sort method for the graham scan for the convex hull that we did at the end of the last lecture. so that's the basis for the graham scan method for the convex hull that we used at the last, at the end of the last lecture. 
finally, we talk about stability. this is really one of the rules of the game but it's much easier to talk about in the context of the real algorithms that we've seen so far. and really it doesn't make sense if you don't know about comparators which we just introduced. so, the typical application that i just used as an example is say the set of student records. so we have them sorted by name and this is say, something that we do just before assigning final grades. maybe the third line there is the final grade. so it's all fine sorted by name and but then in order to distribute it out to the people leading it to the sections, what we want to do is sort by the second fields, sort by section. the problem is that when we do that, it messes up the sort by name and that's annoying. you might assume that once you have it sorted by name, then when you sorted by the second field then it should maintain the sort of by name for all that have equal keys in that second field. actually not all sorts preserve that property that is called stability. and clearly, it's worthwhile to think about for your application whether you want or need a stable sort. and so, it's an annoying surprise for many people and many applications. so a stable sort is a sort that preserves the relative order of items with equal keys. whichever sort are stable? that's an interesting question that we'll take a look at now. the quick bottom line is that insertion sort and mergesort are stable but not selection sort or shellsort. and even within that bottom line, there's implementations that maybe are not stable. you have to carefully check the code to be sure. always, in this class, we have an exercise or exam question is this version of this sort stable or not? so, students learn to recognize whether the code is stable. so this is just another typical example where we've got things sorted by time, and then what we want to do is maybe these are important events. people buying tickets to a rock concert and i'm going to sort by location what we'd hope is that it would keep the sort by time but this is a non-stable sort that doesn't do bad so then out in the location they're going to have to resort it if they use one of these. but if they use a stable sort, then it stay sorted by time and lots of applications you want stability. alright, so let's just look at each of the algorithms that we've considered so far. insertion sort. insertion sort is stable. why is it stable? well, we never move equal items pass one another. in this example here, when we get a1, well that's so in this case, the index is just one that appears in the array, it's just a's and b's. when we get our second a, we stop the sort as long as we're not less. we're equal, we're not less, we stop it so we never move an equal item pass another one. if this less or less than or equal, then it wouldn't work. or if we did the other way around and proceeded accordingly. so, equal items never move past each other in this code so therefore insertion sort is stable. but selection sort is not stable. usually way, the way to show that a sort is not stable and it's just to see if it has a long distance exchange that might move an item pass some equal item. so, [cough] in this case, for example, for selection sort, when we do that first exchange oops, [cough] where we found the minimum a and b is in position zero. we did a long distance exchange and that catapulted that first item past any item that it might be equal putting them out of order. and that's may not get fixed so that sort is not stable. it might move items past some equal item and leave a result where items that are equal or in different order than they were originally in the file. selection sort is not stable. shellsort also has long distance exchange and so it's not stable. it moves keys past other keys that could be equal and so its easy to construct examples showing that selection sort is not stable. and what about mergesort? mergesort is stable well, it's stable as long as the merge operation is stable and that operation is going to be stable depending on how we code it. and, and in our code, if the two keys are equal, it takes from the left subarray so that means that, it will always take the, if there's a two sets of equal keys, it will preserve the relative order and that's enough to show that the merge operation is stable and then therefore mergesort is stable. stability is an important property in sorting algorithms. mergesort is not only efficient, it's also 
welcome back. today we're going to look at quicksort. it was named as one of the most important algorithms of the twentieth century and it's widely used for system sorts and many other applications. last lecture, we looked at mergesort, another classic sorting algorithm, that's used in many systems, and today we are looking at quicksort which is used in many others. you can even get a quicksort t-shirt nowadays. so what is the quicksort method? it's also a recursive method, but the basic idea behind quicksort is that it does the recursion after it does the work, whereas mergesort did it before it did the work. so, the idea is first randomly shuffle the array. that's an important step that we'll talk about later, and then partition the array, so that's to divide it so that for sum value j the entry a of j is in place in the array. there's no larger entry to the left of j and no smaller entry to the right of j. once we have the array partitioned in that way, shown here in the middle. right here, we have k in its position. and we have everybody to the left. there's nobody greater than k. and everybody to the right, there's nobody less. once we have it arranged in that way, then we recursively sort the two parts. sort the left part, sort the right part. and then after those two things are done, the whole thing is sorted. this method was invented in 1961 by tony hore, who won the turing award in 1980 for this and other work. so let's look at a demo of how quicksort partitioning works. the idea is to arbitrarily choose the first element to be the partitioning element. since we shuffled the array, that's our random element from the array. and then we're going to maintain an i pointer that moves from left to right, and a j pointer that moves from right to left. let's look how it works in the demo. so we start again by picking k as the partitioning element. and then our method is to move the i pointer from left to right. as long as what we have is less than the partitioning element. and move the j pointer from right to left as long as it points to an item that's greater than the partitioning element. so, in this example the i pointer stops right away because it's pointing to an r which is bigger than the partitioning element. the j pointer decrements until it gets to the c which it stops there which is less than the partitioning element. and so now what's going to happen is those two elements are out of place. the partitioning elements in between them and they're in the wrong order. so what we want to do is exchange those. and then move on. now we increment i, as long as it's pointing to an element that's less than the partitioning element. stop here at t cuz that's bigger. and now we decrement j, as long as it's pointing to something that's bigger than the partitioning element. stop her at i because that's less. again, t and i are in the wrong places. if we exchange them, we'll maintain the invariant that everything to the left of i is less than the partitioning element, or nothing to the left of i is greater than the partitioning element, and nothing to the right of j is less than the partitioning element. so exchange increment i as long as it's less. stop at l increment j decrement j as long as it's greater. stop at e those two elements are out of position so exchange them. now increment i, stop at the l which is greater than k decrement j stop at the e which is less than k and now at this point the partitioning process is complete, coomplete cause the pointers have crossed and we have looked at everything in the array. in fact. j points to the, rightmost element in the left subfiles, everything that's not greater than k. so we can just exchange j with our partitioning element. and now we've achieved the goal of partitioning the array. so that a of j is in its position. nobody to the left is greater. nobody to the right is less. now, the code for partitioning is straight forward to implement. down below. shows the state of the array before partitioning. during and after partitioning. so in the end, the j pointer is pointing to the partitioning element v, which was in position v in the first place. in the, all during the partitioning process, the code is maintaining this invariant. where everything to the left of i is less than or equal to v. everything to the right of j is greater than or equal to v. and we haven't looked at things in between. so, finding, incrementing i, as long as it's less is a simple while loop. and then we put a test to make sure we don't run off the right end of the array. and decrementing j. as long as it's pointing to a bigger element that's similarly just a wide loop we put in to test to make sure we don't run off the left end of the array. then there's a test to see if the pointers cross. swap the elements of i and j. when we get to the pointers cross we break out of the loop and exchange the partitioning element into position. so that's a quick implementation of the quicksort partitioning method. now, quicksort itself then is going to be a recursive program that uses that partitioning method. first thing we do is the public sort method that takes the array of comparable items as its argument. it's gonna to do a shuffle. and that shuffle is needed to make sure that we can guarantee that the performance is gonna be good. and then it calls the recursive method that takes as arguments the limits of the subarray that's gonna be sorted. so then partitioning. simply does the partitioning. tells us where, which element is in position, and then recursively sorts the last part that's loaded, j -one. and then the right part, that's j + one to high. that's a complete implementation of quicksort. again, as with mergesort, studying a recursive trace is instructive. and this one is kind of upside down as compared to mergesort. the first line shows the partitioning where k is put into position. then the method calls the sort for the left subfile first, and then that's gonna be partitioned on this e, and so forth. and eventually we get down to small subfiles, actually our code doesn't do anything at all for subarrays of size one, so we just leave those in gray, and then it does the right subfile, and so forth. again, studying this, a, a trace like this, gives a, a good feeling for exactly what's going on in the recursive program. let's look at an animation of quicksort in operation. there's the partition. now it's working on the left. now it's partitioning the right. now it's working on the left part of the right. now it's partitioning what's left. doing the left part of that. and working from left to right, by dividing each sub-array in half as it goes. so let's look. consider some of the details in implementation of partitioning with quick sort. so first thing is the partition is in place. you could use an extra array and the partitioning code would be a little bit easier. but one of the big advantages of quicksort over mergesort is that it doesn't take any extra space. it gets the sort done in place. now you have to be a little bit careful with terminating the loop. when we give you working code it's not hard to see why it works. and you might go trough the exercise of trying to implement quicksort without looking at our code, and you'll find that testing when the pointers cross can be a little bit tricky, particulary in the presence of duplicate keys. also staying in bounds. and i, actually, in our implementation the test of the j pointer running off the left end is redundant. why is it redundant? well, the partitioning element is sitting there and it'll stop when it hits the partitioning element. but the other test is not in our implementation. and the key thing, one key thing is that the way that these implementations work. if the in-, the file is, the array is randomly ordered, then the two sub-arrays after partitioning will also be randomly ordered. actually, some implementations of quick sort out in the wild don't have this property, and they suffer a little bit in performance. that random shuffle at the beginning is important and needed for guaranteeing performance. and the other thing i have referred to but not talked about in detail is the presence of equal keys. you might think it would be better to handle equal keys in some special way. we'll talk about that in a second. but this general purpose implementation stops the pointers on keys equal to the partitioning items key and we'll take a look at why that's important in a minute. so now let's look at the running time estimates about why we care about quicksort vs mergesort. this is extending the table we looked at last time, and you can see over in the right column here, quicksort is quite a bit faster than mergesort. and again, a good algorithm is much better than having a super computer. even on your pc you can sort huge array of a million items in less then a second and a million items in only a few minutes. so again this time, sort of timing is why quicksort is so widely used. cuz it's simply just faster than mergesort. well in the best case quick sort will divide everything exactly in half. and that makes it kind of like merge sort. it's about analog in. and in the worst case if the random shuffle winds up putting the items exactly in order, then partitioning doesn't, doesn't really do anything except find the smallest, peel off the smallest item. kind of discover that everything to the right is greater. that's a bad case. but if we shuffled randomly, it's extremely unlikely to happen. most interesting thing about the study of quicksort is the average case analysis. this is a somewhat detailed mathematical derivation, but it is worthwhile going through the steps, to really get a feeling for why it is that, quicksort is quick. so what we do is, as we did for merge sort, is write down a mathematical recurrence relation that corresponds to what the program does. in the case of quick sort, the number of comparisons taken to sort n items is n+1 for the partitioning. plus what happens next depends on what the partitioning element was. if the partitioning element is k. any particular value happens with probability one over n, and if it's k, then the left subfile has k - one items in it, and the right subfile has n - k items in it. so, for every value of k, if you add those up the probability that the partitioning element is k, plus the cost for the two subfiles, we get this equation. this looks like a fairly daunting equation, but actually it's not too difficult to solve. first thing we do is just multiply by n and collect terms. so ncn n times n + one. and then these terms, every size appears twice. so it's twice the sum of from c0 to cn - one. it's a simpler equation already. now what we can do is get rid of that sum by subtracting the same equation for n minus one. so ncn - n - one, cn - one then the n, n + one - n - one n is just 2n. and then the sum collapses just leaving the last term. this sum, minus the same sum for n - one, just leaves the 2cn - one. now that's looking like a much simpler equation. rearrange the terms, so we get n+1 cn-1 and then divided by n, n+1. that's a kind of a magic step, but we will see that it makes possible to solve the equation easily. because that equation, with c over n plus one equals cn minus one over n, is an equation that telescopes the first term at the right. it's the same as the term on the left. so we can apply the same equation so its two over n + one. we apply for n - one we get one less here and we can throw out a lot two over n. and continue that way throwing out two over decreasing numbers all the way down until we get down to two elements, c1 which is zero. substitute the previous equation telescope. and then that gives us an easy sum that we can approximate by an integral. it's one over x from three to n+1. and that's a pretty close approximation, in this case. and that approximation gives us, it's about two m+1 natural log n comparisons for quicksort. about 1.39 n log n. that's the average number of comparisons taken by quicksort, and actually they for a random permutation of the elements which is what we do with the shuffle. they the expected number of comparisons is concentrated around this value. it's very likely to be very near this value is then as large. so the worst case quick sort is quadratic. so complexity's going to tell us that it's a quadratic algorithm if that's what its worst case is. but with random, the random shuffle it's more likely that this lecture will end, because of a lightning strike. or your computer will be struck by a lightning bolt. so we can discount that. the average case, which is extremely likely for any practical application, is going to be about 1.39 n log n. so that's more compares than mergesort uses. but quicksort is much faster, because it doesn't do much corresponding to each compare. it just does the compare and increment a pointer. whereas, mergesort has to move the items into and out of the auxiliary array, which is more expensive. so the random shuffle is a key for good performance in quicksort. it gives us the guarantee that the worst case is not gonna happen. and also, it allows us to develop a math model that we can go ahead and validate with experimentation. you run quick sort and you count compares. if you did the random shuffle, it'll be about 1.39 n log n compares. and its running time will be proportional to n log n, and it'll be a fast sort. and that's what people do, and that's why people use it. now there are some things that you have to watch out for with quicksort because the implementation is a bit fragile and it's easy to make mistakes. and you'll find textbook implementations or implementations out on the web that wind up running in quadratic time in certain situations. you have to be a little bit careful of that and even if everything is randomized if there's lots of duplicates and the implementation is not done quite right the quick sort might take quadratic time. so, let's summarize the properties of quicksort. it's in place. it doesn't use any extra space. the depth of recursion. so tha, that's. again, dependent on the random shuffling, is going to be logarithmic. you can, limit the depth of recursion by always doing the smaller sub-array before the larger sub-array. but that's not really necessary nowadays, as long as you've done the, random shuffle oh, and by the way, quicksort is not stable cuz partitioning does one of those long range exchanges that might put a, a key with equal value over a key another key with the same value. so it's a little more work to make quicksort stable, maybe using extra space. un, so what about in actually in practice? this is our fastest sorting algorithm, and there's a few ways to make it even faster. and these, we looked at some similar things with for the word, mergesort. and it's definitely worthwhile taking implementing for a quicksort. first thing is small sub-arrays. even quicksort has more overhead than you want for a tiny array, like one of size two or three or four. so can implement it to cut off to insertion sort for small arrays. and the exact number they use is not too, critical. okay. anywhere between ten and twenty will improve the running time by maybe twenty%. also you could just not do anything for small arrays, and then do the insertion sorting in one pass at the end. so, that's a first improvement. a second improvement is to, try to estimate the partitioning element to be near the middle. rather than just arbitrarily using the first element. which on average will be at the middle. so one thing that we can do is sample the items, and then take a median of the sample. and that's actually not worth the cost for enlarged samples, not usually. but for three it's worthwhile. slightly reduces the number of compares. increases the number of exchanges paradoxically, cuz more exchanges are required when the partition is right in the middle. so that'll also improve the running time by maybe ten%. so this is a summary of the optimized quicksort with cut off the small subfiles in median-of-three partitioning. so partition usually happens pretty close to the middle when you do that sample median-of-three and then small subfiles can just be left unsorted to be picked up with insertion sort right at the end. so this gives a feeling for the. number of items that have to be touched during quick sort. and kind of an explanation for how it gets the sort done so quickly. that's a summary of quicksort, our best sorting algorithm that we've seen to date. 
now we'll look at the problem that's related to sorting called selection that's also well solved by quicksort partitioning. this is a simpler problem. we're given an array of n items that are ordered and our task is to find the k-th largest. there's lots of important applications for this. so like if we wanted to find the minimum item that's k = zero or the maximum item that's k = n - one or the medium that's k = n/2. and there's many kinds of applications from people processing data. i wanted to find the top k or the medium or other order statistics so that's what selection is all about. now, here's an example where we want to use theory as a guide. what kind of efficiency might we expect in a selection algorithm. well, first of all, it's easy to see that we can solve selection and in law at end time. how would we do that? well, we just sort the array and then if we want to find the smallest, we'll look at the first position or the largest, we'll look in the last position or the medium, we'll look in the middle. in fact, if k is small, the running time is going to be proportional to n. because if you're looking for the smallest, you can just go through the array and find the small or the smallest in one pass through or if you're two, you'll find it and two passes through. so, you can imagine trying to look for a selection algorithm that takes time proportional to n and also the lower bound is n because you have to look at everything. if you don't look at everything, you might miss the one item that you're looking for. so, from these observations it's clear that what we, what we'd like is a selection algorithm that takes linear time. but at this point, the question is, is there a linear time algorithm that works for every k? or possibly selection is as hard as sorting. this kind of question plagued a lot of people in this late 60's or early 70's as these types of problems emerge for computing applications. so, it's an interesting question to think about for sure. well in his original paper in 1961 hoare gave a solution to the selection problem based on partitioning. and the idea is just a version of quicksort in a way. we're going to do our partitioning so that we get entry a(j) in place of the array. nobody to the left is larger, nobody to the right is bigger. but then, when we're doing selection, what we'll do is just go in one sub array or the other depending on where j is. if j = k, we're done, we've found the k is the largest. if k is to the left of j, then, we just do the left sub-file which is set high to j - one. and if k is to the right of j, we just do the right subfiles that load the j + one and that's all this code does is that it, we could do a recursive, a recursive call but this just does it by resetting the values of the parameters. do one partition then check whether you to your k-th element is going to be on the left part or the right part and reset lower high accordingly. if it's equal, then you found it and you return it and you keep going until you get to a point where you have only one element. that's the a quicksort like implementation solving the selection problem. notice again that it depends on the random shuffle at the beginning that's going to be important for performance. alright. so there needs to be a mathematical analysis to, to characterize the running time of this program in the fact is that quick select this method takes linear time on the average. we won't give the full proof. it's actually quite a bit more complicated than the one just on for quick sort. but intuitively, we can see kind of what happens each partitionings that maybe splits the array approximately in half. so that, that means you'd have, if you did exactly and [inaudible] + n/2 + n/4 and so forth which adds up to about two n compare so linear cross. if you do the, actually it doesn't cut it in half at exactly each time only on average so you need a fuller analysis like the one we did for quicksort and the bottom line of that analysis gives the number of comparisons required as a function of n and of k in terms of this formula here and if you plug in k = n/2, you get the result that the number of compares required to fine the median that's the highest value this formula can take is two + two natural log of two. so, linear time to find the k-th largest for any value of k. now again it's going to use, this is a method that's linear time on the average. it's actually going to be quadratic in the worst case but again, the chance of that it will happen with a random shuffle is less than the chance that we'll be struck by lightning. its a probabilistic guaranteed fast algorithm. now, from a theoretical standpoint that's a little unsatisfied and in, in 1973, there's a famous paper that found a compared base selection algorithm that guarantees to solve the problem in linear time. this is areal landmark in the theory of algorithms because for a long time, it's not known, we knew we could have the average case, the linear time but could we find a worst case? and this paper found such a construction. now in practice, this construction is, is rather high. so, the method is not really used in practice. and so, there is still the goal of a, of a fast guaranteed linear time selection algorithm maybe somebody in this class will invent someday. this is another example where we use theory as a guide. it's still worth while to look for a practical linear time worst case algorithm. well then, maybe somebody in this class will invent that but until something like that is discovered use the quick select based on quicksort partitioning you can get linear time selection when you don't need a full sort. that selection of simple problem like sorting that is well sound with quicksort partitioning. 
now we're going to take a look at what happens when we have significant numbers of duplicate keys which is not at all unusual in practical applications. so, in, in fact, often, the purpose of a sort is to bring items with equal keys together for like the example that i gave where we had cities and time. there's a lot of detailed data and the time and maybe the whole goal of the sort is to group them by cities so we can ship out the data for each city, to each city and there's plenty of other examples like that in data processing where we find maybe remove duplicates from a mailing list or all the job applicants that we get, we might want to sort them by the college attendant. so the sort does huge files with huge numbers of duplicate keys. so, a sort, it's worthwhile to take a careful look at what the implication of that is. so again, typical characteristics we have a huge file but small number of different key values. so let's look at our algorithms in that situation. so mergesort, it doesn't matter that much what the key values are like and it's actually, we can show that mergesort always uses between one-half, n log n and n log n compares. quicksort actually, they're up until the 1990s the most widely used implementation took quadratic time. for files with large numbers of equal keys and that was actually found by applications user and, and that's the standard quicksort that was in all the textbooks almost all the textbooks if you did not stop the partitioning on equal keys it would run in quadratic time. so we want to do better than this. so the mistake happens if we put all the items equal to the partitioning item on one side which is a natural way to implement it and the consequence is if you have all the keys equal, then partitioning doesn't really do anything. you just peels off one key to do file size n then you get a sub file size n - one and then n - two and so forth and the result is a quadratic tim e algorithm. our implementation, we stopped the partitioning scans on items equal to the partitioning item and then in that case, when all the keys are equal, it's going to divide it exactly in the middle. and then in that case, when all the keys are equal, it's going to divide at exactly in the middle. and then in that case you get an n log n compares. but actually when you think about it, why don't we just put all the items equal to the partitioning item in place. that's, that's really a desirable way to look at it and let's take a look at that option. so the goal is so called three way partitioning. so what we want to do is get the array into three parts so then now we have two pointers into the middle. one that is the boundary between the keys that are less than the partitioning element and those that are equal of the partitioning element. another one that's the boundary between the keys that are equal of partitioning elements and the one that is greater. and then in the middle are all the equal keys and that's what we'd like to arrange. now until the 1990s, conventional wisdom among people implementing system was, wasn't worth doing this. but, but actually it's a problem that edsger dijkstra had proposed in the 70s as an example of, of programming problem. he was really interested in analyzing correctness of programs and showing that this how you could convince yourself that this program was operating as expected. but in the 1990s we figured out that really this was going to be an effective way to sort. and this was taking a look at the qsort that a user found was broken and, and now, this method is incorporated into some plenty of system sorts. so let's take a look at how it works with the demo its more complicated than standard quicksort partitioning. a bit more complicated because there's more to do. so now we have our i pointer which is right to the left of stuff we haven't seen ye t and then, we have two other pointers that maintain, maintain these boundaries everything to the right of gt is known to be greater than partitioning element. everything to the left of lt is known to be less and between lt and i is known to be equal. and all the method does is maintain this in variants so let's do an example or two and see how that works. so we increment i and then figure out what to do. so, now it's less than the partitioning element. so, what we want to do is increment lt's. so now we have one that's definitely less than the partitioning element and lt is, is pointing at the partitioning element. and so now in, increment both lt and i so that's the first case there. so now we have a second item b. that's less than the partitioning element so exchange with lt and increment them both. so, we're moving the smaller ones than the partitioning element to the left of lt and keeping lt pointing on a partitioning element. now, what about when we get one that's greater than the partitioning elements? so, in that case, we exchange greater the one over at the right with i and decrement gt. so now, we have one over to the right that we know is greater than the partitioning element. notice that we didn't increment i because that element z that is over in the right, really hasn't been compared to the partitioning element yet. but we did decrement gt so we made progress. so now what happens here, now i is pointing to a bigger one so we're going to exchange it with the one at gt and decrement gt again. and again, y is bigger, so exchange it, decrement gt. now we have the c, that one smaller so that's the first case. so we'll move it to the left of lt and increment both lt and i. w is a bigger one, let us to go over to the right. now we have i pointing to an element that's equal to the partitioning element. and what are we supposed to do then? well, to maintain the variant there we just need to increment i. and again it 's pointing to one that's equal of partitioning element increment i. and now one more time and now it's pointing to one that's greater so we exchange that with gt and decrement gt and i is pointing to the one that was there and that ones smaller. so we exchange it will lt and increment both i and lt and now where the point, where the pointers have crossed i and gt across there's nothing that we haven't examined yet. so, our partitioning is complete. here's a trace of dijkstra 3-way partitioning for his problem which is when there's just three different values in the file. our problem we were treating partitioning, equal of partitioning element as one value less than as another and greater than as another. and so this, this trace illustrates how we always make some progress and eventually we get the file sorted. the implementation is amazingly simple. the whole partitioning process for three-way partitioning and the modern programming language like java simply maintains the invariances described in the demo. if we find one that's less we exchange i and lt and increment them both. if it's greater we exchange i and gt and decrement that. otherwise, we increment i. could, could hardly be simpler. in fact, is simpler in many ways than these standard implementation of quicksort. in fact, there's an argument for just using this implementation of quicksort and forgetting about horse because it performs so well in so many practical situations. here's a visual trace of 3-way quicksort for situation with plenty of equal keys. and again, when there's a lot of equal keys then there's going to be place where one of those is chosen, it's partitioning element then a big chunk of the array gets handled just in a partitioning process. now, there's actually some deeper reasons why this method is important and one thing to do is to realize that the lower bound that we talked about before depended on the keys being distinct. so, worst case for lower bounds is when the keys are all distinct. but if we have a situation where there are a lot of equal keys, that model is wrong. it's not too difficult to get a similar lower bound for the model when we know that there are some equal keys. so, for example this is a rather complicated formula but not too bad but in a sense that if you know that the i-th key, it occurs xi times you can write down a lower bound for the number of comparisons that are going to be required in the worst case. and, what's interesting about three way partitioning is that the number of compares that it uses is equal to this lower bound within a constant factor. so that's entropy-optimal and what that means is whatever the distribution of equal keys in there, this thing is going to use a number of compares that's proportional to the best that you could possibly do. the proof for this fact is quite beyond the scope of this course but it's still an important fact. and the, the bottom line is that if you randomize the order and use three-way partitioning then there's lot of applications where your sort routine is going to be linear not n log n so it will be much more faster than mergesort and you know, the methods for really a broad class of applications. so, taking a look at equal keys is carefully is something that can lead us to very efficient quicksort 
now, we'll take a look at how the sorting algorithms that we talked about or expressed in the systems that we use everyday. now, the key point is that sorting algorithms rhythms are essential in a very broad variety of applications and, and all of us use sorting algorithms pretty much every day. many obvious out applications like or, organizing your music library or displaying your search results or listening feeds in your in your web browsers. there's some other applications that are not so obvious where we use sorting as a to make a problem easy once you know that they're sorted. and so, for example, finding the median and if it's already sorted, it's much easy to find the median. and now, the statistical problems are like that or finding duplicates. probably finding duplicates by itself is not quite obvious what to do but the easy way to solve it is to just go ahead and sort. and then there are plenty of applications that we'll see later in this course like data compression or computer graphics like finding the convex hull, applications in science such as computational biology or, or in systems development. we're having a efficient sort as absolutely crucial. so, because there's all these applications most programming systems have a fast sort as an important part of their infrastructure and java is no exemption. so, java has a method called arrays.sort and it's intended to be a general purpose sorting method for use by java programmers. and now, that you have some understanding of the classic methods you can have a better idea of why arrays.sort is the way that it is. so it has the infrastructure that allows us to be used for all types of data types and all types of ordering so it's got a method that implements comparable then its got methods easy compare order. so that you can use the natural order or you can provide a compare order and provide your own order for any type of data. it uses actually both quicksort and mergesort. it uses two quick sort for primitive types of data and a two mergesort for objects. why two different well it's just the designer's assessment of the idea that if a programmer is using object maybe spaces, not a, a critically important consideration. and so, the extra space used by merge sort maybe is not a problem. and if the program is using primitive types, maybe performance is the most important thing. and so, then we'll use the quicksort. to invoke arrays that sort, you have to import the name space from java.util.arrays and then all you need to do is invoke arrays.sort. so i just answered this question, why do we use different algorithms for the two types? and this is, is maybe arguable. now are referred to this idea of a good system sort, there was a good system sort that a lot of people used for many years. and in 1991, there were some scientists that, that bell labs that were using qsort for a scientific problem and they were used to taking just a few minutes and then they realized that it was taking hours of cpu time. and the fact was that all the qsort implementations at that time in unix had this flaw well, there are two flaws and one of them is a little complicated about the way they are raised order and the other one was for a raise that had lots of equal keys and this is wilks and becker problem and have lot of equal keys, it was quadratic time. so, the system designer, jon bentley was one of the designers to take a look at these problems and that lead ultimately to the development of the 3-way quick sort that were used today. he worked with doug mcilroy and they wrote a, a, a paper that outline this problem and talk about some of these things and they had a three-way partitioning method that was somewhat like the dijkstra method that we showed but a bit more complicated. anoth er thing they did was rather than shuffling the array. they [cough] used what's called a method for choosing partitioning element called tukey's ninther. tukey is a statistician and he had this particular method for order statistics that has some interesting properties and use that for the partitioning element. this paper was very influential and, and that basic method is widely used. and tukey's ninther is just pick nine items out of the array and take the median of the mediums and that's the ninther. so very inexpensive and they had macros to do this so and use not too much cost to find a partitioning element that's much closer to the middle than, and if you use a, a random one. so the, the reason they used that is they thought they got them closer to the middle and they also don't like the, some system designers don't like the idea of using random choices in a system method because of way that it changes the state of the system. so they felt that they got better partitioning than a random shuffling and it was also less costly and then generating random numbers including this change of state problem. but there's a problem so you would think that the system sort would be completely solid with all this resource with all these research and all of the development that's going into it. in fact there's a file out there in your book site and get it that will actually break the java system sort. there was a killer input that will make the thing run in quadratic time. actually it usually crashes because it's recursive and it crashes the system stack. and it can cause all sorts of problems. there's a killer input for the system sort and, and it can be disastrous in various systems and the reason is, they didn't do the random shuffling. mcilroy, himself, actually found this problem that you could while the sort is running figuring out an inp ut that would make it crash. and so, you can just run that program and if the sort doesn't use randomness then it's vulnerable to this attack. so which algorithm should we use to sort that's, that's really a key question. we've looked at lot of sorting algorithms and actually, there's hundreds of sorting algorithms out there and we have chosen the most important and the most interesting for you but you could literally spend a year reading all the papers on sorting and then you still continue to be invented new algorithms are developed and that are found to have good characteristics all the time. and really, the key idea is really important to think about cuz it applies to all sorts of algorithmic problems. on our way, we've been talking about rules of the game. what is it that we care about in a sort? it's a little bit more complicated than just put stuff in order. there's a lot of attributes, the different applications have. like stability, that's a fairly sophisticated attribute that you really have to think about, you maybe not be aware of. maybe your computer is parallel and the sort has to be parallel and we found that equal keys make a huge difference. and i didn't really think about that at the beginning but it can make a huge difference. maybe the way your computer's memory is organized make a difference. maybe the keys are small and the items are large or maybe the keys are really huge. do we need guaranteed performance? are we happy with random performance? do we know, is the array randomly ordered? you can think of a matrix shown in the right here where we list out all the possible attributes and then there's algorithms that worked well for different combinations of attributes. but the thing is, there is way more possible combinations of attributes than there are algorithms. so there is going to be situations that are going to require an understanding of what it takes to engineer a, a sort method that's appropriate for your application. there can't be a system sort out there that's going to cover all possible combinations of attributes. now, usually it's going to be good enough but it's definitely worth while to understand what's going on with different sorting algorithms in order to even find improved performance over the system sort. so, here's the summary of some of the things that we've talked about. we've talked about six different sorting algorithms. then, we've talked about a bunch of attributes. for example, the top line, selection sort is in place it always takes about n^2 / two comparisons. and one of the things to remark about it is that it only uses n exchanges and so forth. insertion sort best case linear, quadratic, and place unstable. and it'll work well if the file is small or partially ordered. shellsort, we don't know it's a running time, it's not stable but it's a fast method for intermediate size files and not much code. mergesort and log n guarantee unstable but not in place, need that auxiliary array. quicksort not stable. quadratic time worst case but that's unlikely to occur if you do the random shuffling. and its the fastest and most useful in practice particularly if you make improvements to deal with duplicate keys. and it's interesting to note we've looked at important and classic algorithms that are widely deployed but we don't have a, a useful, practical algorithms that are widely used that's got all of these characteristics that's in place and stable worst case n log n. there's versions of merge sort that come close but they are too complex for practitioners to have adopted them. those are some of the situations that we encounter when developing a system sort. and, quicksort certainly plays a role in most system sorts. 
welcome back. today, we're going to look at priority queues which is a variant of sorting that generalizes the idea to provide more flexible data structure that we can use for all sorts of applications. to get started, we'll look at the api and some elementary implementations. so at a week or so ago, we looked at collections in java and the idea of elementary data structures where we insert and delete items. and then, the data structures differ on the basis of which item we choose to delete. we looked to the push down stack where we removed the item that was most recently added, and the queue where we remove the item that was least recently added. and then, we talked about randomized queue or bag where we might remove a random or an arbitrary item. today, what we're going to talk about is called the priority queue and for that, we insert items, but when it's time to remove, we consider the items to have a total order and we want to remove the largest or the smallest item. so this little example if we insert p, q, and e then when we do remove max, we want to get the q out and for later, we insert x, a, and m and then we removed max. the largest one in there is x. we'll insert p, l, and e and then, after a while, we remove max p. so, that's our basic setup. that's our definition of what a priority queue is. so, the api will look very similar to our stack or queue api with a difference that we want to have generic items that are comparable. so the java language for that is in the class header. we say that our generic type key extends comparable of key. and that just means that our keys must be comparable and we must have a compareto() method that will compare it to another key. otherwise we have a constructor and actually for some applications, it's convenient to have a constructor that takes an array of keys as argument. then, an insert() that puts something in like push in a stack or enqueue in a queue. then delete the maximum. i referred to delete the minimum just to avoid confusion, we have the implementation separate implementation usually minpq, where we delete the minimum. then test isempty() and we also sometimes have extra method that just gives us the value of the largest key and also size which is useful sometimes in collections. you might also want it to be iterable but we'll skip that for now. there are lots and lots of applications of priority queues. even though it emerged as a data structure relatively late in the game now that we see that there are many algorithms that are much easier to implement when we think about the priority key abstraction. we have data that we are processing we can't process it all at once. we have to save it some of way. and then, what the priority queue tells us is - let's organize it in some ways that we are always taking the best one to process next. and it turns out to be very close to a generic algorithmic design technique that we will be looking at in many, many different applications. today, we're going to talk about event-driven simulation which is an interesting idea that is based on priority queues but it's also used in numerical computation and we'll see in algorithms for data compression and graph searching that it's useful. and in many other applications in computer science and in scientific computing. it generalizes the stack and the queue and gives us a data structure that we can use to effectively design algorithm of all sorts. so here's just a particular client that will help explain the idea. so our, our challenge is let's say this is on the web we have billions of transactions, you know, and they are streaming through our data warehouse or processor in some way. and just a very, very huge number of transactions. in fact, we couldn't possibly hope to even store them all. there's trillions of them there coming through as fast as possible. but we're interested in the biggest ones and so maybe it's the biggest amount of dollars, or the biggest cost, or whatever it might happen to be. and so we can pick some numbers that we can store. i would like to, to store the, the thousand biggest ones. so, you can imagine a credit card company looking for fraud - it's going to care about keeping track of the largest transactions. so, maybe we can store millions or thousands of them. so that's our parameter m - that's the number we can afford to store but the total number of items we couldn't possibly afford to store them. so and this is just some test data where we've got all, all these transactions and so we are going to be able to take in data like this and again an unbounded stream of data. but let's say, we want to keep track of the top five [cough] values using the third column as the value. so we're going to look at a client program called topm that takes the command-line argument, how many and this case, it's going to say five and then it's going to take from standard input the transactions and it will print out the top five. so, that's a canonical example of a, a priority queue client that we need to design a program that can do that. so, with the priority queue abstraction that's not too difficult to do. so we are going to use a min-oriented priority queue so that's going to keep, it'll [cough] it'll be one where we can delete the minimum and, and it'll be generic so we'll have a transaction type that holds this information including natural ordering where it's ordered by dollars that last column. so, we'll build a new priority queue, min priority queue or we'll have the capability to delete the minimum. and then we read from standard input. we read the line, build the transaction from the information on that line. and that will fill in the fields and then, we put that transaction on the priority queue. now, if the priority queue has more than m items because we inserted that one, then we want to delete the smallest one there and that way, we're keeping track of the largest m. whenever we get a new one, then we throw away the smallest one that's there. so, even with this huge stream of items coming through, we're only keeping track of the m largest items and that's a fine canonical client for priority queue. now how we are going implement or solve this problem or you can think of lots of ways to go ahead and solve this problem of finding the largest m items in the stream of n items. so, for example, you could sort them. and then just look at the m at the end but by sending up the problem, i already kind of ruled that one out because we don't have the space to sort them all, to store them all. so sorting is out of the question. we'll look at couple of elementary priority queue implementations that are straightforward. you know, keep the items like we would in the stack and then when we need to find the smallest or the largest look at, look at them all. but that's going to be too slow. m is large and n is huge, and m<i>n is going to be too slow. what would what we we'll look at is</i> very simple and practical implementation using a data structure called the binary heap that gets the job done in time proportional to n log m and only m space. and that's pretty close to the best that we could do in theory and is very important and useful, practical implementation and data structure. alright. so here's just an overview of two elementary implementations for priority queues using the example operations that i gave before. so you can imagine keeping the item, say, in a linked list or in a doubling array and just keeping just an order just as we would in the, in the stack just keeping in the way that they come in. and we'll put a new item at the end of the array and remove it from the end of the array. or you could do it in a linked list, and then when it's time to find the, remove the maximum, you have to scan through everything to find the maximum. so, so, that's a one way you could implement this with, with a linked list or with the resizing array. or you might to say well let's try to keep things in order. and then that might involve some work with the, it's like insertion sort, you find a place to put the new item and then put it in the right place. and again, you could do this with a linked list or with the resizing array but then, with array, you'd have to move all the larger ones over one position to fit the new item in. when we insert e and that's supposed to keep them in order, we have to move over l, m, p, and p to get the e and then so forth. but the advantage of that might be that removing the maximum is easy. we just take away the one at the end. to remove the q - we know it's at the end to remove the max. at this point, that's x - it's right at the end, and p is right at the end. so you can imagine the implementations of priority queues using these two basic strategies. not much code involved. this is a an ordered array implementation of priority queues and it's quite straight forward. we didn't put in the this is the cheat version where we require the client to provide a capacity but we could easily change this to a resizing array. so insert() just puts it at the end, and since its unordered delete maximum has to go through the entire array to try to find the maximum when it refines it and the changes that we're the one at the end and then removes it the same way that we do within the stack. it'll use less and exchange just like we would in sorting methods. so that's a fine implementation if the priority queue was going to be tiny all the time. so if you, without even implementing it, you can understand this table that if we use an unordered array implementation we can get insertion done in constant time but we have to look at everything to delete the maximum or even find the maximum. if we keep it in order, we can find the maximum or delete it at constant time but it takes us linear time to insert. and since as with stack and queue operations, these insert and deletes might be intermixed in arbitrary ways and there might be huge numbers of them either one of these is very attractive because they're going to take n times the number of operations. whereas what we can hope for and what we actually will achieve is to get log n time for all operations, time proportion to log n for all operations. with the clever data structure and interesting implementation we can actually achieve that goal. that's the basic api and some elementary implementations for priority queues. 
now we're going to look at binary heaps which is an ingenious and very simple data structure that's going to help us implement all the priority queue operations quickly. so, the idea of a binary heap is based on the idea of a complete binary tree. so, a complete binary tree, well first of all, a binary tree is either empty or it's a node with links to left and right binary trees so that's an example of a binary tree. a complete binary tree is one that's perfectly balanced, except possibly for the bottom level. so there might be a few nodes on the, on the bottom level and one level lower than the bottom level. but otherwise, all the levels are full. we'll see how that looks in just a second. the property of complete tree is that the height of a complete tree with n nodes is the biggest integer less than log base two of n, and that's really easy to convince yourself that that's true because the height, if you add nodes one at a time going from left to right on the bottom level say, the height only increases when n is a power of two. complete binary trees actually happen in nature. here's an example of one that goes one, two, three, four levels at least, so sixteen pushes at the end there. alright. now, the way we're going to use complete binary trees to implement priority queues is to first of all, associate information with each node. we'll put our keys in the nodes, and also we're going to represent it with an array. so, [cough] when we start putting the keys in the nodes we're going to impose one more condition that's called heap ordering. and the idea is that the parent's key is going to be no smaller than its children's key for, and that's true for every node in the tree. the array representation, all we do is, we put we start with indices at one, it's a little less calculation. that way we leave a(0) empty and then we just take the nodes in level order. so first, we put the root. then, we put the two nodes on the fi rst level going left from right. and then, all the nodes on the third level, going from left to right and so forth. this is interesting because we can draw the tree to get more intuition about what's happening. but in the actual data structure representation, we don't need any links at all. it's just an array. the way that we move around the tree is by doing arithmetic on the indices. so let's look at a few properties of binary heaps. so that's complete binary trees represented in array with keys in the nodes satisfying the heap order property. well, first thing is that a(1) is the largest key. it's larger than the keys and its two children and they're larger than theirs and so forth so it's the largest key in the data structure. the other thing is, as i just mentioned, you can use the array in the seeds to move through the tree. for example, if the node is at position k, index k in the array, then it's parent is a k over two and that's integer divide. so the parents of say h and g are both n. h is ten, g is at eleven, n's at five so both of those are ten over two, eleven over two, integer divide is five. and going the other way it's easy to see that the children of the node at k are 2k and 2k + one. so we don't need explicit lengths at all to represent these data structures. we can just use array indices. so [cough] that's the basic setup or the invariant that we're going to maintain in this data structure. and now, what we're going to do is take a look at just a couple of different scenarios that where we violate that invariant temporarily and then fix it. and that's going to give us the flexibility that we need to implement priority queue operations. so one scenario shown here, is if for whatever reason a child's key becomes larger than its parent's key. so in this case, we have an example where t, the node t here, its value changes and it becomes larger than its parent key p. so, the heap order condition is satisfied everywhere, except at this node. well, it's easy to fix this one. all we do is exchange the key in the child with the key in the parent. after that exchange, then, that would have t up here and p down here then the heap order condition is satisfied at that node because the parent was smaller, so that one's smaller. and so that one is still smaller so t is after its exchanged up here will be bigger than both its children. but the heap condition will be violated cuz t is still smaller than s. so we have do it again, exchange it with s. so we move up the tree, exchanging the larger key with its smaller parent, until we get to a point where its larger than both its children. that's restoring the heap order along the path from the place where it's violated to the root. you can think of that as kind of like the well known peter principle where a node gets promoted to a level where it finally can't be better than, than it's boss. it's a level of it's maximum incompetence. and implementing that in code is really easy. we call that the swim operation, it swims up to the top. and if we have a node at index k and we know the heap condition is violated there, as long as we're not at the root and k's parent, k over two is less than a of k. then, we just exchange it with its parent, and move up. that's the swim operation to eliminate a violation when a key value increases. [cough] so for example, this gives us a way to insert a new element into a heap. what we do is, we add a new node at the end of the heap, so this one position over. the thing is, remember represented in array, one, two, three and so forth. so the [cough] next empty position in the array there's a place to put a new node. and then we just declare that, that's part of the heap and that node, well if it's less than its parent, we're fine. but in general, we have to check whether the heap condition is violated and exchange it with its par ent as long as it's smaller and that's just perform the swim operation. so if n is the number of items in the heap, defined to be in the heap we're going to increment it, store a new key there, there and then perform the swim operation. so that's a quick implementation of the insert operation. and notice since it's just going from bottom to top in the heap it takes at most one plus log base two of n compares. [cough] now there's another scenario where a key becomes smaller. for whatever reason, a parent becomes key decreases, it might become smaller than one or both of its children. in this case, the value at position two has changed to h for whatever reason and that's smaller, in this case, than both its children. so how do we fix that violation? well, that one's also easy. we figure out which of the children is larger. in this case, it's the s, and we exchange our exchange that one with the one that's violating the condition. so that's moving the smaller key down. after that exchange, then s is in position two, and it's bigger than both p and h. and the heap condition is only violated again where h is sitting. and again, we keep going until getting to the bottom, or getting to a place where both children are smaller. and that's maybe a little bit what happens when a, a new boss is hired from the outside and then the two subordinates struggle to take over that position and then the bros, boss would get demoted to it's level of competence. and again, that level of flexibility. here's the implementation of it. and again, it's quite straightforward using the [cough] index arithmetic to move around in the heap. if we're, and that's called the sink operation, cuz we're going down the heap. if were at position k, then what we need to worry about it is the nodes at 2k and 2k + one. and the first thing to check is find out which one's bigger. it's either 2k or 2k + one and so set j accordingly. so that's j now, is, after this st atement is the larger of the two children. and don't forget to check that we're going off the end of the heap. and then if, [cough] the k is not less than either one of those, then we're done. if it is, then we exchange with the larger of the two children and move down the heap. again, just a few lines of code to eliminate the violation when a key value in a heap decreases. and that one we're going to use to implement delete the maximum in a heap. so delete the maximum, we have to do two things, one thing is the size of the heap has got to go down by one. the other thing is return the maximum. well, we know that [cough] one that we want to return is the one at the root. so we'll save that value away to return to the client. and then, since it has to go down by one the place to get the to remove the element from the heap is at the end of the array cuz it's now going to have to not occupy that position, so we take that element and replace the root with it. so i move the h up and actually, put the root value there just exchange them but it's not longer in the heap. now, that element which went from the bottom to the top is most likely going to violate the heap order. it's going to be smaller than one of its, both of its children. so, we do a sink. [cough] now in this case to implement the lead max we save away that value at the root in max and we eliminate loitering by nulling out that vacated position then return the max value. so that's an implement, implementation of the delete max operation for heap using sink where a key value that decreases, go down, goes down in the heap. so let's just take a look at what happens with a, a real heap with the demo when we do these things. and you'll have a good feeling for how this data structure works. so, we're starting at some point where we have these ten keys in heap and it's heap order. so we've drawn the data structure with the links, so we have an intuition for what's going on. but all the program sees is the array and grey at the bottom where t's in position one, p and r are position two and three, and so forth. so now, suppose that we're supposed to add s. so to add it to the heap, that's going to go in the position at the end. then now we've added it to the heap just by incrementing n, putting it in there but now we have to bring the heap order back into condition. and so that's going to, now that key is larger than its parent so we're going to swim it up exchange it with its parent as long as it's smaller than its parent. so, first thing it goes up exchange with the s, it's still bigger than p. so we exchange it with the t and now we're done because s in not bigger than t and the heap order condition is now satisfied everywhere in the heap. so, with just two exchanges we insert that new element in the heap in this case. i now suppose the next operation is remove the maximum. so, we're going to take t and exchange it with the last element and then declare that to be no longer part of the heap. so now [cough] we have to bring the heap order back because it might be violated at the root so now we invoke the sink where we exchange that node with the larger of its two children until we find a place where its larger than both its children. so s is the larger of the two children r and s and now h is still smaller than both it's children so we promote the larger, which is p. now h has no right child, just a left child and it's larger than that one, so we're finished with that operation. we've removed the maximum and we still have our data structure heap ordered and our n keys stored in first n positions in the array. let's remove the maximum again. again we take it out by exchanging this time g with the root and then [cough] decrease the size of the heap by one. just take that out. now, t he node of the root violates the heap border so we have to exchange it with the largest of it's two children, in this case that's r. again, g is not larger than it's two children so we have to exchange it with the larger of the two, that's o and now we are done, we've removed the largest again. now suppose we insert s back into the heap. so [cough] that's adding it at the end, violates the heap order exchange it with the parent smaller and keep doing until we get to a place where it's larger than its two children. in this case, s goes all the way up to the root. and it's all heap ordered again. so that's a little survey of some operations on a heap and you can see how every operation is done with just a few exchanges along the path from the bottom to the top or the top to the bottom. okay, here's the complete java implementation of a priority queue using the binary heap data structure. [cough] it's actually not so different from the elementary implementations that we looked at in the last section. our representation is an array of keys and a size, that's the number of items in the heap. [cough] for simplicity, we'll show the code where the client gives the capacity of the heap. we can use resizing array, in industrial strength implementation, the same that, we did for stacks and other data structures where we use arrays. so we'll build a new array of keys and we have to use an ugly, ugly cast because we can't have generic arrays in java. and that, so it's comparable and, and we need one more than the capacity to handle this thing where, we don't use position zero. so the priority queue operations, is the insert and del max that we showed in the previous slides, is empty, is just checking whether n is equal to zero. we have the swim and sink functions that we showed earlier. and then we have helper functions less and exchange, that access the array directly so that the co de doesn't have to access directly. that's a complete implementation of priority queues in java. and this is, this implementation by itself is extremely significant because, it's really very simple, optimal representation of the data. and only a little arithmetic with array indices. but as you can see by looking at this table, it gives us a way to implement priority queues where, both operations are guaranteed to happen in log n time. now, experts have worked to come up with improvements on this and there are slight improvements possible. we can make the heap d way rather than just two way and depending on the frequency of execution of the uncertain del max operations that might work out better. there's an advanced data structure called a fibonacci heap, where inserts are done in constant time and delete max done in log n time on an average over all the operations. that ones generally too complicated to use in practice. but still again, using theory as a guide maybe there's a way to, [cough] to decrease costs a little bit from binary heaps. and of course, we cannot get down to having constant time for all operations. why? well, we can sort with a heap by inserting all the elements and then deleting the maximum of getting a sort done and that would be in linear time if we had this kind of variation. if, if we have constant time operations for both insert and del max. but for certain applications, we can get close to constant time for one or the other operations and that'll be useful in different implementations. now, there's an important consideration, that, in, that we have to bring up related to the programming language and [cough] this is, a more general consideration than usually we bring into focus in algorithms but it's worthwhile mentioning. we're assuming that the client doesn't get to change the keys while they're on the priority queue. and it's better not to ass ume that it's better to arrange for that in our implementations by using keys that are immutable, who's values don't change. there's many reasons that immu, immutable keys are [cough] that programming languages provide the capability to build immutable keys and, and this is a fine example of one. so and we'll talk more about that in a minute. the other things that, that, we didn't talk about in the implementation should throw in, exception. if the client tries to delete from an empty priority queue and we should have a no argument constructor, and use a resizing array, to, account for gradual growth and shrinkage in an industrial strength implementation. usually we provide two implementations, one that's max oriented, one t hat's min oriented so that nobody get's confused and they're the same except the less and greater switch. and we'll see later on, there's times when we want to add, expand the api and provide other operations like removing an arbitrary item from the priority queue, or give the client in the api the capability of changing the priority of an item. our sink and swim methods are good for making this happen, but we'll, delay these implementations until we need them in a more complicated algorithm. so what about mutability? so in every thing in java is implemented as a data type, a set of values and operations on those values and the idea of immutable data type, is you can't change the value once it's created. so that's kind of like, when you [cough] when you, when you create a literal value to be assigned to an integer, it has that value. so here, here's an example say using the data type for vectors might be a way to implement vectors. so we put the word final to means that instance methods can't be overridden. and not only that, instance variables are private, they can't be seen from the outside and they don't change. and so a constructor for an immutable vector data type, it might take an array [cough] as it's argument, and that array has got values stored in it, say doubles, and those are, those can change but what immutable implementation would do would be to copy those values into the local [cough] data array instance variable and then those values are not going to change. and the instance methods won't change them and the client can't change them. so that value stays the same. lots of, implementations, data-type implementations in java are immutable, like string is immutable. when you create a string that value doesn't change. [cough] if you want a new string, you have to create a new string, using concatenation or some other operation. and the same with the wrapper types, like integer and double, or color, and, what lots of things. whereas on the other hand, sometimes, the whole purpose, of a data type is to maintain a changing value like a good example is like a counter, or a stack. so you wouldn't put those things on a priority queue cuz the value is changing but the other ones you would. so the advantages of immutability and again, maybe this isn't the place to really sell those advantages more for a programming language course is that it, it really simplifies debugging. we can be have more confidence that our priority queue operations are going to work correctly if we know that the type of data that's on the priority queue is immutable. if the client could change the values, how do we know that the heap ordering operation is preserved? if we want the client to be able to change values, we're going to provide methods for that purpose as i just mentioned. and there's many other reasons that people use immutable data types. there is a disadvantage that you have to create a new object for every data type value but for a lot of applications that disadvan tage is not viewed to be significant compared to the advantages. here's a quote from a one of java's architect, josh black. classes should be immutable unless there's a very good reason to make them mutable. if a class cannot be made immutable, you should still limit its immutability as much as possible. and many programmers live by that kind of precept. so that's a basic implementation of priority queues using the heap data structure represented as an 
next we're going to look at the use of the binary heap data structure to implement a clever sorting algorithm known as heapsort. so here's the basic plan. what we're going to do is we have our end keys and we'll have them in an array. we'll view that array as eventually being a max heap. so what we have to do first is to rearrange the keys in the array to heap order it. so just make it so that every key is larger than it's two children. and for example, the largest of all the keys is at the root. and then, the next phase would be to take that heap ordered array and get, get it to be a sorted result in, in place. and again, the, heap is stored in the array, with the first key position one, next to position two and three and like that. so the end result would be like that, with, no keys in the heap, but all the keys in the array in sorted order. so it's a little exercise in abstraction. part of the array is the heap. part of the array is the sorted sub array. and eventually we bring it down to the whole thing being sorted. it's very little code beyond the basic heap code that we've looked at can get this implemented. and that's called heapsort. let's take a demo of how heapsort works in our example. so the idea is we're going to use a bottom up method. so all that means is we start with an array in arbitrary order and then we're going to work from the bottom up to make sure that it's heap order. well all the nodes with no children are heap order, they are only a size one, the first one we have to worry about is this one here the root, the root. we haven't examined yet, it's children are heap ordered so it's a small heap of size three that may not be heap ordered. in this case it's not because one of the children is larger, so that's where things are going to start. we have a lot of one node heaps and then we're going to have to perform the sync operation on this one, that node five, that's, in this case just to change it with it's parent. and then proceeding in that way, moving bottom up or moving from right to left, the next thing we do is but then worry about a three node heap that's heap ordered and we're fine. now we'll move over to the t and again, that's the root of a three node heap that's heap ordered except at the root. we may need to fix it with the sync operation. in this case nothing is required because it's larger than its children, so we have a three node heap. and then we move one more to the left, now we're looking at the r. again root of a three node heap may or may not be heap ordered, we do have to do the sync operation. in this case that brings the x up. a three node heap. now we go to two. now that's the root of a seven node heap. we know the two three node heaps that are the children are heap ordered but we may have to correct the heap ordering at the root so we do a sync on two. and that's going to involve, exchanging with the t, because t is larger than o. and exchanging with the p because p is larger than o. now that heap is a seven node heap that's all heap ordered, and then the last thing is to do the root of the whole thing and again, now the two sub trees are heap ordered, that's what we mean by bottom up, we took care of the heep ordering from the bottom up. and so we'll do a sync on the s and bring it into a heap ordering, so that's with just a few exchanges we got that whole array heap order, and now what we want to do is take advantage of the heap ordering in the array to do a sort. and the concept is very simple. right away we have the maximum element in the array right at the root, we want that to be at the end so that's what we're going to do and that's what we're going to do is just put it at the end. we exchange the element at the root with the last element. pull it off the heap and then that's our example. we might have violated the heap order condtion at the heap right now. so now we have to do a sync operation on, on the e. and so, it's larger than, it's both children, and the larger of the two children is t, so we promote the t. and the p is larger, the two children promote that and then finally, the e comes down to the bottom. so now that's one step in the sort, we got the largest element off. now the next largest element in the array is now at the root of the heap. we're going to do the same thing, exchange it with the last element in the heap. then now that t is in its final position in the sorted array, we take it off the heap. so now, we've got a heap with nine elements and two of the elements in the array are already in their final position. and now this one's not heap ordered, so we have to exchange over the largest of its two children. in this case that involves regarding the s and the r. now it's heap ordered. so that's the end of the two steps in heapsort. and then we just keep going. pulling off the largest element from the heap. exchanging it with the. element in the heap in the largest position in the array which brings that element into its final position in the sorted array. and then adjusting the heap ordering with the sync operation. so that e again is going to come down and now it only goes down one step in this case. so now r exchanges with m. it's in it's final position and you can see down at the bottom, the large elements in the array filling in, in their final position, in the, the left part of the array is representing the heap. the r goes off the heap, do the sync operation on the m, and now we have a heap ordered array. so now do the p, exchange that with the a. take it off the heap. do the sync operation on the a. now we're going to do the o. exchange that with the e. take it off the heap. do the sync operation on e which involves promoting the larger of its two children, until it gets to the bottom, or a place where it's larger than both its children. so now we have, just five elements left. we'll, get the m. do heap ordering on the, heap of four and that only involves one exchange. now we get the l. a exchange for the larger of its two children. while, they're both the same, so i t goes with the left one. that's the heap of size three. pull off the first e, it's already heap ordered. pull off that e. and, now we are left with only one element in the heap in this in the first position, so there is nothing to do. so with a series of in exchange and then sync operations, we pull the sorted array out of the heap. okay. this, slide summarizes the code for, heap construction. and as you can see, it's a one liner. we go backwards through the heap. starting at n over two because the, n over, half of the, right most half of the array is just little heaps of size one. we just go backwards doing a sync starting at k. so that's the first piece of code for heap ordering an array with arbitrary values and then these diagrams summarize the sync calls that, that we just went through in the demo starting at five, four, three, two, one. as you can see, only one, two, three, four, five exchanges are needed to bring this into heap order. then the second pass again that's only a two liner, we exchange the first element with the one at the end and then decrement the size of the heap and then do a sync operations. and these diagrams summarize the sync operations that we showed in the demo. on every smaller heap, now we continue just performing sync operations at the root until we get a completely sorted array. so given the sink implementation, we had done a one liner for the first pass and a three liner for the second pass so that gives a complete implementation of heap sort with the code that we have given so for, so far. there's is one little detail when you are sorting an array of course position zero comes into account and we've been building our heaps from position one. so, but we can take care of that in the less and exchange methods by just decrementing the indices in those methods to have it work as if the array were zero through n. that's a little implementation detail, but otherwise this is a fine sword implementation, that actually is very little code, and its got a place in, in the theory of algorithm, that i will talk about in a second. this is just another trace without the data-structure shown, to just show in our standard way, the elements in black and red are the ones that are touched and the elements in grey are the ones that are not touched at all. and to just show that this thing gets the sort done with touching relatively few elements. that's a trace. let's look at an animation, an animation with heapsort is interesting to watch so the construction of the heap happens in a blink and now it's pulling off the largest elements, moving from right to left. so again, a very efficient way to get a sorting job done. so what about the mathematical analysis? well the mathematical analysis, for the heapsort part is pretty easy. n times, we're doing a sink operation, and the size of the heap is at most lg n so it's n lg n. the construction, actually, it turns out although it's a little more complicated to prove, that it always uses just a linear number of comparison exchanges. and that's an interesting result in itself. you can build a heap from n values in linear time. and then, and then lg n more time. you can sort from that heap and that's significance be, significant because it's the first sorting algorithm that we've seen that is both in place. and manages to get the sorting job done with guaranteed analogs and compares. mergesort doesn't do that. it takes linear extra space. quicksort doesn't do that. it takes quadratic time in a worse case even though we make that unlikely by random shuffling. it still takes quadratic time in the worse case but heapsort does both. now there is more complicated versions of mergesort and quicksort that can do this in theory but heapsort is pretty simple algorithm that gets both done, so in a job interview somebody asks you what's an in-place sorting algorithm that's guaranteed n lg n? your answer's going to be heapsort. now in practice heapsort is actually not used that much for a couple of reasons. and they might ask you these on your job interview too. first thing is the inner loop is longer than quicksorts. like mergesort there is more things to do in the inner loop. there is that compare are the two children bigger, then compare. so there are two compares that get done at n lg n times. and then there is some that array index arithmetic. the other thing that is probably more significant on modern machines is. that the references to memory are all over the place when it's a huge array, so it's not a good algorithm for a situation where there's caching which is almost everywhere nowadays. it doesn't have a local reference, like quicksort does. it's always refers to something that's nearby something else that i just referred to. so if a big block of things comes into memory, there's no more extra costs, whereas heapsort is going to look far away from the current place as it goes down the tree and that makes it slower in a lot of situations. and on the other thing is it's not stable, sometimes people choose to use mergesort in practice because of the stability but heapsort isnot stable for the usual reason that it does long distance exchanges that might bring items that have equal keys back out of order. so that, that, that's our full summary of sorting algorithms to and completes our treatment of sorting algorithms with heapsort. and this is just adding the heapsort line to the table. it's in place we don't use any auxiliary array it's not stable, but its worst-case guaranteed time is proportional to n lg n as well as the average and, and the best this is not a result but that's also the case so it's n lg n guarantee n place, but it's not stable, and we still have the hope that someday somebody will develop a simple in-place stable worst case n lg n algorithm but we're not quite there yet. and that completes our treatment of sorting algorithms with the heapsort algorithm. 
now, look at an interesting application of priority queues that is actually representative of whole family of a critically important applications in applications of computing. it's called event driven simulation. and the idea is we want to study some property of the natural world by simulating it. and that's something that's very, very common in, in scientific inquiry nowadays. and this is a very natural idea. and actually, the idea goes back to einstein. so, we want to simulate the motion of n moving particles that might collide with the priority. this, this kind of stimulation is enabled by priority queues. and without something like priority queues, you couldn't do this for a large number of particles because it would require quadratic time and simply can't be afforded for a huge number of particles. so, and let's take a look at how we can possibly make this happen. so we use a simple scientific model called the hard disc model. and then, this is just for simplicity to get this done and just part of a lecture. clearly, these things can be extended in many ways. so, we're going to have moving particles that either collide with each other and with the walls. and each particle is a disc that's got known position, velocity, mass, and radius. and there's no other forces involved. it gets more complicated if there's more forces, like gravity involved. and this point by itself is very significant. as i mentioned, it goes back to the study of physics with [cough] the trying to understand the pressure and temperature in einstein's famous experiment on a pollen grain showing that their motion was brownian and random. so whether it's individual atoms and molecules or some bigger kinds of particles. it's a complex dynamic situation that is better understood through computer simulation. and nowadays that means priority queues. [cough] so, as a wa rm-up, here's code to implement bouncing balls without the collisions. and this is an elementary programing exercise that is the, the code at the left has the effects shown at the right. so, we have a data type called ball that represents just one of the particles and has instance variables that has the position and the velocity. so, that's why we make a bunch of them and then we have a, a while loop which is just every 50 milliseconds clear the, the whole drawing and then move the balls a little bit and then draw them in their current position. and then the only to move [cough] operation does is to update the position of the ball by the velocity, which is just another number and then it does the bouncing off the walls. if it happens to hit the left of the wall then you reflect the x-coordinate in the right wall, you reflect the x-coordinate bottom to top, you do the same for the y-coordinate. so, this the is an easy programming exercise given the right display primitives. and it's a good exercise in object-oriented programming showing how just one implementation then we can use that same implementation to simulate a number of instances. so, that's our starting point in terms of the code. so this is the implementation of the ball class. so, it's got position and velocity as i mentioned, and every ball has a, a radius. and then there is a constructor and maybe we have a constructor that takes arguments that would initialize the position and the velocity or maybe initialize them to a random position if there's no arguments. and then, here's the move method. and the move method again, most of the times, just takes the x and y coordinates and adds the current velocity times the speed constant. the dt speed, speed variable that's given as argument dt. and then these tests are for whether it hits the walls in which case, you have to flip the x or y velocity. and then draw, it's just using standard draw. just draw the ball. so, that's all the code for doing the bouncing ball simulation. now, what's missing in this is what happens when the balls collide with each other. and to cope with that we need to do both. a little bit of high school physics and a little bit of basic computer science. the physics problem is exactly what happens when two balls hit and they bounce off each other according to some well-understood physical process, and that's the high school physics. and the cs problem is how and when to we exactly do these computations for each of the balls. and how can we do it efficiently that is in, in log n time versus quadratic time. because if we have a computational process that takes quadratic time, then it's not going to scale, we're not going to be able to do large number of particles. simulations in the real world, usually, we wind up doing huge amounts of data and we cannot have a quadratic algorithm. this is just first indication of that of why if you want to do this simulation, you better know about some data structure like priority queues. if you try to do it without it, you're not going to be successful. alright, so, let's take a look at what happens. so there's a number of things that you might consider trying. so, one idea is the so-called time driven simulation. and we just say, we're going to update everything every dt seconds. then we go ahead and then we could check if there's a collision, if the two balls, pieces of the two balls are occupying the same space. and if there is, then we could roll back time just a little bit and i'll try to figure out exactly, the moment of which they collided and then figure out how the position and velocity should change accordingly and then continue the simulation. but this has a huge problem. the first one is that you have t o check all pairs of balls for overlap so that's quadratic, so it's going to be really, really lot of overall texture you're not going to be able to do it for a huge, huge value of n. but the other thing is even if n is small if you do a very small dt, then you're just doing this calculation over and over again and there's just too much computation moving the balls little bit at a time. on the other hand, if you try to improve things by making dt too large you might completely miss a collision as shown in the example at right. so figuring out the value of dt that would really work is a huge problem for the time driven simulation. instead, what we want to do is called an event driven simulation. and this is a very general concept that's useful in all kinds of context. and we are going to change things when something happens. so, since the only thing that matters is collisions, we are going to figure the particles move in a straight line, between collisions. and what we are going to do is focus only on the times when the collisions are going to occur. and the way we are going to that, is to maintain a priority queue and that priority queue is going to have all the possible collisions that could happen in the future and they're going to be prioritized by time. and when we remove the minimum element from the priority queue, that's the next collision that we have to deal with. and so we have two phases, we have prediction and resolution. so, that's sometime t, we can take two particles. we know their position and velocities shown at the bottom here and we can predict exactly the moment, which they'll collide assuming that something else doesn't happen to them in between and then so they will put that predicted collision time on the priority queue and later on, when that time comes to pass we will be right at moment when they collide and we can figure out what to do. now, there is a possibly that something else happened to t hem in between and we'll talk about that change, too. so, we have to do collision prediction, which is given position, velocity, and radius when's it going to hit with another particle or, or the wall. and then there's resolution which is to figure out how to change the velocities of the particles according to physical laws. now this part i'm not going to talk about in that much detail right now because it's high school physics. and so, i think most students have had high school physics and will be able to do, do this math or at least be convinced that the code that does this math is correct. so, if you know that you have a particle that's at a certain position or x or y and has got a certain velocity, the x in the x-direction and y in the y-direction, then you can from the distance to the pro, vertical wall you can figure out how many seconds this is going to take until it hits it. it's basically that distance divided by the by the velocity. and so that's the prediction. and then, the resolution. when it hits the wall is, is just going to change the velocity. so that's in, you know what the position is. so that's just an example of collision, of collision prediction, when's it going to hit the wall and resolution what do you do when it gets to the wall. when you have two particles there's definitely more math. and again, this is high school physics. and we're not going to test on it or even go through the details. but it's just a little bit of arithmetic with the velocities and positions to deal with what happens when, when how to predict when a given particle is going to collide with another given particle knowing their velocity and position. so, you have to take both velocities and divide their distance by those and, and so forth. so there's simple formulas to tell us what to do and we can also figure out the formulas for what we do o nce they do collide. and again nobody's claiming that this is easy but this is the physics part and it's worked out and it comes from newton's second law. and, and, anybody taking high school physics will, be able to deal with these formulas and the rest of this may have to go to a reference book to get up to speed on them. [cough] but once it's reduced to code we can be, it might have some trouble debugging at first but at least we can be convinced that it works. but now, let's look at the computer science code. so, this is just extending our ball data type that we use for the bouncing balls that didn't collide to take in, into account these extra things. so, ours will have mass, so there will be some big heavy ones that make things more interesting. and there's also a variable called count, which is the number of collisions of particles have been involved in. and that's useful for a couple of purposes. so, we're going to need a bunch of procedures which do the prediction and the collision resolution. i want, what's the, given a particle what's the time till we hit that particle? what's the time till we hit vertical horizontal wall? and the same thing is if we're at the point where we're hitting a particle, what would we do, the, the same way with the vertical and horizontal wall. so, that's the skeleton. we need those procedures that implement those physics rules for every particle. and, and this is what they look like and again this is high school physics so we're not going to do it in detail other than to point out it's really not a huge amount of code. lots of the xs and the ys and the vs but really not a huge amount of code. and the other point is we're going to return infinity if there's no collision at all so that it's going to keep, keep that on the priority queue, that ran on the priority queue forever. okay, so that's the procedures that we need and then they're similar ones for the horizontal and vertical walls. so now, let's look at the main loop for the event driven simulation. so, the first thing is we're going to for every particle we're going to compute the next time that it might hit every horizontal and vertical wall. well, actually if it's going away from a wall, it's not going to hit it so that would be infinity. but if it's going towards a wall, then we'll compute the time. and then that's a time in the future and we'll put that event on the priority queue with that time as the key. and then, we'll do the same thing for all pairs of particles. so, we do have a quadratic initialization phase that we perform just once to get the priority queue filled up. now, all collisions are, might not happen so we might have two particles that are on a collision course that and we're going to predict that point for both of those particles, you know, even right at the beginning. but it might be the case that there's a third particle that knocks one of those out before that thing happens and that event would be invalidated. so, the simulation has to be careful to take that into account. but that's not difficult to do. so, here's what the main loop is. so, we're going to take the next event from the priority queue. that's the next collision that's going to happen from all our calculations. there's one collision that's going to happen next. then, we test whether that event has been invalidated. and we do that by using that count field in the particle. so, then that tells us what time it's going to be next. so, then we have to go through all the particles and change their positions on a straight line trajectory, where would they'll be after that much time? then we have to take the two particles that collide and change their velocity. they bounce off one another. now those two particles' velocities have changed , essentially that invalidates the future collisions involving those. and then we, what we have to do is for those two particles is go through and predict the future collisions with any walls and collisions with any other particles. and put all those new events on to the priority queue. but that's it. you got two particles, change your velocities figure out the future collision of those particles with the wall and update the priority queue and then the main loop is take the next thing off the priority queue and keep going. that's the code that we'll look at next. so we have a, a, a bunch of conventions just to reduce the code. and if we this the, the thing called event which involves it says between two particles, something is going to happen at a certain time and we're going to adopt the conventions that, if, neither particle is null then we're talking about two particles. if one of the particles is null then we're talking about a wall, a vertical or horizontal wall. and if both particles are null we're saying we just want to redraw things. that's a bit of a hack, but it cuts down on a lot of code. our compared to is by time. and then again, we need an, is valid to check about intervening collision. and then here's the skeleton of what's going to happen with the collision system which is the key thing is this prediction method that takes a particle as argument, and adds to the priority queue, all the possible collisions involving this particle. so, it's going to go through every particle and call the time to hit method for that particle. and then, it'll put an event on the priority queue for that time, this particle with that particle. and then, it'll also put an event for the vertical wall and the horizontal wall, again, using this null convention to say that the event second argument null is vertical. first argument null is horizontal. so that's a key method t hat gets used in the simulation for each of the two particles that are going to collide. so, now we can look finally at the main event driven simulation loop. so there's build a priority queue. there's do this prediction for every one of the particles. and then, also we're going to put as the first thing that happened always a, an event that says redraw everything. so that's just a, a way of make suring that the simulation keeps proceeding. it's an easy way to get things drawn. okay. so, now the main loop is while the priority queue is not empty we're going to pull off an event. we're going to test whether it's valid. and that's just checker if anything happened with those two particles. we're going to pull off the two particles and then we're going to all, we're going to move all particles by the amount of time that has elapsed since the last event. and then, we're going to test which of the four types of events that it is. it's either redraw, bounce, b of a or, or bounce off a vertical wall or, or a horizontal wall. and then we'll go ahead and do the predictions of each of those particles, a and b, against all other particles. that's the pretty much all the code for the simulation. so this is data driven code. so, one thing we can do is just run it for a 100 balls in random position at random velocity. but what's nice about data driven code is now that the code's working and again we, we're not saying that this is a trivial code to write but it's definitely manageable. and it's enabled by priority queues. without priority queues, it would be quite a bit more complicated to figure out how to do this. and also, it wouldn't be reasonably efficient at all for large data sets. so, that's a, a simple simulation to just generate random positions. people might be interested in this one. now this isn't exactly precisely wh at would happen in the real world mainly because we didn't put in the simulation what happens when three particles are touching or there's two touching in another one hits them. and also nobody racks up a, a set of billiard balls such that all fifteen are touching in all places. so life can be complicating when you try to simulate the natural world. this is a little bit about einstein's experiment. if you got one big particle like a pollen grain and lots of little particles like atoms molecules and bouncing against it the big one is going to move about randomly. and then this is another famous physics experiment showing diffusion. and there's many other things that you can do with this basic collision system. if you have huge numbers of particles and you measure the number that hit the size and the frequency with which they hit they sides you can do experiments relating temperature and pressure and many other things or do three-dimensional versions. again simulation of the natural world is an increasingly important application of computing and need efficient data structures like priority queues to get it done. 
welcome back. in this and the next few lectures, we're going to look at symbol tables. a fundamental and extremely important data type that have led to all kinds of fascinating implementations and we're going to see at several of them in this course. to begin, we'll take a look at the api and some elementary implementations and various operations that people want to perform on symbol tables. start with the api. the idea behind symbol tables is to implement the following abstraction. we are going to have keys, like our keys in priority queues but, the whole ideas that we are going want to associate, values with each key. so two operations that we're going to perform in symbol tables is the insert operation where we're really putting a value, a key value pair into the symbol table, a value with a specified key, and then given a key we want to search for a corresponding value. those are the two basic operations. now the keys and the values can interchange roles. and that's, that's why we, have the abstraction to separate them. so for example, a domain name server might have a look up where you've got a table that's got an ip address, and a url associated with that ip address. and different clients might want to use this data in different ways. one might want to use the url as key. given the url, give us the corresponding ip. the address. another client, might want to use the ip address as key, have an ip address, give me the corresponding client. so those are just a couple of examples. this is a very fundamental and basic abstraction. in the list of applications is huge, in fact almost any computer application system is going to have symbol table or multiple symbol tables at its core, all the way down to the basic memory system of the computer or the networking system that your computer accessed information depends on. you can think of it intuitively, as, like a dictionary. well, there used to be books, and people would open up those books to look for a word, to find the definition. nowadays, you're more likely, to do that online. or, when you're trying to find a song to download, you provide the name of the song. and then a value will tell you what computer to go to, to get that or in commercial computing, the key might be an account number, and the value might be the transaction details, for that account. web search is something, we all do multiple times every day. and the key is a keyword, or a list of keywords. and the value is a list of places where that keyword is found. and there's many, many other applications, including scientific applications where say in genomics people use simple tables to keep track of finding markers in the genome and again many other applications. so, its a very fundamental concept and we will look at plenty of applications. but first we want to look at some algorithms so the way that it's convenient to set up a symbol table is to implement the so-called associative array abstraction. and the idea behind that is to think about just associating one value with each key. and well it's like in java array of integers say. we're only, our keys in that case are indices that are restricted between, to be between zero and the array size. but we're only associating one value with each index. we think of storing the value in the array position given by that index. and a good way to think of a symbol table is as shown in the right here. when we put a key value pair onto the symbol table. think of that as using the key to index an array, and storing the value there. now this, isn't legal in java if key is not an int. and, and we're going to do this generic. it can be any type of data. but, it's a good way to think about it. and then, to retrieve it, you just give that same key, and it'll return the value. so that's our, two primary operations. put a key value pair into the table. so that is associate the value with key, and then get the value paired with the key. now th ere's particular rules for null that i'll talk about in a second. and then to properly maintain the symbol table in a dynamic situation in many clients you want to support and delete operation and contains is, is simpler operation than depth. it's convenient for many clients where it just tells us whether there's some value paired with that key in the table isn't in size. and then another thing that you might want to do is iterate through all the keys in, in the table. so those are the basic operations that we're going to want to implement to get the associative array abstraction. and then there's many, many possibilities for clients and we'll look at some later on. now there is a couple of conventions around null. and these are not critical, but they make it bit more convenient for several implementations. so we are not going to allow null values, we cannot associate null with any key. and then we are going to adopt the convention that the get method returns null, if the key is not present in the table. and also the associative array abstraction is the put method, well, overwrite an old value with a new value. so these are our consequences. so, it's, the contains implementation is the same for all our, symbol typal implementations. if get returns, a non null value, then there's a value corresponding to that key in the table if it returns null. it's not get returns null keys not present. and the other thing that we could do is we can use null in some situations or temporary situations to implement a lazy version of the delete operation. we can associate the key with null internally and then a client won't know the difference whether that's in there or not. and some algorithms take advantage of the ability to use null in this way. these are just conventions and somewhat details but it's important to point them out at front. so now, we're going to want the value to be any generic type at all, but the key type we have to make some natural assumptions about them. and actually there's different assumptions that we make in our implementations depending on the application. though one of the most useful ones is to have comparable keys. just as in sorting algorithms we'll assume that the keys have values that have come from a total order. and we can use compare to, to compare whether one key is less than, than other or not. this is for two reasons. one is we can get more efficient app implementations if we can use the ordering of the keys to help us find our way around the data structure. and the other reason is that we can support a broader set of simple table operations that are very convenient for many clients. and it's very typical for keys to come from an ordered set. now, for example in the dictionary application or if keys are stings or numbers. or account numbers or many other situations. so if they're going to be comparable we might as well take advantage of it. both to get more efficient algorithms and to be able to take advantage of a broader set of operations. now in other situations, maybe they're not comparable. and all we're allowed to use is to, use the equals operation. that is everything every type of data in java has to support and equals operation that reads out to test whether they're equal. and there's another family of methods where there's no ordering. then there is a special method called hash code that helps us [cough] inject randomness into the process and that's built into java and also some classic algorithms depend on that. we're going to start out with the comparable mostly. and again, as with priority queues, the best practice is to use immutable types, and experienced programmers know this and it's not difficult to arrange for the natural types of data that people are going to use for simple table keys. unreasonable to expect the implementation to work well if the client can change the values of keys that are in the table. if you want that, you have to provide that as a specific operation. in the case of symbol tables , we are not going to do that. you have to remove it and put it back in. alright, so there's equalities. now, equality again we're getting into programming language issue but it's still it's important to be explicit about what's going on with, equality. how do we test if two objects are equal? so, the job has got requirements as for compared to in, here's the basic requirements about equals. there is a method that all java for equals, but the default implementation is simply to test whether the references are equal. are those precisely the same objects or not. usually in applications when we want to have something more general than that and have a concept of a value or like a key in our case. and then we want to know if two references refer to objects that have the same value and we want to call that equal, that's what equals is about. so anyway we're required to make sure that x is always equal to x and that, x = y is the same y = x, and if x = y, y = z, then x = z. so that means that mathematical terms equals is called an equivalence relation. and also no, no object is equal to null. so those are absolute requirements for java. and again, the default implementation is to check whether they refer to the same object. and that's rarely what we want. java systems programs maybe want that. but client programs usually have customized implementations that are based on comparing some sort of value and the standard built-in types of the java language are going to have those customized implementations and we can rely on them doing what we expect. if we're going to implement our own types and then use those types as keys and symbol tables you have to exercise a little bit of care and we'll talk about that briefly. say we have this simplified date implementation we talked about before it's a mutable type and every day it's got a month a day in a year. it seems like it should be easy to implement equals basically, we're just going to check that all the significant fields are the same. two dates should be equal if they have the same day, month, and year. and if any one of those are not the same value, then just return false. so that seems as if it should work. but that doesn't have all the characteristics that we need in a job implementation. and so all of this code in red shows a model for what you might do if you're going to implement your own type of data equals for your own type of data. so we shouldn't use it in connection with inheritance so we don't use inheritance that much so i won't talk about that the type of the argument in the equals must be object, do you think it should be date? and experts debate about that, and people who are interested can look on the web for that kind of date. if it is the case that you happen to be testing two objects that are the same object for equality, you might as well, optimize everything and just test that. if y is a reference that's pointing to the same object as this object just returned true because, if you're going to test the values they're going to have the same values anyway. and that's a good optimization for lots of, situations. why go through all that risk to that code if you know right away they're, equal. there's this test for null, that has to be there. and if not there can lead to nefarious plugs and, and, unusual problems. so on your equals test you'd better, test that, the client didn't give you null. they have to be in the same class. and while there's a couple of different ways to check about the same class, and that's another religious debate. that we'll ignore. we'll use, get class and that's something that's got to work or they'll get, they'll get an exception in this later code. because since y had to be an object, now we have to, cast it to a date. and then it better be the right class, or else it's not going to have these fields, that, we can test for. so, details but anyway, you can use this code as a model to implement equals for any data type that you might wind up using as a simple table key. okay so that's a standard this is just in words the standard recipe for user find, type optimize for reference equality, check against null. make sure they're the same type and do the casting, and then compare all the similar, significant fields. it could be that if one of the fields is an object, then you use that object's equals, which reapplies the world the rule recursively. and then if you ever feel that it's an array you can go ahead and try applying it to each entry. and there's implementations in java. you don't want to use a, a. = b. that checks if those arrays are the same objects. and that's not what you want you want to check that all the values are the same. and if it's array of objects you can see that testing for equals can actually involve a lot a code and a lot a cost. alright so and certainly you want to follow some of these best practices. so fields that are most likely to differ. those are the ones you might want to compare first. and your also going to want to make compare to consistent with equals. the rule generally if we're have a comparable types we'll use to compare to. if we don't have a comparable types then we'll use equals. okay, so now let's look at a couple of test clients before we look at any particular implementation. so this is a test client so symbol tables are st is the type, symbol table, they're generic on key and value. and so this, this statement builds a new symbol table with string keys and integer values that's going to associate integers with strings. and so what the test client is going do is going to a just go in a loop as long as standard n is not empty, and it's going to read strings, read a string off standard input, and then put it in the symbol table associated with the value i where did it appear in the value input. so this is an index same client where we associate each string with its position most recent position in the input. and, notice it's an associative array implementation so for example, we have two es and at the end e is a associated value twelve. the place where it most recently appeared. we could also keep these things in a bag and do a client that does all the positions that appeared. this is a simple indexing client that we use for our traces. [cough] for analysis for bigger problems we'll use a client called the frequency counter client. and so that one is going to read a sequence of strings from standard input and print out the one that occurs with highest frequency. so, so for this small data from the beginning of dickens' tale of two cities if we run a frequency count-, or the frequency counter client. and this first argument is just ignore words of fewer than this many letters. it'll say that the most frequent word where there's no word that appears more frequently than it which appears ten times. and we'll want this client to work well for huge data sets so liepzeig is a, a data set from the web of about twenty million words. about half a million distinct ones and in that corpus, the word government appears about 25,000 times. so, if you have a quadratic time algorithm for implementing simple tables or linear time for each operation. you're not going to be able to run this client in a reasonable amount of time for a big amount of data. so that's the client that we're going to use for analysis. here's the code for that frequency counter client. again, it's similar to the other one, we're creating a simple table that associates strings with integers. we take that command line argument which is the minimum length that we care about. we will read a new word. we'll ignore the short strings. just trap out if the word length is too small. and now the integer we are going to associate with each word is the frequency of occurrence of that word in a symbol table. so if word is not in the symbol table, we'll put it there with a frequency of occurrence of one. that's the first time we saw the word. if it is in the symbol table, we will just over write. the old value, which is st get word, with the new value, st ge t word plus one. so increment the frequency in the symbol table. so that's, read, this loop reads in all the data and associates each word with its frequency of occurrence. and then we'll have a client that uses the iterator, going through all the keys in the symbol table. it'll get the value associated with each key. and if that's bigger than the maximum found so far, we'll save that away. and then print out the, the word that occurs the, the most often along with its frequency. so this is a useful and non trivial client that's enable by symbol table. and, but it won't work well unless we have an efficient symbol table operation. and we'll use this client to compare different symbol table implementations. so that's the symbol table api. and next, we'll take a look at implementations. 
next we'll look at some elementary symbol table implementations. these are so simple that we won't go in too much detail, but still it's worthwhile to take a look at them to set the stage for the more advanced implementations we'll consider next. well, one thing we could do is maintain a linked list. we could keep it in order or keep it unordered. this version keeps it unordered. so we're going to have nodes in the linked list that have key value pairs. they have every key in the symbol table and a value associated with that key. for search, we have to, since it's unordered, scan through the whole list to find a match, a key that's there. for insert, i would also have to scan through all keys to find the place to update a value, if it's a value that's already there. and if there's no match, we could add it to the front. so here's our simple client for traces, so if we associate s with zero, we just had it. that's our one node linked list that's got that information. associate e with 1, that's not there, so we just add it to the beginning of list a with 2 r, with 3c, with 4 h, with 5 and so forth. so now when we associate e with 6, we have to search through the list to see if there's an e. and in this case, there is. and then we just update that value. that's the associative array abstraction. it's possible to implement symbol tables that allow multiple values with the same key and so forth. and that leads to different types of clients, different types of implementations. we're going to stick to this associative array abstraction and no duplicate keys in the symbol table, because it both simplifies implementations and leads to simpler client code. okay, x7 is a new value. a8, we found a in there and update the value 8. and then, m9 and p10, l11 are all not there and they go at the beginning. and then the last one changes the value at e, again, 12. so implementing this is a simple example of linked list processing, a slight modification of our stack and queue code. and we will skip the details. and just note that what's the cost of implementing this, well if there's been, if there are n things on the symbol table, you have to for both search and insert, look all the way through. and if everything's random, then on average you only have to look halfway through for a successful search. well, you still have to insert. another issue is, for many clients, if the keys are ordered, it's nice to be able to iterate through the symbol table and order. and this one, by definition, doesn't provide that. and this one just uses equals, so the keys don't have to be comparable for this. it just uses equals. so our challenge is to look for methods that give us more efficient implementations for these search and insert operations. and we've already looked at an algorithm that can do this, and that's binary search. so, for binary search, [cough] what we're going to do is use an ordered array and actually use parallel arrays, one for the keys and one for the values. and the idea is to keep the array of keys in sorted order. and then find the index associated with the key that we're searching for using binary search. and then use that index to get the value that's associated with that key, that's stored in a parallel array. and we looked at the binary search algorithm earlier in the course. and so, for example, if these are the keys in our symbol table and we're doing a search for the index where p is stored, we look at the middle. p is bigger than l, so we look to the right. look in the middle of the right half. p is less than r, so we look to the left. continue until we find p. when we find p, we return its index and we use that index to get us the value that we need. or another way to look at this is it implements the function, how many keys are there that are less than k. so, for example, for q, that's an unsuccessful search. you can figure out from the last index when you don't find your element that you're seeking. you can figure out the return value which is the number of keys that are less than it. so that's a trace of implementing binary search to find the rank of a key in ordered array. and again, for successful, you can use that rank to return the value. and if it's unsuccessful, you can use that rank to figure out where to insert the new key. [cough] all right, so this is the code for the get operation and this rank which is binary search. so this is precisely the binary search code that we looked at before. so let's look at the gap. so if the whole table is empty, return null. otherwise, we call rank and that gives us a number of keys less than the current key. and so, that is where we look to check to see if that key is there. if it's there, then we return the value with the same index and parallel array. if it's not there, then we return null, saying the key's not there. now, the problem with binary search is, well, not necessarily the problem, but the situation is that when it's time to insert a new element, we have to move everything larger over one position, just like an insertion sort. so if the table has a, e, r, and s, and we have to insert the value c, then we have to move the e, r, and s over one position to put the c. and then put the value associated with c. you'll have to do the same thing in the vals array. move all the values associated with those keys over in one position and put the associated value in. so this is a trace of what would happen for our trace. and again, every insertion involves making a new position by moving all the larger keys over one position. do the same thing in the vals array. [cough] and if it's changing the value associated with a key that's already there, then it's just a matter of finding where the key is and changing the value at that index. so from that trace, it's pretty easy to see what's involved for the code. and we'll skip that code and just take a look at the comparison between this elementary implementation for symbol tables with the sequential search in an unordered list. so, one thing is we're using a different key interface. we're taking advantage of the fact that the keys are comparable to give us an efficient search. we can do search in worst case, in average case, in time proportional to log n. that's what binary search provides for us. and this is a fine data structure for symbol tables where there is, [cough] that are relatively static, where the values don't change much, and most of the operations are search. it's hard to beat binary search. on the other hand, in a dynamic situation where there are a lot of inserts, this method is going to be problematic, because the cost of its insert is linear. it's proportional to n over 2. if you have a huge number of operations and every one of them is proportional to the symbol table size, then you're just not going to be able to support huge numbers of keys. what we want is sufficient implementations of both search and insert. those are elementary implementations. next, we'll look at more advanced ones. 
when keys are comparable and we can put them in order. we saw that we can use binary search to get an efficient symbol table implementation. but we can also provide a lot of convenient functionality for the client that's what we are going to look at next. so this is just an example of an application that might try to associate keys with values. an illustration of all the different operations that a client might want. so we already looked at the get operation so we might want to know what city is associated with the event that happened at time nine o'clock, thirteenth and so that should return that value. but there's plenty of other things that we might want. like, for example, what's the earliest time? that's the min or what's the latest time? that's the max. what about, being able to iterate between, among all the keys between two given times? that, certainly is convenient. then there's, what's the seventh largest times, that's select that like a median, it generalizes min or max? which key is that, happens second or seventh? so that's, order statistics, a dynamic thing what happened, whats the closest time, thing that happened just before, five past nine. certainly, plenty of clients might want, want that. so this one is, i've only got ten tickets to sell. so that's the cut off point for, selling, seven tickets that's the cut off point. for, anybody after that time doesn't get a ticket. and, and this one might be, if there's a time cut off. i'm not going to sell tickets to anyone that came after that time. and then the corresponding one is, what's the first thing that happened after that time? that's call in to the radio show, i'm going to take that caller, the first call that comes at nine:30. and so forth. so then see how many things happen between nine:15 and nine:25. and how many calls were there before nine:10:25? so you can see that there's, all of these operations are quite natural when we have the, table in sorted order. and that's what we have for our binary search implementation. so we can, implement these, efficiently and they are, convenient and useful for the clients. so typically for ordered simple tables, when keys are comparable will provide a much wider interface it's very useful for many clients. so we say that we're dealing with keys that are comparable by simply adding this extents comparable key to our declaration. same way we did for sorting methods. so all that means is that our implementations can use compared to but for the client it means that all these operations have meaning. give me the minimum key, give me the largest key, and then i can get the value associated with that using that. give me the largest key less than or equal to this key value or the smallest key greater than that key value, give me the number of keys less than that key. you can actually implement priority queues this way. delete the minimum key, delete the largest key. now usually we argue against why the interface is just adding operations to an interface, usually our reason for doing so is that we can't guarantee that all the operations can be performing efficiently. in this case, as we'll see, ultimately we have ways to guarantee that all the operations can be formed efficiently. and they're so convenient for clients. it's certainly worth adding them. so we have iterate through all the keys, and iterate through all the keys in a given range, and count the number of keys in a given range. all of these operations are very useful for clients and we'll see plenty of examples later on. so we have to greatly expand our, our table. what, what are going to be the cost of all of these things. and this is a big difference between the binary search implementation where the keys are kept in order in an array, in the sequential search implementation, when they're all in a link list. so, for example, to provide order and iteration, you have to get them sorted. and that's, going to be a lot of work, and take n lg n time. whereas binary search, you just iterate through, the things in order. the give me the seventh key we just go and look there, they are in order. rank operation, that is essentially what binary search provides. that's the, our basic implementation is providing rank. floor and ceiling that's again is an outgrowth of the rank operation. minimum and maximum well again those are like select. their just right there, you look at the first or the last. to insert or delete however takes linear time. to maintain the sorted array in dynamic fashion is going to take linear time you have to go through the whole thing. and so that's really are the key to thinking about what are symbol table and symbol tables in general. how could we guarantee that all operations are fast? binary research is pretty good but that's a major flaw. it can't maintain a dynamic table efficiently with binary search and that's going to be your focus in the next lecture 
welcome back. next we're going to talk about binary search trees, a classic data structures that'll enables us to provide efficient implementation of symbol table and out rhythms. [cough] let's look at the basic binary search tree data structure with heaps we talk about implicit representation of trees with an array. for binary search trees we're going to talk about actually explicit tree data structures. a binary search tree is a binary tree in symmetric order. let's look at the meaning of those words. so, a binary tree is an explicit data structure. it's got what we call nodes which contain information and every node's got two links to binary trees that are disjoint from one another, a left tree and right tree associated with each node. each node has a left tree and a right tree. links can be null. left tree can be null and right tree can be null or both. we refer to every node in the tree as the root of a sub-tree and [cough] referred to, the nodes below. each node is its children so this is the right child of the root. and that's a left link, and so forth. so that's the definition of a binary tree. a binary search tree, each node has a key and every nodes key is larger than all the keys in its left subtree and smaller than all the keys in its right subtree. this is a different ordering than we have to heap if we have a node larger than both its children, this one, every node is between the values, the value of every node is between the values of the nodes in its two subtrees. so the nodes to the left of e are smaller and nodes to the right of e are larger. [cough] now we're going to use these to implement symbol tables and there's values associated with each key when appropriate, we'll write the values in smaller numbers next to the keys. but usually, we're just going to worry about the keys and we'll keep the values, in the nodes along with them [cough] so that's binary search trees. a binary tree in symmetric order that's the data structur e that we're going to use to implement symbol table operations. so how are we're going to represent binary search trees in java? well, we're going to extend our implementations of linked list structures to have two references instead of just one. so first of all, is the, there's a node at the root. so, a binary search tree in java is just going to be referenced to a root node. and every node's got four fields, a key and a value, and references to the left subtree, that contains the smaller keys, and the right subtree that contains the larger keys. so, here's what the, [cough] code is based on. the, inner class that we used to implement nodes has, one, two, three, four instance variables. all of which are private as usual. a key of type key, a value of type value and then references to a left and a right node. for convenience, we'll provide a constructor that takes the key and value as argument and fills in the key and value instance variables then the left and right links are initialized to null. and our data structure then will be a root that points to a node in the tree and then that node will point to subtrees and that will be the data structure that we use for symbol tables. so here's the skeleton of our symbol table implementation. it's for comparable keys associated with values and those are both generic types. the only instance variable is a link to the root node called root. the inner class node is the code that was given on the previous slide, and then we'll need implementations of put and get, and we'll also look at an implementation of delete, and an iterator as well. so that's our skeleton implementation, let's look at the keys. so let's look at search first. so here's a binary search tree let's do a demo of what different searches will look like in this tree. so there's a tree so s is at the root everybody to the left of is less than s over to the right is bigger. so this is a dynamic data structure that kind of follows the same rule as binary search. so to look for a, to do a search for the key h in this tree, we start at the root and we compare our key against the key at the root. and in this case, h is less so all that says to us is that if h is in the tree, it has to be to the left cuz everybody to the right is greater than s. so we move to the left and compare h against the root of the left subtree. in this case that's e. now h is greater so that says we should go right. now we can pair h against the root of the right subtree of e, and that's r and it's less so we have to go left cuz everybody to the right of r is bigger and h is smaller. and eventually if the key is in the tree, we're going to hit it. in this case we, we find h as the left sub tree of r in [cough] that's a search hit and then for the get operation, we can return the value that's stored in that node along with the key h. what about an unsuccessful search? well the same rules follow. if it's in the tree, it's gotta be according to the left or right, according to whether it's smaller or larger than the key at the route. in this case, if we're searching for g, it's gotta go left, because it's less than s. when we come against the e, we gotta go right because it's bigger than e against the r, we have to go left, because it's less than r. we come against the h, we have to go left. and then we come off a null link, and all that says is that there's no place in this tree where g could be so g is not there. so that's a search miss. and the get operation would return null in that case. what about insertion? well, to insert a new key, all we do is follow the same steps as we did for search. that following off that null link and again, we'll just, for g, travel down the tree until we come to the, null link. really, what we're saying is when we go to the left link of h, it says, if g is in the tree, it has to be down this link. since it's not there to in sert g, all we need to do is just put it there and that's how we insert a new node into a binary search tree. alright, here's the code corresponding to the process that we just demo. and it's quite straight forward simple code as simple as binary search really. we start at the root then we set variable x to be the root and that's going to be the pointer to the current node as we move down the tree. as long as our, our current node x is not null what we'll want to do is a comparison between the key at node x and our search key. if our search key is less, we go to the left. if it's greater we go to the right. and if it's equal we don't even have to test that, that's why it's in grey. if it's not greater or lesser it has to be equal, than we return the value right at that node. if we get to the bottom and our current node is null and that's falling off the bottom of the tree we return null and that's equivalent to saying our buyer convention that, that key is not in our data structure, or not in our symbol table. so that's very straightforward implementation of the get operation for symbol tables with a binary search tree representation. now, what's the cost? well, we went down a path in the tree so it's one plus the depth of the node in the tree. [cough] so what about search well search for put there's two cases. if the if we're supposed to associate a value with a key. if the key's already in the tree then we're just going to reset the value. if they key's not in the tree then we add a new node at the bottom. so now it's a little bit tricky the way that we implement it since we're using we use a recursive implementation. and the reason we do this is that it generalizes to give us more efficient data structures later on. so, what we'll do is use a recursive implementation that as it moves down the tree it'll return a link up higher in the tree. and so when we insert a new node l say in this tree we go down that path, we create a new node and then return the link to that node higher up. there's ways to implement that don't involve this, but its, the code is so simple and it extends to more powerful data structures later on that we'll introduce this right now and, and you'll see how it works. so here's the, this is very concise recursive code but its tricky because of that last point so its worth reading carefully. so, we're going to use a recursive method put. that put a associate a value with a key in the tree. and that recursive [cough] method is going to return a node. so the client method put of course, just is supposed to do the association so it has a void return. but what we'll do is invoke a recursive method starting at the root and whatever link gets returned, we'll set that to root. so right away, we can see, let's suppose that we have an empty tree where root is null. so then if we put with null as the first argument, then null is the first argument. what we do is we say if, if the argument is null, return a reference to a new node that associates key with value and then that one has null links. so in this case, that first call will return a link and whatever link gets returned, that will be set to root. so, without any extra special code we insert a node into an empty tree. and that works, again, recursively say we have one node in the tree, and we have a new key to associate. and let's say that key is less than the key at the root. so, so now we do put in it's actually a link to that one node that's got two null links. so it's not null so we'll compare our key against the key in that node. if that comparison comes out left, here's how we do the insert. we change our left link which is right now it's null to whatever put returns. so what's put going to return? well, that left link is null, so what's going to happen is, in that call x is null. it's going to be cre ated a new node and the link to that node will be returned and that's the link that we'll put in the left. this is a very concise code that otherwise we'd have various cases about saving which link we went down in order to reset that later on. so now the best way having looked at those small examples, the best way to understand this code is recursively. let's believe that it works for small cases which we have just done. so, lets see what the code does. so if x is null, we want to create a new node and return the link to that node. so, even if it's a huge tree down at the bottom, we just came of a null link. we just want to create a new node with our new key and return a link to that node, that's all we want to do. now, we can assume that put is going to return a link to a sub-tree that contains our new key and if our new key is smaller than the key at the node that, that we're processing now, then [cough] we want to insert the new key value there and the new node on the left otherwise, we want to insert on the right. most of the time, the link that we get back will be same as the link that we put in but for the bottom node it will be different. so, if put works properly inserting a new node on the left, then that's what we want our left link to be. if it works properly, putting in the subtree on the right, that's what we want our right link to be. and by the way, if we find a key that's in the tree already, then we just want to reset the value. and in all of these cases where we're on a node that already existed, we just want to return the link to that node. again, when we look at more sophisticated values we'll be returning something else. so it's worthwhile you know, checking that you believe that this code implements the simple binary search tree algorithm that we demoed where when we fall off a null link we created a new node and replaced that null link with the new node . so that's insert for a binary search tree in a symbol table. and again, the cost of this is the number of compares is equal to one plus the depth of the node. we just go down a path in the tree. now, what's interesting about binary search trees is that there are many different binary search trees that correspond to the same set of keys. so the number it compares is going to depend on the order in which the keys come in. and that's a key feature of binary search trees that we'll come back to again when we look at more sophisticated data structures. so it depends on how the keys come in. the shape of the, of the tree could be well in the best case so it would be perfectly balanced. and one of the things we'll look at is algorithms that come very, very close to achieving that goal. the typical, typical case it'll be sort of balanced. now but one problem is if the keys come in and, and really unfortunately, if they come in, in a natural order. like if they come in, in order, that's the worst case. we don't get any benefit from having it in a tree shape. it's no different than a link list. so we'll, we'll come back to dealing with that worse case in the next lecture. but the point is, the tree shape depends on the order of insertion. now, but let's look at what's happened or visualize what happens when keys come in, in random order. so the tree grows from the bottom in the little side to side motion it's just accommodating room for each new key as it's added. but you could see that even for this case which is hundreds of keys, the length of the path from top to bottom is not so much. in this case, the maximum distance from the top to the bottom is sixteen the average is only nine and the best you could in a perfectly balanced tree it would be seven. so it's pretty well balanced which means that our search and insert cost in this case for 255 keys is only going to be sixteen quite a bit less. so one remark before we do the analysis is that actually binary search trees correspond exactly to quicksort partitioning. in the binary search tree, we have a node at the root and we have everybody smaller to the left, and everybody larger to the right. in quicksort partitioning, after the random shuffling we have the partitioning element and then we process everybody to the left independently of everybody to the right. so it's a [cough] there's a direct correspondence. if there is no duplicate keys quicksort processes them and referred them out in bsts and if there's no duplicate keys there's a one-to-one correspondence between what happens with quicksort and what happens with binary search trees. and we point that out because that helps with the mathematical analysis. in fact, this correspondence with quicksort partitioning tells us we can take that proof and prove that if you insert in distinct keys into a bst, in random order, then the expected number of compares for a search and an insert is two natural log n. so again about 1.38 log base two of n almost the best that you could do. it also has been shown actually not that long ago, that the expected height of the tree if they're inserted in random order, the height that's the worst case length of a path in the tree. this is the average path in a tree, this is the, the worst of all the keys. this is about four natural log n. so, if you have the keys in random order the binary search tree gives efficient search and insert. now but there is this problem that the actual worst case height if the keys come in, in order and reverse order and other natural orders that the time could be proportional to n. so, we have this summary which is looking pretty good, because we have the average case for both operations, the search and insert, to be 1.39 log n and that's probabilistic if they are in random order, its extremely likely to be there. but the problem by comparison with sorting is, we don't get to randomize the order the client is providing the keys. so we're going to need something better to provide the guarantee than just randomly ordering the keys. that's what we're going to be looking at next when we look at more efficient algorithms. but first we're going to look at the implementation of order and operations with the binary search tree structure. it expands like binary search to handle all these convenient client operations in a very natural manner. that's the implementation of binary search trees for symbol tables. they give sufficient implementation of both search and insert. 
now, we're going to take a look at ordered symbol table operations using the binary search tree data structure as the underlying implementation. well, each one of these operations are fairly straight forward but just to check our ability to manipulate this data structure, we'll take a look at each. suppose, we wanted to find the minimum key in a binary search tree, or the maximum key. well, just looking at one example you can see almost immediately what to do to find the minimum, we move left from the root until we find a null key, that's where the smallest key in the data structure is. to find the maximum, we move right from the root, until we find a null key. what about floor and ceiling? well, those are a little bit more complicated and we'll have to, not quite the same as in the ordered array for the binary search so we have to do a little bit more work for that. so just for example, let's take a look at the problem of computing the floor. so, what we want to find is so say, we're seeking the floor of g. so, that's the largest key in the data structure that is less than g. in this case, the answer is e. so let's just take a look at what we have to do in the tree, the path we have to take in the tree to figure that out. well so, we are looking for the largest key that's less than g. and have s well, that key is definitely going to be in the left subtree. its not going to be bigger than s because s is bigger than g so we go to the left. so now, we are sitting at e. and so what's the largest key that's less than g in this, in this tree here. well, it might be e but there's no way it's to the left of e because those keys are all smaller than e and therefore smaller than g. so, e is a key candidate. but it might also be in the right so we move to the right in this case. alright [cough]. so that's [cough] if k is equal to the key at the root, the floor of k is k. if k is less than the key, it roots i n the left subtree. that's the one we just did. if it's greater than the key at the root. the floor of k is in the right subtree, if there is any key smaller than k in the right subtree. so, in this case, there's no key smaller than g in the right subtree. so therefore, the answer is e. so, our code has to check for these three cases. and here's the code that does it. it's not that much code. it's just complicated code to understand. so if we find our key, that's the floor. if we're going to the left we find the floor, the one on the left. and in on the right we have to do a, a little bit of tricky code to make sure that we return the floor on the right subtree, if there's some tree there. if there's, if there's no node there then, then, then we, we return the root itself. so, that's a, a implementation that, that code is definitely tricky and a similar code for ceiling. so now, what about operations like rank and select? how many keys are there less than a given key? and, give us the seventh largest key to facilitate implementing those operations and also size all we do is keep an extra field in each node, which is the number of the nodes in the subtree rooted at that node. so, this tree has got eight nodes in it. this subtree has six nodes in it and so forth. and those counts are going to not only enable us to immediately implement the size function, just return the count at the root but also, they'll give us good implementations of rank and select. so, let's look at those now. so, we add account field to every node and then to implement the size function well, if it's null, we return zero. so a client might call us for a null tree or [cough] or an empty tree. otherwise we return, x.count, which is the number of nodes in that, in that subtree by definition. the way we maintain, there's a number of ways we can maintain the thing but the one that we'll adopt un iformly because it adapts to more complicated situations is just before we're done with the put operation we'll say, okay we've done all our work and before we return the pointer to the given subtree we're going to take the size of what's on the left and the size of what's on the right and add one for us and that's going to be our count. so, whether or not there was a new node added we don't have to test for that this recursively takes care of the problem of maintaining the size in every node when there's a new node inserted. and, it also handles more general situations, as we'll see later on. so, that's how to maintain size. so now, how do we implement rank? well, it's a little like floor. it's an easy recursive algorithm, but there are three cases. so let's look at the, at the three cases. so, we want to know the number of keys less than k. so [cough] we're going to have a recursive algorithm for our given key. so, let's, one of the easy ones is, if our key is equal to the, if were [cough] to the, the key at the current node then the number of keys less than our key is the size of the left subtree of that node. so, if we're looking for the rank of e say, how many keys are there less than e there's exactly two, that's by definition in the data structure that's the number of keys that are less than e. so, that's that one for rank. what about the [cough] starting at the root if we have the case where e is less than s. so, the rank of e in this whole tree is the same as the rank of e in the left subtree. so, there's that case and then if we're going to the right, then we have to add one for the root and one for the left subtree of the root and then find the rank of us on the right. so, that's an easy recursive algorithm for finding out the rank. and it's definitely an instructive exercise to check that you believe that, that method works. the other thing we h ave to do is iteration. and iteration is a fundamental operation on tree structure. and it's based on so called in-order traversal. and that's also a simple recursive algorithm. traverse the left subtree enqueue the key, traverse the right subtree. so, to iterate we're going to maintain a queue of keys. and then, we're going to call this recursive in-order [cough] method. and that method is going to add all the keys in the tree to the queue. and then we'll just return that queue. and that's, a queue is an iterable data structure, and the client can iterate that. and, in order, it's just a simple recursive method. put everybody to the left on the queue then put the root on the queue, then put everybody to the right on the queue. and to believe this method, you just have to think recursively and prove by induction that this in order method puts all the keys in the data structure on the queue in their natural order. first, it puts all the ones to the left on the queue. if that, that happens in their natural order, then the next thing that has to appear is the key at the root. and then if the ones on the right go in their natural order, and then, by induction, they're all in their natural order. that's a very simple implementation of an iterator for these symbol table with comparable keys. so we have to again prove that property by induction. and that's easy to do. the diagram at the right gives another simple way to look at it pictorially. all the keys that are smaller on the left we are going to put them out, and then we put the key at the root and then we put all the keys on the right out in order. and then that key is going to have all those keys in order by induction. so, here's the operation summary for ordered symbol table. and the quick summary is that every one of those operations, while ordered iteration is optimal, it just gets them in linear time. and all the res t of'em take time proportional to the height of the tree. now, if the, the keys are inserted in random order, we know that height by analysis, is going to be proportional to log n. or if it's some application where the order of insertion of the keys is well modeled by random order and that's not unusual at all. a binary search tree is a simple and extremely effective data structure that can support all of these operations in a quickly, much better than binary search in an ordered array which is not dynamic and slow for insertion. so, that's a look at binary search tree implementations of ordered operations when keys are comparable. 
if our symbols table's really going to be dynamic, we need to be able to delete key valued pairs from the table. as we'll see, all symbol table implementations have, lead to complications when we try to do this operation. binary search trace is our first example. so we need to fill in this one table. what's, what's the cost of deletion in a binary search tree? how are we going to really do that? well let's take a look at a very lazy approach which we set up for in our basic conventions for simple tables. what we can do to remove a node with a give key is just mark it with a tombstone. say, well, we'll leave the key in the tree to guide searches, but we won't, count it as being in the symbol table. and actually you can, make some progress with this kind of method. leaving tombstone throughout the tree. and, make sure that you keep, as long as there aren't too many deletions, you can keep the search costs, and deletion and insert costs to be logarithmic. but it definitely becomes, inconvenient to manage large numbers of tombstones in highly dynamic situations with large numbers of keys and values. eventually you're going to get an overload of memory and you're going to have to rebuild the thing or clean out the tombstones in some way so we need to look for a better way. this is a general method that people often use on all different types of implementations, but in modern systems it's rather unsatisfactory. also got a simpler problem. what about deleting the minimum? well actually, that's maybe not too difficult to delete the minimum in a binary search tree. again, we just go to the left until we get a null left link. and then, what we can do is just return that node's right link then that old node, nobody's pointing to it, so it's available for garbage collection. and then we use our usual trick of returning the link that we went down to update the other links after the recursive calls. and also we have to update the counts, something happened down below and we used that code to update the counts, in a co nsistent way, so this code implements deleting, not too bad at all. if x. left null, return x right. otherwise x left = delete min x left. and then when you're done with that, it fix the count so maybe a node got deleted down there, but always, the invariant is that the count of the node is one + size of the left and right. and then return x and fix the links and the counts on the way up. that's a fine implementation for delete min, and it also works for delete max. and that's the basis for a general method for deleting nodes from bsts known as hibberd deletion. so, that's the second case, the first case for hibberd deletion is what we want to do to delete a, a node with key k, is we search for the node that contains the key, and the easiest case is that node has no children. so to, to delete a node that has no children just return null. and then go back up to update the counts as usual, that's the easy case. the next most difficult case is like the delete min case we find a node t that contains our key, so like deleting r in this tree, it only has one child. just go ahead and return the link to that child and that updates the link and everything works fine and then the node that the leads that available for garbage collection that nobody's pointing to it. and then again update all the accounts after the recursive calls. zero children no problem one child no problem. the problem is what happens when there is two children. so say we want to delete node e in this tree. we have only one link, and we can get rid of the node but we have only one link pointing to it. but we have two links pointing down from it. so what are we going to do with those two links? well the hibbard deletion mechanism which is pretty old. 50 years ago, it was proposed. says, go ahead and find the next smallest node in the right subtree of that tree. so in the case that's h and what's that node? well it's the minimum in t's right sub tree. and we know how to delete the minimum. so we just find that minimum node and, in this case it's h, and we put that node in t spot and then delete the minimum. so, find the h that's the minimum, hang on to it, and then delete the minimum nt sub tree and then, so we take the e, replace it with the h, delete the h, and then everything is fine. it is still a bst. so we, essentially we are finding a node that has only one link, deleting that node, and then replacing the node that we need to delete with that one. that's hibbard deletion. it's a little bit asymmetric. why are we using the successor and not the predecessor? no real reason. and it's not really satisfactory because of that, and we'll come back to this but it works. so this is the code for hibbard deletion. so we search for the key. if it's got no right child, we're fine. we just return, x out left, and that handles both cases zero and one. if it does have a right child, then we do this, find the minimum, on the right. delete min on the right and then fix the links and then update our count that covers all cases. so actually not that much code, it's complicated but not particularly more complicated than other code we have seen like rank and floor and ceiling and that implements hibbard deletion. so now we have a fully dynamic symbol table where we can insert and delete. the number of nodes that we have in the tree as always proportion to the number of key value pairs in the symbol table. and the problem is, and this was quite a surprise when it was first discovered actually many years after hibbard proposed the algorithm is this lack of symmetry that tends to lead to difficulties. and here we are just inserting the leading alternating insert and delete a random key. so that maybe well models a situation, practical situation. and as you watch it go for awhile. you can see that this thing about going to the right and taking the successor all the time. the tree's becoming much less balanced than it was. and, this seems to be a, a problem. we can't be having, supposedly having a dynamic situation, that is going to, allow support of lots of different inserts and de letes. and in the end, win up with a less balanced tree. what's worse, if you, so how you are going to fix it? at the end researches show that after, sufficiently long sequence of random inserts and deletes, the height of the tree becomes square root of n not lg n. square root of n is usually bigger than lg n. it might make difference between acceptable and unacceptable performance in real applications. then what's worse is you try to fix it by say randomly choosing between the left and the right, that doesn't work, it still becomes square root of n. and that's a very long standing open problem to find a natural, simple, efficient delete for binary search trees. that's another one like merging in place, that you think there ought to be another easy way to do it, but in 50 years no one's really discovered one. now we're going to look at something pretty close in the next lecture, but here's the situation that we're left with. we have a binary search tree algorithm, which is fine in that gives us lg n performance for search and insert, in a situation where we can think that these things are happening randomly. but we're kind of stuck if we allowed delete. in fact everything degenerates to square root of n, and we also have a problem with, with the worst case if the keys happen to have some order in them our trees are not going to be balanced at all. and that's going to make the difference between acceptable and not acceptable performance. what we're going to look at next time, called a red black binary search tree, will guarantee logarithmic performance for all operations. so that's extremely significant and much better then binary search trees but the delete operation for binary search trees shows us the kind of complexity that we can encounter with working with these kinds of data structures. 
welcome back, today we're going to talk about balance search trees, which will lead us to an ultimate symbol table implementation that can provide fast performance for all the simulative options we've looked at, guaranteed. so here's the review of where we were with single tables. we took a look at the last time at the binary search tree, which if things are well modeled by random exertions, have a great performance. they get search and insert on in time proportion for log base two of n and they support ordered operations. but really our goal is to have these operations be guaranteed to take time proportional to log n, because we don't have control over the order of operations and they may not be random at all. and in fact, in many real applications, they're not very random. so that's what were going to look at now is try to find an implementation that can guarantee to be fast for all the symbol table operations. . that's our challenge. so what we're going to talk about to do it, is an algorithm, that actually pretty old algorithm called 2-3 trees, and a particular implementation that requires very little code, called left leaning red black bsts and then we'll talk about a generalization called b-trees. and these methods are all widely used throughout our computational infrastructure. to start, we'll talk about 2-3 search trees, which is a model that underlies the concise and efficient implementation that we're going to look at. so, the 2-3 tree is a way to generalize bsts to provide the flexibility that we need to guarantee fast performance. and the idea is very simple, we allow one or two keys per node. that is, we allow for the possibility of something called a 3-node that can hold two keys, but then it has to have three children. in a regular bst node, the 2-node, we have one link for the keys that are less than the key in the node, and one link for the keys that are greater. in a 3-node, we need three links, one for less, one for between and one for greater. another property of these 2-3 trees is that we are going to have perfect balance, that is every path from the route to a null link is going to have the same link in the 2-3 tree. so, as i mentioned, the symmetric order is part of the definition of a 2-3 tree. every 3-node has three links and two keys. the left link is for the keys that are, points to a 2-3 tree with the keys that are smaller than the smaller of the two keys in the 3-node. the middle link points to a 2-3 tree that contains all the keys that are between the two keys. and the right link points to all, 2-3 tree containing all the keys that are larger than the larger of the two keys in the 3-node. okay, let's take a look at a demo of searching in a 2-3 tree. so say we have this 2-3 tree here and we want to search for whether or not h is one of the keys in the tree. so we start at the root, compare the search key against the key or keys in the node. and follow the link corresponding to the interval that we know must contain the search key by definition of the tree and then we recursively continue the search. so, if we're looking for h, it's less than m, so the only place it could be in this 2-3 tree is in the 2-3 tree on the left link, so we follow the left link. now, we compare h with e and j, and in this case it's between, so now we are going to take the middle link, that's the only place that h possibly could be. in this case, that node, one node 2-3 tree contains h, so that's a search hit. let's take another example for unsuccessful search, a key that's not in the tree. as usual, we start at the root. it's less, so we go left. and it's less than both keys, so, if it's in the tree, it would have to be in the left link and it's between those two keys. so if it's in the tree, it would have to be on the middle link. and that link is null, so that's a search miss. so the search is a natural generalization of the search in, binary search trees. now what about inserting? well, it's a similar type of strategy as with regular binary search trees, except that we manipulate the two and 3-node to keep perfect balance in the tree. so the easy case is if the key winds up in a 2-node at the bottom, like this one. suppose we're inserting k. k is less than m so we go left. k is greater than both the keys, so we go right. k is less than l, so the search ends at the left link of l. and to perform an insertion all we need to do is replace that 2-node with a 3-node containing k. now, k is inserted into the 2-3 tree and it satisfies all the rules. now if we're inserting into a 3-node at the bottom, we have to do more work. and specifically, the work we do is, we add the key to a 3-node to create a temporary 4-node and then split up that four node and move it's middle key into the parent. so, let's see an example. if we are going to insert z into this tree, it's greater than n, so we go to the right. it's greater than r, so we go to the right. and now it's greater than x, and that's a null link to the right of x, so the search ends there and are, what we want to do is insert z into that 3-node. and the way we do it is, first make a temporary 4-node that replaces that 3-node. and then that's not a 2-3 tree cuz it's got that one 4-node with three keys and four links. but what we can do is split that 4-node and pass the middle key up to its parent. so split into two 2-node and pass the middle key to the parent. that's kind of a magical operation and believe me, it's easier to get done in the implementation than the graphics. but now you can see that, that local transformation on the 2-3 tree completes the insertion. now, if that parent were a 3-node, it would become a temporary 4-node and would continue the process moving up the tree. that's a demo of search and insertion in a 2-3 tree. so, let's look at a double split like that. so, say we're inserting l in, into this tree. so, it goes down to the middle, and winds up needing to be inserted in the, 3-node in the middle. so we're going to convert that into a 4-node. now, l is the middle key of that one, so we're going to split that 4-node into, two 2-nodes and move l to the parent. the 4-node had four links, and the two 2-nodes have four lengths, so nothing has to be changed below. and then this insertion into the parent changed it from a two, a 3-node into a 4-node essentially adding a length cuz of the split with the two 2-nodes where there was only one 3-node before. but now, that's not a 2-3 tree, so we have to split again. and in this case there is no paren, so we create a new one and the height of the tree increases by one. that's the only time the height of a 2-3 tree changes, when the roots splits the height introduces increases by one. so, that's a demo of insertion into a 3-node at the bottom, in a 2-3 tree that percolates all the way to the top. now let's look at constructing a 2-3 tree from an initially empty tree. so if we start by just inserting a key, well, that just creates a 2-node containing that key, and that's legal 2-3 tree, so we're fine. now, inserting e into that well, it's going to b if it's in the tree left of s, that's a null lin. so we need to convert that 2-node into a 3-node. okay? and that's the legal of 2-3 trees, so we stop inserting a into that. we convert that 3-node into a temporary 4-node, but then we need to split that 4-node moving e to the parent and that creates a new, root node and increases the size of the tree by one, but now that's a legal 2-3 tree so we stop. insert r into that, it goes to the right of e. convert into a 3-node, now insert c into that. it goes to the left of e, has to be joined with a into a new 3-node. again, that's a legal 2-3 tree and we stop. now we insert h, that kind of goes to the right of e. that 3-node gets converted into a 4-node. that's a temporary 4-node and we split and move r to the parent, now that parent's a legal and there's nothing more to be done. we, have a legal three tree, 2-3 tree. insert x, it's bigger than r, goes to the right. there's a 2-node, there's room for the x. insert p, that goes between e and r. the 2-node containing h, right link is null, so we convert that 2-node into a 3-node and now we have a legal 2-3 tree. now, you, you can see this next insertion is going to cause some splitting wherever it is. so insert l, that's between e and r. so it goes in the, 3-node containing h and p and we convert that into a temporary 4-node. split that 4-node, moving l to the parent. now that parents of 4-node and that has to be split, and we create a new root node. and then the height of the tree grows by one. and that's a legal 2-3 tree, so we stop. so, those local transformations, converting a 2-node to a 3-node or converting a three to a four, and then splitting and passing a node up. those are the, only operations we need to consider to get balance in our search trees. alright. so as i've mentioned and this diagram shows, the splitting of 4-node and a 2-3 tree is a local transformation. it only involves changing a constant number of links. so, in this example, it shows the general situation, when the 4-node to be split is the middle length, but the same is true if it's a left or right. and those six subtrees drawn could be huge. they could contain millions of keys, but it doesn't matter what they contain. we don't touch them at all, nor do we touch anything above this node in the tree until the split happens. so the transformation that splits that b, c, d, node and inserts the c into the 3-node at the root, just involves, making that 3-node into a temporary 4-node. and making that, 4-node into two 2-nodes and adjusting the lengths appropriately. just a constant number of operations and that's why, this operation, is, in general, efficient. so let's look at the just the global properties that these manipulations preserve. the two things that are critical is that the, in a, in a 2-3 tree, we always have symmetric order. that is the word that we defined, for 2-nodes and 3-nodes, and we also have the perfect balance. the distance from the root to the bottom is always the same. and to prove that, we just need to show that each transformation maintains symmetric order and perfect balance, and these are all the possible transformations that we could do. if we split the root, then, that's the, what happens at the root, and if there was perfect balance before, there's perfect balance after, with the height one bigger. if the parent was a 2-node then the transformation is a local transformation and if you look at where the links are, then it's easy to see by induction that if there was perfect balance before there's perfect balance afterward, because we didn't change anything about the perfect balance in any of those subtrees. and that's true in every case. if the 3-nodes at the right and this one is one higher and those four are one lower and afterwards it's the same. if there was perfect balance before there's perfect balance afterwards, because we didn't change the height of any nodes. we just moved things around locally within nodes. and this is when this parent is a 3-node, then there's the tree cases, if we split up the last split at the middle and split at the right, and again, changing the four node to, to a 2-nodes and adding links. if there was perfect balance before, there's perfect balance after, because we didn't change the heights of anything else in the tree. so our operations maintain symmetric order and perfect balance in a 2-3 tree. so, that's going to give us, a very easy way to describe a performance. the call, or operations have costs that's proportional to the path link from the, height to the bottom, and every path from the root to a null link has the same length. how long can those paths be? well, it's not hard to see that the, in the worst case, if they're all 2-nodes, that's the longest they can be is log base two of n. now, and if they're all 3-nodes, it would be log base three of n, which is less, it's about 0.63 log base two of n. so, all the paths in a 2-3 tree with n nodes have to have length between those two bounds and those are pretty small numbers. for a million nodes, that's between twelve and twenty. and, if, if it's a billion nodes, that's between eighteen and 30. those are remarkably small numbers, so we're going to have guaranteed performance, even for huge databases, we're going to be able to guarantee that we can get search and insert them with just eighteen to 30 operations and it's quite remarkable, really. so, here's what our table, will look like, when we finish the implementation of 2-3 trees. every operation is guaranteed to be a constant times log n. now, the constant depends on the implementation, exactly what kind of manipulations we need to do to convert, 3-nodes to 4-nodes and so forth. but it's, easy to see from demo and from the diagrams that those are going to be constant, guaranteed logarithmic performance for all operations, which is certainly what we want in a symbol table implementation now what about the implementation? well, we're actually not going to talk about a direct implementation of 2-3 trees, because it's kind of complicated. it's cumbersome to maintain multiple node types. you might need, a multiple compares to move down the tree. if there's a two, a 3-node, it takes more compares than a 2-node, so, it's complicated to analyze. we have to take track, keep track the links as we go up and down the tree to take, handle the splitting, and there's, and there's a lot of cases. i drew all the cases and, and, there's a, whether you're splitting into the middle of a 4-node or the right of a 2-node, there's just a lot of cases. so, you could do it but we're not going to because there's a much easier way. so that's 2-3 trees, a, a model for implementing balanced trees in guaranteed logarithmic time. 
now, we'll look at red black bsts which is a simple data structure that allows us to implement 2-3 tree with very little extra code beyond the basic binary search tree code. so this is, and actually the version that we're going to, looking at is called left-leaning red-black bsts. on a personal note, i wrote a research paper on this topic in 1979 with leo givas and we thought we pretty well understood these data structures at that time and people around the world use them in implementing various different systems. but just a few years ago for this course i found a much simpler implementation of red-black trees and this is just the a case study showing that there are simple algorithms still out there waiting to be discovered and this is one of them that we're going to talk about. so, the idea is that we are, are going to represent every two, three tree as a binary search tree. and in order to get that done, we just need a simple representation for three notes. so, what we are going to do is use internal left-leaning links to glue the three nodes together. so, the larger of the two nodes in the tree node will always be the root of a little binary search tree of size two for the three node and the link between the two nodes, the left link that links the larger key to the smaller one we'll color red. and that's to distinguish those links from the other links in the binary tree so that we can tell when we're inserting things which nodes belong to tree nodes and which ones don't. and you can see from this transformation that it's easy to perform this, see this correspondence the middle link between a and b, those are the keys that are less than b and larger than a. so, that takes two comparisons to get to them the ones that are less than a, less than, less than a, that's two comparisons for that and the ones that are greater than b are the right link of b. so, just following those three cases, i see t hat this correspondence is going to work. so, any 2-3 tree corresponds to a left leaning red-black bst in this way. just take the three nodes and split them into little binary search tree of size two held together by a red link. and correspondingly given a red-black bst then you can get the 2-3 tree if you wanted it. but just look at the properties of looking at the properties over a left leaning red-black bst. you know, with reference to what we know about 2-3 trees. first of all no node has two red links connected to it cuz the only red links are internal to three nodes. and those have to have ex, external links or tree links connecting them to some other node. every path from the root down to a null link has the same number of black links that just follows directly from the corresponding property for 2-3 trees. a left-leaning red-black bst has perfect black balance. and all the red links lean left. so, given a bst with some of the links colored red that has those properties that's going to correspond to a 2-3 tree. and that's a key property is this one-to-one correspondence between 2-3 trees and left-leaning red-black trees. given a 2-3 tree, we saw how to do it. given a, a, given a red-black tree, we just make the red links horizontal, and merge the nodes together to be three nodes. so, all of the operations that we're going to look at for red-black trees can be understood in terms of the corresponding operations on 2-3 trees. now the first, and really one of the most critical observations, is that search in a red-black bst is exactly the same as for an elementary bst, we just ignore the color. now, it's going to be much faster because of better balance in the tree, but in terms of the code, we don't have to change the code at all. our regular search code doesn't examine the color of a link and so we can just use it exactly as is. and in fact, most of the other operations that we implemented on bsts are also identical. they d on't need the colors, but they can all benefit from the fact that the trees are much better balanced. so this aspect of red-black bsts is an extremely nice one because of the operations that we implemented for regular bsts that involves some complicated code for floor and ceiling and rank and so forth, and we don't have to change that code at all. we just during the insertion, make sure that we, we [cough] maintain the properties the balance properties and by doing that, we wind up with balance trees and we make all the operations quick and we don't have to re-implement, we don't have to change it at all. so first before we get to the code for insertion, we have to look at the representation. we don't actually have explicit representation of links or links in trees or just references to nodes. you could implement this by building explicit links but the an easier thing to do is to know that every node is referenced by just one link in a tree the one from it's parent. so, you can put the color of a link in the node that it references. so, in this case we have a red link connecting e and c. what we do is put a bit, a color bit in each in the node class. and then, if the link coming into a node is red we set that to true. so this simple thing just tests is a node red. we consider all null nodes to be black null links to be black, we don't have red links dangling off, that would be incomplete pre-nodes. and [cough] otherwise if the color's red, we return true otherwise return false to test whether a node is red. so in this tree, the color of h.left is red, the color of h.right is black and so forth. so, that's the way we represent colors by putting the, a color bit in the node for the color of the length that points to it. [cough] alright, so now, there's a couple of elementary operations that we have to perform on red-black trees, called rotations. and the idea is that during the construction of a tree, or during an insertion operation, sometimes we wind up with red links that are leaning in the wrong direction. and so we'll need what's called a left rotation, and the job of that operation is to take a, a right leaning red link that is there for whatever reason and reorient it to lean to the left. [cough] so, in this case, we have the right link of e points to s and s is red so that's a right-leaning red link and so now that's the before and what we want to do is reorient things so that it leans to the left. and again, that has to be a local operation that only changes a few links and just from the diagram it's not difficult to see that this little bit of code will do the job. if we start with a right-leaning red link. so, first thing we do is take the reference of h.right, and save that in x. so, that's the node that's going to be the new root of the three nodes so to speak. and, and then, x.left after the rotation is going to be h. and also whatever color h was, well, it looks like it should be black. but actually this situation is where it could be red. then x is going to have that color cuz the link coming into h is going to be the link coming into x. and then h's color is going to be black afterwards. and then, we return x to link further up the tree which happens during our standard recursive insert. so, that's a rotate left operation. now, the property of this operation that's very important is it maintains a symmetric order. the keys between e and s are still there. we just changed the way we get to them. and the keys less than e and so forth. and also we maintain perfect black balance because we didn't change the black height, height of anything by doing this transformation. all those subtrees, those three subtrees are exactly the same relative to the top and bottom of the tree, as they were before the rotation. now paradoxically and you'll see why very soon it also turns out that to get the insertion done properly we sometimes need to take a left-leaning red link and temporarily make it lean right. and later on, we'll get it back to the left again. but anyway, that's a basic operation that we sometimes need. and so that's just the symmetric code to the code that we just did. now again, now x is h.left. and h.left is going to be x.right after the rotation. x's color is still going to be h's color. and h's color is going to be red. and the right rotation implements this and again that's going to maintain a, a symmetric order in perfect black balance we change the way the red goes but we didn't change anything about the black. okay, that's a right rotation. now, here's the third elementary operation that we're going to perform. it's called a color flip. sometimes during the insertion, we might wind up with a node that's got two red links coming out of it. that's corresponds precisely to our temporary four node when we're doing 2-3 trees. and what we wanted to do with the temporary four node was to split it and pass the center node up to the root. well, you can see from this structure that we're all set to do that all we need to do actually is not change any links, just change all the colors. and so, that is, we change the link from e to a and from e to s to be black. that essentially splits the four node. and then, we want to insert the e into its parent. and we just do that by changing its link to be red. so, that's flipping the colors. and that's the way we split a temporary four node in a left-linear red-black tree. and again, that's just flipping colors. it doesn't change any links so it still, of course, maintains symmetric order and perfect black balance. so, those are the three basic operations we're going to use. rotate left, rotate right and flip colors. so, the basic s trategy is, with those operations, maintain one-to-one correspondence with 2-3 trees when we do insertions. so, here's an example. if we want to insert c into this red black tree which is a representation of this 2-3 tree, then c is going to be less than e greater than a. so, it will get added to, as the right link of a and every time we add a node we just create a red link to its parents and so, that's changing the two node into a three node. in this case it's three nodes that's oriented the wrong way so we need to do a left rotate. after we do the left rotate, we have a legal left-leaning red-black tree, and it exactly corresponds to that 2-3 tree, so the insertion of c gives us exactly what we would want, that correspondence with the 2-3 tree. we have to work through other cases that can arise but there's not too many so we'll work through and we have the basic operations, left rotate, right rotate, and flip colors. alright, so first, warm up, insert into a tree with exactly one node. well, if it goes on the left, then we just make a red link and add it on then we're done. if it goes on the right, then we attach a new node with the red link on the right but we have to rotate it to the left to make a legal three node. so, that's inserting to a tree with the one node and make it a tree with two nodes. and that one generalizes to help us insert into a two node at the bottom. so, we do the standard bst insert color the new link red and then if that [cough] new three node happens to lean right, rotated to the left. that's the case that we just did. so now, let's look at the second warm-up. so, say, we have just two nodes in the tree, so it's we have two nodes and that means it's a single three node. then there's three cases. so, one is that the new one is larger than both of the keys. if that's true, then we attach the new node with the red link as always. and that gives us a temporary four node. and what we want to do is split that four node and in this case, since we are at the root that's all so that just flips the colors. now, the color of the root in our code will temporarily turn red and then we turn it black again. so, that's inserting into a tree that's a single three node a node that's larger than both of them, a key that is larger than both of them and we get wind up with a four node. well, let's look at the other two cases and these understanding needs is crucial to understanding the whole algorithm. let's say, the new key is smaller than both of the keys in our three node. now, we attach a new link at the left of the smaller node. and now, we've gotta find bst. but it has two red links in a row. and that's something that's not allowed. so, what we're going to do is we're going to rotate the top link to the right. so that puts b at the root. and now, it's got two red children. it reduces to this case. and we flip the colors and we have a single four node. sorry a, a red black. a tree that's got three two-nodes and no red links so same situation as before. so, we had a single temporary four note and we split it up into a two, two note not connected to a four note. and then, so that's the case when it's smaller. now, we have to look at the third case, which is, when it's, the new node inserted this in between and comes out of this link here. again, we just had a red link and now we have a bst with two red links along the path connected to a and that's not allowed. in this case it's a bit trickier to affix the situation, what we do is we rotate the bottom link left. so, and that gives us this and reduce it to the other situation that we had before. and then we rotate the top link right and then, we flip the colors. so, this one we used all three of our operations, rotate left rotate right and flip the colors. and that gets us an insertion into a tree that has from a tree that i s a single three node to a tree that is three two nodes that is containing three keys. so that sort of operation is going to work in a big tree when we insert into a new three node at the bottom. we do the standard bst insert, color the new link red, and we do the rotations that we need, either one or two rotations to balance the temporary four node, and then we flip colors to pass the red link up one level and then remind me to rotate to that to make that one lean left. so, for example if we insert h in to this tree here, it comes off as the left link of r so that gives us a temporary four node that's not balanced so we need to rotate the link from s to the right and that gives us now temporary four node that is balanced and again, these are all local transformation it's not changing the rest of the tree. now, we flip colors and that gives us a, a good red-black tree, except that, that one red link that we just is leaning the wrong way. so, now we need to rotate left and then once we've done that, now we have a legal left-leaning red-black tree. so [cough] that's a insertion into a three node at the bottom. so, here's another one that involves, remember, we passed that red link up. there might, if that gets passed up to a three node, then we have to continue moving up the tree and just treat it in the same way as we just treated inserting at the bottom. we have a new red link appearing into some three node. there's the three cases that could happen and here's an, an a, an example. so, say, we're inserting p into this left-leaning red black tree it goes to the right event so we get a temporary four node that's got two red links both children are red in that thing so we want to flip the colors. we flipped the colors and now our temporary 4-node is up higher in the tree but it's not balanced so we are going to have to do two rotations to make that balanced. first one is to make the bottom link left-leaning and then the second one is to make the top link right-leaning so that we can have the temporary four node balance. and then the last thing we do is flip the colors and now that's the result of that insertion. it's a bunch of transformations but they're all simple using our flip colors or left or right rotation. and [cough] that one happened to be at the root. if that red link were, were way down in the tree and there were another three node about it, we might have to do it again. again, exactly as what would happen in a 2-3 tree. so, let's do a demo of constructing the red-black bst from our standard set of keys. so, we start with a single key. now, if we want to insert e, if it goes to the left, that's fine. that's a legal left-leaning red-black tree. a would go to the left of e two lefts in a row so we have to rotate that to right. and then we have to flip the colors. and that's a legal red-black bst. so now, if we insert r into this one then it goes on a red link to the left of x, s and that's fine, it's a red-black bst. and now, if we insert c into this one, it goes less than e, greater than a it's a red link connecting a and c but it's leaning the wrong way. so, we have to do a left rotation, legal-red black bst. and you want to insert h that goes to the left of r, two reds in a row, rotate the top. rotate the top, our temporary four node is balanced, flip colors. now, we have a three node, but the red link is leaning right so we have to rotate. and now, we have a legal red-block bst. insert x into that one that goes to the right of s, it's leaning the wrong way, rotate left. insert m into this one, goes to the right of h, leaning the wrong way, rotate left. most of the operations are simple ones like this happening at the bottom. insert p, that goes to the right of m that makes m a temporary four node that happens to be balanced, so flip the colors. flip the colors, now we h ave a temporary four node that's out of balance so we need a double rotation. first rotate e to make that link point lean to the left, then rotate r to make the, bring the temporary four node into balance. and then, flip the colors and that's a legal red-black bst. insert l into that one. it goes to the right of h, leading the wrong way rotate left. [cough] and that's an example of building a red-black bst from our standard set of keys. now, we're ready to look at the implementation for, of the code for inserting into a left-leaning red-black tree. and the key to understanding this code is to realize that the same code, code handles all of the cases. and the way that it works is we are always reducing one case to another. we get this most complicated case we did a left rotate on the bottom node and that, that transformed it to this case where they're both leaning left. and then we did a right rotate on the top node, and that transformed to the case where our temporary four node is balanced. and then we flipped colors on that. so, for a particular insertion, we can take advantage of this reduce one case to another by, in, in the way that we're moving in the tree, not to get everything happen with just a, a few extra lines of code in our standard binary search tree. so, in gray is our, standard insertion code for binary search trees. and remember we took some pains to think about the recursive implementation where when we go down a link we replace that link by whatever the recursive routine gives us back and that strategy is going to pay off in giving us a really simple code. [cough] and because in this implementation for left-leaning red-black trees we're going to return the link whenever we're done, and then that will get that link installed up in the node above whether it be left or right. typical implementations of red-black trees that do not use this recursive strategy wind u p having lots of cases depending on whether left or right or double rotate to the left or double rotate to the right can be critical of this code because my own was this way for the first three editions of the book. and it's only in this edition that we figured out how to make the code this simple. okay. so what are the things left to be done? let's just check. when we insert a new node all we want to do is create a new node with the, i've given, associating the given value with a given key, as before but now we just make that node red. so, that's adding a new node with a red link at the bottom inserting that into whatever the two or three node it's attached to. and then we do the comparisons as, as before and that, and that's all fine. now, when it's returned then that's the point at which we're going to check whether the left, the links are leaning to the left as they are suppose to and whether or not there are any double links or not. so the first thing is if, if is red h.right and not is red h.left? so, that means h is h.right is red so that means the right link of h is leaning the wrong way. so, h is a three node leaning the wrong way. so we just rotate left h. so, whenever we find a right link, we're sitting on a right red link we just rotate it left and return that. so, that would be in this case here, we'd rotate it left and reduce it to that one. [cough] or in, in you know, in the case when we're just inserting a new node and it's turns out to be the right red link attached to a black one, if that handles that case. now, if h.left is red and h.left is also red that's this case here where we have two left-leaning red links. and then in that case, we just rotate the top one right and that brings us to this one. so, notice, we're in this case we do this rotation first, we're on this node and then , that returns and we come up to deal with the situation on this node after the return, and then we do that rotation. and then after that rotation, or if there were no rotations at all, if the insertion happened over here then we'd test and flip the colors. it's a little mind bending at first because of the recursive structure but it won't take you long to convince yourself that this little bit of extra code completes the implementation of left-leaning red-black trees. it's quite remarkable, actually. so, let's look at a visualization. watching the [unknown], this is a balanced tree getting constructed in the worst case where everything that comes in is in ascending order. a regular binary search tree will just be all strung out in a single line and wouldn't have quadratic time for this input but a left-leaning red-black tree actually when, whenever it becomes a power of two is completely balanced as you can see from this example. even though they came in, in ascending order, the tree winds up being perfectly balanced. and what about descending order. well, it's left leaning and the process is a little bit different and sometimes the left path can get long but not that long. the worse that can happen is that it alternates red and black. and then after it gets to that worse case it also winds up being completely balanced when we have a power of two. interesting to think just, just about this case and to prove to yourself that it's always going to be perfectly balanced when it's descending. and this is just for random insertions. the tree stays very balanced. it's guaranteed that the longest path which is alternating red and black can be no more than twice as long as the shortest path which is all blacks. and so in this case the longest path is ten and the average path is seven for 255. very close to log based two of n. so easy to prove by correspondence with 2-3 trees that t he height is guaranteed to be less than two log base two n. every search in left-leaning red black three is guaranteed to take less than two log base two of n cuz every path gets the same number of black links so you never have two red links in a row. [cough] and actually, in typical applications with any kind of randomness or even if there is a lot of order its difficult to find situations orders of keys that build the trace of height is bigger than actually one log n in, in a real application, its very close to fully balanced all the time. so, that completes our summary for a symbol table implementations with red-black bsts. we have full code it's the regular bst code with the couple of lines adding the calls and the basic operations. she rotate right, rotate left. in color flip, we could guarantee logarithmic performance not just research, insert, in delete code. delete code is a bit more complicated but it's on the book side and in the book. but also, since it's the compare-to interface, and since it's a binary tree representation all the other comparable operations extended operations for ordered symbol tables are going to be implemented and take time proportional to the log n. a lot of people ask why use the name red-black. well we invented this data structure this way of looking at balance trees at, at xerox parc which was the home of the personal computer and many other innovations that we live with today entering graphic user interface and internet and object oriented programmings and many other things. but one of the things that was invented there, was the laser printing and we were very excited to have nearby color laser printer that could print things out in color and out of the colors, the red looked the best. so, that's why we picked the color red to distinguish red links the types of links in three nodes. so, that's an answer to the question for people t hat have been asking. now, here's another war story about red-black bsts. as i mentioned they're widely used. and there was an example not that long ago, where a telephone company contracted with a database provider to build a database that could store customer information and the provider implemented the database using red-black bsts for search and insert. now, our, our original paper on red black trees was the way the paper was laid out, it turned out that the delete implementation happened to be placed after all the references. so, a lot of people didn't see the delete implementation. and also we didn't have the simple left-leaning representation. so it was more complicated and involved a lot more cases and so usually not all the cases were put in the text books. so, people found deletion more difficult. in fact, that's what lead to [unknown] analyze the situation then come up with a left-leaning variant. so, what they did in this implementation was they just put in regular hibbard deletion in the binary search in the red-black bst. not the deletion algorithm that's guaranteed to keep the constant black height all the time. and so but, but they still thought that it should be balanced and it shouldn't matter much. and they had a complex error recovery process that, that got triggered if the height limit got too big. and they rebuild the whole tree and, and then because of the way they did this deletion, well, the end of the story was that they had extended the client had extended outage because the implementer didn't use the full algorithm. and there was a lawsuit and some legal testimony and i am happy to report that, that it was clear that hibbard deletion was the problem once the expert analyzed it and the expert witness, who's a colleague of mine, said if implemented properly, the height of a red-black bst with n keys is at most two log n. and so that's the st ory of red-black bst's guaranteed logarithmic performance for all symbol table operations. 
okay. we're going to finish up by talking about some, practical applications of red black trees. and in particular, b trees which are a general, general version. so the idea the sign behind bee trees is that often, the data that we're, trying to store is, is really huge. there's a, a large amount of data. and we're, we're going to look at a more general model for external storage. where, we work with continuous blocks of data that are big. maybe four, 4k or bigger, or maybe even a whole file. and all we want to count is the first time we access a page, because the main cost is trying to find where the page is. once it's read in we get to read all of the page for free pretty much. so the real property of external storage that not your local memory, is that the time required to get to a page is way larger than the time to access data within a page. so what we want to do is try to access data that's out, externally, using a minimum number of probes. that's a model of a file system that is pretty workable. and so bee trees are a generalization of. balance trees, that allow for this. the idea is to, allow not just two or three, keys per node, but a large number like the number that can fit in a page. so, might be, m = 1000 or m = 4000. and well, we've gotta have at least, two, keys at the root. and, and the only other restriction is that, we don't want the nodes to get too empty. so we have less than m. but we want to have at least m over two. and as you'll see, this is, a generalization of, two three trees. that allows us to, build balance trees that, are very, very shallow. typically, these are set up so that, the, all the data is in the external nodes. and so the external nodes have no links, they just have keys. and their in, kept in sorted order. so, for example, this is a external, this is m = six. this is an external five node. so it's got five keys. it's got, room for one more temporary one, and then what will happen is, when you insert into a full node, it'll split in the same as before. and then we'll pass the s plit up causing a split up higher so the red keys in the internal nodes are copies of keys down below that direct the search. and it is that, that's a little extra detail. it just makes the implementation a little bit easier and that's the way it's usually done. so but for now the main idea is that it's like a 233 except that we allow way more keys per node. and then when a node gets filled it splits. into two so a node is always between half full and full. so it's in 1000, it splits in two. and then each side has 500. and then we can use, that property of the trees, in the analysis to, show that, it's not going to be very many probes to get to any key. so the, search is, you know, just the same as we've been doing, just generalized. there's a list of keys at every internal node and that key, tells you that, then links for every key that give you, a place where your key would have to be. so, this link is for all the keys in the b tree that are between this key and the next one and in every case, it's that way. so if we're looking for e. b and this b tree would go down the left link. and then looking on the second link cuz e is between d and h. and that's just the way it organized. and then when you get to an external node you just look for and so that's a, that's the all searches terminated in external node, in other words that's just a generalization of what we just did. and insertion, works the same way, where we get to the bottom, and then, and then we split. so let's look at just inserting a into this b tree. it comes into the node on the left. and then that makes that temporarily overful. it's got one too many. so we split it into two nodes. and that causes us to add a new entry into this internal, node. in this case, it's the c, which is the smallest one in this new page. and that had to be added. and we can move all those over. there's plenty of time by the memory model. we're only counting the number of times we access the pages. we get to move things around for free. and you could have some hybrid struc ture where you use something different for the internal model. but usually it's fine just to do that. now that one becomes overfull. so it has to split and we have to create a new route, just in the same way as we've been doing. so without seeing all the details yo can understand that the same basic idea is going to work in this situation where we're dealing with much, much more memory. and so the end result is that a search or an insertion in a b-tree in a order m, that's where we're putting m keys per page, requires between log base m - 1n and log. base m over two m probes and that's going to be a really small number, so say m is a 1000, log base m over two is, is log base 500. so what power do you have to raise 500 to get bigger than n? in practice that's going to be like four or five. and we can keep the root page in memory so that it means, for any conceivable application, you can get to any piece of data. even if it's trillions of, of pieces of data in this huge, huge file. you can get to any one with only five or six probes. that's quite amazing. it's really an astounding example of algorithmic technology. doing something that, you wouldn't really, necessarily think that you could do so easily. maintain a dynamic search symbol table with trillions of keys, so that you can get to any key just by looking five or six places but that's what b-trees provide for us. this is a simulation that shows a, a growing b-tree so when a page, at the top, there's just one page that fills up. when it fills up, it's red, and that splits into two half pages and then keys get added on one side or the other so each. lying in this table some pages getting a new key and eventually one of them fills up and splits. now we have three pages and we keep going eventually one of them fills up and splits. now we have four pages and now this time the first one fills up and splits and so forth. so the black is the occupied part of the page. the white is the unoccupied part. and the full page about to split then right below there's two pages. s o this shows the process of building a large b-tree. and that and you can see the amount of black. it's kind of half empty. it's a little more than half empty usually now, as this shows. and, and people have variants of these algorithms that keep it more, much more than half empty if that kind of space is a, is a consideration. so, as i've mentioned, red black trees and b-trees are widely used as system symbol tables. the java implementation of tree map and tree set is red black trees, c++, the standard template library uses, red black trees. and it's also, used in the, linux kernel, and in many other systems. b-trees, there's many different variants that, give different characteristics of, space usage and other characteristics. in most databases, nowadays that, that you might use. sql or oracles database and others, are based on, some variant of b-trees because they're so, so effective. but you really know that your data structure and algorithm is used by a lot of people when it appears in the popular culture. my friend philippe flajolet who recently died was a famous french mathematician send me an e-mail late one night. he was quite excited because he was watching a re-run on, of an english actually canadian tv show on french tv. i didn't know he spend his time doing that but he was very excited because he saw this clip. >> it was the red door again. >> i thought the red door was the storage container. >> but it wasn't red anymore, it was black. >> so red turning to black means what? >> budget deficits, red ink, black ink. >> it could be from a binary search tree. the red black tree tracks every simple path from a node to a descendant leaf that has the same number of black nodes. >> does that help you with the ladies? >> so not only is there some excitement in that dialogue but it's also technically correct which you don't often find with math in popular culture of computer science. a red black tree tracks every simple path from a node to a descendant leaf with the same number of black nodes they got that rig ht. and that's also true of b-trees and both of these methods are very effective and widely use. 
welcome back. today, we're gonna take a look at a number of interesting applications of symbol tables and the binary search tree data structure to address problems with processing geometric data. so let's take a look at it. the idea is that we're gonna be talking about geometric objects, not simple keys like strings and numbers. so here's an example. so say your geometric objects are points in the plane and you specify a rectangle that's oriented with the horizontal/vertical axes. and you might wanna ask, which points are inside the rectangle or how many points are inside the rectangle? or maybe what you are processing is rectangles. you have a set of rectangles, and we want to know which of these rectangles intersect? or how many rectangles intersections are there? these are interesting problems that have lots and lots of applications, from computerated design, to games and movies and also in abstractions such as data bases and other situations where you might have multiple keys or multiple dimensions. and it's a very interesting extension of the ideas that we've looked at for symbol tables for all sorts of familiar applications. and, surprisingly binary search trees and these associated algorithms that we've looked at are going to provide very efficient solutions to a number of important problems in this area. and really have enabled a new developments and new technology in all of these kinds of applications. so, to get started, we're going to look at a simple problem called one-dimensional range search. and it really forms the basis for what we're going to do. it's a little bit of an extension of the ordered symbol table api that we gave before and we're going to have operations range-search and range-count. so, a one-dimensional just means we have one key, so we'll insert a key value pairs before and what we want to do is to be able to search for a key, and a value associated with it, want to b e able to delete. but then we want these operations range-search and range-count. so, find all the keys that are between two given keys, or give how many keys are there between two given keys. so, for this example at right we have insert a number of keys and, and we're just showing them in sorted order. but then, you might say, well, how many keys are there that are between g and k? in this case, there's just two. and then the client might say, well, what are those keys and you want to be able to return them. and this is a very common operation, say, in databases. you want to return how many taxpayers have salaries between one million and ten million and then which ones are they and so forth. so, range searching is a very important fundamental operation. now, in geometric interpretation, we just think that the keys as points on a line. and so, the key values well, are just specified as points on a line. we might convert the letters to numbers, or we might, keys might be numbers. and then, what we're looking for is to find or count the points in a given interval in one dimension. so how we're going ti implement that? well this is the basic problem that is very similar to our symbol table problem. we might consider keeping the things in an unordered array. just put them in an array, and then, well, insertion is, is fast. we just add it to the end of the array. we might have to use resizing to make the array grow. but this is unattractive because for large numbers of keys, in order to count the keys that fall within a given range, you have to go through all the keys and test whether they're in the range or not and to return them the same way. so take linear time for large number of keys. if you keep the things in order like in a binary search situation then to insert in order to keep it in order in an array, you might need to move the larger ones over one pos ition and so forth or elementary implementation of binary search when we did symbol tables did this. so, the insertion time might be linear, but then you can use binary search, to look for the two endpoints, that's only going to take time proportional to log in. and then from that, you can figure out how many keys there are or return them all between the index, the lowest one in the range, index the highest one in the range. so, those elementary implementations are no acceptable for a large numbers of keys cuz they have the linear time operation. so, what we really want is to have time proportional to log in. for insert and, and for counting. for range search, of course, we, we have to touch every key that we return, so the running time is going to be proportional to the number of keys that match. but anyway, those are reasonable goals. and they're easy to achieve. so [cough] so, for example what about one-dimensional range counting? well, what we're going to do is just keep the keys in a binary search tree and we looked at the implementation of the rank function for binary search trees where for every key, we can compute how many keys are there that are strictly less than that key. so in this case, the rank of e is two and h is three and so forth. so, in a binary search tree, those rank numbers go in an increasing order as we do in an ordered traversal and that's easy to compute. you need to keep that rank tree as a field, or keep a field which has the size of the tree and it's easy to complete the rank from that. so how many keys between, say e and s? well one, two, three, four, five. it's actually just the difference between the ranks plus one if the high [cough] entry in the range query is in the table and not +one over. so, there's the same number of keys between e and s as there are between e and t five. between f and t, there's only f our. so, that's a, a really 1d range count is a very easy computation to perform in, in log time with a binary search tree. the [cough] number of nodes examined when we do a search is the length of the search path to low plus the length of the search path to high to [cough] find their ranks and that's going to be time proportional to log n. [cough]. so and a range search. well, we just do a recursive search and to find all the keys between low and high you look in the left subtree if any of them could fall in the range. you look at the current node and you look at the right subtree, if any of them could fall in the range. and it's easy to tell whether any of them could fall in the range by just checking whether they're range overlaps the root or not. so, if we are looking for all the keys between f and t then we have to look at both the subtrees of the root s. but we don't to look at the left subtree of e because all of those are less than e and therefore are less than f. so, we don't have to have to look there. but otherwise, it's a simple modification of recursive tree search to find all the keys and it's easy to see the running time to that is going to be proportional to the number of keys returned plus log n. so, that's one dimensional range search using binary search trees. 
so now let's look at a basic geometric data processing problem of determining intersections among a set of line segments. so, it's called the orthogonal line segment, segment intersection search where the lines segments or constrained to be either horizontal or vertical. and so, suppose we have a large number of such line segments and what we want to be able to do is to find all places where they intersect. and as we'll see this extends to a practical problem in a number of situations. so, in this case there's four places where these lines intersect. so, how are we going to be able to determine these intersections efficiently? now, the natural algorithm, or the naive brute-force algorithm, is quadratic in time. so that is, for every line segment, you check whether it intersects with every other line segment. and again, as we know, such an algorithm is not going to be practical, for huge numbers of line segments. so, just, to simplify our code in the slides in it's off, off from the case for geometric data processing. we don't have to worry about degeneracies where lots of things have the same x or y coordinate. and just to simplify the code and to get it the main principles of the algorithms, we're going to assume that all the coordinates that we have are distinct that we've preprocessed in some way to remove the ones that touch without intersecting. so the method that we're going to look at is a so called sweep line algorithm and the idea is to think of vertical line that sweeps left to right through the data. in to. consider it as a every time it hits some line segment as an event where we have to do something. so sweeping from left to right means we consider each x coordinate as an event. so first thing is if we hit a horizontal line segment. well we're going to hit the left endpoint first, and so what we'll do when we hit the left, endpoint is, insert, the y coordinate of that line into a binary search tree. so, we're going to keep track of y coordinates in a binary search tree. so that's what's happening over in the right there. so now again, sweep from left to right. what's the next smallest x coordinate? in this case it's the line number one there, and we'll remember its y coordinate in a binary search tree. and then two and three. so those that's one kind of event that can happen as we sweep from left to right. another kind of event is that we hit the right endpoint of a horizontal line segment. in this case we hit the right endpoint of line segment two. so, at that point the right point of a horizontal line segment we just remove it because we've processed that line completely. in this case we didn't find any intersections. so, left endpoint insert the y coordinate into a bst, right endpoint remove that ycoordinate from the bst. so, the bst contains the y coordinates of all the horizontal lines that currently might involve an intersection. and then the third kind of event is what happens when we hit a vertical line segment? well, in that case all we want, need to do is just do a range search, for the interval of y end points. so, any point that's inside that interval, is going to represent a horizontal line segment that is an intersection. that's the basic idea behind the sweep line algorithm, to find intersections in sets of horizontal and vertical lines. and it's actually a very simple algorithm, and it's very easy to see that the running time is going to be proportional to n log n plus the number of intersections returned. where there's n horizontal vertical line segments. and it's, and a couple of ways to implement it, one thing is you could sort according to the x coordinates, or you could just put them all on a priority queue. and then, so, so that's going to take n log n for every one of the lines to process them all either n to build the priorit y queue and then log n to take the smallest off each time, or n log n for the sort. and then putting the y coordinates into, into a binary search tree is, again, n log n. and same for deleting. each point has to be inserted, deleted. it could be as many as n in the tree for each one. so it's a total of n log n. and then the, range search, in the binary tree, for each, each one of the range searches. it might take, log n, it might be as many as n. and then there's, plus the number of points return. so, that's a, quick sketch of the proof of this proposition. and with that 1d range search, implementation, we get an efficient n log n, 2d orthogonal, orthogonal line segment, intersection 
now we're going to look at kd trees, which is an extension of bsts that allow us to do efficient processing of sets of points in space and it's very flexible and very useful in lots of applications. so now we're going to extend the api to talk about two dimensional keys. so that's just, you can think of two dimensional keys as, points in the two dimensional geometric space. so we're going to talk about insertion, and search. we won't talk about deletion and then range search and range count. so, we want to be able to insert and delete, points. you can think of a two dimensional key as a point in two dimensional space. and we want to be able to find all keys that lie within a two dimensional range. that's a rectangle. as i mentioned at the beginning or count the number of keys that lie in a two dimensional range. so again, the geometric interpretation is the keys or points in the plane. and we have a, a, a range, 2d range is a, a rectangle, or is oriented, to align with the horizontal, vertical axes. and we want to be able to find or count the points in a given rectangle. in this one has many, many applications and we'll talk about some of them later on. and even if it's not points in the plane, just databases you might ask for all the people with incomes between 1,000,000 and 10,000,000 who are between 40 and 50 years of age. and this kind of algorithm and data structure would be useful for that kind of situation too. so how are we going to solve this problem, implement this api? we build a data structure containing points that can efficiently support range searching and range counting. well one easy way to do it is to just think about dividing space into a grid of squares. so we'll pick a parameter m and divide space into an m by m grid of squares and then process all the points and make a list of points that are contained in each square. we can use a two dimensional array to directly index, relevant squares. and for insert, you just, take x, y. figure out which square it belongs to. simply divide by, both coor dinates by n, and look into the two dimensional array. and just add the point to the list for the corresponding square. and then range searches, only find the squares that intersect the query, and process the points, in that square. and depending on the value of the parameter m, you have a space/time tradeoff. the amount of space required is m^2 for the grid + n. you have to have a linked list element, or whatever for each point. and then the time, though, gets divided by m^2. your number of points and were spread out over the n squared, different squares. and so on average you examine n over m^2 points per square. so you don't want to make m too big that would be too much space, you don't want to make m too small that would be too much time. so what we want to choose is the square size that would best balance these two needs and then it is easy to see that what you should choose is m to be about n square root of n. so then your space is within a constant factor of n and your time is constant. so if the points are randomly distributed, then this is ideal. it takes us, linear time to initialize the data structure. and to insert and search, it take constant time per point in the range. and this is absolutely a fine method that, is not that difficult to implement, in the case that the points are evenly distributed. unfortunately, it's usually the case that, in geometric data, that the points are not evenly distributed. there's a well known phenomenon known as clustering that says that, the, the points aren't going to be evenly distributed all over the whole thing. in the case of the, the grid implementation, they might all fall on the same square. and so, the average list length is short. this is like what we encountered with hashing. if you take, all the points in one square, and zero and all the rest of them. your average is still, n over m squared. but. they are all in that long list and you're going to have a slow algorithm if it's, if it's based on this. so we need a data structure that more gracefully adapts to the distribution of the data. and again it, it's well known that most geometric data has this kind of problem. so for example here's some data which cities in the us. it's got 13,000 points, but if you try to use the grid implementation for this, you'd find that half the squares would be empty. and half the points are in just ten percent of the squares. so, the clustering in the data is going to make the implementation inefficient. we need to adapt to the data. and this is very, very typical, in geometric data. particularly, in higher dimensional data, as we'll see in a minute. so, people have developed all different kinds of, methods for, adapting in this way. and what we're going to look at is one of the most widely used. which is basically to use a tree to represent a recursive subdivision of the plane, of two dimensional space. it's going to be recursive. it's going to be based on the points the way in which we divide into half planes. and its one of many different algorithms that, have been, studied for this, but again its a simple one and widely used. so, for example if you played the game doom or use the flight simulator, that, these types of graphical simulations and animations are made possible only through the use of space partitioning trees, like 2d trees and quad trees and also in all different types of scientific data processing these things are extremely important whenever you're processing. geometric data, doing some kind of geometric search. where is the closest thing? how am i going to find the closest thing efficiently? what things are nearby, and so forth? so, rest assured, these types of algorithms, lie at the heart of, any program that you use that, is involving a lot of geometric data. so, those are just, two examples. so let's look at how it looks now. so, a 2d tree, is, again, it's going to be a data structure based on a bunch of points that's going to facilitate, efficient data processing of these points. so, just as we do for, symbol tables, where we take, keys. now we're going to ta ke points, and we're going to build a data structure based on these points. and the idea is to, build a tree that corresponds to recursively partitioning the plane. so arbitrarily our first point we're going to divide the plane into two parts based on a vertical line through that point. so now, in the tree on the right there, all the points that fall to the left of the first point are going to be on the left, and all the points that fall to the right. that first point, you're going to be on the right. but then we get to the next point, we'll switch and we'll partition on a horizontal line. so now, our second point, in the tree, the left sub-tree corresponds to everybody below that horizontal line, and the right sub-tree corresponds to everybody above it. similar if our third point comes on the left again we'll partition according to the horizontal line through that point on the left. so if we go left and then left that means all the points to the left of one and above three, so the square in the upper left is represented. by, that node in the tree. and, again. now, when we go one level below, we switch again to vertical. alternate between horizontal and vertical partitioning, of the plane. so it's a regular binary search tree. but it's got this interpretation based on the geometric data, where we switch which key we use for, the comparison, the x coordinate or the y coordinate, at each level. and that corresponds to this partitioning of the plane. so now five comes in, that's to the left of four because it was partitioned at a vertical and five's going to partion on a horizontal. this is simple, and completely well defined partion of the plane corresponding to a binary tree. now the ninth point well it's to the left of eight, above to and to the left of eight and then corresponds to horizontal partitioning, tenth point is to the right of one, it's below two and we go to the left and it's to the right of seven so we go to the right. so that's a way to build a binary tree corresponding to a partitioning of the pla ne. and it's really the same as the binary search tree. it's just that we alternate which coordinate we use as the key. at the even levels, we think of a vertical line. and the left subtree is all the points to the left, and the right subtree is all the points to the right. on odd levels, we use a horizontal line, in the left subtrees all points below. in the right subtrees, all points above. and the, and the code for this is, you know, the same as for binary search trees. it's simply, which, coordinate we use for the comparison. that's the only difference. so that's 2d tree implementation. so now what about solving a problem like this rain search problem for a 2d tree. so now we have a query like this green rectangle and what we want to find is all the points in the data structure that fall within that rectangle. well, we're going to use, the 2d tree represents our points and we are going to use the structure and definition of that tree, to go ahead and help us find the points that are in the rectangle. if, if the root node lies in the rectangle then we're done. we can return that, that point but we have to look on both sides to look for more, but if the rectangle lies to the left of the root node then we have to look at the left and so forth. so let's look at how this works in the demo. all right, so, we're going to try to find all the points that are contained in that green query rectangle. so first thing is, to check if our rectangle contains the node of the root. in this case it doesn't. so since, it's to the left of the splitting line of the root we only have to search in the left sub-tree. now, we search the left sub-tree and we're going to check if it contains.3. it does not contain.3, but now which, sub-trees do we search? in this case, now the rectangle intersects a splitting line, so we have to search both subtrees, both above and below. so, first we search the left subtree that's the one below does it contain .4? no. it's to the left so we're going to have to search the left sub-tree of .4. and so we search the left sub-tree and we check if it contains point five and it does, that's the one that we return. it, it also intersects the splitting lines, we have to search both the sub-trees, in this case they're both empty. so we're done with that but now we have to go back and we have to search the other sub-tree of point three and that's the above, so now we check this .6 in the rectangle. in this case, it's not. and it's still a left sway if it's to search the left sub tree a .6 and that one's empty and now we return and we're done. so we don't always go down just one branch if our splitting line hits a rectangle we have to go down both branches but still this is a very efficient algorithm, particularly think about the rectangle being small, it's going to be not that different than a regular search in a binary search tree. alright. so what about the analysis of how long is this going to take? well again, a typical case. a rectangle's small that we're only going to examine, really, a path of the three, maybe a couple of other nodes along the path. and the running time will be proportional to the number of points returned plus lg n. with geometric data the worst case can be bad. so, like all the points could be arranged in a circle. all, all different types of problems that might occur in, with some difficulties. it's possible to prove, that even if the tree is balanced, you can get a worst case proportional to square root of that. so analysis of 2d trees that the under scope. but, for many practical applications they are easily implement and worth using. let's look at another using 2d trees to solve another problem, a so called nearest neighbor search. so now, instead of a rectangle, we have a query point. and our goal is to find the closest point to that point. so in this case our query point is, over here in green. and our algorithm's going to want to return to 0.5. that's the closest one to the query point. so let's see how that looks in a demo. so again, we start at the root. and wh at do we want to do? well, we're going to check. and i, whenever we're at a node, it represents a point so we're going to check that point and we'll compute the distance from that point to our query point. and, if that distance is less than the best found so far, then we'll keep that as the champion. so the first point, that's the closest we've found so far to the query point. so we'll say, number one is the distance. and we'll only worry about the possibility of finding something closer, than that. and so just using that distance we recursively search, any part of the tree that could contain a closer point. and so that's well it continued to do so in this case the query point is to the left of the splitting line and will always go towards the query point first and so in this case we have to search both to see if there might possibly be a closer point than one over on the right if you come like straight across, there might be a closer point. we're going to have a look at both as far as we know now but we'll go towards. the query point and see if we can find something closer. so in that case now we go to .3. compute the distance of that point to the query point. it's closer so we update three to be our new champion. so now we are going to look in parts of the tree that could give us a point that is closer to our query point then three. so already that would mean when we search the point one we wont search the right sub tree because there could be no point on the right sub-tree right of the splitting line. so lets closer to query point than three and so that idea getting closer and closer to the query point is going to cut out different parts of the tree as we process so, but anyway starting at point three as far as we know that we may have to look at both sub trees, so sometimes when we look at both sub-trees but as we get closer and closer we only look at one so lets look at point three now. so, again, go towards the query point. so we'll, go to the top first, and that takes us to six. six is not any closer than three was. so that's not going to, update our champion. and so we'll search 6's left sub-tree which is empty which is right sub-tree and the nearest neighbor can't, we don't have to go down the right sub-tree of six because you can't have a point in that rectangle that's closer to the query point than three. so now we can return from that, and now we have to look at the bottom sub tree associated with three. and so that takes us to four, and that one is, not closer. so we still have three as our current champion. so now, we'll search the left subtree of four first because that query point is to the left of that splitting line. and that takes us to five and five is our new champion. so that's the closest point that we know about. could there be a node that's closer to five, to our right query point than five in the right subtree of four? oh. we have to go above. sorry to look at the top sub-tree associated with five, and we find that it's empty. and now we're back at four. do we have to search the right sub-tree of four? no, because there can't be a closer point, than five in the right sub-tree of four. so we're done with four, and we return to, come to three, and now we search the, suppose to search and return from there we are now at one, suppose to search the right subtree one next but we can term that nearest neighbor couldn't be in there. so, then we are done and we found that the nearest neighbor, is five. and this is going to be, very efficient because as we get closer and closer, the query point, we are cutting out all the subtrees that are away, and again in practice, the running time of this algorithm, is going to be close to logarithmic. so in, in typical cases that the running time of nearest neighbor search in a 2d tree is going to be proportion to logarithmic. it is possible to concoct cases where you are going to have to examine all the points for example if they're all arranged in a circle and your query points to the center or something of that sort. but for typical data it's very efficient. now we're going to look at an application where we simulate a phenomenon in nature. and this is what kind of patterns do things like starlings and geese or cranes or, or fish or fireflies? how do they flock together. and we'll look at a simulation that corresponds to that. and then, when the moment is right, they behave in a way, that should be impossible. [music] and it happens everyday, right through the winter. just a couple of miles from my doorstep. help you desire. >> so to, there's a simple model developed by craig reynolds awhile ago for simulating this situation called the boid. and the idea is to use three simple rules to you get something very close to this complex flocking behavior. so, you have col, collision avoidance where you always try to point away from the k nearest boids. you have centering where you try to point near the center of mass of the k nearest boids, and velocity matching where you update your. philosophy to the average of the k nearest boids. and that algorithm works like this. so as that example shows, 2d trees are extremely effective in quickly processing huge amounts of geometric data, and what's more, it expands to more dimensions. with a very simple modification we can take it to d tree and created data structure known as a kd tree, which even works for k dimensions and the idea is even if there is k dimension, what we will do is recursively partition one dimension at a time, so that's called a kd tree and we use the same ideas for two d trees, but instead of cycling through just horizontal vertical, we cycled through, however many dimensions there are, so its where in three space, we use a plane and do above and below and then simply cycle through the dimensions. at level i, we put on the left points whose i-th coordinates are less than p and on the right, we put the points to whose i-th coordinates are greater than p and at level in cycle three of the dimensions at the level i might k we just use that dimension of the point to do the comparison. the implementation is simple, ex cept for the comparison. and we get the same kind of partitioning for three dimensional data, so we could do boids in three dimensions or for databases with large number of dimensions. you could do even much higher dimensional data. and find nearest neighbors and do range searching extremely efficiently. it's a very efficient and simple data structure for processing k dimensional data that's very widely used and the whole idea is that data clusters, particularly, in high dimensions. and also one point to make for this class is that, this algorithm was discovered by an undergraduate in an algorithms class, so that's, john bentley, who discovered this while an undergraduate at stanford. and, so it's a simple idea that, but, experts scientists where struggling with, dealing with huge amounts of geometric data, and, bentley found this way, to process it efficiently that's been widely used ever since. and in, in particular just as another example consider the idea of n body simulation which is a classic problem in physics. where you've got n particles mutually affected by gravity and basically the computation is based on computing the interacting force for each pair of particles. and so then there'd be mutual gravitational pull. [inaudible] and this is what happens with a large number of particles in a certain simulation and people understand properties in the universe by coming up with, doing these kinds of calculations and comparing against what's observed in space. now but the thing is for each pair of particles, so if you have m particles and you have to do it for each pair, that's m^2 so the progress of scientific investigation is going to be affected by how quickly, you can do this calculation for a large number of particles. there's a lot of particles out in the universe. and, you can't do a quadratic calculation for large n. so, another, undergraduate in an algorithms class discovered, this idea, for n body simulation. and that's, andrew appel. and his idea was that if some part. particle is way away from som e cluster of particles, we can treat that cluster as a single aggregate particle. and not do the individual force calculation between our particle and every one of those in the aggregate. but use the center of mass. and you get a very accurate approximation to the n body doing that. and the algorithm that he used is based on 3d trees. with the n particles as nodes and storing the center of a mass of a sub-tree in each node. and then to compute the total force, traversing the tree of all the information that you need, to, complete the n body calculation. but in time, much closer to n lg n than to n^2. and that, idea that, you didn't need to take time proportional to n^2 but with a, a geometric algorithm, like a 3d tree, you could get the time to n lg n. that enabled, all sorts of new scientific investigation in this example of the use of algorithms to enable new 
okay next we're gonna look at another extension of geometric algorithms to process slightly more complicated objects and then we'll see an important application. this is called interval search. so now instead of points, our data is intervals. so this is, we'll start with one dimension as before and right away you can see that it's a more complicated problem than we've been dealing with. so we want to support the following operations. we wanna be able to insert an interval. so an interval is just a left endpoint, right endpoint of a 1-dimensional data or points on the line. we wanna be able to insert an interval search for an interval, delete an interval but the main thing is we want the interval intersection query. so given a query interval, we want to find all intervals in the data structure that overlap that interval or find any interval we'll start with that simpler problem. so how are we going to support that? so this is the api in java code [cough], so we, have, intervals, so instead of one key we have two, which is left and right end points of the interval for input. and [inaudible], and then we have delete, and then we have intersects. and again simplify the code, we are going to make the non degeneracy assumption that no two intervals have the same left end point. [cough] and, [cough]. easy, easy to fix but, but we don't simplify the code. so now we'll look at a data structure called an interval search tree that helps to solve this problem. and, it's a extremely, simple algorithim, but surprisingly, complicated to understand, so we'll go slowly. so the first thing is what we're going to do is use the left end point of each interval as the binary search tree key. so our, eh, our node stored intervals, but we only use our left end point as the key. so this is the binary search tree that's built from those five intervals, six intervals in our example. seventeen, nineteen is at the root, so everybody with a le ft end point less than seventeen is to the left, the left end point greater than seventeen is to the right and so forth. so that's a binary search tree built, from those intervals. so that's easy. i just build a binary search tree. i just use, the left end point, as the search key. but we're also in the, each node of the tree, we're gonna store, not just the interval. but we're gonna store the, largest endpoint in the subtree rooted at that node. so at every node, we're gonna store the maximum endpoint and subtree rooted at that node. so at the root, the maximum endpoint or the rightmost point covered by an interval, is 24. so we [inaudible] 24 at the root, and, of course, the right subtree. and the left subtree. the max end point is that eighteen so that's what we store for the associated data with the note to the left of the root and so forth. so. we going to have to, that's data that we're going to have to maintain when we do an insert and it's data that we'll use when we're doing an interval-intersection search. so let's take a look at an insertion into an interval search tree with a demo. all right, so, the, insertion algorithm is pretty simple. we do the bst insertion, just so we have to do that, update of the maximum in each node on the search path. so, to insert 16/22 in this tree, while we use the, left endpoint as the search key, sixteen is the left endpoint of our insert interval [cough]. we compare that with seventeen and therefore go left. how sixteen is bigger than five so we go right. now sixteen is bigger than fifteen so we go right. and that's null, so that's where we insert our new, interval. [sound]. so now, we're gonna go back up the tree. and, for every node that we encounter, it could be that, our right endpoint of our interval, is bigger than what was there. so we have to check, all the way up the path, the maximum in each node on the path. so we have to check each node, to see if 22 is bigger, and, for the three nodes to the left it is bigger than eighteen. for the node at the root, it's not. that stays to be 24. so, it's just binary tree insertion, but then after the insertion on the way up, we go ahead and, check, if the maximum that we have is bigger than the maximum there and update it if necessary. so easy to code. [sound]. alright, so now about, how do we do a, a search. so the searches is definitely more complicated and kind of mysterious, but let's look at the rules for search in an interval search tree. alright so now we're gonna look to see if we have an intersection what a. we want to find just. any interval that intersects this query interval 23 25. we're not gonna try to find them all we'll get back to that in a minute. try to find any interval that intersects our query interval. so let's, let's see what we have to do. so first thing is if at the root, we have an intersection, then we're done. we just return. in this case, 2325 does not intersect, 1719. so, we have to go down the tree somewhere. so left subtree is [inaudible] right, okay? otherwise, we have to check whether the max endpoint in the left subtree is less than, the low point in our interval. [inaudible] it's easy to see, well, if that's the case, then we're not gonna find an intersection. in the left. the maximum end-point in the left is 22, and we're looking for 23, and we're not gonna find anything there, so we just wanna go right. so in this case we'll go right 22, 23 no inter sectional left, so we go right and now we do find an intersection 21, 24 does intersect with 23, 25 because 23 is in the middle there, so we find an intersection. now on the other hand, let's say they were looking for 1214, so no intersection. so. all the algorithm says is that, if you didn't go right, go left. so let's go left, in this case. so we weren't able to show that there was no intersection, on the left. so, so we're gonna go left. in this we compare twelve fourteen to five eight, so now we apply the same rules. does it intersect? no, it doesn't intersect. so should we go left. well no, the maximum, end point in the left node is eight. so we can have intersection there, so we gonna go right, [inaudible] to twelve and go right. so, now does twelve, fourteen intersect fifteen, eighteen it does not so there's no intersection so now what do we do. should we go left no the max in point on left is ten so we shouldn't go left. so we're going to go right. those 12-14 intersect 16-22. it does not, so, now, the left end point's null. and so we're just gonna go right. and there's no intersection. so in both cases we just went along one path in the tree to determine whether or not there was an interval or intersection. let's look at one more example. 21, 23. so let's see. 21 thru 23 to seventeen, nineteen. they do not intersect, so now, what are we gonna do next? well we're gonna compare the left sub-tree, and it's not, 22 falls within our interval so it's not less than'r' so there might be an intersection there so we better go to the left, so we do go to the left. now we compare against 5-8, and there's no intersection. so now, we're gonna go left to right. well, we're gonna go to the right, because, the max endpoint in the left subtree is eight, and our interval's 21, so no intersection on the left. so we're gonna go right. intersection 21231518. they do not intersect. so now, do we go left or right? again ten is less than our left endpoint 21. so we better go to the right. [cough]. and now 2123 does intersect 1622, so we return and intersection. again one path through the tree to determine an intersection. so from these rules you can see that the man of code required to implement this intersecting inter role is extremely low. just check for an intersection, if we find it ret urn if left is no we go right. otherwise if the max is less than low we go right. otherwise we go left. could hardly be simpler. really amazingly simple and efficient algorithm. we should convince ourselves really that it always works and so we'll spend just a moment on a short proof. so let's look at the, the cases that could happen. so first thing is if the search goes right. then there's no intersection on the left. and that's easy to convince ourselves of that just from, from what we did in the demo. of course, if the last sub-tree's empty, there's no intersection there. but if the max endpoint in the left sub-tree is less than low, that means every interval in the left sub-tree has a max endpoint less than mah, low, and so therefore it can't intersect. so if you go right, there's no intersection in the left. any possible intersection would have to be in the right, and then the other point is that if you go left, then either there's an intersection there, or there's no intersections at all. so lets suppose that there is no intersect, and that's equivalent to saying, if there is no intersection in the left then there is no intersection in the right. so lets look at it if there is no intersection in the left, since we went to the left and then we have got, low less than max. but, for any interval, in the right subtree, its got to appear after. low. be, because since there's no intersections in the left sub tree high has gotta be less than c. where, because they're sorted by left n point. and then that means that c-s got to be less than a if it is in the right, so therefore there can't be any interesection in the right either. no intersection in the left means no intersections at all, so those two cases is enough to show that this algebroid finds an intersection, if there is one. and the other thing we can do with this is just use a red-black bst to guarantee that we solved this in time proportional to log in. so insert, find, delete, and find any interval that intersects. all take time, guaranteed, proportional to log in. and if we wanna find all intervals we just have to run the algorithm fur each interval that's, until we come up against no intersection, so it'll take time proportional to r log n if there's r intervals that intersect. the theoretical best that you could possibly do would be r plus log n but in practice r log n is quite efficient. this is an easy and very efficient algorithm to solve this interval search problem and as we'll see this algorithm. it's applicable to an important application that we'll see in a 
to finish up, we're going to look at the rectangle intersection problem that's got important practical applications and, uses the techniques that we've been studying so far. and it's a simple generalization of our line intersection problem. so now, we have a bunch of rectangles. they're all oriented horizontal or vertical. and what we need to do is find all the intersections in that set of rectangles. and again n could be huge in applications, as we'll talk about in a second. and the [cough] naive brute-force algorithm involves checking each pair of rectangles for intersection. and what we want is a more efficient algorithm than that as usual. and again, to keep the code simple we're going to assume that all the coordinates are distinct. we don't have any, any equal lines that we have to worry about whether we consider rectangles that touch to be intersecting, and so forth. so that's, that's the problem, rectangle intersection search. this is historically, an extremely, important problem. in the 1970s, when we switched to very large scale integration for computers, we were switching from a situation where we were wiring physical devices together, to a situation where we were essentially drawing the computer. and there were machines that would take drawings and, and return, [cough] and from those drawings, like this, make, physical things that implemented computers with different layers and different, physical materials interacting, in different ways. some things are wires, and some things are switches that, are used to, implement memory bits and computer logic. but the key point about it is that designing a computer became a geometric problem. and so, people, to design new computers, would, make huge drawings that just showed the lines that corresponded to the materials that had to be created to make the computer. now, it was very expensive. you didn't want to have any bugs when you're making a chip. and, there were various rules about what you can do on these drawings. and basically, these rules had to do with doing this ortho, orthogonal rectangle intersection search. you, you can't have [cough] lines that come too close to other lines, certain types of lines can't intersect. need spacing between certain types of wires and, you wanted to, before you tried to make the physical circuit to do this checking, which involved this orthogonal rectangle intersection sort. and it was actually the case that the progress of faster and faster processors with more and more components was slowed because people were using the naive quadratic algorithm to do this design rule checking. and its example of, of moore's law. so, as we built a faster computer say, in 1970x, we needed to check in rectangles. but now, maybe a year and a half later, you have a computer that's two times faster but you also want to build a bigger computer so you have twice as many rectangles to check. so you have two end rectangles to check now, and your computer's twice as fast. so, we get to use the faster and bigger computer to build faster and bigger circuits but that doesn't help if you're using a quadratic algorithm. if you're using a quadratic algorithm and it takes you n days to check your design rules, and people were running these things on the order of days, then for the next computer, it would take 2n days, it would take twice as long. and so people that were using quadratic algorithms were definitely held back and, it was, ed, ed mccreight at xerox park who, discovered interval search trees and the logarithmic algorithm that allowed us to sustain moore's law and keep building bigger and bigger computers. by changing this quadratic algorithm to a linear logarithmic algorithm, and let's see how it works. really, it's a modification of the sweep line algorithm that we looked at for intersecting lines. but now we're going to use that for intersecting rectangles rather than using range search as our basic operation, we're going to use interval search. so now, every time the line sweep hits a rectangle, that corresponds to an interval. if it's the left part of a rectangle, then we put that interval into our interval search tree. so in this case we put on zero and then we put on one and then we put on two. and, and that will give us now three rectangles on our sweep line. and so now, the question is when we hit a, a new rectangle, we want to do an interval search to, if we're at the left to check which ones intersect and the interval search tree algorithm is going to tell us which intersections there are right away. when we reach the right then we remove intervals and so forth. but with the basic interval search tree algorithm and the sweep line process that we've talked about, you can get the orthogonal, orthogonal rectangle intersection search problem solved in time proportional to analog n log n + r log n, where r is the number of intersections. and typically, in design rule checking, you wouldn't expect too many intersections. so again, just as with, line intersection search, using a priority queue or a sort is only n log n for processing the x coordinates. and because the interval search trees take log n for every operation, the insert and delete intervals is n log n totaled and the searches is n log n + r log n. so, the bottom line is that the sweep line algorithm takes this rectangle intersection problem and reduces it to 1d interval search and we have an efficient algorithm for that problem and that enables us to solve the problem in linear rhythmic time instead of quadratic time. and that definitely enabled new progress in technology and it's a fine example of the importance of algorithmic technology. so here's our summary of applications of binary search trees for geometric problems. we started with one dimensional range search and just used regular binary search tree to compute ranks to get the answer. but that as the basis, we're able to solve the two dimensional line segment intersection search using the sweep line algorithm. then we looked at range search and other operations using kd trees. again, modification of binary search trees. and then the interval search tree to solve the one dimensional n over search problem and then how that corresponds to the basic algorithm that you get to if you use the sweep line algorithm to solve rectangle intersection. many of these problems are the basis for geometric processing of huge amounts of data that we see all over the web. and our basic search tree mentality and apis, and binary search tree data structure give us efficient solutions to these important practical problems. 
welcome back. today we're going to look at hashing. which is another approach to implementing symbol tables that can also be very effective in a practical applications. here's our summary of where we left off with red black bsts. where we could get guaranteed logarithmic performance for a broad range of symbol table operations. and the question is, can we do better than that? is logarithmic performance the best we can do? and the answer is that, actually, we can. but it's a different way of accessing the data. and also, it doesn't support ordered operations. but there's plenty of applications where the extra speed for search and insert that we can get this way is worthwhile. the basic plan is to think of the symbol table as really try to reduce the problem to being like an array. and what we do is use a function known as a hash function that takes the key that our symbol table key and reduces it to an integer, an array index, and we use that array index to store the key and the value in an array. maybe the value in a parallel array. now, there's a lot of issues in doing this. the first thing is we need to be able to compute the hash function. that is easy for some types of data but it can get complicated for more complicated types of data. then the other thing is that instead of doing compare to's, we're going to be doing equality tests. so we have to be sure we've got the method that we want for checking whether two keys are equal. all we're going to do is look in the table and try to see if the key that's there is equal to the key we're looking for. and then there's the problem of collision resolution. where, it's since there are so many possible values for a typical data type. we're going to get the situation where two values hash to the same array index and we need a collision resolution strategy to try to figure out what to do in that case. and these things are not difficult but they're all worth articulating as separate issues that we have to deal with in order to get an effective single table implementation. hashing really at its core is a classic space-time tradeoff. if we had no limitation on space at all, then we can have a very huge array with space for every possible key and just use the key itself as an index. if our keys are 32 bit integer keys and we've got a table of size 2 to 32 second then we're just fine. if there were no time limit computation at all, then i'll just hash everything to the same place and then do sequential search. but a sequential search can be slow if we have lots of keys. so what hashing is kind of in the real word where we're trying to tradeoff this idea that we don't have unlimited space and we also don't unlimited time so we're trying to find something in-between. so we'll look at hash functions, separate chaining and then two collision resolution methods called separate chaining and linear probing. now, we'll look at the implementation of hash functions. so idealistically, what we'd like is to be able to take any key and uniformly scramble it to produce a table index. we have two requirements, and one is that we have to be able to compute the thing efficiently in a reasonable amount of time. and the other is that it should be the case that every table index is equally likely for each key. now, mathematicians and computer scientists have researched this problem in a lot of detail. and there's quite a bit known about it. but in practice this is something that still we have to worry about somewhat. so for example, let's suppose that our keys are phone numbers. probably a bad idea to use the first three digits of the phone number as a hash function because so many phone numbers will have the same area code. and it's not equally likely that each phone number has the same first three digits. you have a better chance using the last three digits. but actually, in most cases, you want to find the way to use all the data. another example, social security numbers. again, it's not too good to use the first three digits because they're associated with some geographic region and it's better to use the last three digits. and the real practical challenge with hashing is that developing a hash function is that every type of key needs a hash function and you need a different approach for every key type. now for standard keys like integers and strings and doubles and so forth, we can count on the designers and implementors at java to implement good hash functions. but if we're going to be implementing symbol with our own types of data we're going to have to worry about these things in order to get a hash function that's effective, that leads to an effective symbol table implementation. so hashing is widely used for systems programming and applications, so some conventions for hashing are built into java. in particular, all java classes inherit a method called hash code which is returns a 32-bit int value. and it's a requirement that if x and y are equal then their hash code should be equal. so that's something that is a convention that's built into java and that enables the hash code to be used for hashing. also, of course, if they're not equal then you'd like it to be that they're hash code's are not equal but you can't always get. now, the default implementation for hashing is the memory address of the object. for hashing an object just some memory address of an object. so that kind of meets these two requirements for java. the one that it doesn't maybe meet is the idea that every table position should be equally likely. so usually we'll do some more work to try to make that one happen. as far as the algorithms go, as far as the rules go, you can always return 17. that's legal. it doesn't have this highly desirable attribute but everything would compile. so you'd have to be a little careful that somebody is in there doing that. so java has a customized implementations for the standard data types that people would use for similar table keys and that's the sweet spot for hashing. where some expert has done implementation of the hash code and also your application does not need ordering. but for user defined types, you're on your own and we'll talk a little bit about how to implement hash codes. so here's the java library implementations for a few standard types and they are what they are and what we'll do is we acknowledge that that's what the hash code is. we'll do some extra work to try to get this extra property that every table position should seem to be equally likely. so if it's an integer the hash codes suppose to be 32-bits, integer supposed to be 32-bits. so they just returned the value. if it's a boolean, they pick out a couple of particular values that they return, so hashing boolean type, there's only two different values, so it's hard to think about what you really might want there. for a double value, this is the code. they convert to 64-bit, and x or the most significant 32-bits with the least significant 32-bits. now, this illustrates something that you want to do if you have a lot of bits, you want to try to involve all the bits somehow into hash function. and for strings, it kind of creates the string as a huge number and then, really computes the value of that number. mod 32. it uses an arithmetic. a way of evaluating a polynomial or a number. so called horner's method. where for each digit, you just multiply. so it treats it as a base 31 number. and to compute that whole number you multiply 31 times what you have so far and add the next digit. and that's called horner's rule. and if you're familiar with it, fine. if you're not, you can look at this little example and decide what it is. and again it involves all the characters of the string in computing the hash function. so and actually, since strings are immutable, what java does is keep the hash value in an instance variable so it only gets computed once. and that is going to be very effective for performance and lots of applications. once it computes the hash code, it stores it as an instance variable. and the next time you ask for the hash code of that string, it will just provide it and that works because strings are immutable. so how about implementing a hash code for our own type of data? and so our transaction type might have a couple of instance variables, a string, a date, and a double. and we need to compute a hash code so return a 32-bit value. and again, we want to try to make use of all the pieces of data that we have. and we also want to make use of the hash code implementations for the types of data that we're using. so one thing to do is start out with some small prime number and this kind of mimics horner's method to just add in more data as we get it. so we pick some other small prime number and for each field we multiply by 31. and then add the hash code for that field. so if it's a reference type, you just use the hash code. so who was a string, so string has a hash code method. so we add that in. and dates, when is a date so we add that hash code, multiplied by 31 and add that hash code in. trying to take all the bits and scramble all the bits and use them. and for primitive types take the wrapper type and use the hash code. so that's a simple example of implementing a hash code for our own type of data that might include several different types of instance variables. so that's the standard recipe. here's the 31x plus y rule to combine all the fields. if it's a primitive type, use the wrapper hashcode. if the field is null, return 0. if it's a reference type, use that hashcode and apply recursively. and if you have an array, you have to apply it to each entry. or actually java implements that in its arrays library. so this recipe works pretty well in practice and it's used in several java's libraries. now in theory, it's possible to do something that has the property that all positions are equally likely. it's called universal hash functions. this things exist but they're not so widely applied at in practice. so the basic rule is that if you're computing your own try to use the whole key but consult an expert if you're seeing some performance problems. or you really want to be certain that it in some performance critical situation. now, what we get back from a hash code is a int value that is between minus 2 to the 31st and 231st- 1. now, what we need is if we have a table of size m, an array of size m that we are going to use to store the keys, we need an int value between zero and m minus one. the value of m is maybe a power of two or sometimes we'd pick a prime because of the way that we normally would get the big hash code value down to be a number between zero and m minus one. this is just do mod m and if m is a prime then from that modular arithmetic we know that we're using all the bits in the number in that point to. now, sinse the hash code can be negative, this doesn't quite work the way this arithmetic implement and java, because it's one in a billion times. you really have to take the absolute value. well, sorry, you have to take the absolute value because otherwise it'd be negative and you can't have it negative. you want it to be between 0 and m- 1. but even if you take the absolute value. there's going to have -2 to the 31st. it's possible so you have to just take the 31-bits. get the hash code out, make it positive and mod m is the way to go. the math doesn't quite work out right. so anyway, that code down at the bottom is you can use that as a template for what you might want to do. and that's what we do in order to get the hash code to be a number between 0 and m-1. and if m is prime, it gives us some comfort that we have some possibility of each table position appearing with equal likelihood. so that's our assumption that each key is equally likely to hash an integer between zero and m minus one. and this assumption, again, it would work. it's possible to come close to this. lots of researchers have done good work to show this. we'll assume that is a starting point. and that allows us to model the situation with a so-called bins and balls model that directly relates the study of hash functions to classical probability theory. so we've got m bins, that's our correspondence to our hash table. and we get m balls. and we have some number of balls, however many keys we have. and we'd throw them universally at random into m bins. and these things are studied in classical combinatorial analysis. for example, there's the birthday problem. which how many balls do you throw before you find two hitting the same bin, when do you get the first collision? and the answer to that is it's about square root of pi m over two. when does all the bins fill up? that's called the coupon collector problem. after about natural log m tosses, every bin has at least one ball. and those are just examples of classic results from combinatorial analysis that help us understand what happens when we do this, which is what we're doing with hashing. and we'll look at more advanced versions of these problems when we want to study hashing. in particular, it's known that after you've thrown m balls into the m bins then the most loaded bin has about log m over log m balls. so that's going to help us get a handle on the performance of hashing algorithms when we get to the implementations. so this is just an example showing all the words in a tale of two cities using the modular hashing function for strings like the one that java uses. and they're pretty uniformly distributed. that's the summary for hash functions. 
next we'll look at separate chaining, a collision red solution strategy that makes use of elementary link list. so, what are we supposed to do when two different keys hash to the same index? the birthday problem tells us that we're going to have collisions. you need a quadratic amount of memory to avoid collisions. in the lower balancing, a coupon collector analysis tell us that the collisions are going to be evenly distribute, distributed among the table, around the table. so, what we want to do is have an easy way to deal with collisions. and so the first way we'll look at is called separate chaining and it's a very diagonal idea back1953, and the idea is just build a link list for each of the table positions. so, we'll have a table that's smaller than the number of keys that we have, the hash function will map each key to some integer. so in this case, we have a table of size five. so the hash function will map any key. in this case, we use single letter keys. it'll map any key to an integer between zero and four and then to [cough] do an insertion we'll just keep a link list at the table position corresponding to the hash value. so s hash is to position two, it'll be on the link list that is first link is at position two. and e goes to zero, and a goes to zero. and for search, we're going to have to go to, if we're going to look at is c in this table say, we're going to find the hash value for c and we'll look down the list to see if we can find c. so we have to look through the whole list for search but you only have to look through one list out of all the lists. essentially if you have m entries in the hash table and m keys the link of list you're going to look at is about n over m cuz they're evenly distributed. so that's a straightforward and simple scheme for implementing symbol tables with hashing. now, we could use a interval bag or some data structure like that and hide the link list structure underneath and that's a perfectly fine way to proceed in modern programming. this implementation directly implements the link list. now, for a practical situation we picked some kind of, some value of m. you could make it so that the hash table itself grows once it gets really huge and such hybrid methods are easy to implement. so we won't talk to much about that. we need to just in terms of implementation details, our keys and values have to be objects. because we can't have an array of generics. so, since we're making array of nodes, a node would have generics if we use to key in value. so we have to make them objects then when we get things off, we're going to have cast. so [cough] this is the get procedure. so to look for a key in the hash table we compute the hash value. in our hash function is pull out the system hash code, make it positive by ending off the sign bit and then mark with m to get a number of, zero and -one. so we pick that number, i and then we just go to that list and this is the standard code for diversing a link list start at the first node as long as it is not null go x = x dot x. and if you find a key that's equal to the key you're looking for, return the value and we have to cast it to value because of the generic recreation problem in java, otherwise return null. so that's not much code, and it's trivial code at that for doing an efficient symbol table search using hashing. and insertion is not much more difficult if you do the same thing and if you find a node where key equal to key on the link list, reset the value and return. otherwise, you make a new node and put it at the beginning of the link list with the standard code. now, replace stfi with a new node that links to the old stfi. so again, very little code to implement search and insert using hashin g and that's why it's so popular. and what about the analysis? well, again this the [cough] standard probabilistic analysis of the balls and bins problem tells us a lot of information of what goes on. and again, if the uniform hashing assumption holds the probability that the number of keys within a list is within a constant factor of n over m is extremely close to one. so, it means that we've divided the search cost which would be n if we have a sequential search by a factor of m. and, and in many applications even setting m = 100 or 1,000 is going to be very effective. and that's why so many system programs refuse that. so, number of pros for search and insert's proportional to n over m. now typically, what we'd what a programmer would do is try to figure on making m about equal to the number of keys divided by five say. so, you can't make m too large, you have too much space and you'll have empty chains or short chains. and if you make m too small then they're too long, you have to search through them all. so let's say, n over five and then you get constant time searches and not much extra space. you have extra space for the links to implement the link lists but the rest of the table is not much extra space. and those are typical parameters. if you want a full service symbol table which is going to, going to grow from small to huge and then back down to small again then you'd want to use array re-sizing to make sure that m is always within a constant factor of n but we will leave that detail out for now. so that brings us to this summary where red-black trees, we were happy with a log based two of n for search and insert with separate chaining, you can really get it down to a constant number of operations for search and insert. so hashing's going to be preferred for short keys where the hash function's easy to compute. and where we don't need ordered iteration or any of the ordered symbol table operations because it has really fast access to the symbol table. that's our first collision resolution method, hashing with separate chaining. 
another popular closure resolution method is known as linear probing. in this you know many different versions of hashing that are based on this idea. with linear probing is called open addressing and is also around the same time in the 50's the idea is just use an array. instead of using space for the length in a length list. i use that same space, and just, allocate an array. in this case, the size of the array is going to have to be bigger than the number of keys that we [inaudible] expect. and we use the empty slots in the array. to. essentially terminate the length of the [inaudible] list that we have to search through when we're doing a insertion. so let's look at a demo of how it looks. so to hash again we do the same thing, we just map the key to a index. but, in linear probing, to insert what we do is when we put it in position i if that's free, if not we just look at i plus one, and i plus two, and wrap around to the beginning if we reach the end. now that's also simple to implement and it works well as long the size of the array is, significantly bigger than the number of keys. let's look at, well it's a demo. so we start with an empty table, insert s, it's hash value is six, six is empty so we put it there. now we look at e, hash of e is ten, we look at ten, it's empty so we put e there. so at the beginning we're going to be fine. a is four, empty, put it there. r is fourteen, empty, put it there. so we just essentially, using the hash funtion as an array index. c is five, that's empty and we put it there. so h now, the hash value of h is four. so now we look at four, and that's occupied, so we can't put the h there. and then linear probing says, just look at the next position, look at five. that's still not empty. so we look at six. and we keep going till we find an empty place, and then we put h there. now when we search, we're going to have to do the same thing. we'r e going to have to look at all those positions to look at h. the. group of four key, continuous keys in a table space there is called a cluster and clearly we want to keep those clusters small. and we do that by juts by not putting too many keys in to the table. so x hashes to fifteen, that's empty so we put it there, m hashes to one, that's empty and we put it there. p hashes to fourteen, 14's occupied, 15's also occupied, now we run off the end of the table, and look at zero, and that's empty so we put it there. l hashes to six. six is occupied. we look at seven, seven is occupied. we look at eight, and we put it there. and, so that's an example of inserting, keys into a hash table. and now, for a search, we just do the same thing. we, use the hash function. to search for e, e's hash value is ten so we look in ten and there it is. so that's a search hit. if we're going to search for say l l's hatch value is six so it's not there. so in order to look at every place in the table where l could be, we have to keep looking til we found an empty table position, or we find l itself. so now we look at seven l not there, we look at eight l is there, that's a search hit. if we have a value that's not in the table like k, well hash and is in position five, no, six no, seven no, eight no and we find an empty position at that point we can conclude that k is not in the table. because if k were in the table it would be somewhere between it's hash point five and that empty position nine. that's a search miss, and we return all. so that's a short demo of linear probing hashing. so here's a summary of linear probing, hashing. to. to get started we map a key to a integer between zero and m-1 where m is the sides of our array where we are storing the keys. to insert we put the key value pair. use parallel arrays [inaudible] and the value array with the same index. we put the entry at the table index a for three. if not try i+1 i+2 until getting to a empty position. and for search you do the same thing you hash to the table position and you look there into the right. to find the key and you stop when you find an empty table position. find the key or find an empty table position. now, it's essential that the array size is greater than the number of key value pairs n. and for linear probing hashing, really, the implementation needs to include array resizing, whenever the hash table gets too full. here's the implementation. and it's, quite straightforward, given the demo that we talked about. you use the same hash function. and we use parallel arrays for the value in the keys. and we have to use ugly cast, 'cause we can't have a race of generics. then let's do the search. so. we just have a for loop starting at hash of key and going until we get to a position that's null. as long as it's not null, we stay in the loop and increment i mod m. so that's when i gets to the end it gets to the end, it's in the position m minus one and it goes... in the next increment goes back to zero at the left end of the table and we just test for all the non null keys. if it's equal, if it is, go ahead and return the associated value and if you get to an empty position, then return null. and the implementation of put is similar. find a, a position, if it's, that's equal, and then, reset the key, in the value. if the key's there, it just resets the value. if they key's not there, it puts a new entry in. so again, that's, fairly little code, to implement, a fast symbol table and insert, search and insert. but it's only going to be fast, if the, table size is set appropriately. in ancient times, memory was, at quite a premium and so people were very concerned in m-m-making sure that the hash table never, got too empty. remember in the first computers, each bit was a physical thing, a magnetic core that somebody had to string a wire through, so. the bits were really expensive, and people wanted to make sure, that they were making best use of the memory. and just leaving empty positions around, in a hash table, or using links in a link list, did not seem like an appropriate use of space. and, so there was quite a bit of effort, devoted to figuring it out, how full we could get the hash table, in linear probing. and how close it could get to full without sacrificing performance. and one way to think about what goes on is to just watch what happens when a hash table fills up. so here we just as, as it goes up we're showing each key getting inserted in the number of probes of the table that are needed for the insertions are j hash to the same position that a; you had to look for a while, and the one thing to notice is as the table gets full, is that first of all. you have, these clusters or these chains building. and so, what's clear about that is that, it means that, the new hash is likely to hash into a big cluster. >> and not only that once you have a big cluster and you hash into the middle of it you've got a good chance that, that clusters going to get longer, or worse. that's it's even going to merge with another big cluster. and so, that's the situation as the table fills up. you get long clusters and they're likely to get longer. and the math bares that out. now this was studied in detail by knauf, don knauf, in the 1960's and actually this problem, knauf says, was the origin of the origin of analysis of algorithms. mathematicians were trying hard to understand this problem and were ready to give up and he realized you could use classical balls and bins type probabilistic analysis. not an easy analysis, but we actually could make precise accurate statements about the performance of this algorithm. and those statements can be borne out in practice, because the hash functions approximate random, the math assumes random and the formulas predict what actually happened in practice. no way can you formulate the problem as so called parking problem. so, what happens is that you are on a one way street and you are looking for a parking place and, it's, the idea's you start looking for a parking place at particular times and say "okay, now i need a parking place", and what you're doing is linear probing hashing. if the current space is taken, you try the next space and the one after and so forth. and the question is. if every car. starts looking for a place at a random time. that. then that models the hash function, then how far do they have to go to look for a place? that's canoot's parking problem. and he was able to show, and we'll talk just a little bit about this, that if, there's, only half of the parking spaces are occupied, then, on average, half the people find, find it after one place and the other half have to look one extra. so that's the kind of performance that we want. but as it gets full. the displacement gets up to square root, of pi m over eight. which is obviously much higher than we want. we don't want our searches to take that long. and that actually, the analysis, is amazing function that goes back to famous roman nuygen and other classical results from our commentorial analysis. what canute's theorem says is that under the uniform hashing assumption, the number of probes in the linear hash table size m, that is alpha percent full, so the number of keys is a fraction of m, is for a search miss half one plus one over alpha, and a search miss one plus one over one minus alpha squared. one myse alpha is for the hit, one myse alpha for the squared for the insert. now as alpha gets close to one, you can see these things are going to grow, and particularly the search miss is growing to grow quite, quite a bit. if it's 9/10's full one over one minus alpha squared is 100 one over 100, so it means it's going to be 50 p robes for a search miss if it's 9/10's full, and that's independent of n and m, whereas if it's half full then we get the nice. numbers of only 3-house for a hit, and only 5-house for a miss. and, again these formulas are nice approximate formulas, but knuth, once he figured this out, in 1963, tells stories, that time, he decided to write his famous series of books on algorithms. now there's four volumes out and more planned, and this is where, all computer scientists go. for detailed information on a performance, eval grievance. so, in, in summary. you can't have m too large, what we want to use nowadays is array resizing to make sure that the array is always about half time, half full. and if we can keep the array about half full then we get constant time performance for search hit and search miss. and linear probing is very attractive in this case. there's other things that we can do algorithmically to bring down the search time a little bit. like using another hatch function rather than looking at the next entry. use another hatch function to determine the stride that we're going to use. and that brings it down somewhat and allows us to keep the tables more full. but the bottom line is that now we have two methods that under the uniform hashing assumption can give us constant time, search, search it insert and delete. for symbol table implementations where we don't need ordering. and we've got a reasonable hash function. so, that's a summary of linear probing or second hash, collision avoidance strategy. 
so let's just look at a little bit of the context of hashing in practical applications. as i mentioned, it's very widely used. so here's an, here's an example right from java. the first. implementation of java 1.1. the designers found that the cost of computing the hash function for strings seemed to be excessive, particularly for long strings. and that was one of the main uses of hashing, was just to be able to do searching with string keys. and so what they decided in the first implementation was let's just look at every eighth or ninth character, and that way, we don't have to spend a lot of time computing the hash function. so they had a hash function pretty much like the one that we use. except that it compute a skip that would mean that, that only look at about every eight key and they wouldn't have to do quite so much work performing the hash function. and that's diffidently one thing to consider when using hashing is that the cost of computing the hash function for a complicated key might exceed the cost of searching and using a simpler structure like a binary search tree. and anyway for java 1.1 what happened was that there was a huge potentail for really bad collision patterns on typical data. so here's the example of typical data, which is a url. all of these keys, which are totally different, would wind up having the same collision. and so client programs and system programs on the java system were having terrible performance on their symbol table because of the shortcut in hashing. so this well illustrates that you need to use all of the data in the hash function and sometime we do a closer analysis. the cost of computing the hash function can mean that something like red black trees will even outperform hashing even for just searching and insert. so there is another thing about the uniform hashing assumption is that it is an assumption and if you are writing code where we have to have guaranteed performance like when your aircraft is landing or you are controlling a nuclear reactor or somebody's pa cemaker. that, if that assumption doesn't hold and you get bad performance you're going to have disastrous consequences. so that's another reason to think about maybe paying a little extra and using to guarantee that you get with red black search trees. instead of hashing. and there's another surprising situation that happens in today's world. for example java publishes its hash function. and so if you're trying to provide a service over the web. an adversary can learn your hash function and just send you data that causes huge performance problem by just making all that data hash to one particular item. and that's definitely something to worry about. and, and in the real world you can nowadays find on the web particular sequences of keys that will cause particular services to crash. and again, that's a little harder to do with something like a red black tree where we have performance guarantees. when you make an assumption you better be sure and you're depending on that assumption, you better be sure that it holds somehow. this is different than for example for quick sort when we, our assumption was we're going to create randomness and we are going to depend on that randomness. in this care we're kind of hoping for randomness and maybe that doesn't really always hold. so that's certainly something to be aware of when using hashing in practice. so here's just simple example on hashing in java. so what we can do is it's pretty easy to find a family of strings that have the same hash code for example with just a little fooling around now days you can just look it up on the web, you can see that these two character keys, both have the same hash code because when you just do the math in a base 31 hash code it'll tell you that answer. well what that means is that actually, just like working in binary you got, you can combine those things. in all possible ways, and you can get two to the n strings, for any n of length to n that all hash to the same value. and somebody's implemented a service in java that it uses a simp le table that takes string keys, you can cause that to crash in this way. little bit scary for some systems designers. at least reason for pause in using hashing. now, hashing also has a extremely important application in today's internet commerce. and so the, it's the concept of so called one way hash functions which mean that we, we, use it for secure to try to be, have some secure fingerprints for use on the web. and there's been a lot research done to develop functions that take keys as input, and then produce values that look random. in such a way that, it's hard for someone else to find another key that collides with that. this technology is, is useful for storing passwords and digital fingerprints and things. but it's too expensive for use, in a symbol table. so the bottom line is separate chaining versus linear probin collision resolution message methods. now there's a number of considerations to take into account. separate chaining is really easy to implement both insert and delete it performs, it degrades, it does so gracefully and the clustering is, is maybe less of a problem if you have a bad hash function. linear probing tends to make better use of space. and also it'll perform better for huge tables whereas caching is involved. and if, in the classic algorithm or computer science problems for people to think about is what do we do to delete in these two situations and exactly how do we resize. those are all at the level of exercises in the context of the kinds of algorithms that we've seen. and as i mentioned, there's been many, many improved versions of hashing that have been studied. i mentioned the two probe, or double hashing version. another way to use two hash functions is just to hash the two positions and put the key in the shorter of the two chains. in, in that case, then the expected length of the longest chain will be lg, lg n which is quite an improvement. you get constant time expected and lg, lg n worst case. double hashing is the variant of layer probing where you just skip a variable amount, not one each time. and that pretty much wipes out clustering but it, it is more difficult to implement delete for that one. in a new method called, relatively new method called cuckoo hashing. it's another variant of linear probing where we hash a key to two positions and insert the key in either one. if occupied you, you reinsert the displaced key into its alternative. it was in one, each one can go to two. and that one actually gives constant worst case time for search. that's another variation on the team. and all of these things allow us to make better use of memory, allows the table to become nearly full. it would have been very exciting. thing to be researchers in the 1950's who cared so much about memory and nowadays a little extra memory is not something that people care about so much and most people just go with the easy algorithm except for really performance critical applications. what about hash tables versus balance search trees? well hash tables are really simple to code usually if you don't have to do the hash function. and if you don't have order in the keys at all then you need the compare to, to implement balance search trees. so you have to use hashing if you don't have the comparison. and it'll probably be faster for simple keys to use hashing. it's a few arithmetic operations to do the hash versus lg n and compares for the balance tree. and there's some better system support in java for strings that cache hash code means that you don't even have to compute the hash if your, your simple table for strings is in an inner loop. on the other hand, balanced search trees have a much stronger performance guarantee. it, you don't have to assume anything. it's going to be less than lg n and compares and it's got support for all those ordered st operations, and compared to and is pretty easy and natural function to implement. so it's more flexible and more broadly useful. and actually the java system and other systems include both so that programmers can make use of either one in diff erent situations. that's our context for hashing algorithms. 
welcome back. today we are going to look at some symbol table applications, give you some idea of how symbol tables might be used, by client program for practical problems. first when we look at, seems even simpler than the regular symbol tables, and that's about sets. so, a mathematical set is just a collection of distinct keys. and so, there are plenty of applications where we want to just be able to implement, this really simple api. i want to be able to create an empty set, we've got methods to add a key to the set, and to check whether a given key is in the set or not. to remove a key, and maybe return the number of keys in the set, and also have an iterator to iterate through keys in the set. this is simpler than symbol tables because it's got no associated value. so, it's a very simple api but of course, we're going to be able to do these operations efficiently, how we'd we go ahead and implement that. well, if you think about it for just a minute, you see that what you need to do is just remove all references to value from any of the symbol table implementations that we'd look at. the implementation is easy. take one of our symbol table implementations and get rid of the code that refers to values. okay, so let's look at a couple of applications where this set api might be useful in client programs. one thing that is very common is the idea of an exception filter. so the way we'll set that up is to think about having a list of files a list of words in a file that are exceptional in some way. in this case, we'll have the word, the file list.text that has these four words in it: was, it, the, and of. so this two, complimentary ways to look at this. one is so-called white listing where we want to take the words in that file and, and then we have some other much bigger file. and what we want to do is print out all the occurrences of our exceptional words in our given file. those are the ones that we care about, that we want to get through. so in this case, tinytale.txt the first couple of words from "a tale of two cities." and these words appear often, "it was the of, it was the of." another, a complementary approach is to think of these words as words that we don't want to ever see. they're blacklist, and we want to take them out of our source file. so a blacklist client would print out all the words in our source file, tinytale.txt except was, it, be, and of. in this case, best times, worst times, and so forth. so that's the exception filter, and that's useful in lots of applications such as the ones listed here. for example, you might have a spellchecker where you want to identify misspelled words. so, then your key would be a word or a string. and in your exceptional list would be words that are in the dictionary. and you'd want to print out all the words that are not in the dictionary. that's an example of a, an exception filter. or in a browser you might want to mark your visited pages or block sites and so forth. or like the one at the bottom credit cards. maybe, you run a credit card company and you want to check for stolen cards then your keys would be numbers. i. and in your list, might be kind of short, which would be the stolen cards that you know about, and you'd want to run a, a white list filter for those cards and print out in your long list of transactions which ever transactions have that stolen cards, so, that's just a couple of examples of exception filters. what's the implementation of an exception filters? here's a simple one using the said api that we just articulated. so, we start by creating an empty set of strings, and again since we don't have associated values, we just have the one generic for strings, and then create a new input stream from, from the first argument so that's the name of the file that contains the exceptional words and so this just reads the strings while the input string is not empty and then adds the m to the set. so that now, we have our set of exceptional words. and now, from standard input we read words, as long as our set contains the word, we print it out. and if doesn't contain it, we don't print it out. so that's an example of a white list filter. and to implement black list we just this call to contains, we just change that to a, a not. if it's not in the exceptional list, then we print it out. so that's a simple example of a filter using sets. 
now, let's look at a dictionary client, another very useful and common application of symbol tables. so, in this case, we are going to write a client called lookup csv that [cough] is going to take three arguments. the first will be a file name, a so-called common separated value file and the next two arguments are integers which will tell us what to treat as keys and values in the file. in this example or csv file relates urls to ip addresses. so, each line has a url and ip address and are separated by commas. and in general, a csv file might have many fields separated by comma, comma. so, we number them zero, one, and so forth starting from the left. so, what we are going to do with this client is specify with integers which field is the key, and which is the value. so, if we call this client with second argument zero and third argument one, that means we want to use the url field zero on the csv file as the key, no one use the ip address that's field one in the csv as the value, you want to associate keys with values. so the client will build a symbol table that makes us associations for every line in the file and this could be huge file. and then, if we want to look up the ip address associated with a given url we can just type in urls and the client will return the ip address, it'll do the look up. so, adobe.com has got this ip address that's shown if this line here in the table, and so forth. princeton.edu has this ip and ebay.edu is not in the file. now, on the other hand, we could, from this same file, we could build a symbol table where we treat the ip address as the, as the key and the url as the value. so, in that case, it'll build a symbol table with ip addresses as keys and we can type in an ip address and get the associated url. so, with one client we can handle lookups of all kinds in csv files. for example, here's another csv file that from biology that deals with, amino acids and codons and names. so, in this case, the first field is three letters from the dna sequence which, represents a codon. and certain codons have names, that's the amino acids. so, tcc is called serine, and so forth. and that's an association that's well known to biologist and then you can use this lookup csv client to quickly get the name associated with any given codon. that's just another simple example. this is a very general tool. any csv file, you can pick any field as the key, any other field as the value. so here's still another example where we might use for a class list, which has the person's year of graduation, last name, first name precept name and login name. and so in the first call over here, we might use the login name as the key and the first name as the value. so, we type in somebody's login name we get their first name. and again, with the same client login is key and get the section as a value. so, all kinds of information processing that we might need to do for large amounts of data, represented in comma, comma separated value files this one client which is based on a symbol table will provide useful functionality. and here's the implementation there's very little to it given the symbol table api that we've articulated and the implementations that we have. so what do we do to get lookup csv implemented? well, first thing is to set up the input stream from the first argument, so that's our input file. and then, get the numbers of the fields, so the key and the value. and now build a simple table that associates strings with strings. then there there's a while loop where we just read a new line in that read line, read line and then split according to comma into tokens in an array. and then, the index in the array is going to build the fields that we're going to use. so, the key is the string in the key field entry of the array, and the value is the string in the value field entry in the array, and we simpl y put that into symbol table. so, this value loop just builds the symbol table from the file. then from standard input we take queries, just read a string, check if the symbol table contains the string. it, it doesn't, print not found. and if it does, print the value associated with the key. so, a very small amount of code based on a symbol table implementation that gives us the dictionary functionality. 
here's another simple client program for symbol tables related to indexing. again, another common function that's easily handled by symbol tables. there's all kinds of cases where we just have a lot of information, maybe on our pc or all over the web, and we want to create an index that allows us to specify. a search key and get all the associated information. and you've, you've used programs like this on, on your computer many instances, most likely. so, more generally, here's the goal. so, we're going to list of files that are specified. and then, and maybe it's all files on the computer, or maybe it's specified in, in some other way. and what we want to do is create an index, so that we can efficiently find all the files that contain a given query. string. so, in this small example. so, our client is going to called file index. and in this small example say we are going to have five text files. and these are just pieces of literature. and so what we want to do is, build and index from that set of text files. and then given a key, a string key, we want it to print out the files that might contain that key. so, for example, the word freedom appears in the magna ca, carta and in moby dick and a tale of two cities [cough] in all three of them but not in tom sawyer and not in aesop's fables. the word whale is only in, moby dick. the word lamb appears both in tom sawyer and aesop's fables, and so forth. so we're looking for a piece of information somewhere in a computer, and it'll give us the names of the files that contain that information. so, with the, more [cough] complex user interface, this is, very much what the spotlight or find function on your computer is doing. and a programmer might use the same program, to find places where certain, programming terms are used in a bunch of programs. so, normally we have a bunch of dot java files when we're working on an application. like these are all the ones associated with this, lecture, so we might build an index from that set of files, and then we might wonder well where do we use the import statements. so we just search for import and it'll tell us, look in those three files. or, did we use comparators anywhere here? in, this index will tell us no. so again, a very general capability that's got lots and lots of applications and easy to implement with our symbol table api. and the idea is, that what we're going to do is associate. on keys with values well the key is the string we type what's the value? well what we're going to use for value is a set of files, the files that contain the query string. so just given that high level description then the implimintation is pretty direct. here's the implementation of file index using our symbol file implimintation. so now we're going to build a symbol table. that associates string keys with sets of files. so this creates a new symbol table associating string keys with sets of files. so first thing we do, as before, is build the index. so we're going to take, this time we're going to take the list of file names from the command line. and [cough] for each file we're going to create an input stream. we're going to read every word in the file if the symbol contains does not contain the word. will create a new set for that word and put that in the symbol table, and then, we simply get the set associated with the key and add the new word to that set, the new file name to that set. so, this one, for each word in the file. it'll add that file name to the corresponding set for that word. that's building a symbol table associating keys with sets of files. and then processing queries is the same as before, as long as standard in is not empty we take a query. and then simply print out the set of strings associated with that word which is a list of file names. so again, our symbol table api gives a very easy way to implement this file indexing function, which is very widely useful. and similarly, maybe we want to build a, a book index, maybe for a real book, or maybe for an ebook. or more generally, people want to often process, preprocess text to, maybe a huge amount of text, to support, so called, concordance queries. so what you want to do, is given a word, find all occurrences of that word along with immediate contexts. and context just means a few words before and a few words after. so for example, in tail of two cities. the words cities actually only appears in this one place and this is [cough] context that it appears. with the forwards tongues of the two before and that were blended in after. the word majesty appears in three places, and there's, there's the context. in well, this is, the very special case that you're very familiar with. in web searches. you type in a word, and then you get places that, where that word appears in context. again, our symbol table api provides for an easy way to implement this. this is a, a. [cough] concordance client that that does the job. now what we want to do is read text, and we want to build an index for each word. so this is a [cough] a, a java construct for. reading all the strings on an input stream, on standard input, and, splitting em by blank space and putting em into array, so now all the words, are, in the an array. on standard input. and then we're going to create a new symbol table that associates strings with sets of integers. and again the string is the key and the sets of integers are going to be the places in the array where the given work appears. so, we go through. all the words, pick out our key, s, and again, if, it's not there yet, we create a new set associated with that s, and, then, afterwards, we go ahead and. get that set, and put the new index on that set. and then that allows us to process queries, where we take a query, and then get the set of indices associated with that query. and then we left out, left out the code where you print out the nine words that you want. the four to the left and the four to the right. again, quite useful and familiar functionality and very easy to implement with our simul table client. 
as a final example of a symbol table client, we'll take a look at a mathematical application where we want to implement sparse vectors and matrices. so, this is a standard matrix vector multiplication that you learn in math where we have a square matrix and a column vector and we want to do a dot product of, of first row with the column vector to get the first entry in the result. so, in this case, they're all zero, except for 0.04 0.9 which is 0.36. and then similarly, dot product of this with that column is 0.297 and so forth. so standard implementation of this is quite easy. we have a two-dimensional matrix one-dimensional column vector for the multiplicand and the result. and then they get initialized in some way, but the main computation is a pair of nested four loops for each row in the matrix we have to go through each entry in the column vector and compute a running sum of for that row in the matrix, that corresponding expanding entry with the entry in the column and them, keep the running sum and then that's the result that we put in the result column factor for every value of i. and the key thing about this standard implementation that it's two nested four loops that each run up to n. so, that's n^2 or quadratic running time. and that's fine in typical applications when the matrix is small, or when there's lots of entries in the matrix. but the fact is that in many practical applications, the matrices are what's called sparse. they, most of the entries are zero. and so, symbol tables provide us with a way to provide a more efficient implementation of, of this process when we have lots of zero entries. so in a typical thing, say, maybe the matrix dimension would be 10,000, and maybe there would only be ten non-zero entries per row. or, even nowadays you might have matrices that are, are even bigger. 10,000 by 10,000 if, if there was, if it was full, that would be a billi on or 100 million entries. and so that's definitely going to be costly eh, if you're doing this operation a, a lot. and the idea is to cut down on that cost by taking advantage of idea that there's a lot of zeros. so let's start by just looking at vectors. so the standard representation that we use for vector is to simply use a one dimensional array. we have constant time access to every element, but the space is proportional to n. so, even if there's a lot of zeros we, we still have to take the space to store them all. instead, we're going to use a symbol table representation where our key is the index and the value is the entry. and we just use that for every non-zero entry in the vector. so, this has got the same amount of information. it says that index one has got 0.36, index five also has 0.36, index fourteen has 0.18 and so forth. but the space is proportional instead of to n, it's just proportional to the number of non-zero entries which again, in typical applications, may be way, way less. and so now, just we, we know we have a symbol table implementation that has efficient iterator. and also access is not bad. it's just that we're able to do it with way less space. so, here's what the implementation of a sparse vector might look like. so first thing is the representation is going to be a symbol table. and in this case, we might as well use a hash table because the order in which we process things is not important. uh-huh, we just want to get at the, all the non-zero entries. so the constructor is going to create in this [cough] symbol table. just a new symbol table that associates integer indices with double values. so, the put which is to store a value associated with an index i, is just put into that hash table, associate key i with value x. associate an integer with a double. and get i'll return zero if the index key is not in the symbol table. we didn't, that's the whole poin t was, we don't represent zeroes. other, otherwise it returns the value associated with the index. and the iterable just returns all the key to iterate. and the most important thing is that if we want to do a dot product with a vector, say, then the time that it takes is only proportional to the number of non-zero keys. the zero keys are going to be zero on the dot product so what we're going to do is take the item key of the vector and multiply it by whatever value we get for the non-zero entries. so, it's a dot product that takes time proportional to the number of non-zero entries in the vector. and, and that's going to be important in the use of a matrix. so instead of using the standard matrix representation, where every row of a matrix is an array, that's what a two dimensional array is and the space is proportional to n^2. now we're going to use a sparse matrix representation, where each row of the matrix is a sparse vector. we can iterate through the elements in, in constant time, and with a hash table, we can get at them in near constant time and then constant time in the average. but the space is only proportional to the number of non-zero elements plus n for the extra symbol table overhead. so, those are independent symbol table objects. but they allow us to have a much more efficient matrix multiplication method. so now if we have a sparse matrix times a vector our running time is going to be constant for each row or proportional to the number of non-zero entries for each row which means that the running time is going to be linear for a sparse matrix just by the use of a symbol table. and this clearly can make the difference between being able to address a huge problem. if we have a 10,000 by 10,000 matrix we can get it done nearly instantly linear time versus 10,000^2. if we now run out of space, we might run out of time. but with the symbol table implementation we can ef ficiently process huge sparse 
hi, my name's tim roughgarden. i'm a professor here at stanford university. and i'd like to welcome you to this first course on the design and analysis of algorithms. now, i imagine many of you are already clear on your reasons for taking this course. but let me begin by justifying this course's existence. and giving you some reasons why you should be highly motivated to learn about algorithms. so what is an algorithm anyways? basically it's a set of well defined rules, a recipe in effect for solving some computational problem. maybe you have a bunch of numbers and you want to rearrange them so that they're in sorted order. maybe you have a roadmap and an origin and a destination and you want to compute the shortest path from that origin to that destination. may be you face a number of different tasks that need to be completed by certain deadlines and you ant to know in what order you should accomplish the task. so that you complete them all by their respective deadlines. so why study algorithms? well first of all, understanding the basics of algorithms and the related field of data structures is essential for doing serious work in pretty much any branch of computer science. this is the reason why here at stanford, this course is required for every single degree that the department offers. the bachelors degree the masters degree and also the phd. to give you a few examples routing and communication networks piggybacks on classical shortest path algorithms. the effectiveness of public key cryptography relies on that of number-theoretic algorithms. computer graphics needs the computational primitives supplied by geometric algorithms. database indices rely on balanced search tree data structures. computational biology uses dynamic programming algorithms to measure genome similarity. and the list goes on. second, algorithms play a key role in modern technological innovation. to give just one obvious example, search engines use a tapestry of algorithms to efficiently compute the relevance of various webpages to it's given search query. the most famous such algorithm is the page rank algorithm currently in use by google. indeed in a december 2010 report to the united states white house, the president's counsel of advisers on science and technology argued that in many areas performance gains due to improvements in algorithms have vastly exceeded event he dramatic performance gains due to increased processor speeds. third, although this is outside of the score, the scope of this course. algorithms are increasingly being used to provide a novel lens on processes outside of computer science and technology. for example, the study of quantum computation has provided a new computational viewpoint on quantum mechanics. price fluctuations in economic markets can be fruitfully viewed as an algorthmic process and even evolution can be usefully thought of as a surprisingly effect search algorthim. the last two reasons for studying algorthims might sound flippant but both have more than a grain of truth to them. i don't know about you, but back when i was a student, my favorite classes were always the challenging ones that, after i struggled through them, left me feeling a few iq points smarter than when i started. i hope this course provides a similar experience for many of you. finally, i hope that by the end of the course i'll have converted some of you to agree with me that the design and analysis of algorithms is simply fun. it's an endeavor that requires a rare blend of precision and creativity. it can certainly be frustrating at times, but it's also highly addictive. so let's descend from these lofty generalities and get much more concrete. and let's remember that we've all been learning about and using algorithims since we were little kids. 
sometime when you were a kid, maybe say third grade or so, you learned an algorithm for multiplying two numbers. maybe your third grade teacher didn't call it that, maybe that's not how you thought about it. but you learned a well defined set of rules for transforming input, namely two numbers into an output, namely their product. so, that is an algorithm for solving a computational problem. let's pause and be precise about it. many of the lectures in this course will follow a pattern. we'll define a computational problem. we'll say what the input is, and then we'll say what the desired output is. then we will proceed to giving a solution, to giving an algorithm that transforms the input to the output. when the integer multiplication problem, the input is just two, n-digit numbers. so the length, n, of the two input integers x and y could be anything, but for motivation you might want to think of n as large, in the thousands or even more, perhaps we're implementing some kind of cryptographic application which has to manipulate very large numbers. we also need to explain what is desired output in this simple problem it's simply the product x times y. so a quick digression so back in 3rd grade around the same i was learning the integer multiplication algorithm. i got a c in penmanship and i don't think my handwriting has improved much since. many people tell me by the end of the course. they think of it fondly as a sort of acquired taste, but if you're feeling impatient, please note there are typed versions of these slides. which i encourage you to use as you go through the lectures, if you don't want to take the time deciphering the handwriting. returning to the integer multiplication problem, having now specified the problem precisely, the input, the desired output. we'll move on to discussing an algorithm that solves it, namely, the same algorithm you learned in third grade. the way we will assess the performance of this algorithm is through the number of basic operations that it performs. and for the moment, let's think of a basic operation as simply adding two single-digit numbers together or multiplying two single digit numbers. we're going to then move on to counting the number of these basic operations performed by the third grade algorithm. as a function of the number n of digits in the input. here's the integer multiplication algorithm that you learned back in third grade illustrated on a concrete example. let's take say the numbers 1, 2, 3, 4 and 5, 6, 7, 8. as we go through this algorithm quickly, let me remind you that our focus should be on the number of basic operations this algorithm performs. as a function of the length of the input numbers. which, in this particular example, is four digits long. so as you'll recall, we just compute one partial product for each digit of the second number. so we start by just multiplying 4 times the upper number 5, 6, 7, 8. so, you know, 4 times 8 is 32, 2 carry to 3, 4 times 7 is 28, with the 3 that's 31, write down the 1, carry the 3, and so on. when we do the next partial product, we do a shift effectively, we add a 0 at the end, and then we just do exactly the same thing. and so on for the final two partial products. [sound] and finally, we just add everything up. [sound], what you probably realized back in third grade, is that this algorithm is what we would call correct. that is, no matter what integers x and y you start with if you carry out this procedure, this algorithm. and all of your intermediate computations are done properly. then the algorithm will eventually terminate with the product, x times y, of the two input numbers. you're never going to get a wrong answer. you're always going to get the actual product. well, you probably didn't think about was the amount of time needed to carry out this algorithm out to its conclusion to termination. that is the number of basic operations, additions or multiplications of single digit numbers needed before finishing. so let's now quickly give an informal analyses of the number of operations required as a function of the input length n. let's begin with the first partial product, the top row. how did we compute this number 22,712? well we multiplied 4 times each of the numbers 5, 6, 7 and 8. so that was for basic operations. one for each digit at the top number, plus we had to do these carries. so those were some extra additions. but in any case, this is at most twice times the number of digits in the first number. at most two end basic operations to form this first partial product. and if you think about it there's nothing special about the first partial product. the same argument says that we need at most 2 n operations to form each of the partial products of which there are again n, one for each digit of the second number. well if we need at most two n operations to compute each partial product and we have n partial products. that's a total of at most two n squared operations to form all of these blue numbers, all of the partial products. now we're not done at that point. we still have to add all of those up to get the final answer, in this case 7,006,652. and that's final addition requires a comparable number of operations. roughly, another say two n squared, at most operations. so, the upshot, the high level point that i want you to focus on, is that as we think about the input numbers getting bigger and bigger. that is as a function of n the number of digits in the input numbers. the number of operations that the grade-school multiplication algorithm performs, grows like some constant. roughly 4 say times n squared. that is it's quadratic in the input length n. for example, if you double the size of the input, if you double the number of digits in each of the two integers that you're given. then the number of operations you will have to perform using this algorithm has to go up by a factor of four. similarly, if you quadruple the input length, the number of operations going, is going to go up by a factor of 16, and so on. now, depending on what type of third grader you were. you might well of accepted this procedure as the unique or at least the optimal way of multiplying two numbers together to form their product. now if you want to be a serious algorithm designer. that kind of obedient tumidity is a quality you're going to have to grow out of. and early and extremely important textbook on the design and analysis of algorithms was by aho, hopcroft, and ullman. it's about 40 years old now. and there's the following quote, which i absolutely adore. so after iterating through a number of the algorithm design paradigms covered in the textbook. they say the following, perhaps the most important principle of all, for the good algorithm designer is to refuse to be content. and i think this is a spot on comment. i might summarize it a little bit more succinctly. as, as an algorithm designer you should adopt as your mantra the question, can we do better? this question is particularly apropos when your'e faced with a naive or straight-forward solution to a computation problem. like for example, the third grade algorithm for integer multiplication. the question you perhaps did not ask yourself in third grade was, can we do better than the straight forward multiplication algorithm? and now is the time for an answer. 
if you want to multiply two integers, is there a better method than the one we learned back in third grade? to give you the final answer to this question, you'll have to wait until i provide you with a toolbox for analyzing divide and conquer algorithm a few lectures hence. what i want to do in this lecture is convince you that the algorithm design space is surprisingly rich. there are certainly other interesting methods of multiplying two integers beyond what we learned in third grade. and the highlight of this lecture will be something called karatsuba multiplication. let me introduce you to karatsuba multiplication through a concrete example. i am going to take the same pair of integers we studied last lecture, 1, 2, 3, 4, 5, 6, 7, 8. i am going to execute a sequence of steps resulting in their products. but, that sequence of steps is going to look very different than the one we undertook during the grade school algorithm, yet we'll arrive at exactly the same answer. the sequence of steps will strike you as very mysterious. it'll seem like i'm pulling a rabbit out of the hat, and the rest of this video will develop more systematically what exactly this karatsuba multiplication method is, and why it works. but what i want you to appreciate already on this slide is that the algorithm design space is far richer than you might expect. there's this dazzling array of options for how to actually solve problems like integer multiplication. let me begin by introducing some notation for the first and second halves of the input numbers x and y. so the first half of x, that is 56- we're going to regard as a number in its own right called a. similarly b will be 78, c will be 12, and d will be 34. i'm going to do a sequence of operations involving only these double digit numbers a b c and d. and then after a few such operations i will collect all of the terms together in a magical way resulting in the product of x and y. first let me compute the product of a times c and also the product of b times d. i'm going to skip the elementary calculations, and just tell you the answer. so you can verify that a times c is 672, where as b times d is 2652. next i'm going to do something even still more inscrutable. i'm going to take the sum of a and b. i'm going to take the sum of c and d. and then i'm going to compute the product of those two sums. that boils down to computing the product of 134 and 46. mainly at 6164. now, i'm going to subtract our first two products from the results of this computation. that is, i'm going to take 6164. subtract 2652, and subtract 672. you should check that if you subtract the results of the first 2 steps from the result of the 3rd step, you get 2840. now, i claim that i can take the results of step 1, 2 and 4 and combine them into super simple way to produce the product of x and y. here's how i do it. i start with the first product, ac. and i pad it with four zeros. i take the results of the second step, and i don't pad it with any zeros at all. and i take the result of the fourth step, and i pad it with two zeros. if we add up these three quantities, from right to left. we get two, five, six. six, zero, zero, seven. if you go back to the previous lecture you'll note that this is exactly the same output as the great school algorithm, that this is in fact the product of one, two, the, three, four and five, six, seven, eight. so let me reiterate that you should not have any intuitions for the computations i just did, you should not understand what just went down on this slide. rather i hope you feel some mixture of bafflement and intrigue but, more the point i hope you appreciate that the third grade algorithm is not the only game in town. there's fundamentally different algorithms for multiplying integers than what you learned as a kid. once you realize that, once you realize how rich the space of algorithms is, you have to wonder can we do better than that third grade algorithm? in fact, does this algorithm already do better that the third grade algorithm? before i explain full-blown karatsuba multiplication, let me begin by explaining a simpler, more straightforward recursive approach. to integer multiplication. now, i am assuming you have a bit of programming background. in particular, that you know what recursive algorithms are. that is, algorithms which invoke themselves as a subroutine with a smaller input. so, how might you approach the integer multiplication problem recursively? well the input are two digits. each two numbers. each has two digits. so to call the algorithm recursively you need to perform inputs that have smaller size, less digits. well, we already were doing that in the computations on the previous slide. for example the number 5678 we treated the first half of digits as 56 as a number in its own right and similarly 78. in general, given a number x with n digits. in can be expressed decomposed, in terms of two, n over two digit numbers. namely as a, the first half of the digits shifted appropriately. that is multiplied by ten raised to the power, n over two. plus the second half of the digits b. in our example, we had a equal to 56, 78 was b. n was 4, so 10 to the n over 2 was 100, and then c and d were 12 and 34. what i want to do next is illuminate the relevant recursive calls. to do that, let's look at the product, x times y. express it in terms of these smaller numbers, a, b, c, and d, and do an elementary computation. multiplying the expanded versions of x and y, we get an expression with three terms. one shifted by n, 10 raised to the power n, and the coefficient there is a times c. we have a term that's shifted by 10 to the n over 2, and that has a coefficient of ad and also plus bc. and bringing up the rear, we have the term b times d. we're going to be referring to this expression a number of times, so let me both circle it and just give it a shorthand. we're going to call this expression star. one detail i'm glossing over for simplicity, is that i've assumed that n is an even integer. now, if n is an odd integer, you can apply this exact same recursive approach to integer multiplication. in the straightforward way, so if n was 9 then you would decompose one of these input numbers into say the first five digits and the later four digits and you would proceed in exactly the same way. now the point of the expression star is if we look at it despite being the product of just elementary algebra, it suggests a recursive approach to multiplying two numbers. if we care about the product of x and y, why not, instead, compute this expression star, which involves only the products of smaller numbers, a, b, c and d. you'll notice, staring at the expression star, there are 4 relevant products, each involving a pair of these smaller numbers. namely ac, ad, bc, and bd . so why not compute each of those four products recursively. after all, the inputs will be smaller. and then once our four recursive calls come back to us with the answer, we can formulate the rest of expression star in the obvious way. we just pad a times c with n zeros at the end. we add up a, d, and bc, using the grade school algorithm, and pad the result with n over two zeros, and then we just sum up these returns, again using the grade school addition, and algorithm. so the one detail missing, that i've glossed over, required to turn this idea into a bonafide recursive algorithm, would be to specify a base case. as i hope you all know, recursive algorithms need a base case. if the input is sufficiently small, then you just immediately compute the answer rather than recursing further. of course, recursive algorithms need a base case so they don't keep calling themselves til the rest of time. so for integer multiplication, which the base case, well, if you're given two numbers that have the just one digit each. then you just multiply them in one basic operation and return the result. so, what i hope is clear at the moment is that there is indeed a recursive approach to solving the integer multiplication algorithm resulting in an algorithm which looks quite different than the one you learned in third grade, but which nevertheless you could code up quite easily in your favorite programming language. now, what you shouldn't have any intuition about is whether or not this is a good idea or a completely crackpot idea. is this algorithm faster or slower than the grade school algorithm? you'll just have to wait to find out the answer to that question. let's now refine this recursive algorithm, resulting in the full-blown karatsuba multiplication algorithm. to explain the optimization behind karatsuba multiplication, let's recall the expression we were calling star on the previous slide. so, this just expressed the product of x and y in terms of the smaller numbers a, b, c, and d. in this straight forward recursive algorithm we made four recursive calls to compute the four products which seemed necessary to value, to compute the expression star. but if you think about it, there's really only three quantities in star that we care about, the three relevant coefficients. we care about the numbers ad and bc. not per se, but only in as much as we care about their sum, ad plus bc. so this motivates the question, if there's only 3 quantities that we care about, can we get away with only 3 rather than 4 recursive calls. it turns out that we can and here's how we do it. the first coefficient a c and the third coefficient b d, we compute exactly as before, recursively. next, rather than recursively computing a d or b c, we're going to recursively compute the product of a plus b and c plus d. if we expand this out, this is the same thing as computing ac plus ad plus bc plus bd. now, here is the key observation in karatsuba multiplication, and it's really a trick that goes back to the early 19th century mathematician, gauss. let's look at the quantity we computed in step 3 and subtract from it. the two quantities that we already computed in steps one and two. subtracting out the result of step one cancels the a c term. subtracting out the result of step two, cancels out the bd term, leaving us with exactly what we wanted all along, the middle coefficient a d plus b c. and now in the same that on the previous slide we have a straightforward recursive algorithm making four recursive calls, and then combining them in the obvious way. here we have a straightforward recursive algorithm that makes only three recursive calls. and on top of the recursive calls does just great school addition and subtraction. so you do this particular difference between the three recursively computed products and then you do the shifts, the padding by zeros, and the final sum as before. so that's pretty cool, and this kind of showcases the ingenuity which bears fruit even in the simplest imageable computational problems. now you should still be asking the question yeah is crazy algorthim really faster than the grade school algorithm we learn in 3rd grade? totally not obvious, we will answer that question a few lecture hense and we'll answer it in a special case of an entire toolbox i'll provide you with to analyze the running time of so called divide and conquer algorithms like karatsuba multiplication, so stay tuned. 
in this video i'll talk about various aspects of the course, the topics that we'll cover, the kinds of skills you can expect to acquire, the kind of background that i expect, the supporting materials and the available tools for self assessment. let's start with the specific topics that this course is going to cover. the course material corresponds to the first half of the ten week stanford course. it's taken by all computer science undergraduates, as well as many of our graduate students. there will be five high level topics, and at times these will overlap. the five topics are first of all, the vocabulary for reasoning about algorithm performance, the design and conquer algorithm design paradigm, randomization and algorithm design, primitives for reasoning about graphs, and the use and implementation of basic data structures. the goal is to provide an introduction to and basic literacy in each of these topics. much, much more could be said about each of them, than we'll have time for here. the first topic is the shortest, and probably also the driest. but it's a prerequisite for thinking seriously about the design and analysis of algorithms. the key concept here is big-o notation, which, conceptually, is a modeling choice about the granularity with which we measure a performance metric like the running time of an algorithm. it turns out that the sweet spot for clear high level thinking about algorithm design, is to ignore constant factors and lower-order terms. and to concentrate on how well algorithm performance scales with large input sizes. big o notation is the way to mathematize this sweet spot. now, there's no one silver bullet in algorithm design. no single problem solving method that's guaranteed to unlock all of the computational problems that you're likely to face. that said, there are a few general algorithm design techniques. high level approaches to algorithm design that find successful application across a range of different domains. these relatively widely applicable techniques are the backbone of a general algorithms course like this one. in this course, we'll only have time to deeply explore one such algorithm design paradigm, namely that of the divide and conquer algorithms. in the sequel course as we'll discuss, there's two other major algorithms on paradigms to get covered. but for now, divide and conquer algorithm, the idea is to first break the problem into smaller problems which then gets solved recursively, and then to somehow quickly combine the solutions to the sub problems into one for the original problem that you actually care about. so for example, in the last video. we saw two algorithms of this sort, two divide and conquer algorithms from multiplying two large integers. in later videos we will see a number of different applications. we'll see how to design fast divide and conquer algorithms for problems ranging from sorting to matrix multiplication to nearest neighbor-type problems and computation of geometry. in addition, we'll cover some powerful methods for reasoning about the running time of recursive algorithms like these. as for the third topic. a randomized algorithm is one that, in some sense, flips coins while it executes. that is, a randomized algorithm will actually have different executions if you run it over and over again on a fixed input. it turns out, and this is definitely not intuitive, that allowing randomization internal to an algorithm, often leads to simple, elegant, and practical solution to various computational problems. the canonical example is randomized quick sort, and that algorithm and analysis we will cover in detail in a few lectures. randomized primality testing is another killer application that we'll touch on. and we'll also discuss a randomized approach to graph partitioning. and finally we'll discuss how randomization is used to reason about hash functions and hash maps. one of the themes of this course, and one of the concrete skills that i hope you take away from the course, is, literacy with a number of computational primitives for operating on data, that are so fast, that they're, in some sense, essentially free. that is, the amount of time it take to invoke one of these computational primitives is barely more than the amount of time you're already spending just examining or reading the input. when you have a primitive which is so fast, that the running time is barely more than what it takes to read the input, you should be ready to apply it. for example, in a preprocessing step, whenever it seems like it might be helpful. it should just be there on the shelf waiting to be applied at will. sorting is one canonical example of a very fast, almost for-free primitive of this form. but there are ones that operate on more complex data as well. so recall that a graph is a data structure that has, on the one hand, vertices, and on the other hand, edges. which connects pair of vertices. graphs model, among any other things, different types of networks. so even though graphs are much more complicated than mere arrays, there's still a number of blazingly fast primitives for reasoning about their structure. in this class we'll focus on primitives for competing connectivity information and also shortest paths. we'll also touch on how some primitives have been used to investigate the structure of information in social networks. finally, data structures are often a crucial ingredient in the design of fast algorithms. a data structure's responsible for organizing data in a way that supports fast queries. different data structures support different types of queries. i'll assume that you're familiar with the structures that you typically encounter in a basic programming class including arrays and vectors. lists, stacks, and queues. hopefully, you've seen at some point both trees and heaps, or you're willing to read a bit about them outside of the course, but we'll also include a brief review of each of those data structures as we go along. there's two extremely useful data structures that we'll discuss in detail. the first is balanced binary search trees. these data structures dynamically maintain an ordering on a set of elements, while supporting a large number of queries that run in time logarithmic in the size of the set. the second data structure we'll talk a fair bit about is hash tables or hash maps, which keep track of a dynamic set, while supporting extremely fast insert and lookup queries. we'll talk about some canonical uses of such data structures, as well as what's going on under the hood in a typical implementation of such a data structure. >> there's a number of important concepts in the design and analysis of algorithms that we won't have time to cover in this five week course. some of these will be covered in the sequel course, design and analysis of algorithms ii, which corresponds to the second half of stanford's ten week course on this topic. the first part of this sequel course focuses on two more algorithm design paradigms. first of all, the design analysis of greedy algorithms with applications to minimum spanning trees, scheduling, and information theoretic coding. and secondly, the design analysis of dynamic programming algorithms with example applications being in genome sequence alignment and the shortest path protocols in communication networks. the second part of the sequel course concerns np complete problems, and what to do about them. now, np complete problems are problems that, assuming a famous mathematical conjecture you might have heard of, which is called the "p not equal to np" conjecture, are problems that cannot be solved under this conjecture by any computationally efficient algorithm. we'll discuss the theory of np completeness, and, with a focus on what it means for you as an algorithm designer. we'll also talk about several ways to approach np complete problems, including: fast algorithms that correctly solve special cases; fast heuristics with provable performance guarantees; and exponential time algorithms that are qualitatively faster than brute force search. of course there are plenty of important topics that can't be fit into either of these two five-week courses. depending on the demand, there might well be further courses on more advanced topics. following this course is going to involve a fair amount of time and effort on your part. so it's only reasonable to ask: what can you hope to get out of it? what skills will you learn? well. primarily, you know, even though this isn't a programming class per se, it should make you a better programmer. you'll get lots of practice describing and reasoning about algorithms, you'll learn algorithm design paradigms, so really high level problem-solving strategies that are relevant for many different problems across different domains, and tools for predicting the performance of such algorithms. you'll learn several extremely fast subroutines for processing data and several useful data structures for organizing data that can be deployed directly in your own programs. second, while this is not a math class per se, we'll wind up doing a fair amount of mathematical analysis. and this in turn will sharpen your mathematical analytical skills. you might ask, why is mathematics relevant for a class in the design and analysis of algorithms, seemingly more of a programming class. well let me be clear. i am totally uninterested in merely telling you facts or regurgitating code that you can already find on the web or in any number of good programming books. my goal here in this class, and the way i think i can best supplement the resources that you probably already have access to is to explain why things are the way they are. why we analyze the algorithms in the way that we do, why various super fast algorithms are in fact super fast, and so on. and it turns out that good algorithmic ideas usually require nontrivial mathematical analysis to understand properly. you'll acquire fundamental insights into the specific algorithms and data structures that we discuss in the course. and hopefully, many of these insights will prove useful, more generally, in your other work. third, and perhaps the most relevant for those of you who work in some other discipline: this course should help you learn how to think algorithmically. indeed after studying algorithms it's hard enough not to see them pretty much everywhere, whether you are riding an elevator, watching a flock of birds, buying and selling stocks out of your portfolio, even watching an infant learn. as i said in the previous video algorithm thinking is becoming increasingly useful and prevalent if you are outside of computer science and technology like in biology, statistics and economics. fourth, if you're interested in feeling like a card carrying computer scientist, in some sense, then you'll definitely want basic literacy in all of the topics that we'll be covering. indeed, one of the things that makes studying algorithms so fun, is, it really feels like you're studying a lot of the greatest hits from the last 50 years of computer science. so, after this class, no longer will you feel excluded at that computer science cocktail party when someone cracks a joke about dijkstra's algorithm. now you'll know exactly what they mean. finally, there's no question that studying this material is helpful for technical interview questions. to be clear, my sole goal here is to teach you algorithms, not to prepare you for interviews, per se. but over the years, countless students of mine have regaled me with stories about how mastering the concepts in this class enabled them to ace every technical question they were ever asked. i told you, this is fundamental stuff. so, what do i expect from you? well, honestly, the answer is nothing. after all isn't the whole point of a free online class like this one that anyone can take it and devote as much effort to it as they like. so that said, as a teacher it's still useful to have one or more canonical students in mind. and i thought i'd go ahead and be transparent with you about how i'm thinking about these lectures. who i have in mind that i'm teaching to. so again, please don't feel discouraged if you don't conform to this canonical student template. i'm happy to have the opportunity to teach you about algorithms no matter who you are. so first, i have in mind someone who knows at least some programming. for example, consider the previous lecture. we talked about a recursive approach to multiplying two numbers and i mentioned how in certain mathematical expression, back then we labeled it star and circled it in green. how that expression naturally translated into a recursive algorithm. in particular, i was certainly assuming that you had some familiarity with recursive programs. if you feel comfortable with my statement in that lecture, if you feel like you could code up a recursive integer multiplication algorithm based on the high level outline that i gave you, then you should be in good shape for this course. you should be good to go. if you weren't comfortable with that statement, well, you might not be comfortable with the relatively high conceptual level at which we discuss program in this course. but i encourage to watch the next several videos anyway, to see if you get enough out of them to make it worth your while. [sound]. now, while i'm aiming these lectures at people who know some programming, i'm not making any assumptions whatsoever about exactly which programming languages you know. any standard imperative language you know, something like c, java or python, is totally fine for this course. now, to make these lectures accessible to as many programmers as possible, and to be honest, you know, also to promote thinking about programming at a relatively abstract conceptual level, i won't be describing algorithms in any particular programming language. rather, when i discuss the algorithms, i'll use only high-level pseudo-code, or often simply english. my inductive hypothesis is that you are capable of translating such a high level description into a working program in your favorite programming language. in fact, i strongly encourage everyone watching these lectures to do such a translation of all of the algorithms that we discussed. this will ensure your comprehension, and appreciation of them. indeed, many professional computer scientists and programmers don't feel that they really understand an algorithm until they've coded it up. many of the course's assignments will have a problem in which we ask you to do precisely this. put another way, if you're looking for a sort of coding cookbook, code that you can copy and paste directly into your own programs. without necessarily understanding how it works, then this is definitely not the course for you. there are several books out there that cater to programmers looking for such coding cook books. second, for these lectures i have in mind someone who has at least a modest amount of mathematical experience though perhaps with a fair bit of accumulated rust. concretely i expect you to be able to recognize a logical argument that is a proof. in addition, two methods of proof that i hope you've seen before are proofs by induction and proofs by contradiction. i also need you to be familiar with basic mathematical notation, like the standard quantifier and summation symbols. a few of the lectures on randomized algorithms and hashing will go down much easier for you if you've seen discrete probability at some point in your life. but beyond these basics, the lectures will be self contained. you don't even need to know any calculus, save for a single simple integral that magically pops up in the analys of the randomized quick sort algorithm. i imagine that many of you have studied math in the past, but you could use a refresher, you're a bit rusty. and there's plenty of free resources out there on the web, and i encourage you to explore and find some that you like. but one that i want to particularly recommend is a great set of free lecture notes. it's called mathematics for computer science. it's authored by eric lehman and tom layden, and it's quite easy to find on the web if you just do a web search. and those notes cover all of the prerequisites that we'll need, in addition to tons of other stuff. in the spirit of keeping this course as widely accessible as possible, we're keeping the required supporting materials to an absolute minimum. lectures are meant to be self-contained and we'll always provide you with the lecture notes in powerpoint and pdf format. once in a while, we'll also provide some additional lecture notes. no textbook is required for this class. but that said, most of the material that we'll study is well covered in a number of excellent algorithms books that are out there. so i'll single out four such books here. the first three i mention because they all had a significant influence on the way that i both think about and teach algorithms. so it's natural to acknowledge that debt here. one very cool thing about the second book, the one by dasgupta, papadimitriou and vazirani, is that the authors have made a version of it available online for free. and again, if you search on the authors' names and the textbook title, you should have no trouble coming up with it with a web search. similarly, that's the reason i've listed the fourth book because those authors have likewise made essentially a complete version of that book available online and it's a good match for the material that we're going to cover here. if you're looking for more details about something covered in this class, or simply a different explanation than the one that i give you, all of these books are gonna be good resources for you. there are also a number of excellent algorithm textbooks that i haven't put on this list. i encourage to explore and find you own favorite. >> in our assignments, we'll sometimes ask you to code up an algorithm and use it to solve a concrete problem that is too large to solve by hand. now, we don't care what program and language and development environment you use to do this as we're only going to be asking you for the final answer. thus, we're not requiring anything specific, just that you are able to write and execute programs. if you need help or advice about how to get set up with a suitable coding environment, we suggest that you ask other students for help via the course discussion forum. finally, let's talk a bit more about assessment. now this course doesn't have official grades per se, but we will be assigning weekly homeworks. now we're going to assign homeworks for three different reasons. the first is just for self-assessment. it's to give you the opportunity to test your understanding of the material so that you can figure out which topics you've mastered and which ones that you haven't. the second reason we do it is to impose some structure on the course, including deadlines, to provide you with some additional motivation to work through all the topics. deadlines also have a very important side effect that synchronizes a lot of the students in the class. and this of course makes the course discussion forum a far more effective tool for students to seek and provide help in understanding the course material. the final reason that we give homeworks is to satisfy those of you who, on top of learning the course material, are looking to challenge yourself intellectually. [sound]. now, this class has tens of thousands of students. so it's obviously essential that the assignments can be graded automatically. now, we're currently only in the 1.0 generation of free online courses such as this one. so the available tools for auto graded assessment are currently rather primitive. so, we'll do the best we can, but i have to be honest with you. it's difficult, or maybe even impossible to test deep understanding of the design and analysis of algorithms, using the current set of tools. thus, while the lecture content in this online course is in no way watered down from the original stanford version. the required assignments and exams we'll give you, are not as demanding as those that are given in the on campus version of the course. to make up for this fact, we'll occasionally propose optional algorithm design problems, either in a video or via supplementary assignment. we don't have the ability to grade these, but we hope that you'll find them interesting and challenging, and that you'll discuss possible solutions with other students via the course discussion forum. so i hope this discussion answered most of the questions you have about the course. lets move on to the real reason that we're all here, to learn more about algorithms. 
okay. so in this video, we'll get our first sense of what it's actually like to analyze an algorithm. and we'll do that by first of all reviewing a famous sorting algorithm, namely the merge sort algorithm. and then giving a really fairly mathematically precise upper bound on exactly how many operations the merge sort algorithm requires to correctly sort an input array. so i feel like i should begin with a bit of an apology. here we are in 2012, a very futuristic sounding date. and yet i'm beginning with a really quite ancient algorithm. so for example, merge sort was certainly known, to john von neumann all the way back in 1945. so, what justification do i have for beginning, you know, a modern class in algorithms with such an old example? well, there's a bunch of reasons. one, i haven't even put down on the slide, which is like a number of the algorithms we'll see, "merge sort" as an oldie but a goodie. so it's over 60, or maybe even 70 years old. but it's still used all the time in practice, because this really is one of the methods of choice for sorting. the standard sorting algorithm in the number of programming libraries. so that's the first reason. but there's a number of others as well that i want to be explicit about. so first of all, throughout these online courses, we'll see a number of general algorithm design paradigms ways of solving problems that cut across different application domains. and the first one we're going to focus on is called the divide-and-conquer algorithm design paradigm. so in divide-and-conquer, the idea is, you take a problem, and break it down into smaller sub problems which you then solve recursively, ... ... and then you somehow combine the results of the smaller sub-problem to get a solution to the original problem that you actually care about. and merge sort is still today's the, perhaps the, most transparent application of the divide-and-conquer paradigm, ... ... that will exhibit very clear what the paradigm is, what analysis and challenge it presents, and what kind of benefits you might derive. as for its benefits, so for example, you're probably all aware of the sorting problem. probably you know some number of sorting algorithms perhaps including merge sort itself. and merge sort is better than a lot of this sort of simpler, i would say obvious, sorting algorithms, ... ... so for example, three other sorting algorithms that you may know about, but that i'm not going to discuss here. if you don't know them, i encourage you to look them up in a text book or look them up on the web. let's start with three sorting algorithms which are perhaps simpler, first of all is "selection sort". so this is where you do a number of passes through the way repeatedly, identifying the minimum of the elements that you haven't looked at yet, ... ... so you're basically a linear number of passes each time doing a minimum computation. there's "insertion sort", which is still useful in certain cases in practice as we will discuss, but again it's generally not as good as merge sort, ... ... where you will repeatedly maintain the invariant that prefix view of array, which is sorted version of those elements. so after ten loops of insertion sort, you'll have the invariant that whatever the first ten elements of the array are going to be in sorted order, ... ... and then when insertion sort completes, you'll have an entire sorted array. finally, some of you may know about "bubble sort", which is where you identify adjacent pairs of elements which are out of order, ... and then you do repeated swaps until in the end the array is completely sorted. again i just say this to jog your memory, these are simpler sorts than merge sort, ... ... but all of them are worse in the sense that they're lack in performance in general, which scales with n^2, ... ...  and the input array has n elements, so they all have, in some sense, quadratic running time. but if we use this non-trivial divide-and-conquer approach, or non-obvious approach, we'll get a, as we'll see, a much better running time than this quadratic dependence on the input. okay? so we'll get a win, first sorting in divide-and-conquer, and merge sort is the algorithm that realizes that benefit. so the second reason that i wanna start out by talking about the merge sort algorithm, is to help you calibrate your preparation. i think the discussion we're about to have will give you a good signal for whether you're background's at about the right level, of the audience that i'm thinking about for this course. so in particular, when i describe the merge sort algorithm, you'll notice that i'm not going to describe in a level of detail that you can just translate it line by line into a working program in some programming language. my assumption again is that you're a sort of the programmer, and you can take the high-level idea of the algorithm, how it works, ... ... and you're perfectly capable of turning that into a working program in whatever language you see fit. so hopefully, i don't know, it may not be easy the analysis of merge sort discussion. but i hope that you find it at least relatively straight forward, .. .. because as the course moves on, we're going to be discussing algorithms and analysis which are a bit more complicated than the one we're about to do with merge sort. so in other words, i think that this would be a good warm-up for what's to come. now another reason i want to discuss merge sort is that our analysis of it will naturally segment discussion of how we analyze the algorithms in this course and in general. so we're going to expose a couple of assumptions in our analysis, we're focus on worst case behavior, ... ... or we'll look for guarantees on performance on running time that hold for every possible input on a given size, ... and then we'll also expose our focus on so called "asymptotic analysis", which meaning will be much more concerned with the rate of growth on an algorithms performance than on things like low-order terms or on small changes in the constant factors. finally, we'll do the analysis of merge sort using what's called as "recursion-tree" method. so this is a way of tying up the total number of operations that are executed by an algorithm. and as we'll see a little bit later, this recursion-tree method generalizes greatly. and it will allow us to analyze lots of different recursive algorithms, lots of different divide-and-conquer algorithms, including the integer multiplication algorithm that we discussed in an earlier segment. so those are the reasons to start out with merge sort. so what is the computational problem that merge sort is meant to solve? well, presumably, you all know about the sorting problem. but let me tell you a little bit about it anyways, just so that we're all on the same page. so, we're given as input. an array of n numbers in arbitrary order, and the goal of course is to produce output array where the numbers are in sorted order, let's say, from smallest to largest. okay so, for example, we could consider the following input array, and then the goal would be to produce the following output array. now one quick comment. you'll notice that here in input array, it had eight elements, all of them were distinct, it was the different integers, between 1 and 8. now the sorting problem really isn't any harder if you have duplicates, in fact it can even be easier, ... ... but to keep the discussion as simple as possible let's just, among friends, go ahead and assume that they're distinct, for the purpose of this lecture. and i'll leave it as an exercise which i encourage you to do, which is to think about how the merge sort algorithm implementation and analysis would be different, if at all, if there were ties, okay? go ahead and make the distinct assumption for simplicity from here on out. okay, so before i write down any pseudo code for merge sort, let me just show you how the algorithm works using a picture, ... ... and i think it'll be pretty clear what the code would be, even just given a single example. so let's go ahead and consider the same unsorted input array that we had on the previous slide. so the merge sort algorithm is a recursive algorithm, and again, that means that a program which calls itself and it calls itself on smaller sub problems of the same form, okay? so the merge sort is its purpose in life is to sort the given input array. so it's going to spawn, or call itself on smaller arrays. and this is gonna be a canonical divide-and-conquer application, where we simply take the input array, we split it in half, we solve the left half recursively, we solve the right half recursively, and then we combine the results. so let's look at that in the picture. so the first recursive call gets the first four elements, the left half of the array, namely 5, 4, 1, 8. and, of course, the other recursive call is gonna get the rest of the elements, 7, 2, 6, 3. you can imagine these has been copied into new arrays before they're given to the recursive calls. now, by the magic of recursion, or by induction if you like, the recursive calls will do their task. they will correctly sort each of these arrays of four elements, and we'll get back sorted versions of them. so from our first recursive call, we receive the output, 1, 4, 5, 8, and from the second recursive call, we received the sorted output, 2, 3, 6, 7. so now, all the remains to complete the merge sort is to take the two results of our recursive calls, these two sorted elements of length-4, and combine them to produce the final output, namely the sorted array of all eight of the input numbers. and this is the step which is called "merge". and hopefully you are already are thinking about how you might actually implement this merge in a computationally efficient way. but i do owe you some more details. and i will tell you exactly how the merge is done. in effect, you just walk pointers down each of the two sort of sub-arrays, copying over, populating the output array in the sorted order. but i will give you some more details in just a slide or two. so that's merge sort in a picture. split it in half, solve recursively, and then have some slick merging procedure to combine the two results into a sorted output. 
okay, so let's move on, and actually discuss the pseudo-code for the merge sort algorithm. first, let me just tell you the pseudo-code, leaving aside exactly how the merging subroutine is implemented. and thus, high levels should be very simple and clear at this point. so there's gonna be two recursive calls, and then there's gonna be a merging step. now, i owe you a few comments, 'cause i'm being a little sloppy. again, as i promised, this isn't something you would directly translate into code, although it's pretty close. but so what are the couple of the ways that i'm being sloppy? well, first of all, there's, [inaudible], you know, in any recursive algorithm, you gotta have some base cases. you gotta have this idea that when the input's sufficient. really small you don't do any recursion, you just return some trivial answer. so in the sorting problem the base case would be if your handed an array that has either zero or an elements, well it's already sorted, there's nothing to do, so you just return it without any recursion. okay, so to be clear, i haven't written down the base cases. although of course you would if you were actually implementing, a merge short. some of you, make a note of that. a couple of other things i'm ignoring. i'm ignoring what the, what to do if the array has odd lengths, so if it has say nine elements, obviously you have to somehow break that into five and four or four and five, so you would do that just in either way and that would fine. and then secondly, i'm ignoring the details or what it really means to sort of recursively sort, so for example, i'm not discussing exactly how you would pass these subarrays onto the recursive calls. that's something that would really depend somewhat on what, on the programming language, so that's exactly what i want to avoid. i really want to talk about the concepts which transcend any particular programming language implementation. so that's why i'm going to describe algorithms at this level okay. alright, so the hard part relatively speaking, that is. how do you implement the merge depth? the recursive calls have done their work. we have these two sort of separated half the numbers. the left half and the right half. how do we combine them into one? and in english, i already told you on the last slide. the idea is you just populate the output array in a sorted order, by traversing pointers or just traversing through the two, sorted sub-arrays in parallel. so let's look at that in some more detail. okay, so here is the pseudo-code for the merge step. [sound] so let me begin by, introducing some names for the, characters in the, what we're about to discuss. so let's use c. to denote the output array. so this is what we're suppose to spit out with the numbers in sorted order. and then, i'm gonna use a and b to denote the results of the two recursive calls, okay? so, the first recursive call has given us array a, which contains the left half of the input array in sorted order. similarly, b contains the right half of the input array, again, in sorted order. so, as i said, we're gonna need to traverse the two, sorted sub-arrays, a and b, in parallel. so, i'm gonna introduce a counter, i, to traverse through a, j to traverse through b. i and j will both be initialized to one, to be at the beginning of their respective arrays. and now we're gonna do. we're going to do a single pass of the output array copying it in an increasing order. always taking the smallest from the union of the two sorted sub arrays. and if you, if there's one idea in this merge step it's just the realization that. the minimum element that you haven't yet looked at in a and b has to be at the front of one or the two lists right so for example at the very beginning of the algorithm where is the minimum element over all. well, which ever of the two arrays it lands in -- a or b -- it has to be the smallest one there okay. so the smallest element over all is either the smallest element a or it's the smallest element b. so you just check both places, the smaller one is the smallest you copy it over and you repeat. that's it. so the purpose of k is just to traverse the output array from left to right. that's the order we're gonna populate it. currently looking at position i, and the first array of position j and the second array. so that's how far we've gotten, how deeply we've probed in the both of those two arrays. we look at which one has the current smallest, and we copy the smallest one over. okay? so if the, if, the entry in the i position of a is smaller, we copy that one over. of course, we have to increment i. we probe one deeper into the list a, and symmeterically for the case where the current position in b has the smaller element. now again, i'm being a little bit sloppy, so that we can focus on the forest, and not sort of, and not get bogged down with the trees. i'm ignoring some end cases, so if you really wanted to implement this, you'd have to add a little bit, to keep track of when you fall off, either, either a or b. because you have additional checks for when i or j reaches the end of the array, at which point you copy over all the remaining elements into c. alright, so i'm gonna give you a cleaned up version, of, that pseudo-code so that you don't have to tolerate my questionable handwriting any longer than is absolutely necessary. this again, is just the same thing that we wrote on the last slide, okay? the pseudo-code for the merge step. now, so that's the merge sort algorithm. now let's get to the meaty part of this lecture, which is, okay, so merge sort produces a sorted array. what makes it, if anything, better than much simpler non divide and conquer algorithms, like say, insertion sort? other words, what is the running time of the merge sort algorithm? now i'm not gonna give you a completely precise definition, definition of what i mean by running time and there's good reason for that, as we'll discuss shortly. but intuitively, you should think of the running time of an algorithm, you should imagine that you're just running the algorithm in a debugger. then, every time you press enter, you advance with one line of the program through the debugger. and then basically, the running time is just a number of operations executed, the number of lines of code executed. so the question is, how many times you have to hit enter on the debugger before the, program finally terminates. so we're interested in how many such, lines of code get executed for merge short when an input array has n numbers. okay, so that's a fairly complicated question. so let's start with a more modest school. rather than thinking about the number of operations executed by merge sort, which is this crazy recursive algorithm, which is calling itself over and over and over again. let's just think about how many operations are gonna get executed when we do a single merge of two sorted sub arrays. that seems like it should be an easier place to start. so let me remind you, the pseudo code of the merge subroutine, here it is. so let's just go and count up how many operations that are gonna get used. so there's the initialization step. so let's say that i'm gonna charge us one operation for each of these two initializations. so let's call this two operations, just set i equal to one and j equal to one then we have this four loop executes a total number of end times so each of these in iterations of this four loop how many instructions get executed, well we have one here we have a comparison so we compare a(i) to b(j) and either way the comparison comes up we then do two more operations, we do an assignment. here or here. and then we do an increment of the relevent variable either here or here. so that's gonna be three operations per iteration. and then maybe i'll also say that in order to increment k we're gonna call it a fourth iteration. okay? so for each of these n iterations of the four loop we're gonna do four operations. all right? so putting it all together, what do we have is the running time for merge. so let's see the upshot. so the upshot is that the running time of the merge subroutine, given an array of m numbers, is at most four m plus two. so a couple of comments. first of all, i've changed a letter on you so don't get confused. in the previous slide we were thinking about an input size of n. here i've just made it. see i've changed the name of the variable to m. that's gonna be convenient once we think about merge sort, which is recursing on smaller sub-problems. but it's exactly the same thing and, and whatever. so an array of m entries does as most four m plus two. lines of code. the second thing is, there's some ambiguity in exactly how we counted lines of code on the previous slide. so maybe you might argue that, you know, really, each loop iteration should count as two operations, not just one.'cause you don't just have to increment k, but you also have to compare it to the, upper bound of n. eh, maybe. would have been 5m+2 instead of 4m+2. so it turns out these small differences in how you count up. the number of lines of code executed are not gonna matter, and we'll see why shortly. so, amongst friends, let's just agree, let's call it 4m plus two operations from merge, to execute on array on exactly m entries. so, let me abuse our friendship now a little bit further with an, an inequality which is true, but extremely sloppy. but i promise it'll make our lives just easier in some future calculations. so rather than 4m+2, 'cause 2's sorta getting on my nerves. let's just call this. utmost six n. because m is at least one. [sound] okay, you have to admit it's true, 6mo is at least 4m plus two. it's very sloppy, these numbers are not anything closer to each other for m large but, let's just go ahead and be sloppy in the interest of future simplicity. okay. now i don't expect anyone to be impressed with this rather crude upper bound, the number of lines of code that the merge subroutine needs to finish, to execute. the key question you recall was how many lines of code does merge sort require to correctly sort the input array, not just this subroutine. and in fact, analyzing merge sort seems a lot more intimidating, because if it keeps spawning off these recursive versions of itself. so the number of recursive calls, the number of things we have to analyze, is blowing up exponentially as we think about various levels of the recursion. now, if there's one thing we have going for us, it's that every time we make a recursive call. it's on a quite a bit smaller input then what we started with, it's on an array only half the size of the input array. so there's some kind of tension between on the one hand explosion of sub problems, a proliferation of sub problems and the fact that successive subproblems only have to solve smaller and smaller subproblems. and resolute resolving these two forces is what's going to drive our analysis of merge short. so, the good news is, is i'll be able to show you a complete analysis of exactly how many lines of code merge sort takes. and i'll be able to give you, and, in fact, a very precise upper bound. and so here's gonna be the claim that we're gonna prove in the remainder of this lecture. so the claim is that merge short never needs than more than six times n. times the logarithm of n log base two if you're keeping track plus an extra six n operations to correctly sort an input array of n numbers, okay so lets discuss for a second is this good is this a win, knowing that this is an upper bound of the number of lines of code the merger takes well yes it is and it shows the benefits of the divide and conquer paradigm. recall. in the simpler sorting methods that we briefly discussed like insertion sort, selection sort, and bubble sort, i claimed that their performance was governed by the quadratic function of the input size. that is they need a constant times in the squared number of operations to sort an input array of length n. merge sort by contrast needs at most a constant times n times log n, not n squared but n times log n lines of code to correctly sort an input array. so to get a feel for what kind of win this is let me just remind you for those of you who are rusty, or for whatever reason have lived in fear of a logarithm, just exactly what the logarithm is. okay? so. the way to think about the logarithm is as follows. so you have the x axis, where you have n, which is going from one up to infinity. and for comparison let's think about just the identity function, okay? so, the function which is just. f(n)=n. okay, and let's contrast this with a logarithm. so what is the logorithm? well, for our purposes, we can just think of a logorithm as follows, okay? so the log of n, log base 2 of n is, you type the number n into your calculator, okay? then you hit divide by two. and then you keep repeating dividing by two and you count how many times you divide by two until you get a number that drops below one okay. so if you plug in 32 you got to divide five times by two to get down to one. log base two of 32 is five. you put in 1024 you have to divide by two, ten times till you get down to one. so log base two of 1024 is ten and so on, okay. so the point is you already see this if a log of a 1000 roughly is something like ten then the logarithm is much, much smaller than the input. so graphically, what the logarithm is going to look like is it's going to look like. a curve becomes very flat very quickly, as n grows large, okay? so f(n) being log base 2 of n. and i encourage you to do this, perhaps a little bit more precisely on the computer or a graphing calculator, at home. but log is running much, much, much slower than the identity function. and as a result, sorting algorithm which runs in time proportional to n times log n is much, much faster, especially as n grows large, than a sorting algorithm with a running time that's a constant times n squared. 
in this video we'll be giving a running time analysis of the merge sort algorithm. in particular, we'll be substantiating the claim that the recursive divide and conquer merge sort algorithm is better, has better performance than simple sorting algorithms that you might know, like insertion sort, selection sort and bubble sort. so in particular, the goal of this lecture will be to, mathematically argue the following claim, from an earlier video. that, in order to sort an array of n numbers, the merge sort algorithm needs no more than a constant times n log in operations. that's the maximum number of lines of code it will ever execute. specifically 6 times n log n + 6n operations. so how are we going to prove this claim? we're going to use what is called a recursion tree method. the idea of the recursion tree method is to write out all over the work done by the recursive merge sort algorithm in a tree structure with the children of a given node corresponding to the recursive calls made by that node. the point of this tree structure is it will facilitate a interesting way to count up the overall work done by the algorithm and will greatly facilitate the analysis. so specifically what is this tree? so at level zero we have a root. and this corresponds to the outer call of merge sort, okay so i'm going to call this level zero. now, this tree is going to be binary in recognition of the fact that each invocation of mergesort makes two recursive calls. so the two children will correspond to the two recursive calls of mergesort. so, at the root we operate on the entire input array, so let me draw a big array indicating that, and at level one we have one sub problem for the left half and another sub problem for the right half of the input array. and i'll call these first two recursive calls level one. now, of course, each of these two level one recursive calls will themselves make two recursive calls. each operating on then a quarter of the original input array. so those are level two recursive calls of which there are four. and this process will continue until eventually the recursion bottoms out in base cases when there's only an array of size zero or one. so now i have a question for you which i'll give you in the form of a quiz. which is at the bottom of this recursion tree corresponding to the base cases, what is the level number at the bottom? so at what level do the leaves in this tree reside? okay, so hopefully you guessed, correctly guessed that the answer is the second one. so namely that the number of levels of the recursion tree is essentially logarithmic in the size of the input array. the reason is basically that the input size is being decreased by a factor two with each level of the recursion. if you have an input size of n at the outer level, then each of the first set of recursive calls operates on a array of size n over 2. at level two, each array has size n over 4 and so on, whereas if the recursion bottom out, well, down at the base cases, where there's no more recursion, which is where the input array has size one or less. so in other words, the number of levels of recursion is exactly the number of times you need to divide n by 2, until you get down to a number that's most one. and recall, that's exactly the definition of a logarithm base 2 of n. so since the first level is level zero and the last level is level log base 2 of n. the total number of levels is actually log base 2 of n plus 1. and when i write down this expression i'm here assuming that n is a power of 2 which is not a big deal. i mean the analysis is usually extended to the case where n is not a power of 2. but this way we don't have to think about fractions log base 2 of n then is an integer. okay, so let's return to the recursion tree, and let me just redraw it really quick. so again, down here at the bottom of the tree, we have the leaves. and i.e., the base cases, where there's no more recursion. which, when n is a power of 2, correspond exactly to single element arrays. so that's the recursion tree corresponding to an invocation of mergesort. and the motivation for writing down, for organizing the work performed by mergesort in this way, is it allows us to count up the work level by level. and we'll see that's a particularly convenient way to account for all of the different lines of code that get executed. now to see that in more detail, i need to ask you identify a particular pattern. so first of all, the first question is, at a given level j of this recursion, exactly how many distinct sub problems are there as a function of the level j. that's the first question. the second question is, for each of those distinct sub problems at level j, what is the input size. so what is the size of the array which is passed to a subproblem residing at level j of this recursion tree? so the correct answer is the third one. so first of all at a given level j, there's precisely 2 to the j distinct subproblems. there is one outermost subproblem at level zero. it has two recursive calls. those are the subproblems at level one, and so on. in general, since mergesort calls itself twice, the number of subproblems is doubling each level, so that gives us the expression 2 to the j for the number of subproblems at level j. on the other hand via a similar argument the input size is halving each time with each recursive call you pass at half of the input that you were given. so at each level of the recursion tree we're seeing half of the input size of the previous level. so after j levels since we started with an input size n after j levels each subproblem will be operating on an array of length n over 2 the j. okay so now let's put this pattern to use, and actually count up all of the lines of code, and thenmergesort executes. and as i said before, the key idea is to count up the work level by level. now to be clear, when i talk about the amount of work done at level j. what i'm talking about is the work done by those 2 to the j invocations of mergesort not counting their respective recursive calls, not counting work which is going to get done in the recursion lower in the tree. now recall merge sort is a very simple algorithm, it just three lines of code, first there's a recursive call so we're not counting that, second, there's another recursive call. again, we're not counting that at level j. and then third, we just invoke the merge subroutine. so really outside the recursive cause all that mergesort does is a single indication of merge. further, recall we already have a good understanding of the number of lines of code that merge needs. on an input of size m, it's going to use at most 6m lines of code. that's an analysis that we did in the previous video. so let's fix a level j. we know how many subproblems there are, 2 to the j. we know the size of each subproblem, n over 2 to the j. and we know how much work merge needs on such an input. we just multiply by 6. and then we just multiply it out. and we get the amount of work done at a level j. okay at all of the little j subproblems, so here it is in more detail. all right so, we start with just the number of different subproblems at level j and we just noticed that that was at most 2 to the j. we also observed that each level j subproblem is past an array as input which has length n over 2 to the j. and we know that the merge subroutine, when given an array of size n over 2 to the j, will execute at most 6 times that many number of lines of code. so to compute the total amount of work done at level j, we just multiply the number of problems times the work done per subproblem. and then something sort of remarkable happens. we get this cancellation of the two 2 to the j's and we get an upper bound 6n which is independent of the level j. so we do at most 6n operations at the root. we do at most 6n operations of level one, at level two, and so on okay. it's independent of the level. morally, the reason this is happening because of a perfect equilibrium between two competing forces. first of all, the number of subproblems is doubling with each level of the recursion tree. but secondly, the amount of work that we do per subproblem is halving with each level of the recursion trees. and so those two cancel out. we get an upperbound 6n, which is independent of the level j. now, here's why that's so cool, right, we don't really care about the amount of work just at a given level. we care about the amount of work that mergesort does ever at any level. but if we have a bound on the amount of work at a level which is independent of the level, then our overall bound is really easy. what do we do? we just take the number of levels. and we know what that is. it's exactly log base 2 of n + 1. remember, the levels are zero through log base 2 of n inclusive. and then we have an upper bound 6n for each of those log n plus 1 levels. so, if we expand out this quantity, we get exactly the upper bound that was claimed earlier, namely that the number of operations mergesort executes is at most 6n times log based 2 of n plus 6n. so that my friends, is a running time analysis of the merge sort algorithm. that's why its running time is bounded by a constant times nlogn, which especially has n grows large, it is far superior to the more simple iterative algorithms like insertion or selection sort. 
having completed our first analysis of an algorithm, namely an upper bound on the running time of the merge short algorithm. what i wanna do next is take a step back, and be explicit about three assumptions, three biases that we made when we did this analysis of merge short, and interpreted the results. these three assumptions we will adopt as guiding principles for how to reason about algorithms, and how to define a so called fast algorithm for the rest of the course. so, the first guiding principle is that we used what's often called worst case analysis. by worst case. analysis, i simply mean that our upper bound of six n log n plus six n. applies to the number of lines of executed for every single input array of length end. we made absolutely no assumptions about the input, where it comes from, what it looks like beyond what the input length n was. put differently, if, hypothetically, we had some adversary whose sole purpose in life was to concoct some malevolent input designed to make our algorithm run as slow as possible. the worst this adversary could do, is. upper bounded by this same number, 6n log n + 6n. now, this, so, sort of worst case guarantee popped out so naturally from our analysis of merge short, you might well be wondering, what else could you do? well, two other methods of analysis, which do have their place, although we won't really dicuss them in this course, are quote unquote, average case analysis. and also the use of a set of prespecified benchmarks. by average case analysis, i mean, you analyze the average running time of an algorithm under some assumption about the relative frequencies of different inputs. so, for example, in the sorting problem, one thing you could do, although it's not what we do here. you could assume that every possible input array is equally unlikely, and then analyze the average running time of an algorithm. by benchmarks, i just mean that one agrees up front about some set, say ten or twenty, benchmark inputs, which are thought to represent practical or typical inputs for the algorithm. now, both average-case analysis and benchmarks are useful in certain settings, but for them to make sense, you really have to have domain knowledge about your problem. you need to have some understanding of what inputs are more common than others, what inputs better represent typical inputs than others. by contrast, in worst-case analysis, by definition you're making absolutely no assumptions about where the input comes from. so, as a result, worst-case analysis is particularly appropriate for general-purpose sub-routines. sub-routines that you design. find without having any knowledge of how they will be used or what kind of inputs they will be used on. and happily, another bonus of doing worst case analysis, as we will in this course, it's usually mathematically much more attractable than trying to analyze the average performance of an algorithm under some distribution over inputs. or to understand the detailed behavior of an algorithm on a particular set of benchmark inputs. this mathemetical tractabilty was reflected in our merge sort analysis, where we had no a priori goal of analyzing the worst case, per se. but it's naturally what popped out of our reasoning about the algorithm's running time. the second and third guiding principles are closely related. the second one is that, in this course, when we analyze algorithms, we won't worry unduly about small constant factors or lower order terms. we saw this philosophy at work very early on in our analysis of merge sort. when we discussed the number of lines of code that the merge subroutine requires. we first upper-bounded it by 4m plus two, for an array of length m, and then we said, eh, let's just think about it as 6m instead. let's have a simpler, sloppy upper-bound and work with that. so, that was already an example of not worrying about small changes in the constant factor. now, the question you should be wondering about is, why do we do this, and can we really get away with it? so let me tell you about the justifications for this guiding principle. so the first motivation is clear, and we used it already in our merge short analysis. which is simply way easier mathematically, if we don't have to, precisely pin down what the [inaudible] constant factors and lower-order terms are. the second justification is a little less obvious, but is extremely important. so, i claim that, given the level at which we're describing and analyzing algorithms in this course, it would be totally inappropriate to obsess unduly about exactly what the constant factors are. recall our discussion of the merge subroutine. so, we wrote that subroutine down in pseudocode, and we gave it analysis of 4m plus two on the number of lines of code executed, given an input of length m. we also noted that, it was somewhat ambiguous exactly how many lines of code we should count it as, depending on how you count loop increments and so on. so even there it's small constant factors could creep in given the under specification of the pseudo code. depending on how that pseudo code gets translated into an actual program language like c or java. you'll see the number of lines of code deviate even further, not but a lot but again by small constant factors. when such a program is then compiled down into machine code, you'll see even greater variance depending on the exact processor, the compiler, the compiler optimizations, the programming implementation, and so on. so to summarize, because we're going to describe algorithms at a level. that transcends any particular programming language. it would be inappropriate to specify precise constants. the precise constants were ultimately determined by more machine dependent aspects like who the programmer is, what the compiler is, what the processor is, and so on. and now the third justification is frankly, we're just going to be able to get away with it. [sound] that is, one might be concerned that ignoring things like small constant factors leads us astray. that we wind up deriving results which suggest that an algorithm is fast when it's really slow in practice, or vice versa. and for the problems we discuss in this course we'll get extremely accurate predictive power. even though we won't be keeping track of lower terms and constant factors. when the mathematical analysis we do suggests that an algorithm is fast, indeed it will be. when it suggests that it's not fast, indeed that will be the case. so we lose a little bit of granularity of information. but we don't lose in what we really care about, which is accurate guidance about what algorithms are gonna be faster than others. so the first two justifications, i think, are pretty self evident. this third justification is more of an assertion, but it's one we'll be baking up over and over again as we proceed through this course. now, don't get me wrong. i'm not saying constant factors aren't important in practice. obviously, for crucial programs the constant factors are hugely important. if you're running the sorta crucial loop, you know, your start up's survival depends on, by all means optimize the constant like crazy. the point is just that understanding tiny constant factors in the analysis is an inappropriate level of granularity for the kind of algorithm analysis we're going to be doing in this course. okay, lets move on the, the third and final guiding principle. so the third principle is that we're going to use what's called asymptotic analysis, by which i mean we will focus on the case of a large input sizes. the performance of an algorithm as the size n of the input grows large, that is, tends to infinity. now this focus on large input size is it was already evident when we interpreted our bound on merge sort. so, how did we describe the bound on merge sort? we said, oh, well, it needs a number of operations proportional, a constant fact or times in login. and we very cavalierly declared that this was better than any algorithm which has quadratic dependence of it's running time on the number of operations. so for example, we argued that merge sort is a better, faster algorithm than something like insertion sort, without actually discussing the constant factors at all. so mathematically. we were saying the running time of merge short, which we know, which we can represent as the function. six n log base two of n + 6n is better than any function which has a quadratic dependence on n.  even one with a small constant like lets say 1/2 n squared which might roughly be the running time of insertion sort. and this is a mathematical statement that is true if and only if n is sufficiently large once n grows large it is certainly true that the expression on the left is smaller than the expression on the right but for small n the expression on the right is actually going to be smaller because of the smaller leading term so in saying that merge sort is superior to insertion sort the bias is that we're focusing on problems with a large n so the question you should have is that reasonable is that a justified assumption to focus on large input sizes and the answer is certainly yes. so the reason we focus on large input sizes is because, frankly, those are the only problems which are even, which are at all interesting. if all you need to do is sort 100 numbers, use whatever method you want, and it's gonna happen instantaneously on modern computers. you don't need to know say, the divide and conquer paradigm, if all you need to do is sort 100 numbers. so one thing you might be wondering is if, with computers getting faster all the time according to moore's law, if really, it doesn't even matter to think about algorithmic analysis, if eventually all problem sizes will just be trivially solvable on super fast computers. but, in fact, the opposite is true. moore's law, with computers getting faster, actually says that our computational ambitions will naturally grow. we naturally focus on ever larger problem sizes. and the gulf between an n squared algorithm and an m log n algorithm will become ever wider. a different way to think about it is in terms of how much bigger a problem size you can solve. as computers get faster. if you are using an algorithm with a running time which is proportional to the input size then the computers get faster by a factor of four then you can solve problems that are factor of four or larger. whereas if you are using an algorithm whose running time is proportional to the square of the input size then a computer gets faster by a factor of four, you can only solve double the problem size and we'll see even starker examples of this gulf between different algorithm approaches as the time goes on. so to drive this point home. let me show you a couple of graphs. so what we're looking at here, is we're looking at a graph, of two functions. so the solid function. is the upper bound that we proved on merge sort. so this is gonna be 6nlog(base2)n plus 6n. and the dotted line is an estimate. a rather generous estimate about the running time of, [inaudible] sort. namely one-half times n. squared. and we see here in the graph exactly the behavior that we discussed earlier, which is that the small n. down here. in fact because one-half n. squared has a smaller leading constant it's actually a smaller function. and this is true up to this crossing point of maybe 90 or so. again, beyond n=90. the quadratic growth in the n squared term. overwhelms the fact that it had a smaller constant and it starts being bigger than this other function six of n + six n so in the regime below 90 it's predicting that the insertion store will be better and in the regime above 90 it's predicting that merge sort will be faster. now here's what's interesting let's scale the x axis let's look well beyond this crossing point of 90 let's just increase it in order of magnitude up to a raise in size 1500. and i want to emphasize these are still very small problem sizes. if all you need to do is sort arrays of size 1500 you really don't need to know divide-and-conquer or anything else we'll talk about -- that's a pretty trivial problem on modern computers. [sound]. so what we're seeing is, that even for very modest problem sizes here, array of, of, size, say 1500. the quadratic dependence in the insertion sort bound is more than dwarfing the fact, that it had a lower constant factor. so in this large regime, the gulf between the two algorithms is growing. and of course, if i increased it another 10x or 100x or 1000x to get to genuinely interesting problem sizes, the gap between these two algorithms would be even bigger, it would be huge. that said, i'm not saying you should be completely ignorant of constant factors when you implement algorithms. it's still good to have a general sense of what these constant factors are so for example in highly tuned versions of merge sort which you'll find in mny programming libraries. in fact, because of the difference in constant factors, the algorithm will actually switch from merge sort over to insertion sort, once the problem size drops below some particular threshold, say seven elements, or something like that. so for small problem sizes, you use the algorithm with smaller constant factors, and the insertion sort for larger problem sizes, you use the algorithm and better rate of growth, mainly merge short. so, to review our first guiding principal is that we're going to pursue worse case analysis. we're going to look to bounds on the performance, on the running time of an algorithm which make no domain assumptions, which make no assumptions about which input of a given length the algorithm is provided. the second guiding principal is we're not going to focus on constant factors or lower returns, that would be inappropriate, given the level of granularity at which we're describing algorithms and third is where going to focus on the rate of growth of algorithms for large problem sizes. putting these three principles together, we get a mathematical definition of a fast algorithm. namely, we're gonna pursue algorithms whose worst case running time grows slowly as a function of the input size. so let me tell you how you should interpret what i just wrote down in this box. so on the left hand side is clearly what we want. okay, we want algorithms which run quickly if we implement them. and on the right hand side is a proposed mathematical surrogate of a fast algorithm. right, the left hand side is not a mathematical definition. the right hand side is, as well become clear in the next set of lectures. so we're identifying fast algorithms, which, those that have good asymptotic running time which grows slowly with the input size. now, what would we want from a mathematical definition? we'd want a sweet spot. on one hand we want something we can actually reason about. this is why we zoom out and squint and ignore things like constant factors and lower terms. we can't keep track of everything. otherwise we'd never be able to analyze stuff. on the other hand we don't want to throw out the baby with the bath water, we want to retain predictive power and this turns out this definition turns out for the problems we're going to talk about in this course, to be the sweet spot for reasoning about algorithms okay worst case analysis using the asymptotic running time. we'll be able to prove lots of theorems. we'll be able to establish a lot of performance guarantees for fundamental algorithms but at the same time we'll have good predictive power what the theory advocates will in fact be algorithms that are known to be best in practice. so, the final explanation i owe you, is, what do i mean by, the running time grows slowly, with respect to the input size? well, the answer depends a little bit on the context, but, for almost all of the problems we're going to discuss, the holy grail will be to have what's called a linear time algorithm, an algorithm whose number of instructions grows proportional to the input size. so, we won't always be able to achieve linear time, but that's, in some sense, the best case scenario. notice, linear time is even better than what we achieved with our merge short algorithm for sorting. merge short runs a little bit superlinear, it's n times log n, running as the input size. if possible, we. to be linear time. it's not always gonna be possible, but that is what we will aspire toward. for most of the problems we'll discuss in this course. looking ahead, the next series of videos is going to have two goals. first of all, on the analysis side, i'll describe formally what i mean by asymptotic running time. i'll introduce "big oh" notation and its variants, explain its mathematical definitions, and give a number of examples. on the design side, we'll get more experience applying the divide and conquer paradigm to further problems.  see you then. 
in this sequence of lectures, we're going to learn asymptotic analysis. this is the language, by which every serious computer programmer and computer scientist, discusses, the high level performance, of computer algorithms. as such, it's a totally, crucial, topic. in this video, the plan is to segueway between the high level discussion that you've already seen in the course introduction. and the mathematical formalism which we're going to start developing in the next video. before getting into that mathematical formalism, however, i want to make sure that the topic is well motivated. that you have solid intuition for what it's trying to accomplish, and also that you've seen a couple of simple, intuitive examples. let's get started. asymptotic analysis provides basic vocabulary for discussing the design and analysis of algorithms. and while it is a mathematical concept, it is by no means math for math's sake. you will very frequently hear serious programmers saying that such and such code runs in o of n time where such and such other code runs in o of n squared time. it's important you know what programmers mean when they make statements like that. the reason this vocabulary is so ubiquitous is that it identifies a sweet spot for discussing the high level performance of algorithms. what i mean by that, is it is on the one hand, coarse enough, to suppress all of the details that you want to ignore. details that depend on the choice of architecture, the choice of programming language, the choice of compiler, and so on. on the other hand it's sharp enough to be useful. in particular, to make predictive comparisons between different high level algorithmic approaches to solving a common problem. this is going to be especially true for large inputs. and remember as we discussed in some sense, large inputs are the interesting ones, those are the ones for which we need algorithmic ingenuity. for example, asymptotic analysis will allow us to differentiate between better and worse approaches to sorting. better and worse approaches to multiplying two integers and so on. now most serious programmers if you ask them, what's the deal with asymptotic analysis anyways? they'll tell you reasonably, that the main point is to suppress both leading constant factors and lower order terms. now, as we'll see, there's more asymptotic analysis than just these seven words here. but long term, ten years from now if you only remember seven words about asymptotic analysis, i'll be reasonably happy if these are the seven words that you remember. so how do we justify adopting a formalism which essentially by definition, suppresses constant factors and lower order terms? well lower order terms basically, by definition, become increasingly irrelevant as you focus on large inputs. which, as we've argued, are the interesting inputs, the ones where algorithmic ingenuity is important. as far as constant factors, these are going to be highly dependent on the details of the environment, the compiler, the language and so on. so if we want to ignore those details, it makes sense to have a formalism which doesn't focus unduly on leading constant factors. here's an example. remember when we analyzed the merge sort algorithm, we gave an upper bound on its running time that was 6 times n log n plus 6n. where n was the input length, the number of numbers in the input array. so the lower order term here is the 6n. that's growing more slowly than than n log n, so we just drop that. and then the leading constant factor is the 6 so we suppress that as well. after those two suppressions we're left with a much simpler expression, n log n. the terminology would then be to say that the running time of merge sort is big o of n log n. so in other words, when you say that an algorithm's running time is big o of some function. what you mean is that after you've dropped the lower order terms, and suppressed the leading constant factor, you're left with the function f of n. intuitively, that is what the big o notation means. so to be clear, i'm certainly not asserting thay constant factors never matter when you're designing and analyzing algorithms. rather, i'm just saying that when you think about high level algorithmic approaches, when you might want to make a comparison between fundamentally different ways of solving a problem. asymptotic analysis is often the right tool for giving you guidance about which one is going to perform better. especially on reasonably large inputs. now, once you've committed to a particularly algorithmic solution to a problem. of course, you might want to then work harder to improve the leading constant factor, perhaps even to improve the lower order terms. by all means if the future of your start up depends on how efficiently you implement some particular set of lines of code, have at it. make it as fast as you can. in the rest of this video i want to go through four very simple examples. in fact these examples are so simple, if you have any experience with big o notation, you're probably better off just skipping the rest of this video. and moving on to the mathematical formalism that we begin in the next video. but if you've never seen it before, i hope these simple examples will get you oriented. so let's begin with a very basic problem, searching an array for a given integer. let's analyze the straight forward algorithm for this problem where we just do a linear scan through the array. checking each entry to see if it is the desired integer t. that is the code just checks each array entry in turn. if it ever finds the integer t it returns true. if it falls off the end of the array without finding it, it returns false. so what do you think? we haven't formally defined big o notation, but i've given you an intuitive description. what would you say is the running time of this algorithm as a function of the length of the array, capital a? so the answer i'm looking for is c, big o of n. or equivalently we would say that the running time of this algorithm is linear in the input length n. why is that true? well let's think about how many operations this piece of code is going to execute. actually the lines of code executed is going to depend on the input. it depends on whether or not the target t is contained in the array a. and if so, where in the array a it lies. but, in the worst case this code will do an unsuccessful search. t will not be in the array and the code will scan through the entire array a and return false. the number of operations then is a constant. there is some initial setup, perhaps and maybe it's an operation to return this final bullying value. but outside of that constant, which will get suppressed in the big o notation, it does a constant number of operations per entry in the array. and you can argue about what the constant is, if it's two, three, four operations per entry point in the array. but the point is, whatever that constant is two, three, or four, it gets conveniently suppressed by the big o notation. so as a result, the total number of operations would be linear in n, and so the big o notation will just be o of n. so that was the first example. in the last three examples, i want to look at different ways that we could have two loops. and in this example, i want to think about one loop followed by another, so two loops in sequence. i want to study almost the same problem as the previous one. where now we are just given two arrays, capital a and capital b, let's say both are the same length. and we want to know whether the target t is in either one of them. again, we'll look at the straight forward algorithm that we just searched through a. and if we fail to find t and a, we search through b. if we don't find t and b either, than we have to return return false. so the question then is exactly the same as last time given this new, longer piece of code. what in big o notation is it's running time? well, the question was the same and in this case the answer was the same. so this algorithm, just like the last one, has running time big o of n. if we actually count the number of operations of course it won't be exactly the same as last time. it'll be roughly twice as many operations as the previous piece of code. that's because we have to search two different arrays each of link in. so, whatever work we did before we now do it twice as many times. of course that two being a constant. independent of the input length n, is going to get suppressed once we passed a big o notation. so this, like the previous algorithm, is a linear time algorithm, it has running time big o of n. let's look at a more interesting example of two loops, where rather than processing each loop in sequence, they're going to be nested. in particular, let's look at the problem of searching whether two given input arrays each of link n, contain a common number. the code that we're going to look at for solving this problem is the most straight forward one you can imagine, where we just compare all possibilities. so for each index i into the array a, each index j into the array b. we just see a if a[i] is the the same number as b[j]. if it is we return true. if we exhaust all of the possibilities without ever finding equal elements, then we're safe in returning a false. the question of course is in terms of big o notation asymptotic analysis as a function of the array length n, what is the running time of this piece of code? so this time the answer has changed. for this piece of code the running time is not big o with n, but it is big o of n squared. so, we might also called this a quadratic time algorithm. because the running time is quadratic in the input length n. so, this is one of those kinds of algorithms where if you double the input link. then the running time of the algorithm will go up by a factor of four rather than by a factor of two, like in the previous two pieces of code. so why is this? why does it have quadratic running time big o of n squared? well again, there's some constant setup costs which gets suppressed in the big o notation. again, for each fixed choice of an entry i into array a and an index j for array b, for each fixed choice of i and j, we only do a constant number of operations. the particular constants are relevant because it gets suppressed in the big o notation. what's difference is that there's a total of n squared iterations of this double four loop. in the first example we only had n iterations of a single four loop. in our second example because one four loop completed before the second one began we had only 2n iterations overall. here, for each of the n iterations of the outer four loop, we do n iterations of the inner four loop. so that gives us the n times n, ie n squared total iterations. so that's going to be the running time of this piece of code. lets wrap up with one final example, it will again be nested four loops. but this time we're going to be looking for duplicates in a single array a. rather than needing to compare two distinct arrays a and b. so here's the piece of code we're going to analyze for solving this problem, for detecting whether or not the input array a has duplicate entries. there's only two small differences relative to the code that we went through on the previous slide when we had two different arrays. the first change won't surprise you at all which is instead of referencing the array b, i change that b to an a. all right, so i'm just going to compare the i th entry of a with the j th entry of a. the second change is a little bit more subtle. which is i changed the inner for loops so that the index j begins at i plus one. where i is the current value of the outer four loop's index, rather than starting at the index one. i could have had it start at the index one, that would still be correct, but it would be wasteful and you should think about why. if we started the inner four loops index at one, then this code would actually compare each distinct pair of elements of a to each other twice. which of course is silly. you only need to compare two different elements of a to each other once, to know whether they're equal or not. so this is the piece of code, the question is the same as it always is. what in terms of big o notation and the input link n is the running time of this piece of code? so the answer to this question, same as the last one, big o of n squared. that is this piece of code is also has quadratic running time. so what i hope was clear was that whatever the running time of this piece of code is. it's proportional to the number of iterations of this double four loop. like in all the examples, we do constant work per iteration. we don't care about the constant, it gets suppressed by the big o notation. so all we gotta do is figure out how many iterations there are of this double for loop. my claim is that there's roughly n squared over two iterations of this double four loop. there's a couple ways to see that. informally we discussed how the difference between this code and the previous one, is that instead of counting something twice, we're counting it once. so that saves us a factor of two in the number of iterations. of course, this one half factor gets suppressed by the big o notation anyways. so the big o running time doesn't change. a different argument would just say, there's one iteration for every distinct choice of i and j of indices between one and n. and a simple counting argument says that there's n choose two such choices of distinct i and j. where n choose two is the number n times n minus one over two. and again suppressing lower order terms in the constant factor, we still get a quadratic dependence on the length of the input array a. so that wraps up some of the sort of just simple basic examples. i hope this gets you oriented, you have a strong intuitive sense, for what big o notation is trying to accomplish, and how it's defined mathematically. let's now move on to both the mathematical development and some more interesting algorithms. 
in the following series of videos, we'll give a formal treatment of asymptotic notation, in particular big-oh notation, as well as work through a number of examples. big-oh notation concerns functions defined on the positive integers, we'll call it t(n) we'll pretty much always have the same semantics for t(n). we're gonna be concerned about the worst-case running time of an algorithm, as a function of the input size, n. so, the question i wanna answer for you in the rest of this video, is, what does it mean when we say a function, t(n), is big-oh of f(n). or hear f(n) is some basic function, like for example n log n. so i'll give you a number of answers, a number of ways of, to think about what big-oh notation really means. for starters let's begin with an english definition. what does it mean for a function to be big-oh of f(n)? it means eventually, for all sufficiently large values of n, it's bounded above by a constant multiple of f(n). let's think about it in a couple other ways. so next i'm gonna translate this english definition into picture and then i'll translate it into formal mathematics. so pictorially you can imagine that perhaps we have t(n) denoted by this blue functions here. and perhaps f(n) is denoted by this green function here, which lies below t(n). but when we double f(n), we get a function that eventually crosses t(n) and forevermore is larger than it. so in this event, we would say that t(n) indeed is a big-oh of f(n). the reason being that for all sufficiently large n, and once we go far enough out right on this graph, indeed, a constant multiple times of f(n), twice f(n), is an upper bound of t(n). so finally, let me give you a actual mathematical definition that you could use to do formal proofs. so how do we say, in mathematics, that eventually it should be bounded above by a constant multiple of f(n)? we see that there exists two constants, which i'll call c and n0. so that t(n) is no more than c times f(n) for all n that exceed or equal n0. so, the role of these two constants is to quantify what we mean by a constant multiple, and what we mean by sufficiently large, in the english definition. c obviously quantifies the constant multiple of f(n), and n0 is quantifying sufficiently large, that's the threshold beyond which we insist that, c times f(n) is an upper-bound on t(n). so, going back to the picture, what are c and n0? well, c, of course, is just going to be two. and n0 is the crossing point. so we get to where two f(n). and t(n) cross, and then we drop the acentode. this would be the relative value of n0 in this picture, so that's the formal definition, the way to prove that something's bigger of f(n) you exhibit these two constants c and n0 and it better be the case that for all n at least n0, c times f(n) upper-bounds t(n). one way to think about it if you're trying to establish that something is big-oh of some function it's like you're playing a game against an opponent and you want to prove that. this inequality here holds and your opponent must show that it doesn't hold for sufficiently large n you have to go first your job is to pick a strategy in the form of a constant c and a constant n0 and your opponent is then allowed to pick any number n larger than n0 so the function is big-oh of f(n) if and only if you have a winning strategy in this game. if you can up front commit to constants c and n0 so that no matter how big of an n your opponent picks, this inequality holds if you have no winning strategy then it's not big-oh of f(n) no matter what c and n0 you choose your opponent can always flip this in equality. by choosing a suitable, suitable large value of n. i want to emphasis one last thing which is that these constants, what do i mean by constants. i mean they are independent of n. and so when you apply this definition, and you choose your constant c and n0, it better be that n does not appear anywhere. so c should just be something like a thousand or a million. some constant independent of n. so those are a bunch of way to think about big-oh notation. in english, you wanna have it bound above for sufficiently large numbers n. i'm showing you how to translate that into mathematics that give you a pictorial representation. and also sort of a game theoretic way to think about it. now, let's move on to a video that explores a number of examples. 
having slogged through the formal definition of big o notation, i wanna quickly turn to a couple of examples. now, i wanna warn you up front, these are pretty basic examples. they're not really gonna provide us with any insight that we don't already have. but they serve as a sanity check that the big o notation's doing what its intended purpose is. namely to supress constant factors and low order terms. obviously, these simple examples will also give us some, facility with the definition. so the first example's going to be to prove formally the following claim. the claim states that if t(n) is some polynomial of degree "k", so namely a<u>k  n^k. plus all the way up to a<u>1  n + a<u>0. for any integer "k", positive</u></u></u> integer "k" and any coefficients a<u>i's positive or negative. then: t(n) is big</u> o of n^k. so this claim is a mathematical statement and something we'll be able to prove. as far as, you know, what this claim is saying, it's just saying big o notation really does suppress constant factors and lower order terms. if you have a polynomial then all you have to worry about is what is the highest power in that polynomial and that dominates its growth as "n" goes to infinity. so, recall how one goes about showing that one function is big o of another. the whole key is to find this pair of constants, c and n<u>0, where c quantifies the constant multiple</u> of the function you're trying to prove big o of, and n<u>0 quantifies what you mean</u> by "for all sufficiently large n." now, for this proof, to keep things very simple to follow, but admittedly a little mysterious, i'm just gonna pull these constants, c and n<u>0, out of a hat. so, i'm not gonna tell you how i derived them,</u> but it'll be easy to check that they work. so let's work with the constants n<u>0</u> equal to one, so it's very simple choice of n<u>0 and then "c" we are gonna pick to</u> be sum of the absolute values of the coefficients. so the absolute value of "a<u>k"</u> plus the absolute value of "a<u>(k-1)", and so on. remember i didn't assume that</u> the pol..., the original polynomial, had non-negative coefficients. so i claim these constants work, in the sense that we'll be able to prove to that, assert, you know, establish the definition of big o notation. what does that mean? well we need to show that for all "n" at least one (cause remember we chose n<u>0 equal to</u> one), t(n) (this polynomial up here) is bounded above by "c" times "n^k", where "c" is the way we chose it here, underlined in red. so let's just check why this is true. so, for every positive integer "n" at least one, what do we need to prove? we need to prove t(n) is upper bounded by something else. so we're gonna start on the left hand side with t(n). and now we need a sequence of upper bounds terminating with "c" times "n^k" (our choice of c underlined in red). so t(n) is given as equal to this polynomial underlined in green. so what happens when we replace each of the coefficients with the absolute value of that coefficient? well, you take the absolute value of a number, either it stays the same as it was before, or it flips from negative to positive. now, "n" here, we know is at least one. so if any coefficient flips from negative to positive, then the overall number only goes up. so if we apply the absolute value of each of the coefficients we get an only bigger number. so t(n) is bounded above by the new polynomial where the coefficients are the absolute values of those that we had before. so why was that a useful step? well now what we can do is we can play the same trick but with "n". so it's sort of annoying how right now we have these different powers of "n". it would be much nicer if we just had a common power of "n", so let's just replace all of these different "n"s by "n^k", the biggest power of "n" that shows up anywhere. so if you replace each of these lower powers of "n" with the higher power "n^k", that number only goes up. now, the coefficients are all non negative so the overall number only goes up. so this is bounded above by "the absolute value of a<u>k"  "n^k"</u> ...up to "absolute value of a<u>1"  "n^k" ...plus "a<u>0"  "n^k".</u></u> i'm using here that "n" is at least one, so higher powers of "n" are only bigger. and now you'll notice this, by our choice of "c" underlined in red, this is exactly equal to "c" times "n^k". and that's what we have to prove. we have to prove that t(n) is at most "c" times "n^k", given our choice of "c" for every "n" at least one. and we just proved that, so, end of proof. now there remains the question of how did i know what the correct, what a workable value of "c" and "n<u>0"</u> were? and if you yourself want to prove that something is big o of something else, usually what you do is you reverse engineer constants that work. so you would go through a proof like this with a generic value of "c" and "n<u>0" and then</u> you'd say, "ahh, well if only i choose "c" in this way, i can push the proof through." and that tells you what "c" you should use. if you look at the optional video on further examples of asymptotic notation, you'll see some examples where we derive the constants via this reverse engineering method. but now let's turn to a second example, or really i should say, a non-example. so what we're going to prove now is that something is not big o of something else. so i claim that for every "k" at least 1, "n^k" is not o(n^(k-1)).  and again, this is something you would certainly hope would be true. if this was false, there'd be something wrong with our definition of big o notation and so really this is just to get further comfort with the definition, how to prove something is not big o of something else, and to verify that indeed you don't have any collapse of distinctive powers of ploynomials, which would be a bad thing. so how would we prove that something is not big o of something else? the most...frequently useful proof method is gonna be by contradiction. so, remember, proof by contradiction, you assume what you're trying to, establish is actually false, and, from that, you do a sequence of logical steps, culminating in something which is just patently false, which contradicts basic axioms of mathematics, or of arithmetic. so, suppose, in fact, n^k was big o of n^(k-1), so that's assuming the opposite of what we're trying to prove. what would that mean? well, we just referred to the definition of big o notation. if in fact "n^k" hypothetically were big o of n^(k-1) then by definition there would be two constants, a winning strategy if you like, "c" and "n<u>0" such</u> that for all sufficiently large "n", we have a constant multiple "c" times "n^(k-1)" upper bounding "n^k". so from this, we need to derive something which is patently false that will complete the proof. and the way, the easiest way to do that is to cancel "n^(k-1)" from both sides of this inequality. and remember since "n" is at least one and "k" is at least one, it's legitimate to cancel this "n^(k-1)" from both sides. and when we do that we get the assertion that "n" is at most some constant "c" for all "n" at least "n<u>0". and this now</u> is a patently false statement. it is not the case that all positive integers are bounded above by a constant "c". in particular, "c+1", or the integer right above that, is not bigger than "c". so that provides the contradiction that shows that our original assumption that "n^k" is big o of "n^(k-1)" is false. and that proves the claim. "n^k" is not big o of "n^(k-1)", for every value of "k". so different powers of polynomials do not collapse. they really are distinct, with respect to big o notation. 
in this lecture, we'll continue our formal treatment of asymptotic notation. we've already discussed big o notation, which is by far the most important and ubiquitous concept that's part of asymptotic notation, but, for completeness, i do want to tell you about a couple of close relatives of big o, namely omega and theta. if big o is analogous to less than or equal to, then omega and theta are analogous to greater than or equal to, and equal to, respectively. but let's treat them a little more precisely. the formal definition of omega notation closely mirrors that of big o notation. we say that one function, t of n, is big omega of another function, f of n, if eventually, that is for sufficiently large n, it's lower bounded by a constant multiple of f of n. and we quantify the ideas of a constant multiple and eventually in exactly the same way as before, namely via explicitly giving two constants, c and n naught, such that t of n is bounded below by c times f of n for all sufficiently large n. that is, for all n at least n naught. there's a picture just like there was for big o notation. perhaps we have a function t of n which looks something like this green curve. and then we have another function f of n which is above t of n. but then when we multiply f of n by one half, we get something that, eventually, is always below t of n. so in this picture, this is an example where t of n is indeed big omega of f of n. as far as what the constants are, well, the multiple that we use, c, is obviously just one half. that's what we're multiplying f of n by. and as before, n naught is the crossing point between the two functions. so, n naught is the point after which c times f of n always lies below t of n forevermore. so that's big omega. theta notation is the equivalent of equals, and so it just means that the function is both big o of f of n and omega of f of n. an equivalent way to think about this is that, eventually, t of n is sandwiched between two different constant multiples of f of n. i'll write that down, and i'll leave it to you to verify that the two notions are equivalent. that is, one implies the other and vice versa. so what do i mean by t of n is eventually sandwiched between two multiples of f of n? well, i just mean we choose two constants. a small one, c1, and a big constant, c2, and for all n at least n naught, t of n lies between those two constant multiples. one way that algorithm designers can be quite sloppy is by using o notation instead of theta notation. so that's a common convention and i will follow that convention often in this class. let me give you an example. suppose we have a subroutine, which does a linear scan through an array of length n. it looks at each entry in the array and does a constant amount of work with each entry. so the merge subroutine would be more or less an example of a subroutine of that type. so even though the running time of such an algorithm, a subroutine, is patently theta of n, it does constant work for each of n entries, so it's exactly theta of n, we'll often just say that it has running time o of n. we won't bother to make the stronger statement that it's theta of n. the reason we do that is because you know, as algorithm designers, what we really care about is upper bounds. we want guarantees on how long our algorithms are going to run, so naturally we focus on the upper bounds and not so much on the lower bound side. so don't get confused. once in a while, there will a quantity which is obviously theta of f of n, and i'll just make the weaker statement that it's o of f of n. the next quiz is meant to check your understanding of these three concepts: big o, big omega, and big theta notation. so the final three responses are all correct, and i hope the high level intuition for why is fairly clear. t of n is definitely a quadratic function. we know that the linear term doesn't matter much as it grows, as n grows large. so since it has quadratic growth, then the third response should be correct. it's theta of n squared. and it is omega of n. so omega of n is not a very good lower bound on the asymptotic rate of growth of t of n, but it is legitimate. indeed, as a quadratic growing function, it grows at least as fast as a linear function. so it's omega of n. for the same reason, big o of n cubed, it's not a very good upper bound, but it is a legitimate one, it is correct. the rate of growth of t of n is at most cubic. in fact, it's at most quadratic, but it is indeed, at most, cubic. now if you wanted to prove these three statements formally, you would just exhibit the appropriate constants. so for proving that it's big omega of n, you could take n naught equal to one, and c equal to one-half. for the final statement, again you could take n naught equal to one. and c equal to say four. and to prove that it's theta of n squared you could do something similar just using the two constants combined. so n naught would be one. you could take c1 to be one-half and c2 to be four. and i'll leave it to you to verify that the formal definitions of big omega, big theta, and big o would be satisfied with these choices of constants. one final piece of asymptotic notation, we're are not going to use this much, but you do see it from time to time so i wanted to mention it briefly. this is called little o notation, in contrast to big o notation. so while big o notation informally is a less than or equal to type relation, little o is a strictly less than relation. so intuitively it means that one function is growing strictly less quickly than another. so formally we say that a function t of n is  little o of f of n, if and only if for all constants c, there is a constant n naught beyond which t of n is upper bounded by this constant multiple c times by f of n. so the difference between this definition and that of big-o notation, is that, to prove that one function is big o of another, we only have to exhibit one measly constant c, such that c times f of n is upper bound, eventually, for t of n. by contrast, to prove that something is little o of another function, we have to prove something quite a bit stronger. we have to prove that, for every single constant c, no matter how small, for every c, there exists some large enough n naught beyond which t of n is bounded above by c times f of n. so, for those of you looking for a little more facility with little o notation, i'll leave it as an exercise to prove that, as you'd expect for all polynomial powers k, in fact, n to the k minus one is little o of n to the k. there is an analogous notion of little omega notation expressing that one function grows strictly quicker than another. but that one you don't see very often, and i'm not gonna say anything more about it. so let me conclude this video with a quote from an article, back from 1976, about my colleague don knuth, widely regarded as the grandfather of the formal analysis of algorithms. and it's rare that you can pinpoint why and where some kind of notation became universally adopted in the field. in the case of asymptotic notation, indeed, it's very clear where it came from. the notation was not invented by algorithm designers or computer scientists. it's been in use in number theory since the nineteenth century. but it was don knuth in '76 that proposed that this become the standard language for discussing rate of growth, and in particular, for the running time of algorithms. so in particular, he says in this article, "on the basis of the issues discussed here, i propose that members of sigact," this is the special interest group of the acm, which is concerned with theoretical computer science, in particular the analysis of algorithms. so, "i propose that the members of sigact and editors in computer science and mathematics journals adopt the o, omega, and theta notations as defined above unless a better alternative can be found reasonably soon. so clearly a better alternative was not found and ever since that time this has been the standard way of discussing the rate of growth of running times of algorithms and that's what we'll be using here. 
this video is for those of you who want some additional practice with asymptotic notation. and we're gonna go through three additional optional examples. let's start with the first one. so the point of this first example is to show how to formally prove that one function is big o of another. so the function that i want to work with is two raised to the n plus ten, okay, so it's the two to the n function that you're all familiar with, we're going to shift it by ten and the claim is that this function is big o of two to the n, so without the ten. so how would one prove such a claim? well lets go back to the definition of what it means for one function to be big o over another, what we have to prove is we need to show that there exists two constants, so that for all sufficiently large n meaning n bigger than n-nought, our left hand side, so the function should be n plus ten is bounded above by a constant multiple c times the function on right hand side to the n. right so for all sufficiently large n the function is bounded above by a constant multiple of two to the n. so unlike the first basic example where i just pulled the two constants out of a hat let's actually start the proof and see how you'd reverse engineer the suitable choice of these two constants. so, what a proof would look like, it would start with two to the n plus ten, on the left-hand side, and then there'd be a chain of inequalities, terminating in this, c times two to the n. so, let's just go ahead and start such a proof, and see what we might do. so, if we start with two to the n plus ten on the left-hand side, what would our first step look like? well, this 10's really annoying, so it makes sense to separate it out. so you could write two to the n plus ten as the product of two terms. two to the ten, and then the two to the n. also known as just 1024 times two to the n. and now we're in, looking in really good shape. so if you look at where we are so far, and where we want to get to, it seems like we should be choosing our constant c to be 1024. so if we choose c to be 1024 and we don't have to be clever with n-nought we can just set that equal to one, then indeed star holds to the desired inequality and remember to prove that one function is big o of another all you gotta do is come up with one pair of constants that works and we've just reverse engineered it just choosing the constant c to be 1024 and n-nought to be one works so this proves that two to the n plus ten is big o over two to the n. next let's turn to another non example how, of a function which is not big o over another. and so this will look superficially similar to the previous one. instead of taking, adding ten in the exponent of the function two to the n, i'm gonna multiply by ten in the exponent. and the claim is if you multiply by ten in the exponent then this is not the same asymptotically as two to the n. so once again, usually the way you prove one thing is not big o of another is by contridiction. so we're going to assume the contrary, that two to the ten n is in fact big o of two to the n. what would it mean if that were true? well, by the definition of big o notation, that would mean there are constant c and n-nought. so that for all large n, two to the ten n is bounded above by c times 2 to the n. so to complete the proof what we have to do is go from this assumption and derive something which is obviously false but that's easy to do just by cancelling this 2 of the n terms from both sides. so if we divide both sides by 2 to the n, which is a positive number since n is positive, what we find would be a logical consequence of our assumption would be that two raised to the nine n is bounded above by some fixed constant c for all n at least n-nought. but this inequality of course is certainly false. the right hand side is some fixed constant independent of n. the left hand side is going to infinity as n grows large. so there's no way this inequality holds for arbitrarily large n. so that concludes the proof by contradiction. this means our assumption was not the case, and indeed it is not the case that two to the ten n is big o of two to the n. so our third and final example is a little bit more complicated than the first two. it'll give us some practice using theta notation. recall that while big o is analogous to saying one function is less than or equal to another, theta notation is in the spirit of saying one function is equal asymptotically to another. so here's gonna be the formal claim we're gonna prove, for every pair of functions f and g, both of these functions are defined on the positive integers, the claim is that it doesn't matter, up to a constant factors, whether we take point wise maximum of the two functions or whether we take the point wise sum of the two functions. so let me make sure it's clear that you know i mean by the point wise maximum by max f and g. so, if you look at the two functions, both functions of n, maybe we have f being this green function here and we have g hooked to this red function. then by the point wise maximum max(f,g) just means the upper envelope of these two functions. so that's gonna be this blue function. so lets now turn to the proof of this claim that the point wise function of these two function is theta of the sum of two functions. so let's recall what theta means formally. what it means is that the function on the left can be sandwiched between the constant multiples of the function on the right. so we need to exhibit both the usual n-nought but also two constants, the small one, c1, and the big one, c2, so that the point wise maximum(f,g), whatever that may be, is wedged in between c1 and c2 times f(n) plus g(n), respectively. so to see where these constants c1 and c2 are going to come from, let's observe the following inequalities. so no matter what n is, any positive integer n, we have the following. suppose we take the larger of f of n and g of n. and remember now, we've fixed the value of n, and it's just some integer, you know, like 23. and now f of n and g of n are theirselves, just numbers. you know, maybe they're 57 and 74, or whatever. and if you take the larger of f of n and g of n, that's certainly no more than the sum of f of n plus g of n. now, i'm using, in this inequality, that f and g are positive. and that's something i've been assuming throughout the course so far. here, i wanna be explicit about it, we're assuming that f and g cannot output negative numbers. whatever integer you feed in, you get out something positive. now, the functions we care about are things like the running time of algorithms, so there's really no reason for us to pollute our thinking with negative numbers. so, we're just gonna always be assuming in this class, positive numbers. and i'm actually using it here, the right hand side is the sum of two things, is bigger than just either one of the constituent summants. secondly. if we double the larger of f of n and g of n well that's going to exceed the sum of f of n plus g of n, right? because on the right hand side we have a big number plus a small number and then on the left hand side we have two copies of the big number so that is going to be something larger, now it's gonna be convenient it's gonna be more obvious what's going on if i divide both of these sides by two so that the maximum of f of n and g of n is at least half of f of n plus g of n that is at least half of the average and now we're pretty much home free right so what does this say. this says that for every possible n, the maximums wedged between suitable multiples of the sum. so one-half of f of n plus g of n. there's a lower bound on the maximum. this is just the second inequality that we derived. and by the first inequality that's bounded above by once times the sum. and this holds no matter what n is, [inaudible] at least one. and this is exactly what it means to prove that one function is theta of another. we've shown that for all n, not just for insuffiently large, but in fact for all n. the pointwise maximum of f and g is wedged between suitable constant multiples of their sum. and again, just to be explicit, the certifying choices of constants are n-nought equals one. the smaller constant is one half. and the bigger constant equals one. and that completes the proof. 
in this next series of videos, we'll get some more practice applying the divide and conquer algorithm and design paradigm to various problems. this will also give us a glimpse of the kinds of application [inaudible] to which it's been successfully applied. we're gonna start by extending the merge sort algorithm to solve a problem involving counting the number of inversions of an array. before we tackle the specific problem of counting the number of inversions in an array, let me say a few words about the divide and conquer paradigm in general. so again, you've already seen the totally canonical example of divide and conquer, namely merge sort. so the following three conceptual steps will be familiar to you. the first step, no prizes for guessing is you divide. the problem. into smaller sub-problems. sometimes this division happens only in your mind. it's really more of a conceptual step than part of your code. sometimes you really do copy over parts of the input into say new arrays to pass on to your recursive calls. the second step, again no prizes here, is you conquer the sub-problems just using recursion. so for example, in merge sort, you conceptually divide the array into two different pieces. and then you [inaudible] with the conquer or sort to the first half of the array. and you, you do the same thing with the second half of the array. now, of course, it's not quite as easy as just doing these two steps. dividing the problem, and then solving the sub problems recursively. usually, you have some extra cleanup work after the recursive calls, and to stitch together the solutions to the sub problems into one for the big problem, the problem that you actually care about. recall, for example, in merge sort, after our recursive calls, the left half of the array was sorted, the right half of the array was sorted. but we still had to stitch those together. merge into a sorted version of the entire array. so the [inaudible] step is to combine. the solutions to the subproblem into one problem. generally the largest amount of ingenuity happens in the third step. how do you actually quickly combine solutions to subproblems into one to the original problem? sometimes you also get some cleverness in the first step with division. sometimes it's as simple as just spliting a ray in two. but there are cases where the division step also has some ingenuity. now let's move on to the specific problem of counting inversions and see how to apply this divide and conquer paradygm. so let begin by defining the problem formally now. we're given as input an array a with a length n. and you can define the problem so that the array a contains any ole distinct numbers. but, let's just keep thing simple and assume that it contains the numbers one through n. the integers in that range in some order. that captures the essence of the problem. and the goal is to compute the number of inversions of this array so what's an inversion you may ask well an inversion is just a pair of array [inaudible] i and j with i smaller than j so that earlier array entry the i entry is bigger than the latter one the jake one so one thing that should be evident is that if the array contains these numbers in sorted order if the array is simply one two three four all the way up to n then the number of inversions is zero. the converse you might also want to think through if the array has any other ordering of the numbers between one and n other than the assorted one, then it's going to have a non. of zero number of inversions. let's look at another example. so'spose we have an array of six entries. so the numbers one thru six in the following order. one, three, five followed by two, four, six. so how many inversions does this array have? so again what we need to look for are pairs of array entries so that the earlier or left entry is bigger than the later or right entry. so one example which we see right here would five and two. those are right next to each other and out of order, the earlier entry is bigger than the other one. but there's others, there's three and two for example those are out of order. and, five and four are also out of order. and i'll leave it to you to check that those are the only three pairs that are out of order. so summarizing the inversions in this array of length six are 3-2, 5-2, and 5-4. corresponding to the array entries, 2-4, 3-4, and 3-5. pictorially, we can think of it thusly, we can first. write down the numbers in order, one up to six. and then we can write down the numbers again but, ordered in the way that their given in the input array. so, one three five two four six. and then we can connect the dots, meaning we connect one to one. reconnect two to two, and so on. it turns out, and i'll leave to for you to, to think this through, that the number of crossing pairs of line segments prescisely correspond to the number of inversions. so we see that there are one, two, three crossing line segments. and these are exactly in correspondence with the three inversions, we found earlier. five and two, three and two, and five and four. now, [inaudible] wanna solve this problem you might ask. well there's few reasons that come up. one would be to have a numerical similarity measure that quantifies how close to [inaudible] lists are to each other. so for example, suppose i took you and a friend, and i took, identified ten movies that both of you had seen. and i asked each of you to order, or to rank these movies from your most favorite to your least favorite. now i can form an array, and compute inversions. and it quantifies, in some sense, how dissimilar your two rankings are to each other. so in more detail, in the first entry of this array, i would put down the ranking that your friend gave to your favorite movie. so if you had your favorite movie, star wars or whatever. and your friend only thought it was the fifth best out of the ten, then i would write down a five in the first entry of this array. generally, i would take your second favorite movie. i would look at how your friend ranked that. i would put that in the second entry of the array and so on, all the way up to the tenth entry of the array, where i would put your friend's ranking of your least favorite movie. now, if you have exactly identical preferences, if you rank them exactly the same way, the number of inversions of this array would be zero. and in general, the more inversions this array has, it quantifies that your lists look more and more different from each other. now why might you want to do this why might you want to know whether two different people ranked things in the similar way had similar preferences well one reason might be what's called collaborative filtering, probably many of you have had the experience of going to a website and if you've made a few purchases through this website it starts recommending further purchases for you, so one way to solve this problem under the hood, is to look at your purchases look at what you seem to like, find other people who have similar preferences similar history look at things they've bought that you haven't, and then recommend. new products to you based on what similar customers seemed to have bought. so this problem captures some of the essence of identifying which customers or which people are similar based on data about what they prefer. so just to make sure we're all on the same page, let me pause for a brief quiz. we've already noticed that a given array will have zero inversions, if and only if it's in sorted order. if it only contains the numbers of one through n in order. so, on the other side, what is the largest number of inversions an array could possibly have? let's say, just for an array of size six, like the one in this example here. so the answer to this question is the first one. fifteen. or in general in an n. element array the largest number of inversions is n. choose two. also known as n times n minus one over two. which, again, in the case of a [inaudible] is going to evaluate to fifteen. the reason is, the worst case is when the array is in backwards order, reverse [inaudible] order, and every single pair of [inaudible] indices is inverted. and so the number of indices ij, with i less than j is precisely [inaudible] too. let's now turn our attention to the problem of computing the number of inversions of an array as quickly as possible. so one option that is certainly available to us is the brute force algorithm. and by brute force i just mean we could set up a double four loop. one which goes through i, one which goes through j bigger than i, and we just check each pair ij individually with i less than j whether that particular pair of array entities ai and aj is inverted and if it is then we add it to our running count. and then we return the final count at the end of the double four loop. that's certainly correct. the only problem is, as we just observed, there's n [inaudible] two or a quadratic number of potential inversions so this algorithm's almost going to run in time quadratic in the array link. now remember the mantra of any good algorithm designer. can we do better? and the answer is yes. and the method we'll be using, divide and conquer. the way in which we'll divide will be motivated directly by merge sort where we recurs e separately on the left and the right half's of the array. we're gonna do the same thing here. to understand how much progress we can make purely using recursion let's classify the inversions of array into one of three types. so suppose we have an inversion of an array i, j, and remember in an inversion you always have i less than j. we're gonna call it a left inversion. if both of the array indices are at most n over two, where n is the array length. we're gonna call it a right inversion if they're both strictly greater than n over two. and we're gonna call it a split inversion if the smaller index is at most n over two and the larger index is bigger than n over two. we can discuss the progress made by recursion in these terms. when we recurse on the left-half of an array, if we implement our algorithm correctly, we'll successfully be able to count all of the inversions located purely in that first half. those are precisely the left inversions. similarly, a second recursive call on just the right half of an array, the second half of an [inaudible] array will successfully count all of the right inversions. there remains the questions of how to count the split inversions. but we shouldn't be surprised there's some residual work left over, even after the recursive calls do their job. that, of course, was the case at merge short, where [inaudible] magically took care of sorting the left half of the array, sorting the right half of the array. but there was still, after their return, the matter of merging those two sorted lists into one. and here again, after the recursion is gonna be the matter of cleaning up and counting the number of split inversions. so for example if you go back to the six element array we worked through before, 135246, you'll notice that there, in fact, all of the inversions are split. so the recursive calls will both come back counting zero inversions. and all of the work for that example will be done by the count split inversions subroutine. so let's summarize where things stand given underspecified high level description of the algorithm as we envision it. there is a base case. i'll go ahead and write it down for completeness, which is if we're given a one element array, then there's certainly no inversion so we can just immediately return the answer zero. for any bigger array, we're going to divde and conquer. so we'll count the left inversions with a recursive call. the right inversions with a recursive call. and then we'll have some currently unimplemented subroutine that counts the split inversions. since every inversion is either left or right, or split, and can't be any more than one of those three, then, having done these three things, we can simply return their sum. so that's our high level attack on how we're gonna count up the number of inversions. and of course, we need to specify how we're gonna count the number of split inversions. and moreover, we lack that subroutine to run quickly. an analogy to emerge short, where, outside the recursive calls, we did merely linear work. outs-, in the merge subroutine. here, we'd like to do only linear work in counting up the number of split inversions. if we succeed in this goal, if we produce a correct and linear time of limitation to count up the number of split incursions, then this entire recursive algorithm will run in big o. of n. log in time. the reason the overall out rhythm will run in o. of n. log in time is exactly the same reason that merge short ran in n. log in time. there's two recursive calls. each on a problem of one-half the size. and outside of the recursive calls we would be doing linear work. so you could copy down exactly the same recursion tree argument we used for merge short. it would apply equally well here. alternatively, very soon we will cover the master method, and as one very special case it will prove that this algorithm, if we can implement it thusly, will run in o. of n. log in time. now one thing to realize, is this is a fairly ambitious goal, to count up the number of split inversions in linear time. it's not that there can't be too many split inversions. there can actually be a lot of them. if you have an array where the first half of the array contains the numbers n over two plus one, up to n. whereas the second part of the array contains the numbers one up to n over two, that has a quadratic number of inversions, all of which are split. so, what we're attempting to do here is count up a quadratic number of things using only linear time. can it really be done? yes is can, as we'll see in the next video. 
so far we've developed a divide and conquer approach to count the number of inversions of an array. so we're going to split the array in two parts, recursively count inversions on the left, on the right. we've identified the key challenge is counting the number of split inversions quickly. where a split inversion means that the earlier indexes on the left half of the array, the second index is on the right half of the array. these are precisely inversions that are going to be missed by both of our recursive calls. and the cracks or the problem is that there might be as many as quadratics but conversions. it somehow they go the run time they want. we need to do it in a linear time. so, here is the really nice idea which is going to let us do that. the idea is to piggy back on merge sort. by which i mean we're actually going to demand a bit more of our recursive calls to make the job of counting the number of split recursions easier. this is analogous to when you're doing a proof by induction, sometimes making the inductive hypothesis stronger, that's what lets you push through the inductive proof. so we're going to ask our recursive calls to not only count inversions in the array that they're passed, but also along the way to sort the array. and hey, why not? we know sorting is fast. merge sort will do it in n log in time, which is the run time we're shooting for, so why not just throw that in? maybe it'll help us in the combined step, and as we'll see, it will. so, what is this bias, why should we demand more recursive calls? well, as we'll see in a couple of slides, the merge subroutine almost seem designed just to count the number of split inversions. as we'll see, as you merge two sorted sub arrays, you will naturally uncover, all of the split inversions. so, let me just be a little bit more clear about how our previous high level algorithm is going to now be souped up so that the recursive calls sort, as well. so, here is the high level algorithm we proposed before where we just recursively counted versions on the left side, on the right side. and then, we have some currently unimplemented subroutine counts splint if which is responsible for counting the number of split inversions. so we're just going to augment this as follows so instead of being called count now we're going to call it sort and count. that's going to be the name of our algorithm. the recursive calls, again, just invoke sort and count. and so now we know each of those will not only count the number of inversions in sub array, but also return a sorted version. so, out from the first one we're going to get arrayed b back which is the sorted version of the array that we past it and we got the sorted array c back from the second recursive call or sort of version of the array that we past it. and now, the counts split inversions now, in addition to count and split inversions is responsible for merging the two sort of subarrays, b and c. so countsplitinv will be responsible for outputting an array d, which is a sorted version of the original input array a. and so i should also rename our unimplemented subroutine to reflect its now more ambitious agenda. so we'll call this mergeandcountsplitinv. now, we shouldn't be intimidated by asking our combining subroutine to merge the two sorted subarrays b and c, because we've already seen, we know how to do that in linear time. so the question is just, piggybacking on that worth, can we also count the number of split inversions in an additional linear time? we'll see that we can, although that's certainly not obvious. so you should again at this point have the question why are we doing this? why are we just making ourselves do more work? and again the hope is that the payoff is somehow counting split inversions becomes easier by asking our recursive calls to give some additional work of sorting. so to develop some intuition for why that's true. why merging naturally uncovers the number of splits inversions. let's recall with just the definition of the original merge subroutine from merge sort was. so here's the same pseudocode we went through several videos ago. i have renamed the letters of the arrays to be consistent with the current notation. so we're given two sorted subarrays. these come back from a recursive calls. i'm calling them b and c. they both have length n/2 and were responsible for producing the sorted combination of b and c so that's an output array d of length n. and again the ideas simple, you just take the two sorted sub-arrays b and c and then you take the output array d which you're responsible for populating. and using an index k you're going to traverse the output d from left to right. that's what this outer form here does and you're going to maintain pointers i and j to the sorted sub arrays b and c respectively. and, the only observation is that whatever the minimum element that you haven't copied over to d yet is, it's got to be either the left most element of b that you haven't seen yet or the left most element of c that you haven't seen yet. b and c by virtue of being sorted, the minimum element remaining has to be the next one available to either b or c. so you just proceed in the obvious way. you compare the two candidates for the next ones that copy over. you look at b(i). you look at c(j). whichever one is smaller, you copy over, so the first part of the if statement is for when b contains the smaller one. the second part of the else statement is for when c contains the smaller one, okay? so, that's how merge works. you go down b and c in parallel, populating d in sorted order from left to right. now to get some feel for what on earth any of this has to do with the split inversions of an array, i want you to think about an input array a that has the following property. that has the property that there are no split inversions whatsoever. so every inversion in this input array a is going to be either a left inversion, so both indices are at most n/2, or a right end version. so both indexes are strictly greater than n/2. now, the question is, given such an array a, once you're merging at this step, what do the assorted subarrays b and c look like for an input array that has no split inversions? the correct answer is the second one. that if you have an array with no split inversions then everything in the first half is less than everything in the second half, why? well, consider the contra-positive. suppose you had even one element in the first half which was bigger than any element in the second half, that pair of elements alone would constitute a split inversion, okay? so if you have no split inversions then everything on the left is smaller than everything in the right half of the array. now, more to the point, think about the execution of the merge subroutine on an array with this property, on an input array a where everything in the left half is less than everything in the right half. what is merge going to do? all right, just remember it's always looking for whichever is smaller the first element of remaining in b or the first element remaining in c and that's what it copies over. when everything in b is less than everything in c everything in b is going to get copied over in to the output array d before c ever gets touched. okay, so merge has an unusually trivial execution on input arrays with no split inversions with zero split inversions first it just goes through b and copies it over then it just concatinate c. okay, there's no interweaving between the two. so, no split in versions means nothing it copied from c, until it absolutely has to, until b is exhausted. so, this suggests that, perhaps, copying elements over from the second sub-array c has something to do with the number of split inversions in the original array, and that is in fact the case. so we're going to see a general pattern about copies from the second array c through the output array, exposing split inversions in the original input array a. so let's look at a more detailed example to see what that pattern is. so let's return to the example in the previous video, which is an array with six elements, ordered 1, 3, 5, 2, 4, 6. so we do our recursive call and in fact, the left half of the array is sorted and the right half of the array is already sorted. no sorting was going to be done and i'm actually going to get zero inversions for both our recursive calls. remember in this example it turns out all of the inversions are split versions. so now let's trace through the merge sub routine invoked on these two sorted subarrays. and try to spot a connection with the number of split inversions in the original six element array. so we initialize indices i and j to point to the first element of each of these subarrays. so this left one is b and this right one is c and the output is d. and the first thing we do is we copy the 1 over from b into the top of array so 1 goes there and we advance this index over to the 3. and here, nothing really interesting happens, there's no reason to count on this split inversions and indeed the number one is not involved at any split inversions, because you want it smaller than all of the other elements and it's also in the first index. things are much more interesting when we copy over the element 2 from the second array c. and notice, at this point, we have diverged from the trivial execution that we would see with an array with no split inversions. now we're copying over something from c before we've exhausted copying b. so we are hoping this will expose some split inversions. so we copy over the two and we advance the second pointer j into c and the thing to notice is, this exposes two split inversions. the two split inversions that involve the element two. and those inversions are 3,2 and 5,2. so why did this happen? well the reason we copied two over is because it's smaller than all the elements we haven't yet looked at in both b and c. so in particular 2 is smaller than the remaining elements in b, the 3 and the 5. but also because b is the left array, the indices of the 3 and the 5 have to be less than the index of this 2. so, these are inversions, 2 is further to the right in the original input array, and yet it's smaller than these remaining elements in b. so, there are two elements remaining in b, and those are the two split versions that involve the elements two. so, now let's go back to the merging subroutines, and what happens next. well, next we'll make a copy from the first array and we sort of realize that nothing really interesting happens when we copy it from the first array, at least with respect to split in versions. then we copy the four over, and yet again, we discover a split inversion, the remaining one, which is 5,4. again, the reason is, given that 4 was copied over before what's left in b, it's got to be smaller than it, but by virtue of being in the rightmost array, it's also not going to have a bigger index, so it's gotta be a split inversion. now the rest of the merge subroutines executes without any real incident. five gets copied over and we know copies from the left array are boring and then we copy the six over and copies from the right array are generally interesting but not if the left array is empty. that doesn't involve any split versions. and you will recall from the earlier video that these were the inversions in your original array, 3252 and 54. we discovered them all on an automated method by just keeping an eye out when we copy from the right array c. so this is indeed a general principle so let me state the general claim. so, the claim is not just in this specific example, in this specific execution. but no matter what the inquiry is, no matter how many split inversion there might be, the split inversions that involve an element of the second half of the array are precisely those elements remaining in the first array when that element gets copied over to the output array. so this is exactly the pattern that we saw in the example. what were, so on the right array c, we have the elements two, four and six. remember every split version has to, by definition, involve one element from the first half and one element from the second half. so the count for split inversions, we can just group them according to which element of the second array that they involve. so out of the two four and six, the two is involved in the split up inversions three two and five two. the three and the five were exactly the elements remaining in b when we copied over two. the split inversions involving four is exactly the inversion five, four and five is exactly the element that was remaining. in b when we copied over the four, there's no split inversions involving six and indeed, the element b was empty when we copied the six over in the output array d. so what's the general argument? well it's quite simple. let's just zoom in and fixate on a particular element x that belongs to that first half of that array. that's amongst the first half of the element. and let's just examine which y's, so which elements of the second array, the second half of the original input array, are involved in split inversions with x. so there are two cases, depending on whether x is copied over into the output array d before or after y. now if x is copied to the output before y, well then since the output's in sorted order it means x has got to be less than y so there's not going to be any split inversion. on the other hand if y is copied to the output d before x then again because we populate the left to right in sorted order, that's got to mean that y is less than x. now x is still hanging out in the left array so it has a less index than y, y comes from the right array so it's not a split inversion. so putting these two together, it says that the elements x of the array b that form split inversions with y are precisely those that are going to get copied to the output array after y. so those are exactly the number of elements remaining in b when y gets copied over. so that proves the general claim. so this slide was really the key insight. now that we understand exactly why counting split inversions is easy, as we're merging together two sorted subarrays, it's a simple matter to just translate this into code and get a linear time of notation of a sub routine that both emerges and counts the number of split inversions. which then in the overall course of the algorithm we'll have n log n running time just as in merge sort. so, let's just spend a quick minute filling in those details. so, i'm not going to write up the pseudo code. i'm just going to write what you need to augment the merge pseudo code discussed a few slides ago by in order to count split inversion as you're doing the merging. and this will follow immediately from the previous plan which indicated how split version relate to the number of elements remaining in the left array as you're doing the merge. so the idea is the natural one, as you're doing the merging, according to the previous pseudo code, of the two sorted subarrays you just keep a running total of the number of split inversions that you encounter. and so you've got your sorted subarray b, you've got your sorted subarray c. you're merging these into an output array d, and as you traverse through d and k goes from 1 to n, you just start out at zero and increment it by something each time you copy over from either b or c. so, what's the increment? well, what did we just see, we saw the copies involving b don't count, we're not going to look at split inversions when to copy over from b, only when we look at them from c, right? every split inversion involves exactly one element from each of b and c. so, i may as well count them via the elements in c and how many split inversions are involved with the given element of c, well it's exactly how many elements of b remain when it gets copied over. so, that tells us how to increment this running count. and, it follows immediately from the claim on the previous slide that this implementation of this running total counts precisely the number of split inversions that the original input array a possesses. and we'll call that the left inversions are counted by the first recursive call of the right inversions are counted by the second recursive call. every inversion is either at left or right or splitt that's exactly one of those three types. so, with our three different subroutines, the two recursive ones and this one here, we successfully count of all the inversions of the original input array. so that's the correctness of the algorithm. what's the running time? we'll recall in mergesort, we began just by analyzing the running time of merge and then we discussed the running time of the entire mergesort algorithm. let's do the same thing here briefly. so what's the running time of the subroutine for this merging and simultaneously counting the number of split inversions? well there's the work that we do in the merging, and we already know that that's linear. and then the only additional work here is incrementing this running count, and that's constant time for each element of d, right? each time we do a copy over we do some single addition to our running count. so constant time for element of d, or linear time over all. so, i'm being a little sloppy here. sloppy in a very conventional way but it is a little sloppy by writing o(n) + o(n) = o(n). be careful when you make statements like that. right, so, if you added o(n) to itself n times, it would not be o(n), but if you add o(n) to itself a constant number of times, it is still o(n). so you might, as an exercise, want to write out a formal version of what this means. basically there's some constant c1 so that the merge steps takes at most c1 n steps. there's a constant c2 so that the rest of the work is at most c2 times n steps. so when you add them, we get it's at most quantity c1 plus c2 times n steps, which is still big o(n), because c1 plus c2 is a constant, okay? so, linear work for merge, linear work for the running count, so does linear work in the subroutine overall. and now, by exactly the same argument, we'll use in merge sort because we have two reversing calls in half the size. and with your linear work outside the recursive calls, the overall running time is o(n) log n. so, it really just piggybacked on merge sort upped to the constant factor a little bit to do the counting along the way, but the running time remains the big o(n log n). 
in this video, we'll apply the divide and conquer algorithm design paradigm to the problem of multiplying matrices. this will culminate in the study of strassen matrix multiplication algorithm. and this is a super cool algorithm for two reasons. first of all, strassen's algorithm is completely non-trivial. it's totally non-obvious, very clever, not at all clear how strassen ever came up with it. the second cool feature is it's for such a fundamental problem. so computers as long as they've been in use from the time they were invented up til today a lot of their cycles is spent multiplying matrices. it just comes up all the time in important applications. so let me first just make sure we're all clear on what the problem is of multiplying two matrices. so we're going to be interested in three matrices x, y and z. i'm going to assume they all have the same dimensions, n x n. the ideas we'll talk about are also relevant for multiplying non square matrices but we're not going to discuss it in this video. the entries in these matrices, you could think of it as whatever you want. maybe they're integers, maybe they're rationals. maybe they're from some finite field. it depends on the application but the point is they're entries that we can add and multiply. so how is it that you take two n x n matrices x and y and multiply them producing a new n x n matrix z? well recall that the ij entry of z, that means the entry in the ith row and the jth column, is simply the dot product of the ith row of x with the jth column of y. so if ij was this red square, this red entry over in the z matrix that would be derived from the corresponding row of the x matrix and the corresponding column of the y matrix. and recall what i mean by dot product, that just means you take the products of the individual components and then add up the results. so ultimately the zij entry boils down to a sum over n things where each of the constituent products is just the xik entry, the ik entry of the matrix x with a kj entry of the matrix y. where your k is ranging from 1 to n. so that's how zij is defined for a given pair of indices i and j. one thing to note is that where we often use n to denote the input size, here we're using n to denote the dimension of each of these matrices. the input size is not n, the input size is quite a bit bigger than n. specifically, each of these are n x n matrices that contains n squared entries. so since presumably we have to read the input, which has size n squared, and we have to produce the output, which also has size n squared. the best we could really hope for a matrix multiplication algorithm would be a running time of n squared, so the question is how close can we get to it. before we talk about algorithms for matrix multiplication let me just make sure we're all crystal clear on exactly what the problem is so let's just actually spell out what would be the result of multiplying two different 2 x 2 matrices. so we can parameterize two generic 2 x 2 matrices by just giving the first one entries a, b, c, and d. or these four entries could all be anything. and then, we're multiplying by a second 2 x 2 matrix. let's call its entries e, f, g, and h. now what's the result of multiplying these? where again, it's going to be a 2 x 2 matrix where each entry is just the corresponding dot product of the relevant row of the first matrix and column of the second matrix. so to get the upper left entry we take the dot product of the upper row of the first matrix and the first column of the left column of the second matrix so that results in ae + bg. to get the upper right entry we take the dot product of the top row of the left matrix with the right column of the second matrix, so that gives us af + bh and then filling in the other entries in the same way, we get ce + dg and cf + dh. okay, so that's multiplying two matrices, and we've already discussed the definition in general. now, suppose you had to write a program to actually compute the result of multiplying two n x n matrices. one natural way to do that would just be to return to the definition, which defines each of the n squared entries in the z matrix as a suitable sum of products of entries of the x and y matrices. so in the next quiz i'd like you to figure out exactly what would be the running time of that algorithm as a function of the matrix dimension n. where, as usual, we count the additional multiplication of two individual entries as a constant time operation. so the correct response to this quiz is the third answer. that the running time of the straightforward iterative algorithm runs in cubic time relative to the matrix dimension n. to see this just recall what the definition of the matrix multiplication was. the definition tells us that each entry zij of the output matrix z is defined as the sum from k = 1 to n of xik times ykj. that is the dot product of the ith row of the x matrix and the jth column of the y matrix. i'm certainly assuming that we have the matrices represented in a way that we could access a given entry in a constant time. and under that assumption remember each of these products only takes constant time. and so then to compute zij we just have to add up these n products so that's going to be theta of n time to compute a given zij and then there's an n squared entries that we have to compute. there's n choices for i, n choices for j. so that gives us n squared times n or cubic running time overall. for the natural algorithm, which is really just a triple for loop which computes each entry of the output array separately using the dot product. so the question as always for the keen algorithm designer is, can we do better. can we beat n cube time for multiplying two matrices. and we might be especially emboldened with the progress that we've already seen in terms of multiplying two integers. we apply the divide and conquer algorithm to the problem of multiplying two integers. and we had both a naive recursive algorithm and a seemingly better algorithm due to gauss, which made only three recursive calls. now we haven't yet analyzed the running time of that algorithm. but as we'll see later, that does indeed beat the quadratic running time of the grade school algorithm. so it's very natural to ask, can we do exactly the same thing here? there's the obvious n cubed algorithm which follows straight from the definition, perhaps analogous to gauss we could have some clever divide and conquer method which beats cubic time. so that's what we're going to explore next. let's recall the divide and conquer paradigm, what does it mean to use it? well we first have to identify smaller subproblems, so if we want to multiple two n x n matrices. we have to identify multiplications of smaller matrices that we can solve recursively. once we figured out how we want to divide the given problem into smaller ones. then the conquer step, we simply invoke our own algorithm recursively. that's going to recursively multiply the smaller matrices together. and then in general, we'll have to combine the results of the recursive cause to get the solution for the original problem. in our case, to get the product of the original two matrices from the product of whatever submatrices we identify. so how would we apply the divide and conquer paradigm to matrices? so we're given two n x n matrices, and we have to somehow identify smaller pairs of square matrices that we can multiply recursively. so the idea i think is fairly natural. so we start with a big n by n matrix x, right, so there's n rows and n columns. and we have to somehow divide it into smaller pieces. now the first thing you might think about is you put it into it's left half and it's right half analogous to what we've been doing. with arrays, but then we're going to break x into two matrices which are no longer square, which are n over 2 in one dimension, and have length n in the other dimension. and we want to recursively call a subroutine that multiplies square matrices. so what seems like the clear thing to do, is to divide x into quadrants. okay, so we have four pieces of x, each is going to be n over 2 by n over 2 corresponding to the different quarters of this matrix. so let's call these different quadrants or blocks in matrix terminology a, b, c, and d. all of these are n over 2 by n over 2 matrices. as usual for simplicity, i'm assuming that n is even. and as usual it doesn't really matter and we can do the same trick with y. so, we'll divide y into quadrants. n over 2 by n over 2 matrices, which we'll call e, f, g, and h. so one thing that's cool about matrices is when you split them into blocks and you multiply them, the blocks just behave as if they were atomic elements. so what i mean by that, is that the product of x and y can be expressed in terms of its quadrants. and each of its four quadrants, each of its four corners can be written as a suitable arithmetic expression of the quadrants of x and y. so here's exactly what those formulas are. they're exactly analogous to when we just multiplied a pair of 2 by 2 matrices. so i'm not going to formally prove this fact. i'm sure many of you have seen it before or familiar with it. and if you haven't, it's actually quite easy to prove. it's not obvious since you can't see it off the top of your head necessarily. but if you go back to the definition, it's quite easy to verify. but indeed when you multiply x and y, you can express its quadrants into blocks in terms of the blocks of x and y. so what we just did is completely analogous to when talking about integer multiplication, and we wanted to multiply two integers, x and y, and we broke them into pairs of n over 2 digits. and then we just took the expansion, and we've observed how that expansion could be written in terms of products of n over 2 digit numbers. it's the same thing going on here, except with matrices. so now we're in business as far as a recursive approach. we want to multiply x and y. they're n by n matrices. we recognize, we going to express that product, x times y. in terms of the products of n over 2 by n over 2 matrices. things were able to multiply recursively, plus additions. and additions, it's clearly easy to multiply two different matrices with, say, n squared entries each. it's going to be linear in the number of entries. so it's going to be n squared time to add two matrices that are n by n. so this immediately leads us to our first recursive algorithm. to describe it, let me quickly rewrite that expression we just saw on the previous slide. and now our first recursive algorithm is simply to evaluate all of these expressions in the obvious way. so specifically in step one, we recursively compute all of the necessary products. and observe that there are eight products that we have to compute. eight products with n over 2 by n over 2 matrices. there are four entries in this expansion of x times y. each of the blocks is the sum of two products and none of the products reoccurred. they're all distinct. so naively, if you want to evaluate this, we have to do eight different products of n over 2 by n over 2 matrices. once those recursive calls complete, then all we do is do the necessary four additions. as we discussed, so that takes time proportional to the number of entries in the matrix. so this is going to take a quadratic time overall, quadratic in n. linear in the number of entries. now the question you should be asking is, is this a good algorithm? was this good for anything? this recursive approach. splitting x and y into these blocks. expanding the product in terms of these blocks then recursively computing each of the blocks. and i want to say it's totally not obvious. it is not clear what the running time of this recursive algorithm is. i'm going to go ahead and give you a spoiler which is going to follow from the master method that we'll talk about in the next lecture. but it turns out, with this kind of recursive algorithm where you do eight recursive calls. each on a problem with dimension half as much as what you started with, and then do quadratic time outside, the running time is going to be cubic. so exactly the same as with the straightforward iterative algorithm that follows from the definition. that was cubic, it turns out, and that was clearly cubic. this one, although it's not obvious, is cubic as well. so no better, no worse than the straightforward iterative algorithm. so in case you're feeling disappointed that we went through all this work in this sort of seemingly clever divide and conquer approach for matrix multiplication and came out at the end no better than the iterative algorithm. well, there's really no reason to despair. because remember back in integer multiplication, we had a straightforward recursive algorithm. where we have to do four recursive calls. products of n over 2 digit numbers. but then we had gauss' trick, which said if we only did more clever products and more clever additions and subtractions, then we can get away with only three recursive calls. and we'll see later if that is indeed a significant savings in the time we did for integer multiplication. and we've done nothing analogously clever to gauss' trick for this matrix multiplication problem. all we did is the naive expansion, in terms of submatrices, and the naive evaluation of the resulting expressions. so, the $64,000 question is then, can we do something clever, to reduce the number of recursive calls, from 8 down to something lower? and that is where strassen's algorithm comes in. so the high level strassen's algorithm has two steps, just like the first recursive algorithm that we discussed. it recursively computes some products of smaller matrices and over to a roughly n over 2 by n over 2 matrices. but there's only going to be seven of them. but they will be much less straightforward, they will be much more cleverly chosen than in the first recursive algorithm. in step two then is to actually produce the product of x and y. produce each of those four blocks that we saw. with suitable additions and subtractions of these seven products. and again, these are much less straightforward than in the first recursive algorithm. and so, while the additions and subtractions involved will be a little bit more numerous than they were in the naive recursive algorithm. it's only going to change the work in that part of the algorithm by a constant factor. so we'll still spend only theta (n squared) work adding and subtracting things, and we get a huge win in decreasing the number of recursive calls from 8 to 7. now just in case you have the intuition that shaving off one of eight recursive calls should only decrease the running time of an algorithm by one-eighth by 12.5%. in fact it has a tremendously more amplified effect. because we do one less recursive call over and over and over again as we keep recursing in the algorithm. so it makes a fundamental difference in the eventual running time of the algorithm as we'll explore in detail in the next set of lectures when we discuss the master method. so again a bit of a spoiler alert. what you're going to see in the next set of lectures that indeed strassen's algorithm does beat cubic time. it's better than n cubed time. you'll have to watch the next set of lectures if you want to know just what the running time is. but i'm going to tell you now that savings of the eighth recursive call is what changes the algorithm from cubic to subcubic. now, 1969 was, obviously, quite a bit before my time. but by all accounts, from people i've talked to who were around then and from what the books say, strassen's algorithm totally blew people's minds at the time. everybody was just assuming that there's no way you could do better than the iterative algorithm, the cubic algorithm. it just seemed that matrix multiplication intuitively fundamentally required all of the calculations that are spelled out in the definition. so strassen's algorithm is an early glimpse of the magic and of the power of clever algorithm design. but if you really have a serious ingenuity, even for super fundamental problems, you can come up with fundamental savings over the more straightforward solution. solutions. so those are the main points i wanted to talk about strassen's algorithm. how you can beat cubic time by saving a recursive call with suitably chosen clever products and clever additions and subtractions. maybe a few of you are wondering what are these cleverly chosen products, can you really do this? and i don't blame you. there's no reason to believe me just because i sort of spelled out this high level idea. it's not obvious this should work. you might want to actually see the products. so for those of you like that, this last slide is for you. so here is strassen's algorithm in it's somewhat gory detail. so let me tell you what the seven products are that we're going to form. i'm going to label them p1 through p7 and they're all going to be defined in terms of the blocks of the input matrices x and y. so let me just remind you that we think of x in terms of its blocks a, b, c, d and we think of y in terms its blocks e, f, g, h. and remember a through h are all n over 2 by n over 2 sub-matrices. so here are the seven products p1 though p7. p1 = a(f-h), p2 = (a+b)h, p3 = (c+d)e. p4 = d(g-e). p5 = (a+d)(e+h). p6 = (b-d)(g+h) and finally p7 = (a-c)(e+f). so i hope you'll agree that these are indeed only seven products. and we could compute these with seven recursive calls. so we've pre-processed with a little bit of additions and subtractions. we have to compute f minus h, a plus b, c plus d, and so on. we compute all these new matrices from the blocks. and then we can recursively, with seven recursive calls. do the seven products that operate on n over 2 by n over 2 matrices. now the question is why is this useful, why on earth would we want to know the values of these seven products. so the amazing other part of the algorithm is that from just these seven products we can using only addition and subtraction recover all four of the blocks of x times y. so x times y you recall we solved it in terms of blocks. so we previously computed this to be ae+bg in the upper left corner and similarly expressions for the upper right, lower left or lower right blocks. so this we already know. so the content of the claim is that these four blocks also arise from the seven products in the following way. so the claim here is that these two different expressions for x times y are exactly the same, and they're the same block by block. so in the other words, what the claim is that this crazy expression p5+p4-p2+p6 where those are four of the products that we have listed above, that is precisely ae+bg. similarly we're claiming the p1+p2 as exactly af+bh that's actually easy to see p3+p4 is ce+dg. that's also easy to see. and then the other complicated one is the p1+p5-p3-p7 is exactly the same as cf+dh, so all four of those hold. so, let me, just so you believe me because i don't know why you'd believe me unless i showed you some this derivation. let's just look at the proof of one of the cases of the upper left corner. so that is let's just expand out this crazy expression, p5+p4-p2+p6. what do we get? well, from p5 we get, ae+ah+de+dh, then we add p4, so that's going to give us, +dg-de, then we subtract p2, so that gives us a -ah-bh, and then we add in p6, so that gives us a bg+bh-dg-dh. okay so what happens next, well now we look for cancellations, so we cancel the ah's we cancel the de's, we cancel the dh's, we cancel the dg's, we cancel the bh's and holy cow, what do we get? we get ae+bg, that is we get exactly what we were supposed to in the upper left block of x times y. so we just actually verified that this equation holds for the upper left block. it's quite easy to see that it holds for the upper right and lower left blocks. and then a comparable calculation verifies it for the lower right blocks of the two. so summarizing, because this claim holds, because actually we can recover the four blocks of x times y from these seven products. strassen's algorithm works in the following way. you compute the seven products p1 through p7 using seven recursive calls. then you just compute the four blocks using some extra additions and subtractions as shown in the claim. so it's seven recursive calls on n over 2 by n over 2 matrices. plus n squared work due to the necessary additions and as you'll see in the master method lecture that is actually sufficient for subcubic time. now, i sympathize with you if you have the following question, which is how on earth did strassen come up with this. and indeed this sort of illustrates, the difference between checking somebody's proof and coming up with a proof. given that i told you the magical seven products and how you from them you can recover the four desired blocks of x times y. it's really just mechanical to see that it works. it's a totally different story of how do you come up with p1 through p7 in the first place. so how did strassen come up with them? honestly, your guess is as good as mine. 
so in this video and the next, we're going to study a very cool divide and conquer algorithm for the closest pair problem. this is a problem where you're given n points in the plane and you want to figure out which pair of points are closest to each other. so this would be the first taste we get of an application in computational geometry, which is the part of algorithms which studies how to reason and manipulate geometric objects. so those algorithms are important in, among other areas robotics, computer vision and computer graphics. so this is relatively advanced material, it's a bit more difficult than the other applications of divide and conquer that we've seen. the algorithm's a little bit tricky and it has a quite nontrivial proof of correctness, so just be ready for that and also be warned that because it's more advanced i'm going to talk about the material in at a slightly faster pace tha i do in most of the other videos. so let's begin now by defining the problem formally, so we're given as imput endpoints in the plane, so each one just define by its x coordinate and ist y coordinate. and when we talk about the distance between two points in this problem, we're going to focus on euclidean distance. so, let me remind you what that is briefly, but we're going to introduce some simple notation for that, which we'll use for the rest of the lecture. so we're just going to note the euclidean distance between two points, pi and pj, by d of pi pj. so in terms of the x and y coordinates of these two points, we just look at the squared differences in each coordinate, sum them up, and take the square root. and now as the name of the problem would suggest, the goal is to identify among all pairs of points that pair which has the smallest distance between them. next, let's start getting a feel for the problem by making some preliminary observations. first, i want to make an assumption purely for convenience that there's no ties. so that is i'm going to assume all endpoints have distinct x coordinat es, and also all endpoints have distinct y coordinates. it's not difficult to extend the algorithm to accommodate ties. i'll leave it to you to think about how to do that. so next, let's draw some parallels with the problem of counting inversions, which was a earlier application of divide and conquer that we saw. the first parallel i want, want to out is that, if we're comfortable with the quadratic time algorithm, then this is not a hard problem, we can simply solve this by brute-force search. and again, by brute-force search, i just mean we set up a double for loop, which iterates over all distinct pairs of points. we compute the distance for each such pair and we remember the smallest. that's clearly a correct algorithm, it has to iterate over a quadratic number of pairs, so its running time is going to be theta of n squared. and, as always, the question is can we apply some algorithmic ingenuity to do better? can we have a better algorithm than this naive one which iterates over all pairs of points? you might have a, an initial instinct that because the problem asks about a quadratic number of different objects, perhaps we fundamentally need to do quadratic work. but again, recall back in counting inversions, using divide and conquer, we were able to get an n log n algorithm despite the fact that there might be as many as a quadratic number of inversions in an array. so the question is, can we do something similar here for the closest pair problem? now, one of the keys to getting an n log n time algorithm for counting inversions was to leverage a sorting subroutine. recall that we piggybacked on merge sort to count the number of inversions in n log n time. so the question is, here, with the closest pair problem, perhaps, sorting again can be useful in some way to beat the quadratic barrier. so, to develop some evidence that sorting will indeed help us compute the closest pair of points embedded in quadratic time, let's look at a special case of the problem, really, an easier version of t he problem, which is when the points are just in one dimension, so on the line rather that in two dimensions in the plane. so in the 1d version, all the points just lie on a line like this one, and we're given the points in some arbitrary order not necessarily in sorted order. so, a way to solve the closest pair problem in one dimension, is to simply sort the points, and then of course, the closest pair better be adjacent in this ordering, so you just iterate through the n minus 1 consecutive pairs and see which one is closest to each other so, more formally, here's how you solve the one-dimensional version of the problem. you sort the points according to their only coordinate, because you're going to remember, this is one dimension. so as we've seen, using merge sort, we can sort the points in n log n time and then we just do a scan through the points, so this takes linear time. and for each consecutive pair, we compute their distance and we remember the smallest of those consecutive pairs and we return that. that's gotta be the closest pair. so, in this picture here on the right, i'm just going to circle here in green the closest pair of points. so this is something we discover by sorting and then doing a linear scan. now, needless to say, this isn't directly useful, this is not the problem i started out with. we wanted to find out the closest pair among of points in the plane not points in the line. but, i want to point out that, this, even in the line, there are a quadratic number of different pairs, so brute-force search is still a quadratic time algorythm even in the 1d case. so at least, with one dimension, we can use sorting, piggyback on it, to beat the naive brute-force search bound and solve the problem in n log n time. so our goal for this lecture is going to be to devise an equally good algorithm for the two-dimensional case, so we want to solve closest pair of points in the plane, again, in n log n, n time. so we will succeed in this goal. i'm going to show you an n log n time algo rithm for 2d closest pair. it's going to take us a couple steps. so let me begin with a high level approach. alright. so the first i need to try is just to copy what worked for us in the one-dimensional case. so the one-dimensional case, we first sorted the points by their coordinate and that was really useful. now, in the 2d case, points have two coordinates, x coordinates and y coordinates, so there's two ways to sort them. so let's just sort them both ways, that is, the first step of our algorithm, which you should really think of as a preprocessing step. we're going to take the input points. we invoke merge sort once to sort them according to x coordinate, that's one copy of the points. and then we make a second copy of the points where they're sorted by y coordinates. so we're going to call those copies of points px, that's an array of the points sorted by x coordinate, and py for them sorted by y coordinate. now, we know merge short takes n log n times, so this preprocessing step only takes o of n log n time. and again, given that we're shooting for an algorithm with running time big o of n log n, why not sort the points? we don't even know how we're going to use this fact right now, but it's sort of harmless. it's not going to effect our goal of getting a big of o n log n time algorithm. and indeed, this illustrates a broader point, which is one of the themes of this course. so recall, i hope one of the things you take away from this course is a sense for what are the four free primitives, what are manipulations or operations you can do on data which basically are costless. meaning that if your data set fits in the main memory of your computer, you can basically invoke the primitive and it's just going to run blazingly fast, and you can just do it even if you don't know why. and again, sorting is the canonical for free primitive, although, we'll see some more later in the course and so, here, we're using exactly that principle. so we don't even understand why yet we might wa nt the points to be sorted. it just seems like it's probably going to be useful, motivated by the 1d case, so let's go ahead and make assorted copies of the points by x and y coordinate upfront. so reasoning by analogy with the 1d suggests that sorting the points might be useful, but we can't carry this analogy too far. so in particular, we're not going to be able to get away with just a simple linear scan through these arrays to identify the closest pair of points. so, to see that, consider the following example. so we're going to look at a point set which has six points. there's going to be two points, which i'll put in blue which are very close in x coordinate, but very far away in y coordinate. and then there's going to be another pair of points which i'll do in green, which are very close in y coordinate, but very far away in x coordinate. and then there's going to be a red pair of points, which are not too far away in either the x coordinate or the y coordinate. so in this set of six points, the closest pair is the pair of red points. they're not even going to show up consecutively on either of the two arrays, right? so in the array that's sorted by x coordinate, this blue point here is going to be wedged in between the two red points, they won't be consecutive. and similarly in the, in py, which is sort of by y coordinate, this green coordinate is going to be wedged between the two red points. so you won't even notice these red point if you just do a linear scan if your px and py, or py look at the consecutive pairs of points. so, following our preprocessing step where we just invert, invoke merge sort twice we're going to do a quite nontrivial divide and conquer algorithm to compute the closest pair. so really, in this algorithm, we're applying the divide and conquer algorithm twice. first, internal to the sorting subroutine, assuming that we use the merge sort algorithm to sort. divide and conquer is being used there to get an n log n running time in this preprocessing step, and the n, we're going to use it again on sorted arrays in a new way and that's what i'm going to tell you about next. so let's just briefly review the divide and conquer algorithm design paradigm before we apply it to the closest pair problem. so, as usual, the first step is to figure out a way to divide your problem into smaller subproblems. sometimes this has a reasonable amount of ingenuity, but it's not going to. here in the closest pair problem, we're going to proceed exactly as we did in the merge sort and counting inversions problems, where we took the array and broke it into its left and right half. so here, we're going to take the input point set, and again, just recurse on the left half of the points, and recurse on the right half of the points. so here, by left and right, i mean with respect to the points x coordinates. there's pretty much never any ingenuity in the conquer step, that just means you take the sub-problems you identified in the first step, and you solve them recursively. that's what we'll do here, we'll recursively complete the closest pair in the left half of the points, and the closest pair in the right half of the points. so where all the creativity in divide and conquer algorithms is in the combined step. given the solutions to your sub problems, how do you somehow recover a solution to the original problem? the one that you actually care about. so for closest pair, the questionis going to be, given that you've computed the closest pair on the left half of the points, and the closest pair on the right half of the points, how do you then quickly recover the closest pair for the whole point set? that's a tricky problem, that's what we're going to spend most of our time on. so let's make this divide and conquer approach for closest pair a little bit more precise, so let's now actually start spelling out our closest pair algorithm. the input we're given, it's, this follows the preprocessing steps or recall that we invoke, merge sort, we get our two sorted copies of the poin t set px, sorted by x coordinate, and py sorted by y coordinate. so the first dividend is the division step. so given that we have a copy of the points px sorted by x coordinate, it's easy to identify the leftmost half of the points, those with the, those n over two smallest x coordinates and in the right half, those were the n over two largest x coordinates. we're going to call those q and r respectively. one thing i'm skipping over is the base case. i'm not going to bother writing that down, so base case omitted, but it's what you would think it would be. so basically once you have a small number point, say two points or three points, then you can just solve the problem in constant time by a brute-force search. you just look at all the pairs and you return the closest pair. so think of it being at least four points in the input. now, in order to recurse, to call clo pair again, in the left and right halves, we need sorted version of q and r, both by x coordinate and by y coordinate, so we're just going to form those by doing suitable linear scans through px and py. and so one thing i encourage you to think through carefully or maybe even code up after the video is how would you form qx, qy, rx and ry given that you already have px and py. and if you think about it, because px and py are already sorted just producing these sorted sublists takes linear time. it's in some sense the opposite of the merge subroutine used in merge sort. here, we're sort of splitting rather than merging. but again, this can be done in linear time, that's something you should think through carefully later. so that's the division step, now we just conquer, meaning we recursively call closest pair line on each of the two subproblems, so when we invoke closest pair on the left half of the points on q we're going to get back what are indeed, the closest pair of points amongst those in q. so we're going to call those p1 and pq, so among all pairs of points that both lie in q, p1 and q1 minimize the distance between them. similarly, we're going to call q2q2 the results of the second recursive call, that is, p2 and q2 are amongst all pairs of points that both lie in r, the pair that has the minimum euclidean distance. now, conceptually, there's two cases. there's a lucky case and there's an unlucky case. in the original point set p, if we're lucky, the closest pair of points in all of p, actually, both of them lie in q or both of them lie in r. in this lucky case, we'd already be done if the closest pair in the entire point set they happen to both lie in q, then this first recursive call is going to recover them and we just have them in our hands p1q1. similarly, if both of the closest pair of points in all of p lies on the right side in r, then they get handed to us on a silver platter by the second recursive call that just operate on r. so in the unlucky case, the closest pair of point in p happens to be split. that is, one of the points lies in the left half, in q, and the other point lies in the right half, in r. notice, if the closest pair of points in all of p is split, is half in q and half in r, neither recursive call is going to find it. okay? the pair of points is not passed to either of the two recursive calls, so there's no way it's going to be returned to us. okay? so we have not identified the closest pair after these two recursive calls, if the closest pair happens to be split. this is exactly analagous to what happened when we were counting inversions. the recursive call on the left half of the array counted the left inversions. the recursive call on the right half of the array counted the right inversions. but we still had to count the split inversions, so in this closest pair algorithm, we still need a special purpose subroutine that computes the closest pair for the case in which it is split, in which there is one point in q and one point in r. so just like in counting inversions, i'm going to write down that subroutine and i'm going to leave it unimplemented for now, we'll figur e out how to implement it quickly in the rest of the lecture. now, if we have a correct implementation of closest split pair, so that takes us input the original point set sort of the x and y coordinate, and returns the smallest pair that's split or one points in q and one points in r, then we're done. so then, the split, then the closest pair has to either be on the lef or onm the right or it has to be split. steps two through four compute the closest pair in each of those categories, so those are the only possible candidates for the closest pair and we just returned the best of them. so that's an argument for y, if we have a correct implementation of the closest split para subroutine, then that implies a correct implementation of closest pair. now, what about the running time? so the running time of the closest para algorithm is going to be in part determined by the running time of closest split pair. so in the next quiz, i want you to think about what kind of running time we should be shooting for with a closest split pair subroutine. so the correct response of this quiz is the second one, and the reasoning is just by analogy with our previous algorithms for merge sort and for counting inversions. so, what is all of the work that we would do in this algorithm or we do have this preprocessing step we call merge sort twice, we know that's n log n, so we're not going to have a running time better than n log n cause we sort at the beginning. and then, we have a recursive algorithm with the following flavor, it makes two recursive calls. each recursive call is on a problem of exactly half the size with half the points of the original one. and outside of the recursive calls, by assumption, by, in the problem, we do a linear amount of work in computing the closest split pair. so we, the exact same recursion tree which proves an n log n bound for merge sort, proves an n log n bound for how much work we do after the preprocessing step, so that gives us an overall running time bound of n log n. remem ber, that's what we were shooting for. we were working n log n already to solve the one-dimensional version of closest pair and the goal of these lectures is to have an n log n algorithm for the 2d versions. so this would be great. so in other words, the goal should be to have a correct linear time implementation of the closest split pair subroutine. if we can do that, we're home-free, we get the desired n log algorithm. now, i'm going to proceed in a little bit to show you how to implement closest split pair, but before i do that, i want to point out one subtle, the key idea, which is going to allow us to get this linear time correct implementation. so, let me just put that on the slide. so, the key idea is that we don't actually need a full-blown correct implementation of the closets split pair subroutine. so, i'm not actually going to show you a linear time subroutine that always correctly computes the closets split pair of a point set. the reason i'm going to do that is that's actually a strictly harder problem than what we need to have a correct recursive algorithm. we do not actually need a subroutine that, for every point sets, always correctly computes the closest split pair of points. remember, there's a lucky case and there's an unlucky case. the lucky case is where the closest pair in the whole point set p happens to lie entirely in the left half of the points q or in the right half of the points r in that lucky case, we, one of our recursive calls will identify this closest pair and hand it over to us on a silver platter. we could care less about the split pairs in that case. we get the right answer without even looking at the split pair, pairs. now, there's this unlucky case where the split pairs happens to be the closest pair of points. that is when we need this linear time subroutine, and only. then, only in the unlucky case where the closest pair of points happens to be split. now, that's in some sense, a fairly trivial observation, but, there's a lot of ingenuity here i n figuring out how to use that observation. the fact that we only need to solve a strictly easier problem and that will enable the linear time implementation that i'm going to show you next. so now, let's rewrite the high level recursive algorithm slightly to make use of this observation that the closest split pair subroutine only has to operate correctly in the regime of the unlucky case, when in fact, the closest split pair is closer than the result of either recursive call. so i've erased the previous steps 4 and 5, that, but we're going to rewrite them in a second. so, before we invoke close split pair, what we're going to do is we're going to see how well did our recursive calls do. that is, we're going to define a parameter little delta, which is going to be the closest pair that we found or the distance of the closest pair we found by either recursive call. so the minimum of the distance between p1 and q1, the closest pair that lies entirely on the left, and p2q2, the closest pair that lies entirely on the right. now, we're going to pass this delta information as a parameter into our closest split pair subroutine. we're going to have to see why on earth that would be useful and i still owe you that information, but, for now, we're just going to pass delta as a parameter for use in the closest split pair. and then, as before we just do a comparison between the three candidate closest pairs and return the best of the, of the trio. and so, just so we're all clear on, on where things stand, so what remains is to describe the implementation of closest split pair, and before i describe it, let me just be crystal clear on what it is that we're going to demand of the subroutine. what do we need to have a correct in o of n log n time closest pair algorithm. well, as you saw on the quiz, we want the running time to be o of n always, and for correctness, what do we need? again, we don't need it to always compute the closest split pair, but we need it to compute the closest split pair in the events that there is a split pair of distance strictly less than delta, strictly better than the outcome of either recursive call. so now that we're clear on what we want, let's go ahead and go through the pseudocode for this closest split pair subroutine. and i'm going to tell you upfront, iit's going to be fairly straightforward to figure out that the subroutine runs in linear time, o of n time. the correctness requirement of closest split pair will be highly non-obvious. in fact, after i show you this pseudo you're not going to believe me. you're going to look at the pseudocode and you'd be like, what are you talking about? but in the second video, on the closest pair lecture, we will in fact show that this is a correct sub-routine. so, how does it work? well, let's look at a point set. so, the first thing we're going to do is a filtering step. we're going to prune a bunch of the points away and so to zoom in on a subset of the points. and the subset of the points we're going to look at is those that lie in a vertical strip, which is roughly centered in the middle of the point set. so, here's what i mean. by center dot, we're going to look at the middle x coordinate. so, let x bar be the biggest x coordinate in the left half, so that is in the sorted version of the points by x coordinate, we look at the n over two smallest ex-coordinate. so, in this example where we have six points, all this means is we draw, we imagine drawing a line between the third points, so that's going to be x bar, the x coordinate of the third point from the left. now, since we're passed as input, a copy of the points sorted by x coordinate, we can figure out what x bar is in constant time. just by accessing the relevant entry of the array, px. now, the way we're going to use this parameter delta that we're passed, so remember what delta is. so before we invoke the closest split pair subroutine in the recursive algorithm, we make our two recursive calls, we find the closest pair on the left, the closest pair on the right, and delta is whatever the smaller of those two distances are. so delta is the parameter that controls whether or not we actually care about the closest split pair or not, we care if and only if there is a split pair at distance less than delta. so, how do we use delta? well, that's going to determine the width of our strip, so the strip's going to have width 2 delta, and it's going to be centered around x. and the first thing we're going to do is we're going to ignore, forevermore, points which do not line in this vertical strip. so the rest of the algorithm will operate only on the subset of p, the subset of the points that lie on the strip, and we're going to keep track of them sorted by y coordinate. so the formal way to say that they line the strip, is that they have x coordinate in the interval with lower endpoint x bar minus delta and upper endpoint x bar plus delta. now, how long does it take to construct this set sy sorted by y coordinate? well fortunately, we've been passed as input a sorted version of the points py so to extract sy from py, all we need to do is a simple linear scan through p y checking for each point where its x coordinate is. so this can be done in linear time. now, i haven't yet shown you why it's useful to have this sorted set as y, but if you take it on faith that it's useful to have the points in this vertical strip sorted by y coordinate. you now see why it was useful that we did this merge sort all the way at the beginning of the algorithm before we even underwent any recurssion. remember, what is our running time goal for closest split pair? we want this to run in linear time, that means we cannot sort inside the closest split pair subroutine. that would take too long. we want this to be in linear time. fortunately, since we sorted once and for all at the beginning of the closest pair algorithm, extracting sorted sublists from those sorted lists of points can be done, done in linear time, which is within our goals here. now, it's the rest of t he subroutine where you're never going to believe me that it does anything useful. so, i claim that essentially with a linear scan through sy, we're going to be able to identify the closest split pair of points in the interesting, unlucky case where there is such a split pair with distance less than delta. so here's what i mean by that linear scan through sy. so as we do the scan, we're, we're going to keep track of the closest pair of points of a particular type that we've seen so far. so, let me introduce some variables to keep track of the best candidate we've seen so far. there's going to be a vary, variable best which will initialize to be delta. remember, we're uninterested in split pairs unless they have distance strictly less than delta. so, and then we're going to keep track of the points themselves, so we'll initialize the best pair to be null. now, here is the linear scan. so we go through the points of sy in order y coordinate. okay, well, not quite all the points of sy. we stop at the eighth to last point and you'll see why in a second. and then, for each position i of the array sy, we investigate the seven subsequent points of the same array sy. so for j going from one to seven, we look at the ith, and i plus jth entry of sy. so if sy looks something like this array here, in any given point in this double for loop, we're generally looking at an index i, a point in this, in this of the array, and then some really quite nearby point in the array i plus j, because j here's going to be at most seven. okay? so we're constantly looking at pairs in this array, but we're not looking at all pairs of all. we're only looking at pairs that are very close to each other, within seven positions of each other. and what do we do for each choice of i and j? well, we just look at those points, we compute the distance, we see if it's better than all of the pairs of points of this form that we've looked at in the past and if it is better, then we remember it. so we just remember the best, ie c losest pair of points, of this particular type for choices of i and j of this form. so in more detail, if the distance between the current pair of points of p and q is better than the best we've seen so far, we reset the best pair of points to be equal to p and q, and we reset the best distance, the closest distance seemed so far to be the distance between p and q and that's it. then, once this double for loop terminates, we just return it the best pair. so one possible execution of closest split pair is that it never finds a pair of points, p and q, at distance less than delta. in that case, this is going to return null and then in the outer call. in the closet pair, obviously, you interpret a null pair of points to have an infinite distance. so if you call closest split pair, and it doesn't return any points, then the interpretation is that there's no interesting split pair of points and you just return the better of the results of the two recursive calls p1q1 or p2q2. now, as far as the running time of the subroutine, what happens here? well, we do constant work just initializing the variables. then notice that the number of points in sy, well in the worst case, you have all of the points of p. so, it's going to be the most endpoints, and so, you do a linear number of iterations in the outer for loop. but here is the key point, in the inner for loop, right, normally double for loops give rise to quadratic running time, but in this inner for loop we only look at a constant number of other positions. we only look at seven other positions and for each of those seven positions, we only do a constant number of work. right? we just, we want to compare distance and make a couple other comparisons, and reset some variables. so for each of the linear number of outer iterations, we do a constant amount of work, so that gives us a running time of o of n for this part of the algorithm. so as i promised, analyzing the running time of this closest split pair subroutine was not challenging. we just , in a straightforward way, looked at all the operations. again, because in the key linear scan, we only do constant work per index, the overall running time is big o of n, just as we wanted. so this does mean that our overall recursive algorithm will have running time o of n log n. what is totally not obvious and perhaps even unbelievable, is that this subroutine satifies the correctness requirements that we wanted. remember, what we needed, we needed that whenever we're in the unlucky case, whenever, in fact, the closest pair of points in the whole point set is split, this subroutine better find it. so, but it does, and that's being precise in the following correctness claim. so let me rephrase the claim in terms of an arbitrary split pair, which has distance less than delta, not necessarily the closest such pair. so suppose, there exists, a p on the left, a point on the left side and a point on the right side so that is a split pair and suppose the distance of this pair is less than q. now, there may or may not be such a pair of points, pq.. don't forget what this parameter delta means. what delta is, by definition, is the minimum of d of p1q1, for p1q1 is the closest pair of points that lie entirely in the left half of the point set q and d of p2q2, or similarly, p2q2 is the closest pair of points that entirely on the right inside of r. so, if there's a split pair with distance less than delta, this is exactly the unlucky case of the algorithm. this is exactly where neither recursive call successfully identifies the closest pair of points, instead that closest pair is a split pair. on the other hand, if we are in the lucky case, then there will not be any split pairs with distance less than delta, because the closest pair lies either all on the left or on the right, and it's not split. but remember, we're interested in the case where there is a split pair that has a distance less than delta where there is a split pair that is the closest pair. so the claim has two parts. the first part, part a, says the following. it says that if there's a split pair p and, and q of this type, then p and q are members of sy. and let me just sort of redraw the cartoon. so remember what sy is. sy is that vertical strip. and again, the way we got that is we drew a line through a median x coordinate and then we fattened it by delta on either side, and then, we focused only on points that lie in the vertical strip. now, notice our counts split pair subroutine, if it ever returns a pair of points, it's going to return a pair of points pq that belong to sy. first, it filters down to sy, then it does a linear search through sy. so if we want to believe that our subroutine identifies best split pairs of points, then, in particular, such split pairs of points better show up in sy, they better survive the filtering step. so that's precisely what part a of the claim is. here's part b of the claim and this is the more remarkable part of the claim, which is that p and q are almost next to each other in this sorted array, sy. so they're not necessarily adjacent, but they're very close, they're within seven positions away from each other. so, this is really the remarkable part of the algorithm. this is really what's surprising and what makes the whole algorithm work. so, just to make sure that we're all clear on everything, let's show that if we prove this claim, then we're done, then we have a correct fast implementation of a closest pair algorithm. i certainly owe you the proof of the claim, that's what the next video is going to be all about, but let's show that if the claim is true, then, we're home-free. so if this claim is true, then so is the following corollary, which i'll call corollaryl 1. so corollary 1 says, if we're in the unlucky case that we discussed earlier, if we're in the case where the closest point and the whole points of p does not lie both on the left, does not lie both on the right, but rather has one point on the left and one on the right but as it's a split pair, th en in fact, the count split pair subroutine will correctly identify the closest split pair and therefore the closest pair overall. why is this true? well what does count split pair do? okay, so it has this double for loop, and thereby, explicitly examines a bunch of pairs of points and it remembers the closest pair of all of the pairs of points that it examines. what does this, so what are the criteria that are necessary for count split pair to examine a pair point? well, first of all, the points p and q both have to survive the filtering step and make it into the array sy. right? so count split pair only searches over the array sy. secondly, it only searches over pairs of points that are almost adjacent in sy, that are only seven positions apart, but amongst pairs of points that satisfy those two criteria, counts but pair will certainly compute the closest such pair, right? it just explicitly remembers the best of them. now, what's the content of the claim? well, the claim is guaranteeing that every potentially interesting split pair of points and every split pair of points with distance less than delta meets both of the criteria which are necessary to be examined by the count split pair subroutine. so first of all, and this is the content of part a, if you have an interesting split pair of points with distance less than delta, then they'll both survive the filtering step. they'll both make it into the array sy., part a says that. part b says they're almost adjacent in sy. so if you have an interesting split pair of points, meaning it has distance less than delta, then they will, in fact, be at most seven positions apart. therefore, count split pair will examine all such split pairs, all split pairs with distance less than delta, and just by construction, it will compute the closest pair of all of them. so again, in the unlucky case where the best pair of points is a split pair, then this claim guarantees that the count split pair will compute the closest pair of points. therefore, having h andled correctness, we can just combine that with our earlier observations about running time and corollary 2 just says, if we can prove the claim, then we have everything we wanted. we have a correct o of n log n implementation for the closest pair of points. so with further work and a lot more ingenuity, we've replicated the guarantee that we got just by sorting for the one-dimensional case. now again, these corrollaries hold only if this claim is, in fact, true and i have given you no justification for this claim. and even the statement of the claim, i think, is a little bit shocking. so if i were you i would demand an explanation for why this claim is true, and that's what i'm going to give you in the next video. 
alright. so the plan for this video is to prove the correctness of the divide and conquer closest to pair algorithm that we discussed in the previous video. so just to refresh your memory, how does the outer algorithm work? well, we're given endpoints in the plane. we begin by sorting them, first by x-coordinate and then by y-coordinate. that takes n log in time. then we enter the main recursive divide and conquer part of the algorithm. so what do we do? we divide the point set into the left half and the right half, q and r, then we conquer. we recursively compute the closest pair in the left half of the point set q. we recursively compute the closest pair in the right half of the point set r. there is a lucky case where the closest pair on the entire point set lies either all on the left or all on the right. in that case, the closest pair is handed to us on a silver platter, by one of the two recursive calls. but there remains the unlucky case where the closest pair is actually split with one point on the left and one point on the right. so to get our n log n running time bound, analogous to merge short in our inversion counting, we need to have a linear time implementation of a subroutine which computes the best, the closest pair of points, which is split, one on the left and one on the right. well, actually, we don't need to do quite that. we need to do something only a little bit weaker. we need a linear time algorithm, which whenever the closest pair in the whole point set is in fact split, then computes that split pair in linear time. so let me now remind you of how that subroutine works. so it has two basic steps. so first, there's a filtering step. so it looks at, first of all, a vertical strip, roughly down the middle of the point set. and it looks at, only at points which fall into that vertical strip. that was a subset of the points that we called s sub y, 'cause we keep track of them sorted by y coordinate. and then we do essentially a linear scan through sy. so we go through the points one at a time, and then, for each point, we look at only the almost adjacent points. so for each index i, we look only at j's that are between one and seven positions further to the right, than i. so among all such points, we compare them, we look at their distances. we remember the best such pair of points. and then that's what we return from the count split pair subroutine. so we've already argued, in the previous video, that the overall running time of the algorithm is n log n. and what remains to prove correctness. and we also argued, in the previous video, that correctness boils down to the following correctness claim. in the sense that, if we can prove this claim, then the entire algorithm is correct. so this is what remains. our residual work is to provide a proof of the correctness claim. what does it say? it says consider any split pair that is one point p from the left side q, capital q, and another point little q drawn from the right side of the point set capital r. and fur, further suppose that it's an interesting split pair meaning that the distance between them's at most delta. here delta is recall the parameter pass to the count split pair subroutine, which is the smallest distance between a pair of points all on the left or all on the right. and this is the only case we're interested in. there's two claims. first of all, for p and q, both members of the split pair survive the filtering step. they make it into the sorted list s sub y, and second of all, they will be considered by that double for loop, in the sense that the positions of p and q in this array, s sub y, differ by at most seven. so that's the story so far. let's move on to the proof. so let's start with part a which is the easy part relatively of the claim. so remember what we start with, our assumptions. we have a point p, let's write it out in terms of the x coordinates, x1 and y1, which is from the left half of the point set. and we have a point q, which we'll call x2y2, which comes from the right half of the point set. and furthermore, we're assuming that these points are close to each other. and we're gonna use that hypothesis over and over again. so the euclidean distance between p and q is no more than this parameter delta. so, first, something very simple, which is that if you have two points that are close in euclidean distance, then both of their coordinates have to be close to each other, right? if you have two points, and they differ by a lot in one coordinate, then the euclidean distance is gonna be pretty big as well. so, specifically. by our hypothesis, that p and q have euclidean distance less than delta, it must be that the difference between their coordinates in absolute value is no more than delta, and as well, the difference between their y-coordinates is at most delta. okay, and this is easy to see if you'd just return to the definition of euclidean distance that we reviewed at the beginning of the discussion of closest points. okay? so if your distance is at most delta, then in each coordinate, you differ by at most delta as well. now, what does a say? [sound]. so proof of a. so what does part a of the claim assert? it asserts that p and q are both members of sy, are both members of that vertical strip. so another way of saying that is that the x coordinates of p and q, that is, the numbers x1 and x2 both are within delta of xbar. remember, xbar was in some sense the median x coordinate. so the x coordinate of the n over two'th leftmost point. so we're gonna do a proof by picture, so consider, forget about the y coordinates, that's of irrelevant right now, and just focus on the x coordinates of all of these points. so on the one hand we have x bar. this is the x coordinate of the n over two'th point to the left. and then there are the x coordinates which define the left and the right borders of that vertical strip. namely xbar-delta and xbar+delta. and then somewhere in here are x1 and y1, the x coordinates of the points we care about, p and q. so a simple observation, so because p comes from the left half of the point set, and xbar is the rightmost x coordinate of the left half of the point set, the x coordinate is at most xbar. right? so all points of q have x coordinate, at most, xbar, in particular, p does. similarly, since xbar is the rightmost edge of the left half of the point set, everything in the right half of the point set has x coordinate, at least xbar. so in particular, little q does as well. so what does this mean. so this means x1, wherever it is, has to be at the left of x bar. x2 wherever it is has to be to the right of x bar. what we're trying to prove is that they're wedged in between x bar minus delta and x bar plus delta. and the reason why that's true is because their x coordinates also differ by at most delta. okay, so what you should imagine is. you can imagine x1 and x2 are sort of people tied by a rope at the waist. and this rope has length delta. so wherever x1 and x2 move, they're at most delta apart. furthermore x1, we just observed, can't move any farther to the right than xbar. so even if x1 moves as far to the right as it can, all the way to xbar, x2, since it's at most delta away, tied by the waist, can't extend beyond x bar+ delta. by the same reasoning, x2 can't move any further to the left than xbar, x1 being tied to the waist to x2, can never drift further to the left than xbar minus delta. so that's the proof that x1 and x2 both lie within this region, and that defines the vertical strip. so that's part a. if you have any split pair whose distance between them is less than delta, they both have to wind up, in this vertical strip. and therefore wind up in the filtered set, the proof set, s sub y. so that's part a of the claim. let's now move to part b. recall what part b asserts. it says that the points p and q, this split pair that are distance only delta apart. not only do they wind up in this sort of filtered set sy, but in fact, they are almost adjacent in sy, in the sense that the indices in the array differ by, at most, seven positions. and this is a part of the claim that is a little bit shocking. really what this says is that we're getting away with more or less a variant of our one dimensional algorithm. remember when we wanted to find the closest pair of points on the line, all we had to do was sort them by their single coordinate and then look at consecutive pairs and return the best of those consecutive pairs. here what we're saying is really, once we do a suitable filtering focus on points in this vertical strip, then we just go through the points according to their y coordinate. and okay, we don't just look at adjacent pairs. we look at pairs within seven positions, but still we basically do a linear sweep through the points in sy. according to their y coordinate and that's sufficient to identify the closest split pair. so why on earth will this be true. so our workhorse in this argument will be a picture which i am going to draw on next. so i'm going to draw eight boxes, which have a height and width delta over two. so here, delta is the same parameter that gets passed to the closest split pair subroutine. and it's also the same delta which we're assuming p and q are closer to each other than, right? so that's, remember, that's one of our hypotheses in this claim. the distance between p and q is strictly less than delta. so we're gonna draw eight delta over two boxes. and they're gonna be centered at x bar. so, this same center of the vertical strip that defines s y. and the bottom is going to be the smaller of the y-coordinates of the points p and q. so it might be p, it might be q. it doesn't really matter. but the bottom is going to be the smaller of the two. so the picture then looks as follows. so the center of these collections of eight boxes, x bar, the bottom is the minimum of y1, y2. we're gonna have two rows and four columns. and needless to say, we're drawing this picture just for the sake of this correctness proof, right? this picture is just a thought experiment in our head. we're just trying to understand why the algorithm works. the algorithm, of course, does not draw these boxes. the subroutine, the, closest split pair subroutine is just that pseudo code we saw in the previous video. this is just to reason about the behavior of that subroutine. now looking ahead, i'll make this precise in two lemmas that are about to come up, what's going to be true is the following. so, either p or q is on this bottom line, right? so we define the bottom to be the lower y coordinate of the two. so maybe, for example, q is the one that has the smaller y coordinate, in which case, is gonna be, somewhere, say, down here. p, you remember, is from the left half of the point sets. so p is maybe gonna be here or something. and we're gonna argue that both p and q have to be in these boxes. moreover, we're gonna argue that these boxes are sparsely populated. every one contains either zero or one point of the array s sub y. so, what we're gonna see is that there's at most eight points in this picture, two of which are p and q, and therefore, if you look at these points sorted by y coordinate, it has to be that they're within seven of each other, the difference of indices is no more than seven. so, we're gonna make those two statements precise one at a time by the following two lemmas. let's start with lemma one. lemma one is the easy one. and it states that all of the points of s sub y, which show up in between the y coordinates of the points we care about p and q have to appear in this picture, they have to lie in one of these eight boxes. so we're going to argue this in two steps. first, we're going to argue that all such points have to have y coordinates within the relevant range of this picture between the minimum of y1 and y2 and delta more than that, and secondly that they have to have x coordinates in the range of this picture, namely between x bar minus delta and x bar plus delta. so let's start with y coordinates. so again, remember this key hypothesis we have, okay. we're dealing with a split pair p-q that are close to each other. the distance between x and y is strictly less than delta. so the very first thing we did at the beginning of this proof is we said well, if their euclidean distance is less than delta then they have to differ by at most delta in both of their coordinates, in particular in their y coordinate. now remember whichever is lower of p and q, whichever one has a smaller y coordinate is precisely at the bottom of this diagram. for example, if q is the one with the smaller y coordinate, it might be on the black line right here. so that means in particular x has y coordinate no more than the top part of this diagram. no more than delta bigger than q. and of course all points with y coordinates in between them are equally well wedged into this picture. so that's why all points of sy with a y coordinate between those of p and q have to be in the range of this picture, between the minimum of the two y coordinates and delta more than that. now what about horizontally? what about the x coordinates? well this just follows from the definition of s sub y. so remember, s sub y are the points that fall into this vertical strip. how did we define the vertical strip? well it had center xbar, and then we fattened it by delta on both sides. so just by definition, if you're an sy, you've gotta have x coordinates in the range of this picture. x delta plus minus, sorry, xbar plus minus delta. so that completes the proof of the lemma. so this is not. this is just a lemma. so i'll use a lower case qed. remember this is just a step toward proving the overall correctness claim. but this is a good step. and again, the way you think about this is it says we draw this boxes. we know that either p or q is at the bottom. the other one is going to be on the other side of the black line x bar but will be in some other box so perhaps maybe p is here and the lemma is saying that all the relevant points of sy have to be somewhere in this picture. now remember in our double for loop we only search seven positions away, so the concern is that this is a sorta super highly populated collection of eight boxes. that's the concern, but that's not going to be the case and that's exactly what lemma two is going to say. not only do the points between p and q in y coordinates show up in this diagram, but there have to be very few. in particular, every box has to be sparse, with population either zero or one. so, let's move on to lemma two. so formally the claim is [sound], we have at most one point of the point set in each of these eight boxes. and this is finally where we use, in a real way, the definition of delta. this is where we finally get the payoff from our realization long ago, that when defining the closest split pair subroutine, we only really need to be correct in the unlucky case. in the case we're not handed the right answer by one of our recursive calls. we're finally gonna use that fact in a fundamental way. so we're gonna proceed by contradiction. so we're going to think about what happens if there are two points in a single box and from that we'll be able to derive a contradiction. so, call the points that wind up in the same box a and b. so, to the contrary, suppose a and b lie in the same box. so, maybe this is a here, and this is b here, at antipodal corners of this particular box. so from this supposition, we have two consequences. first of all. i claim that a and b lie on the same side of the point set. they're either both in the left side, q or they're both in the right side, r. so why is this true? well it's because every box lies either entirely on the left half of the point set or on the right half of the point set. recall how we define x bar. x bar is the x coordinate of the right most point amongst the left half of the point set capital q. so therefore points with x coordinate at most x bar have to lie inside the left half q. points with x coordinates at least x bar have to lie inside the right half of the point set capital r. so that would be like in this example. a and b both lie in a box which is to the right of x bar. so they both have to come in the right half of the point set capital r. this is one place we are using that there are no ties in x coordinate, so if there's a point with x, x coordinate or x bar, we can count it as part of the left half. so every box, by virtue of being either to the left of xbar or to the right of xbar, can only contain points from a common half of the point set. so that's the first consequence of assuming that you have two points in the same box. the second consequence is, because the boxes are small, the points gotta be close. so, if a and b co-habitate a box, how far could they be from each other? well, the farthest they could be is like i've drawn in the picture, with the points a and b. where they're at opposite corners of a common box. and then you bust out pythagorean's theorem, and what do you get? you get that the distance between them is delta over two, the side of the box times root two. and what's relevant for us is this is strictly less than delta. okay? but, now, here is where we use, finally, the definition of delta. consequences one and two in tandem, contradict how we define delta. remember what delta is. it's as close as two pair of, a pair of points can get if they both lie on the left side of the point set, or if they both lie on the right side of the point set. that is how we defined it. as small as a pair of points on a common half can get to each other. but what have we just done? we've exhibited a pair a and b that lie on the same half of the point set, and are strictly closer than delta. so that contradicts the definition of delta. so that completes the proof of lemma two. let me just make sure we're all clear on why having proved lemma one and lemma two we're done with the proof part b of the claim and therefore the entire claim because we already proved part one, now a long time ago. so let's interpret the 2 lemmas in the context of our picture that we had all throughout. in terms of the eight boxes of side length delta over two by delta over two. so again, whichever is the lower of p and q, and again let's just for the sake of concreteness say it's q, is at the bottom of the picture. the other point is on the other half of the line xbar, and is in one of the other boxes. so, for example, maybe p is right here. so lemma one says that every relevant point, every point that survives the filtering and makes it into sy, by virtue of being in the vertical strip, has to be in one of those boxes, okay? if it has y coordinate in between p and q. lemma two says that you can only have one point in each of these boxes from the point set, so that's gonna be at most eight total. [sound] so combining them. lemmas one and two imply, that there are almost eight points in this picture and that includes p and q because they also occupy two of eight boxes. so in the worst case, if this is as densely populated as could possibly be, given lemmas one and two, every other box might have a point and perhaps every one of those points has a y coordinate between p and q. but this is as bad as it gets. any point of the strip with y coordinate between p and q occupies a box. so, at most, there are these six wedged in between them. what does this mean? this means if from q you look seven positions ahead in the array, you are guaranteed to find this point p. so a split pair with distance less than delta is guaranteed to be identified by our double for loop. looking seven positions ahead in the sorted array sy is sufficient to identify, to look at every conceivably interesting split pair. so that completes the assertion b of the correctness claim and we're done. that establishes that this supremely clever divide and conquer algorithm is indeed a correct o(nlog(n)) algorithm that computes the closest pair of a set of n points in the plane. 
in this series of videos we'll study the master method. which is a general mathematical tool for analyzing the running time of divide and conquer algorithms. we'll begin in this video motivating the method. then we'll give its formal description. that'll be followed by a video working through six examples. finally we'll conclude with three videos that discuss proof of the master method. with a particular emphasis on the conceptual interpretation of the master method's three cases. so let me say at the outset that this lecture's a little bit more mathematical than the previous two. but it's certainly not just math for math's sake. we'll be rewarded for our work with this powerful tool, the master method, which has a lot of predictive power. it'll give us good advice about which divide and conquer algorithms are likely to run quickly and which ones are likely to run less quickly. indeed it's sort of a general truism that novel algorithmic ideas often require mathematical analysis to properly evaluate. this lecture will be one example of that truism. as a motivating example consider the computational problem of multiplying two n digit numbers. recall from our first set of lectures that we all learned the iterative grade school multiplication algorithm. and that that requires a number of basic operations, additions and multiplications, between single digits. which grows quadratically with the number of digits, n. on the other hand we also discussed an interesting recursive approach using the divide and conquer paradigm. so recall, divide and conquer necessitates identifying smaller sub-problems. so for integer multiplication, we need to identify smaller numbers that we want to multiply. so we proceed in the obvious way, breaking each of the two numbers into its left half of the digits, and its right half of the digits. for convenience, i'm assuming that the number of digits n is even, but it really doesn't matter. having decomposed x and y in this way, we can now expand the product and see what we get. so let's put a box around this expression and call it *. so we begin with the sort of obvious recursive algorithm where we just evaluate the expression * in the straightforward way. that is, * contains four products involving n over two digit numbers, ac, ad, bc, and bd. so we make four recursive calls to compute them. and then we complete the evaluation in the natural way. namely, we append 0s as necessary, and add up these three terms to get the final result. the way we reason about the running time of recursive algorithms like this one is using what's called a recurrence. so to introduce a recurrence, let me first make some notation t(n). this is going to be the quantity that we really care about. the quantity that we want to upper bound. namely this will be the worst case number of operations that this recursive algorithm requires to multiply two n-digit numbers. this is exactly what we want to upper bound. a recurrence, then, is simply a way to express t(n) in terms of t of smaller numbers. that is the running time of an algorithm in terms of the work done by its recursive calls. so every recurrence has two ingredients. first of all it has a base case describing the running time when there's no further recursion. and in this integer multiplication algorithm, like in most divide and conquer algorithms, the base case is easy. once you get down to a small input, in this case two one digit numbers, then the running time is just constant. all you do is multiply the two digits and return the result. so i'm going to express that by just declaring the t(1), the time needed to multiply one digit numbers, as bounded above by a constant. i'm not going to bother to specify what this constant is. you can think of it as one or two if you like. it's not going to matter for what's to follow. the second ingredient in a recurrence is the important one. and it's what happens in the general case when you're not in the base case, and you make recursive calls. and all you do is write down the running time in terms of two pieces. first of all the work done by the recursive calls, and second of all the work that's done right here now. work done outside of the recursive calls. so on the left hand side of this general case, we just write t(n). and then we want an upper bound on t(n) in terms of the work done by recursive calls and the work done outside of recursive calls. and i hope it's self evident what the recurrence should be in this recursive algorithm for integer multiplication. as we discussed, there are exactly four recursive calls. and each is invoked on a pair of n/2 digit numbers. so that gives us 4 times the time needed to multiply n/2 digit numbers. so what do we do outside of the recursive call? well, we pad the results of the recursive calls with a bunch of zeros and we add them up. and i'll leave it to you to verify that grade school addition, in fact, runs in time linear in the number of digits. so putting it all together the amount of work we do outside of the recursive calls is linear. that is, it's o(n). let's move on to the second, more clever, recursive algorithm for integer multiplication, which dates back to gauss. gauss's insight was to realize in the expression * that we're trying to evaluate, there's really only three fundamental quantities that we care about. the coefficients for each of the three terms in the expression. so this, leads us to hope that perhaps we can compute these three quantities using only three recursive calls, rather than four. and indeed, we can. so what we do is we recursively compute a times c, like before, and b times d, like before. but then we compute the product of a + b with c + d. and the very cute fact is if we number these three products, one, two, and three. that the final quantity that we care about, the coefficient of the 10 to the n/2 term, namely ad + bc. is nothing more than the third product minus each of the first two. so that's the new algorithm, what's the new recurrence? the base case obviously is exactly the same as before. so the question then is, how does the general case change? and i'll let you answer this in the following quiz. so the correct response for this quiz is the second one. namely the only thing that changes with respect to the first recurrence is that the number of recursive calls drops from four down to three. a couple of quick comments. so first of all, i'm being a little bit sloppy when i say there's three recursive calls, each on numbers with n/2 digits. when you take the sums a + b and c + d, those might well have n/2 plus 1 digits. amongst friends, let's ignore that. let's just call it n/2 digits in each of the recursive calls. as usual, the extra plus one is not going to matter in the final analysis. secondly i'm ignoring exactly what the constant factor is in the linear work done outside of the recursive calls. indeed it's a little bit bigger in gauss's algorithm than it is in the naive algorithm with four recursive calls. but it's only by a constant factor. and that's going to be suppressed in the big o notation. let's look at this recurrence and compare it to two other reccurrences, one bigger, one smaller. so first of all, as we noted, it differs from the previous recurrence of the naive recursive algorithm in having one fewer recursive call. so, we have no idea what the running time is on either of these two recursive algorithms. but we should confident that this one certainly can only be better. that's for sure. another point of contrast is merge sort. so think about what the recurrence would look like for the merge sort algorithm. it would be almost identical to this one except instead of a three we'd have a two. right? merge sort makes two recursive calls, each on an array of half the size. and outside of the recursive calls it does linear work, namely for the merge sub-routine. we know the running time of merge sort. it's n log n. so this algorithm, gauss's algorithm, is going to be worse, but we don't know by how much. so while we have a couple clues about what the running time of this algorithm might be more or less than. honestly we have no idea what the running time of gauss's recursive algorithm for integer multiplication really is. it is not obvious. we currently have no intuition for it. we don't know what the solution to this recurrence is. but it will be one super-special case of the general master method, which we'll tackle next. 
so having motivated and hyped up the generality of the master method and its use for analyzing recursive algorithms, let's move on to its precise mathematical statement. now, the master method is, in some sense, exactly what you want. it's what i'm going to call a black box for solving recurrences. basically, it takes an input a recurrence in a particular format and it spits out as output a solution to that recurrence, an upper bound on the running time of your recursive algorithm. that is, you just plug in a few parameters of your recursive algorithm, and boom, out pops its running time. now, the master method does require a few assumptions, and let me be explicit about one of them right now. namely, the master method, at least the one i'm going to give you, is only going to be relevant for problems in which all of the subproblems have exactly the same size. so for example, in merge sort, there are two recursive calls, and each is on exactly one half of the array. so merge sort satisfies this assumption, both subproblems have equal size. similarly, in both of our integer multiplication algorithms, all subproblems were on integers with n over 2 digits, with half as many digits, so those will all also obey this assumption. if for some reason you had a recursive algorithm that recursed on a third of the array and then on the other two-thirds of the array, the master method that i'm going to give you will not apply to it. there are generalizations of the master method that i'm going to show you which can accommodate unbalanced subproblem sizes, but those are outside the scope of this course. this will be sufficient for almost all of the examples we're going to see. one notable exception, for those of you that watched the optional video on a deterministic algorithm for linear time selection, that will be one algorithm which has two recursive calls on different subproblem sizes. so to analyze that recurrence, we'll have to use a different method, not the master method. next i'm going to describe the format of the recurrences to which the master method applies. as i said, there are more general versions of the master method which apply to even more recurrences. but the one i'm going to give you is going to be reasonably simple, and it will cover pretty much all the cases you're likely to ever encounter. so recurrences have two ingredients. there's the relatively unimportant, but still necessary, base case step. and we're going to make the obvious assumption, which is just satisfied by every example we're ever going to see in this course, which is that at some point, once the input size drops to a sufficiently small amount, then the recursion stops, and the subproblem is solved in constant time. since this assumption is pretty much always satisfied in every problem we're going to see, i'm not going to discuss it much further. let's move on to the general case where there are recursive calls. so we assume the recurrence is given in the following format. the running time on an input of length n is bounded above by some number of recursive calls, let's call it a different recursive calls. and then each of these subproblems has exactly the same size, and it's 1 over b fraction of the original input size. so there's a recursive calls, each on an input of size n over b. now, as usual, there's the case where n over b is a fraction and not an integer. and as usual, i'm going to be sloppy and ignore it. and as usual, that sloppiness has no implications for the final conclusion. everything that we're going to discuss is true for the same reasons in the general case where n over b is not an integer. now, outside the recursive calls, we do some extra work. and let's say that it's o(n to the d) for some parameter d. so in addition to the input size n, there are three letters here which we need to be very clear on what their meaning is. so first of all, there's a, which is the number of subproblems, the number of recursive calls. so a could be as small as 1 or it might be some larger integer. then there's b. b is the factor by which the input size shrinks before a recursive call is applied. b is some constant strictly greater than 1. so for example, if you recurse on half of the original problem, then b would be equal to 2. it better be strictly bigger than 1 so that eventually you stop recursion, so that eventually then you terminate. finally, there's d, which is simply the exponent in the running time of the, quote, unquote combine step, that is, the amount of work which is done outside of the recursive calls. and d could be as small as 0, which would indicate constant amount of work outside of the recursive calls. one point to emphasize is that a, b, and d are all constants. they're all numbers that are independent of n. so a, b, and d are going to be numbers like 1, 2, 3, or 4. they do not depend on the input size n. and in fact, let me just redraw the d so that you don't confuse it with the a. so again, a is the number of recursive calls and d is the exponent and the running time governing the work done outside of the recursive calls. now, one comment about that final term, that big o(n to the d). on the one hand, i'm being sort of sloppy. i'm not keeping track of the constant that's hidden inside the big-o notation. i'll be explicit with that constant when we actually prove the master method. but it's really not going to matter. it's just going to carry through the analysis without affecting anything. so you can go ahead and ignore that constant inside the big-o. obviously, the constant in the exponent, namely d, is very important. so depending on what d is, depends on whether that amount of time is constant, linear, quadratic, or so on. so certainly we care about the constant d. so that's the input to the master method. it is a recurrence of this form. so you can think of it as a recursive algorithm which makes a recursive calls, each on subproblems of equal size, each of size n over b, plus it does n to the d work outside of the recursive calls. so having set up the notation, i can now precisely state the master method for you. so given such a recurrence, we're going to get an upper bound on the running time. so the running time on inputs of size n is going to be upper bounded by one of three things. so somewhat famously, the master method has three cases. so let me tell you about each of them. the trigger, which determines which case you're in, is a comparison between two numbers. first of all, a, recall, a is the number of recursive calls made. and b raised to the d power. recall, b is the factor by which the input size shrinks before you recurse. d is the exponent in the amount of work done outside of the recursive call. so we're going to have one case for when they're equal, we're going to have one case for when a is strictly smaller than b to the d. and the third case is when a is strictly bigger than b of the d. and in the first case, we got a running time of big o of n to the d times log n. and again, this is d, the same d that was in the final term of the recurrence. okay, the work done outside of the recursive calls. so the first case, the running time is the same as the running time in the recurrence, outside of the recursive calls, but we pick up an extra log n factor. in the second case, where a is smaller than b to the d, the running time is merely big-o of n to the d. and this case might be somewhat stunning that this could ever occur, because of course, in recurrence, what do you do? you do some recursion, plus you do n to the d work outside of the recursion. so in the second case, it actually says that the work is dominated by just what's done outside the recursion in the outermost call. the third case will initially seem the most mysterious. when a is strictly bigger than b to the d, we're going to get a running time of big-o of n to the log base b of a. where, again, recall, a is the number of recursive calls and b is the factor by which the input size shrinks before you recurse. so that's the master method with its three cases. let me give this to you in a cleaner slide to make sure there's no ambiguity in my handwriting. so here's the exact same statement, the master method once again with its three cases, depending on how a compares to b to the d. so one thing you'll notice about this version of the master method is that it only gives upper bounds. so we only say that the solution to the recurrence is big-o of some function. and that's because if you go back to our recurrence, we used big-o rather than theta in the recurrence. and this is in the spirit of the course, where as algorithm designers, our natural focus is on upper bounds, on guarantees for the worst case running time of an algorithm. and we're not going to focus too much most of the time on proving stronger bounds in terms of theta notation. now, a good exercise for you, to check if you really understand the proof of the master method after we go through it will be to show that if you strengthen the hypothesis and you assume the recurrence has the form t of n equals a times t of n over b plus theta of n to the d, then in fact, all three of these big-o's in the statement of the master method become thetas and the solution becomes asymptotically exact. so one final comment. you'll notice that i'm being asymmetrically sloppy with the two logarithms that appear in these formulas. so let me just explain why. in particular, you'll notice that in case one, with the logarithm, i'm not specifying the base. why is that true? well, it's because the the logarithm, with respect to any two different bases, differs by a constant factor. so the logarithm base e, that is, the natural logarithm, and the logarithm base 2, for example, differ by only a constant factor independent of the argument n. so you can switch this logarithm to whatever constant base you like, it only changes the leading constant factor, which of course is being suppressed in the big-o notation anyways. on the other hand, in case three, where we have a logarithm in the exponent, once it's in the exponent, we definitely care about that constant. constants is the difference between, say, linear time and quadratic time. so we need to keep careful track of the logarithm base in the exponent in case three, and that base is precisely b, the factor by which the input shrinks with each recursive call. so that's the precise statement of the master method, and the rest of this lecture will work toward understanding the master method. so first, in the next video, we'll look at a number of examples, including resolving the running time of gauss's recursive algorithm for integer multiplication. following those several examples, we'll prove the master method. and i know now these three cases probably look super mysterious, but if i do my job, by the end of the analysis, these three cases will seem like the most natural thing in the world, as will these somewhat exotic looking formula for exactly what the running time is. 
in this video, we'll put the master method to use by instantiating it for six different examples. but first, let's recall what the master method says. so the master method takes as input recurrences of a particular format, in particular recurrences that are parameterized by three different constants, a, b and d. a refers to the number of recursive calls, or the number of subproblems that get solved. b is the factor by which the subproblem size is smaller than the original problem size. and d is the exponent and the running time of the work done outside of the recursive calls. so the recurrence has the form, t(n), the running time on the input of size n, is no more than a, the number of subproblems, times the time required to solve each subproblem. which is t(n/b) because the input size of a subproblem is n/b. plus o(n to the d). the work outside of the recursive calls. there's also a base case which i haven't written down. so once the problem size drops below a particular constant then there should be no more recursion and you can just solve the problem immediately that is in constant time. now given a recurrence in this permitted format, the running time is given by one of three formulas depending on the relationship between a, the number of recursive calls, and b raised to the d power. case one of the master method is when these two quantities are the same, a = b to the d. then the running time is n to the d log n, no more than that. in case 2, the number of recursive calls, a, is strictly smaller than b to the d. then we get a better running time upperbound of o(n to the d). and when a is bigger than b to the d, we get this somewhat funky looking running time of o(n raised to the log base b of a power). we will understand where that formula comes from a little later. so that's the master method. it's a little hard to interpret the first time you see it. so let's look at some concrete examples. let's begin with an algorithm that we already know the answer to, we already know the running time. namely let's look at merge sort. so again what's so great about the master method is all we have to do is identify the values of the three relevant parameters a, b, and d, and we're done. we just plug them in and we get the answer. so a remembers the number of recursive calls. so in merge sort, recall we get two recursive calls. b is the factor by which the sub problem size is smaller than that in the original. well we recurse on half the array. so the subproblem size is half that of the original. so b = 2. and recall that outside of the recursive calls, all merge sort does is merge. and that's a linear time subroutine. so the exponent d is 1, a reflection of the fact that it's linear time. so remember the key trigger which determines which of the three cases is the relationship between a and b to the d. so a obviously is 2. and b to the d = 2. so this puts us in case 1. and remember in case 1 we have that the running time is bounded above by o(n to the d (logn). in our case d = 1. so this is just o(n log n). which, of course, we already knew. but at least this is a sanity check, the master method is at least reconfirming facts which we've already proven by direct means. so let's look at a second example. the second example is going to be for the binary search algorithm in a sorted array. now we haven't talked explicitly about binary search and i'm not planning to, so if you don't know what binary search is, please read about it in a textbook or just look it up on the web, and it'll be easy to find descriptions. but the upshot it, this is basically how you'd look up a phone number in a phone book. now i realize probably the youngest viewers of this video haven't actually had the experience of using a physical telephone book. but for the rest of you, as you know. you don't actually start with the as and then go to the bs, and then go to the cs if you're looking for a given name. you more sensibly split the telephone book roughly in the middle and depending if what you're looking for is early or later in the alphabet, you effectively recurse on the relevant half of the telephone book. so binary search is just exactly the same algorithm when you are looking for a given element in a particular sorted array. you start in the middle of the array, and then you recurse on the left or the right half as appropriate depending on if the element you're looking for is bigger or less than the middle element. now the master method applies equally well to binary search and it tells us what its running time is. so in the next quiz you'll go through that exercise. so the correct answer is the first one. to see why let's recall what a, b, and d mean. a is the number of recursive calls. now in binary search you only make one recursive call. this is unlike merge sort. remember you just compare the element you're looking for to the middle element. if it's less than the middle element you recurse on the left half. if it's bigger than the middle element, you recurse on the right half. so in any case there's only one recursive call, so a is merely 1 in binary search. now in any case you recurse on half the array so like in merge sort the value of b = 2, you recurse on a problem of half the size. and outside of the recursive column, the only thing you do is one comparison. you just determine whether the element you're looking for is bigger than or less than the middle element of the array that you recursed on. so that's constant time outside the recursive call giving us a value for d of 0. just like merge sort, this is again case 1 of the master method because we have a = b to the d, both in this case are equal to one. so this gives us a recurrence, a solution to our recurrence of big o(n to the d log n). since d = 0 this is simply log n. and again many of you probably already know that the running time of binary search is log n or you can figure that out easily. again this is just using the master method as a sanity check to reconfirm that it's giving us the answers that we expect. let's now move on to some harder examples, beginning with the first recursive algorithm for integer multiplication. remember this is where we recurse on four different products of n over two digit numbers and then re-combine them in the obvious way using adding by zero and some linear time additions. in the first integer multiplication algorithm, which, does not make use of gauss's trick where we do the four different recursive calls in a naive way, we have a, the number of recursive calls, = 4. now in each case, whenever we take a product of two smaller numbers, the numbers have n over two digits so that's half as many digits as we started with. so just like in the previous two examples, b = 2. the input size drops by a factor 2 when we recurse. now how much work do we do outside of the recursive call? well again all it is doing is additions and adding by zeros and that can be done in linear time. linear time corresponds to a primer value of d = 1. so next we determine which case of the master method we're in. a = 4, b to the d = 2, which in this case is less than a. so this corresponds to case 3 of the master method. this is where we get the somewhat strange formula for the running time of the recurrence. t(n) = o(n to the log base b of a). which with our parameter values, is n to the log base 2(4). also known as o(n squared). so let's compare this to the simple algorithm that we all learned back in grade school. recall that the iterative algorithm for multiplying two integers also takes an n squared number of operations. so this was a clever idea to attack the problem recursively. but at least in the absence of gauss's trick where you just naively compute each of the four necessary products separately. you do not get any improvement over the iterative algorithm that you learned in grade school. either way, it's an n squared number of operations. but what if we do make use of gauss's trick, where we do only three recursive calls instead of four? surely the running time won't be any worse than n squared, and hopefully it's going to be better. so i'll let you work out the details on this next quiz. so the correct answer to this quiz is the fourth option. it's not hard to see what the relevant values of a, b, and d are. remember the whole point of gauss's trick is to reduce the number of recursive calls from four down to three so the value of a is going to be 3. as usual we're recursing on a problem size which is half of that of the original. in this case n over two digit numbers so b remains 2. and just like in the more naive recursive algorithm, we only do linear work outside of the recursive call. so all that's needed to do some additions and patterns by 0. so that puts this parameter values a, b, and d. then we have to figure out which case of the master method that is. so we have a = 3, b raised to the d = 2. so a has dropped by 1 relative to the more naive algorithm. but we're still in case 3 of the master method. a is still bigger that the b to the d. so the running time is governed by that rather exotic looking formula. namely t(n) = o(n to the log base b), which in our case is 2(a). which is now 3 instead of 4, okay? so the master method just tells us the solution to this recurrence of 3. so what is log of the, what is log base 2(3)? well plug it in your computer or your calculator, and you'll find that it's roughly 1.59. so we've got a running time of n to the 1.59. which is certainly better than n squared. it's not as fast as n log n, not as fast as the merge sort recurrence, which makes only two workers for calls. but it's quite a bit better than quadratic. so summarizing, you did in fact learn a suboptimal algorithm for integer multiplication way back in grade school. you can beat the iterative algorithm using a combination of recursion plus gauss's trick to save on the number of recursive calls. let's quickly move on to our final two examples. example number five is for those of you that watched the video on strassen's matrix multiplication algorithm. so recall the salient properties of strassen's algorithm. the key idea is similar to gauss's trick for integer multiplication. first you set up the problem recursively, one observes that the naive way to solve a problem recursively would lead to eight subproblems. but if you're clever about saving some computations, you can get it down to just seven recursive calls, seven subproblems. so a, in strassen's argument, is equal to 7. as usual, each subproblem size is half that of the original one. so b = 2. and the amount of work done outside of the recursive calls is linear in the matrix size. so quadratic in the end, quadratic in the dimension because there's a quadratic number of entries in terms of the dimension. so n squared work outside the recursive call is leaving you a value of d = 2. so as far as which case of the master method we're in. well it's the same as in the last couple of examples. a = 7, b to the d = 4 which is less than a. so once again we're in case 3 and now the running time of strassen's algorithm t(n) = o(n to the log base) 2(7) which is more or less n to the 2.81. and again this is a win. once we use the savings to get down to just 7 recursive calls. this beats the naive federate algorithm, which recall would require cubic time. so that's another win for clever divide and conquer in matrix multiplication via strassen's algorithm. and once again, the master's method just by plugging in parameters, tells us exactly what the right answer to this recurrence is. so for the final example i feel a little guilty because i've shown you five examples and none of the them have triggered case 2. we've had two in case 1 in the master method and three now in case 3. so this will be a fictitious recurrence just to illustrate case 2. but there are examples of recurrences that come up where case 2 is the relevant one. so let's just look at the following recurrence. so this recurrence is just like merge sort. we recurse twice. there's two recursive calls each on half the problem size. the only difference is in this recurrence we're working a little bit harder on the combined step. instead of linear time outside of the recursive calls we're doing a quadratic amount of work. okay so a = 2. b = 2 and d = 2. so b to the d = 4, strictly bigger than a. and that's exactly the trigger for case 2. now recall what the running time is in case 2. it's simply n to the d, where d is the exponent in the combined step. in our case, d is 2, so we get a running time of n squared. and you might find this a little counter-intuitive, right? given that merge sort. all we do with merge sort is change the combine step from linear to quadratic. and merge sort has a running time of n log n. you might have expected the running time here to be n squared log n. but that would be an over estimate, so the master method gives us a tighter upper bound, shows that it's only quadratic work. so put differently, the running time of the entire algorithm is governed by the work outside of the recursive calls. just in the outer most call to the algorithm, just at the root of the recursion tree 
in this video, we'll begin the proof of the master method. the master method, you'll recall, is a generic solution to recurrences of the given form, recurrences in which there's a recursive calls, each on a sub-problem of the same size, size n over b, assuming that the original problem had size n. and, plus, there is big o of n to the d work done by the algorithm outside of these a recursive calls. the solution that the master method provides has three cases, depending on how a compares to b to the d. now. this proof will be the longest one we've seen so far by a significant margin. it'll span this video as well as the next two. so let me say a few words up front about what you might want to focus on. overall i think the proof is quite conceptual. there's a couple of spots where we're going to have to do some computations. and the computations i think are worth seeing once in your life. i don't know that they're worth really committing to long term memory. what i do think is worth remembering in the long term however, is the conceptual meaning of the three cases of the master method. in particular the proof will follow a recursionary approach just like we used in the running time analysis of the mertshot algorithm. and it worth remembering what three types of recursion trees the three cases is that the master method corresponds to. if you can remember that, there will be absolutely no need to memorize any of these three running times, including the third, rather exotic looking one. rather, you'll be able to reverse engineer those running times just from your conceptual understanding of what the three cases mean and how they correspond to recursion trees of different type. so, one final comment before we embark on the proof. so, as usual, i'm uninterested in formality in its own sake. the reason we use mathematical analysis in this course, is because it provides an explanation of, fundamentally, why things are the way they are. for example, why the master method has three cases, and what those three cases mean. so, i'll be giving you an essentially complete proof of the master method, in the sense that it has all of the key ingredients. i will cut corners on occasion, where i don't think it hinders understanding, where it's easy to fill in the details. so, it won't be 100 percent rigorous, i won't dot every i and cross every t, but. there will be a complete proof, on the conceptual level. that being said, let me begin with a couple of minor assumptions i"m going to make, to make our lives a little easier. so first, we're gonna assume that the recurrence has the following form. so, here, essentially, all i've done is i've taken our previous assumption about the format of a recurrence, and i've written out all of the constants. so, i'm assuming that the base case kicks in when the input size is one, and i'm assuming that the number of operations in the base case is at most c, and that, that constant c is the same one that was hidden in the big o notation of the general case of the recurrence. the constant c here isn't gonna matter in the analysis, it's just all gonna be a wash, but to make, keep everything clear, i'm gonna write out all the constants that were previously hidden in the big o notation. another assumption i'm going to make. now goes to our murtured analysis, is that n is a power of b. the general case would be basically the same, just a little more tedious. at the highest level, the proof of the master method should strike you as very natural. really, all we're going to do is revisit the way that we analyze merge short. recall our recursion tree method worked great, and gave us this [inaudible] log [inaudible], and the running time of merge short. so we're just gonna mimic that recursion tree, and see how far we get. so let me remind you what a recursion tree is. at the roots, at level zero, we have the outermost, the initial indication of the recursive algorithm. at level one, we have the first batch of recursive calls. at level two, we have the recursive calls made by that first batch of recursive calls, and so on. all the way down to the leaves of the tree, which correspond to the base cases, where there's no further recursion. now, you might recall, from the merge sort analysis that we identified a pattern that was crucial in analyzing the running time. and that pattern that we had to understand was, at a given [inaudible] j, at a given level j of this recursion tree. first of all, how many distinct subproblems are there at level j? how many different level j [inaudible] are there? and secondly, what is the input size that each of those level j subproblems has to operate on? so think about that a little bit and give your answer in the following quiz. so the correct answer is the second one. at level j at. of this recursion tree, there are a to, to the j sub-problems and each has an input of size of n over b to the j. so first of all, why are there a to the j sub-problems? well, when j equals zero at the root, there's just the one problem, the original indication of the recursive algorithm. and then each. call to the algorithm makes a further calls. for that reason the number of sub problems goes up by a factor of a with each level leading to a to the j sub problems at level j. similarly, b is exactly the factor by which the input size shrinks once you makea recursive call. so j levels into the recursion. the input size has been shrunk j times by a fctor of b each time. so the input size at level j is n over b to the j. that's also the reason why, if you look at the question statement, we've identified the numbers of levels as being log of base b. of n. back in merge short, b was two. we [inaudible] on half the array. so the leaves all resided at level log base two of n. in general, if we're dividing by a factor b each time, then it takes a log based b of n times before we get down the base cases of size of one. so the number of levels overall, zero through log base b event. for a total of log based b event plus one levels. here then is what the recursion tree looks like. at level zero we have the root corresponding to the outer most call. and the input size here is n. the original problem. children of a node correspond to the recursive calls. because there are a. recursive calls by assumption, there are a. children or a. branches. level one is the first batch of precursor calls. each of which operates on an input of size n over b. that level log base b. of n. we've cut the input size by a factor of b. this many times, so we're down to one. so that triggers the base case. so now, the plan is to simply mimic our previous analysis of merge sort. so let's recall how that worked. what we did is we zoomed in, in a given level. and for a given level j, we counted the total amount of work that was done at level j subproblems, not counting work that was gonna be done later by recursive calls. then, given a bound on the amount of work at a given level j, we just summed up overall, the levels, to capture all of the work done by all of the, recursive indications of the algorithm. so inspired by our previous success let's zoom in on a given level j., and see how much work gets done, with level j. sub problems. we're going to compute this in exactly the way we did in merge sort. and we were just going to look at the number of problems that are at level j and we're going to multiply that by a bound on the work done per sub-problem. we just identified the number of level j sub-problems as a to the j. to understand the amount of work done for each level j sub-problem, let's do it in two parts. so, first of all, let's focus on the size of the input for each level j sub-problem. that's what we just identified in the previous quiz question. since the input size is being decreased by a factor b each time, the size of each level j sub-problem is n over b to the j. [inaudible] now we only care about the size of a level j sub problem in as much it determines the amount of work the number of operations that we perform per level j sub problem. and to understand the relationship between those two quantities we just return to the re currents. the recurrent says how much work gets done in the specific sub problem well there's a bunch of work done by recursive calls the a recursive calls and we're not counting that we're just counting the work done here at a level j and the recurrence also tells us how much is done outside of the recurrent calls. namely it's no more than the constant c times the input size. raised to the d power. so here the input size is n over b to the j, so that gets multiplied by the constant c. and it gets raised to the d power. okay. so c. times quanity n. over b. to the j. that's the emphasized. raised to the d. power. next, i wanna simplify this expression a little bit. and i wanna separate out the terms which depend on the level number j, and the terms which are independent of the level number j. so if you look at it a and b are both functions of j, where the c and end of the d terms are independent of j. so let's just separate those out. and you will notice that we have now our grand entrance of the ratio between a and b to the d. and foreshadowing a little, recall that the three cases of the master method are governed by the relationship between a and b to the d. and this is the first time in the analysis where we get a clue that the relative magnitude of those two quantities might be important. so now that we've zoomed in on a particular label j and done the necessary computation to figure out how much work is done just at that level, let's sum over all of the levels so that we capture all of the work done by the algorithms. so this is just gonna be the sum of the epression we saw on the previous slide. now since c into the d doesn't depend on j, i can yank that out in front of the sum, and i'll sum the expression over all j. that results in the following. so believe it or not, we have now reached an important milestone in the proof of the master method. specifically, the somewhat messy looking formula here, which i'll put a green box around, is going to be crucial. and the rest of the proof will be devoted to interpreting and understanding this expression, and understanding how it leads to the three different running time bounds in the three different cases. now i realize that at the moment this expression's star probably just looks like alphabet soup, probably just looks like a bunch of mathematical gibberish. but actually interpreted correctly this has a very natural interpretation. so we'll discuss that in the next video. 
this video is the second of three that describes the proof of the master method. in the first of these three videos we mimicked the analysis of merge sort. we used a recursion tree approach which gave us an upper bound of running time of an algorithm. which is governed by a recurrence of the specified form. unfortunately, that video left us with a bit of an alphabet soup, this complicated expression. and so in this second video, we're not gonna do any computations. we're just going to look at that expression, attach some semantics to it, and look at how that interpretation naturally leads to three cases, and also give intuition for some of the running times that we see in a master method. so recall from the previous video that the way we've bounded the work done by the algorithm is resumed in on a particular level j of the recursion tree. we did a computation, which was the number of sub problems at that level, a to the j, times the work done per sub-problem, that was the constant c times quantity n over b to the j raised to the d and that gave us this expression. cn to the d times the ratio of a over b to the d raised to the j. at a given level. j. the expression star that we concluded the previous video with was just the sum of these expressions over all of the logarithmic levels, j. now, as messy as this expression might seem, perhaps we're on the right track in the following sense. the master method has three different cases, in which case you're in is governed by how a compares to b to the d. and hearing this expression, we are seeing precisely that ratio. a divided by b to the d. so let's drill down and understand why this ratio is fundamental to the performance of the divide and conquer [inaudible] algorithm. so really, what's going on in the master method, is a tug of war between two opposing forces. one which is forces of good, and one which is forces of evil, and those correspond to the quantities b to the d and a, respectively. so let me be more precise. let's start with the parameter a. so a, you'll recall, is defined as the number of recursive calls made by the algorithm. so it's the number of children that a [inaudible] recursion tree has. so fundamentally, what a is, it's the rates at which sub problems proliferate as you pass deeper in the recursion tree. it's the factor by which there are more sub problems at the next level than the previous one. so let's think of a. in this way. as the rate of subpropabifliation, or r.s.p. and when i say rate i mean as a function of the recursion level j. so these are the forces of evil. this is why our algorithm might slowly, is because as we go down the tree there are more and more sub problems, and that's a little scary. the forces of good, what we have going for us, is that with each recursion level j we do less work per sub problem and the extent to which we do less work is precisely b to the d. so i'll abbreviate this rate of work shrinkage or this quantity b. to the d. by r. w. s. now perhaps you're wondering why is it b of the d. why is it not b? so remember what b denotes. that's the factor by which the input size shrinks with the recursion level j. so for example if b equals two, then each sub-problem at the next level is only half as big. as that at the previous level. but we don't really care about the input size of a subproblem, except inasmuch as it determines the amount of work that we do solving that subproblem. so that's where this parameter d comes into play. think, for example, about the cases where you have a linear amount of work outside the recursive calls, versus a quadratic amount of work that is considered the cases where d equals one or two. if b = two and d = one that is if you reverse on half the input. and do linear work, then. not only is the input size dropping by factor two but so is the amount of work that you do per sub problem and that's exactly the situation we had in merge short where we had linear work outside the recursive calls. the thing about d = two, suppose you did quadratic work per sub problem as a function of the input size. then again if b = two if you cut the input in half, the recursive call's only gonna do 25 percent as much work as what you did. at the current level. the input size goes down by a factor two and that gets squared because you do quadratic work as a function of the input size. so that would be b to the d, two raised to the two or four. so in general the input size goes down by a factor b, but what we really care about, how much less work we do per subproblem, goes down by b to the d. that's why b to the d is the fundamental quantity that quan, that's governs the forces of good, the extent to which we work less hard with each occursion level j. so the question that is just what happens in this tug of war between these two opposing forces? so fundamentally, what the three cases of the master method correspond to, is the three possible outcomes in this tug of war between the forces of good, namely the rate of word shrinkage and the forces of evil, namely the sub-problem proliferation. there are three cases one for the case of a tie one for the case in which the forces of evil win that is in which a is bigger than b to the d and a case in which the forces of good wins, that is b to the d is bigger than a. to understand this a little bit better what i want you to think about is the following. think about the recursion tree that we drew in the previous slide and as a function of a verses b to the d think about the amount of work you've done per level. when is that going up per level? when is it going down per level? and when is it exactly the same at each level? so the answer is all of these statements are true except for the third one. so let's take them one at a time. so first of all let's consider the first one. suppose that the rate of sub problem proliferation a is strictly less than the rate of work shrinkage, b to the d. this is where the forces of good, the rate at which we're doing less work per sub problem is out, out pacing the rate of at which sub problems are proliferating. and so the number of sub-problems goes up, but the savings per sub-problem goes up by even more. so, in this case it means that we're gonna be doing less work. with each recursion tree level, the forces of good outweigh the forces of evil. the second one is true for exactly the same reason. if sub-problems are proliferating so rapidly that it outpaces the savings that we get per sub-problem, then we're gonna see an increasing amount of work. as we go down the recursion tree, it will increase with the level of j. given that these two are true the third one is false. we can draw conclusions depending on whether the rate of sub-problem proliferation is strictly bigger or strictly less than the rate of work shrinkage. and finally, the fourth statement is also true. this is the perfect equilibrium between the forces of good and the forces of evil. sub-problems are proliferating, but our savings per sub-problem is increasing at exactly the same rate. the two forces will then cancel out and we'll get exactly the same amount of work done at each level of the recursion tree. this is precisely what happened when we analyzed a merd short algorithm. so let's summarize and conclude with the interpretation. and even understand how this interpretation lends us to forecast some of the running time bounds that we see in the master method. summarizing, the three cases of the master method correspond to the three possible outcomes in the battle between sub-problems proliferating and the work per sub-problem shrinking. one for a tie, one for when sub-problems are proliferating faster, and one for when the work shrinkage is happening faster. in the case where the rates are exactly the same, and they cancel out, then the amount of work should be the same at every level of the recursion tree. and, in this case, we can easily predict what the running time should work out to be. in particular, we know there's a logarithmic number of levels, the amount of work is the same at every level, and we certainly know how much work is getting done at the root, right, because that's just the original recurrence, which tells us that there's, acentotically, n to the d work done at the root. so, with n to the d work for each of the log levels, we expect a running time of n to the d times log n. as we just discussed, when the rate of. work done per subproblem is shrinking even faster than subproblems proliferate. then we do less and less work with each level of the recursion tree. so in particular, the biggest amount of work, the worst level is at the root level. now, the simplest possible thing that might be true would be that actually, the root level just dominates the overall running time of the algorithm, and the other levels really don't matter up to a constant factor. so it's not obvious that's true, but if we keep our fingers crossed and hope for the simplest possible outcome. with the root has the most work, we might expect a running time that's just proportional to the running time of the root. as we just discussed, we already know that that's n to the d, cuz that's just the outermost call to the algorithm. by the same reasoning, when this inequality is flipped, and [inaudible] proliferates so rapidly that it's outpacing the same as we get for sub problem, the amount of work is increasing the recursion level. and here, the worst case is gonna be at the leaves. that's where the, that level's gonna have the most work compared to any other level. and again, if you keep your fingers crossed and hope that the simplest possible outcome is actually true, perhaps the leaves just dominate, and. up to a constant factor, they govern the running time of the algorithm. in this third case, given that we do a constant amount of work for each of the leaves, since those correspond to base cases, here we'd expect a running time in the simplest scenario, proportional to the number of leaves in the recursion tree. so lets summarize what we've learned in this video. we now understand that fundamentally there are three different kinds of recursion trees. those in which the work done per level is the same in every level. those in which the work is decreasing with the level in which case the root is the lowest level. and those in which the amount of work is increasing with the level where the leads are the lowest level. further more it's exactly the ratio between a the rate of sub problem proliferation and b to the d the rate of work shrinkage sub problem that governs which of these three recursion trees we're dealing with. further more. intuitively, we've now had predictions about what kind of running time we expect to see in each of the three cases. they're n to the d log in, that we're pretty confident about. there's a hope that, in the second case, where the root is the worst level, that maybe the running time is n to the d. and there's a hope in the third case where the [inaudible] are the worse level, and we do constant time per leaf, per base case, that it's gonna be proportional to the number of leaves. let's now stand and check this intuition against the formal statement of the master method, which we'll prove more formally in the next video. so in the three cases, we see they match up. at least two out of three with exactly [inaudible] lies. so in the first case, we see the expected end of the d times log in. in the second case, where the root is the worst level indeed, the simplest possible outcome of big o of n to the d is the assertion. now, the third case that remains a mystery to be explained. our intuition said this should hopefully be proportional to the number of leaves. and instead, we've got this funny formula of big o of n in the log base b of a. so in the next video, we'll demystify that connection, as well as supply formal proof for these assertions. 
let's complete the proof of the master method. let me remind you about the story so far, the first thing we did is we analyzed the work done by a recursive algorithm using a recursion tree. so we zoomed in on a given level j, we identified the total amount of work done at level j and then we summed up over all of the levels resulting in this rather intimidating expression star. c into the d times a sum over the levels j from zero to log base b of n of quantity a over b to the b raised to the j. having derived this expression star we then spent some time interpreting it, attaching to it some semantics sticks. and we realize that the roll of this ratio a to the b over d, is to distinguish between three fundamentally different types of recursion trees. those in which a = b to the d and the amount of work is the same at every level. those in which a is less than b to the d and therefore the amount of work is going down with the level. and those where a is bigger than b to the d in which case the amount of work is growing with the level. this gave us intuition about the three cases of the master method and even gave us predictions f or the running times we might see. so what remains to do is turn this hopeful intuition into. a rigorous proof. so we need to verify that in fact the simplest possible scenarios outlined in the previous video. actually occur. in addition, we need to demystify the third case and understand what the expression has to do with the number of leaves of the recursion tree. let's begin with the simplest case, which is case one. we're calling case one, we're assuming that a equals b to the d. this is the case where we have a perfect equilibrium between the forces of good and evil. where the rate of the sub problem proliferation exactly cancels out with a rate at which we do less work per sub problem. and now, examining the expression, star, we can see how easy our lives get when a equals b to the d. in that case, this ratio is equal to one. so naturally this ratio raised to the j is also equal to one for all j. and then of course this sum evaluates to something very simple. namely one summed with itself log base b of n plus one times. so the sum simply equals log base b of n. plus one, and that's going to get multiplied by. this cn to the d term which is independent of the sum. so summarizing, when a equals b to the d, we find that star equals cn to the d times log base b of n plus one. writing this in big o notation, we would write big o of end of a d login. and again, i'm going to suppress the base of the logarithms. since all logarithms differ only by a constant factor we don't have to specify the base. that's just suppressed by the constant hidden in the big o notation. so that's it for case one. like i said, this is the easy case. so what do we do when a is not equal to b to the d? and remember a could be either less than or bigger than b to the d. to answer that question, let's take a short detour into geometric series. for this single slide detour we're going to think about a single constant number r. now, what you want to think about is r. representing that ratio a. over b. to the d. from the previous slot. but for this slide only let's just call it r. this is a constant. it's bigger than zero, and it's not equal to one. now, suppose we sum up powers of r stopping, let's say, at the kth power of r. i claim that this sum has a nice closed form formula. specifically it is exactly, r. to the k. plus one, minus one. divided by or a minus one. now, whenever you see a general formula like this, it's useful to keep in mind a couple of canonical values of the parameters that you can plug in to develop intuition. and for this expression, you might wanna think canonically about the cases, r=2, and r=1/2. so when r=2, or something that powers a two. one+2+4+8+16, and so on. one hour's a half, [inaudible] have one, plus a half, plus a quarter, plus an eighth, and so on. now i'm not gonna prove this for you, i'd like you to prove this yourself. if you don't already know this fact. so the way to prove this is simply by induction. and i will leave this an, an exercise. what i wanna focus on instead is what this fact can do for us. the way that we use this fact is to formalize the idea that, that in recursion trees where the amount of work is increasing in the levels, the leaves dominate the overall running time. and where recursion trees, where the amount of work is decreasing in the level, the root dominates the running time. in the sense that we can ignore all of the other levels of the recursion tree. so, and in the vision in this slide, we have two upshots. first of all, for the case when r is less than one. and in this case, this expression on the right-hand side. r to the q plus one minus one over r minus one can be upper bounded by one over one minus r. so again, remember, you might want to have a canonical value of r in mind here, namely, one half. so what we're claiming here is that the right hand side is nor more than two for the case of r=1/2. and that's easy to see if you think about one plus one-half plus a one-fourth plus one-eighth and so on. that sum is converging to, to as k grows large. so in general, for our less than one constant, the sum is divided by one minus one over r. now, we're not actually gonna care about this formula, one minus one over r. the point for us is just that this is a constant. and by constant, i mean independent of k, independent of how many terms we sum up. obviously, it depends on r of the ratio, but it does not depend on how many things we sum up on k. so the way to think about this is, when we sum up a bunch of terms where r is less than one, then the very first term dominates. the first term is with a one. and no matter how many terms we sum up, we never get, grow bigger than the sum constant. a similar situation holds for the case where r is a constant bigger than one. when r is bigger than one. a tiny bit of algebra shows that we can upper bound the right hand side by r to the k. times something which is constant, independent of k. so again, let's interpret the second upshot in terms of a canonical value of r. namely, r equals two. then our sum is one plus two plus four plus eight plus sixteen, and so on. and what this is saying is that no matter how many terms we sum up, the overall sum is never gonna be more than twice. the largest and final term. so if we sum up to say 128, the sum, you'll notice, will be 255, which is, at most, twice that largest term, 128. and that saying is true for any k. the entire sum is no more than twice that of the largest term. in this sense, the largest term in the series dominates the whole thing. so to summarize this slide in just into one sentence we sum up powers of a constant r when r is bigger than one the largest power of that constant dominate to the sun when r is smaller than one then the sun is just a constant. let's now apply this to prove case two of the master method. in case two of the master method, we assume that a is less than b to the d. that is, the rate at which sub problems are proliferating is drowned out by the rate at which we do less work per sub problem. so this is the case where the amount of work is decreasing with each level of the recursion tree. and our intuition said that, well, in the simplest possible scenario, we might hope that all of the work, up to a constant factor, is being done at the root. so let's make that intuition precise by using the basic sums fact on the previous slide. so, since a is less than b to the d. this ration is less than one. so let's call this ratio equal to r. so r, you'll notice, does depend on the three parameters, a, b and d. but r is a constant, it does not depend on n. so what is this sum? the sum is just, we're just summing up powers of this constant r, where r is less than one. what did we just learn? we just learned that any such sum is bounded above by a constant, independent of the number of terms that you sum up. so therefore, what is this expression star evaluates to. it evaluates to c, which is a constant, times n to the d. times another constant. so suppressing the product of these two constants in big o notation we can say that the expression starts upper bounded by big o(n to the d). and this makes precise our intuition that indeed the overall running time of the algorithm, in this type of recursion tree with decreasing work per level, is dominated by the root. the overall amount of work is only a constant factor larger than the work done and merely at level zero of the tree. let's move on to the final and most challenging part of the proof, the final case. in case three we assume that a is bigger than b to the d. so in conceptual terms, we're assuming the rate at which sub problems proliferate is exceeding the rate at which we do less work per sub problem. so these are recursion trees where the amount of work is increasing with each level, with the most work being done at the leaves. and once again, using the basic sums fact, we can make precise the hope that, in fact, we only have to worry about the leaves. we can throw away the rest of work, losing only a constant factor. so to see that, you will again denote this ratio between a and b to the d as r. and in this case r is bigger that one. so this sum is a sum of a bunch of powers of r were r is bigger than one, what did we just learn about that two slides ago in the basic sums facts, we learned that such sums are dominated by the largest and last term of the sum. okay so the bounded it by a constant factor times the largest term. therefore, we can we can simplify the expression star to the following. i'm gonna write it in terms of big o notation. and, like, on the last slide, i'll use it to suppress two different constants. on the one hand, i'm gonna be suppressing the constant c, which we inherited way back when from the original recurrence. and on the other hand, i'm gonna use it to also suppress this constant that comes from the basic sums fact. so ignoring those two constants, what do we have left? we have n to the d. times the largest term of the sum. so what is the largest term of the sum? well, it's the last one so we plug in the biggest value of j that we're ever going to see. so what's the biggest value of j that we're ever going to see? we'll it's just this. log base b of n. so, we get the ratio a over b to the d, raised to the log base b of n. power. now don't despair how messy this looks. we can do some remarkable simplifications. so what i want to do next is i want to focus just on this one over b to the d, raised to the log base b of n term. so that's going to be. you can write that as b to the minus d log base b of n. which if i factor this exponent into two successive parts i can write this as b raise to the log base b of n power. and only then raised to the minus d. and now of course what happens is that taking the logarithm of n base b, followed by taking, raising it to the b power, those are inverse operations that cancel, so that leaves us just with the n. so this results in a end to the minus d. and now remarkably this end to the minus d is just gonna cancel out with this end to the d. leaving us with merely. a, raise the log based b event. and thus, out of this crazy sea of letters, rises a formula we can actually understand. so a to the log based b of n, if we step back and pick for a minute, is actually a supernatural quantity. describe something about the recursion trees that we already knew was supposed to pop up in the analysis. i'll let, i'll let you think through exactly what that is in the following quiz. so the correct answer to this quiz is the fourth response. a raised to the logarithm event is precisely the number of leaves of the recursion tree. and remember in our intuition for case three, recursion trees where the amount of work is increasing per level, we thought that perhaps the work would be dominated by the work done at the leaves which is as proportional as the number of leaves. so why is this the answer? well just remember what recursion trees look like at level zero. we have a single node, and then with each level we have eight times as many nodes as before. that is, with each level of the recursion tree, the number of nodes goes up by a factor of a. how far does this, how long does this process go on? well, it goes on until we reach down the, the leaves. recall that in the input size starts at n up at the root. it gets divided by a factor of b each time, and it terminates once we get down to one. so the leaves preside at precisely level log base b of n. so therefore. the number of leaves is just a branching factor which is a raised to the number of times that we actually multiply by a which is just the number of levels which is log base b n. so each time we go down a level we increase the number of nodes by a factor of a and we go down a level log base b of n times. leaving us with a number of leaves equal to a raised to the log base b of n. so what we've done is we've mathematically confirmed, in a very cool way, our intuition about what case three should look like in the master method. we've proven that in case three when a is. bigger than b to the d. the running time is, o of the number of leaves in the recursion tree, just as the intuition predicted. but, this leaves us with one final mystery. if you go back to the statement of the master method, we didn't say, a to the log base b of n. in case three, it says the running time is, n to the log base b of a. and, not only that, we've used this case three formula over and over again, to evaluate gauss's recursive algorithm for integer multiplication, to evaluate the strassen's matrix multiplication algorithm, and so on. so, what's the story? how come we're not getting the same thing, as in the statement of the master method? well there's a very simple explanation, which is simply that, believe it or not. a log base b of n, and n to the log base b of a. are exactly the same thing. this looks like the kind of mistake you'd make in freshmen algebra. but actually, if you think about it, these are simply the same quantity. if you don't believe me, just take the logarithm base b of both sides, and it'll give the same thing in both sides. now, you might well be wondering why i didn't just state in the master method that the running time of case three is this very sensible and meaningful expression, a raised log based b of n, i.e., the number of leaves in the recursion tree. well, it turns out that while this expression on the left hand side is the more meaningful conceptually. the right hand side. n. to the log base b. of a. is the easiest one to apply. so recall when we worked through a bunch of examples, of the master method, this right hand side was super convenient, when we evaluated the running times of out rhythms. when we plugged in the numbers of a. and b. in any case, whether or not you want to think about the running time in case three as proportional to the number of leaves in the tree or as proportional at the end of the log base b of a, we're done. we've proved it. that's case three. that was the last one. so we're done with the master method. qed. so that was a lot of hard work for doing the master method and i would never expect someone to be able to regurgitate all of the details of this proof you know it's something like a cocktail party well maybe except the nerdiest of all cocktail parties but i do think there's a few high level conceptual points of this proof that are worth remembering in the long term, so we started by just writing down a recursion tree for the recursive algorithm and in a generic way. and going level by level, we counted up the work done by the algorithm. and this part of the proof had nothing to with how a and b  related to each other. then we recognized that there are three fundamentally different types of recursion trees. those with the same amount of work per level, those where it increases with the level, and those where it decreases with the level. if you can remember that, you can even remember what the running times of the three cases should be. in the case where you do the same amount of every work at each level. we know there's a logarithmic number of levels. we know we do end in d work at the root. so that gives us the running time in case one had ended the day you log in. when the amount of work is decreasing with the levels, we now know that the route dominates. up to a constant, we can throw out the rest of the levels, and we know end of the d work gets done at the root, so that's the overall running time. and in the third case, where it's increasing in the levels, the leaves dominate. the number of leaves is a raised to the log based of b of n, and that's the same as n, the log based b of a. and that's proportional to running time in case three of the master method. 
so now we come to one of my favorite sequence of lectures, where we going to discuss the famous quicksort algorithm. if you ask professional computer scientists and professional programmers to draw up a list of their top five, top ten favorite algorithms, i'll bet you'd see quicksort on many of those, those peoples' lists. so, why is that? after all, we've already discussed sorting. we already have a quite good and practical sorting algorithm, mainly the merge sort algorithm. well, quicksort, in addition to being very practical, it's competitive with, and often superior to, merge sort. so, in addition to being very practical, and used all the time in the real world, and in programming libraries, it's just a extremely elegant algorithm. when you see the code, it's just so succinct. it's so elegant, you just sorta wish you had come up with it yourself. moreover, the mathematical analysis which explains why quicksort runs so fast, and that mathematical analysis, we'll cover in detail, is very slick. so it's something i can cover in just about half an hour or so. so more precisely what we'll prove about the quicksort algorithm is that a suitable randomized implementation runs in time n log n on average. and i'll tell you exactly what i mean by on average, later on in this sequence of lectures. and, moreover, the constants hidden in the big-oh notation are extremely small. and, that'll be evident from the analysis that we do. finally, and this is one thing that differentiates quicksort from the merge sort algorithm, is it operates in place. that is, it needs very little additional storage, beyond what's given in the input array, in order to accomplish the goal of sorting. essentially, what quicksort does is just repeated swaps within the space of the input array, until it finally concludes with a sorted version of the given array. the final thing i want to mention on this first slide is that, unlike most of the videos, this set of the videos will actually have an accompanying set of lecture notes, which i've posted on, in pdf, from the course website. those are largely, redundant. they're optional, but if you want another treatment of what i'm gonna discuss, a written treatment, i encourage you to look at the lecture notes, on the course website. so, for the rest of this video, i'm gonna give you an overview of the ingredients of quicksort, and what we have to discuss in more detail, and the rest of the lectures will give details of the implementation, as well as the mathematical analysis. so let's begin by recalling the sorting problem. this is exactly the same problem we discussed back when we covered merge sort. so we're given as input an array of n numbers in arbitrary order. so, for example, perhaps the input looks like this array here. and then what do we gotta do? we just gotta output a version of these same numbers but in increasing order. like when we discussed merge sort, i'm gonna make a simplifying assumption just to keep the lectures as simple as possible. namely i'm going to assume the input array has no duplicates. that is, all of the entries are distinct. and like with the merge sort, i encourage you to think about how you would alter the implementation of quicksort so that it deals correctly with ties, with duplicate entries. to discuss how quicksort works at a high-level, i need to introduce you to the key subroutine, and this is really the, key great idea in quicksort, which is to use a subroutine which partitions the array around a pivot element. so what does this mean? well, the first thing you gotta do is, you gotta pick one element in your array to act as a pivot element. now eventually we'll worry quite a bit about exactly how we choose this magical pivot element. but for now you can just think of it that we pluck out the very first element in the array to act as the pivot. so, for example, in the input array that i mentioned on the previous slide, we could just use "3" as the pivot element. after you've chosen a pivot element, you then re-arrange the array, and re-arrange it so that every, all the elements which come to the left of the pivot element are less than the pivot, and all the elements which come after the pivot element are greater than the pivot. so for example, given this input array, one legitimate way to rearrange it, so that this holds, is the following. perhaps in the first two entries, we have the 2 and the 1. then comes the pivot element. and then comes the elements 4 through 8 in some perhaps jingled order. so notice that the elements to the left of the pivot, the 2 and the 1, are indeed less than the pivot, which is 3. and the five elements to the right of the pivot, to the right of the 3, are indeed all greater than 3. notice in the partition subroutine, we do not insist that we get the relative order correct amongst those elements less than the pivot, or amongst those elements bigger than the pivot. so, in some sense, we're doing some kind of partial sorting. we're just bucketing the elements of the array into one bucket, those less than the pivot, and then a second bucket, those bigger than the pivot. and we don't care about, getting right the order amongst each, within each of those two buckets. so, partitioning is certainly a more modest goal than sorting, but it does make progress toward sorting. in particular, the pivot element itself winds up in its rightful position. that is, the pivot element winds up where it should be in the final sorted version of the array. you'll notice in the example, we chose as the pivot the third largest element, and it does, indeed, wind up in the third position of the array. so, more generally, where should the pivot be in the final sorted version? well, it should be to the right of everything less than it. it should be to the left of everything bigger than it. and that's exactly what partitioning does, by definition. so, why is it such a good idea to have a partitioning subroutine? after all, we don't really care about partitioning. what we want to do is sort. well, the point is that partitioning can be done quickly. it can be done in linear time. and it's a way of making progress toward having a sorted version of an array. and it's gonna enable a divide-and-conquer approach toward sorting the input array. so, in a little bit more detail, let me tell you about two cool facts about the partition subroutine. i'm not gonna give you the code for partitioning here. i'm gonna give it to you on the next video. but, here are the two salient properties of the partition subroutine, discussed in detail in the next video. so the first cool fact is that it can be implemented in linear, that, is o(n) time, where n is the size of the input array, and moreover, not just linear time but linear time with essentially no extra overhead. so we're gonna get a linear time of mutation, where all you do is repeated swaps. you do not allocate any additional memory. and that's key to the practical performance of the quicksort algorithm. [sound] secondly, it cuts down the problem size, so it enables the divide-and-conquer approach. namely, after we've partitioned an array around some pivot elements, all we have to do is recursively sort the elements that lie on the left of the pivot. and recursively sort the elements that lie on the right of the pivot. and then, we'll be done. so, that leads us to the high-level description of the quicksort algorithm. before i give the high-level description, i should mention that this, algorithm was discovered by, tony hoare, roughly, 1961 or so. this was at the very beginning of hoare's career. he was just about 26, 27 years old. he went on to do a lot of other contributions, and, eventually wound up winning the highest honor in computer science, the acm turing award, in 1980. and when you see this code, i'll bet you feel like you wish you had come up with this yourself. it's hard not to be envious of the inventor of this very elegant quicksort algorithm. so, just like in merge sort, this is gonna be a divide-and-conquer algorithm. so it takes an array of some length n, and if it's an array of length n, it's already sorted, and that's the base case and we can return. otherwise we're gonna have two recursive calls. the big difference from merge sort is that, whereas in merge sort, we first split the array into pieces, recourse, and then combine the results, here, the recursive calls come last. so, the first thing we're going to do is choose a pivot element, then partition the array around that pivot element, and then do two recursive calls. and then, we'll be done. there will be no combined step, no merge step. so in the general case, the first thing you do is choose a pivot element. for the moment i'm going to be loose, leave the choosepivot subroutine unimplemented. there's going to be an interesting discussion about exactly how you should do this. for now, you just do it in some way, that for somehow you come up with one pivot element. for example, a naive way would be to just choose the first element. then you invoke the partition subroutine that we'll discuss in the last couple slides. [sound]. so recall that the results in a version of the array in which the pivot element p is in its rightful position, everything to the left of p is less than p, everything to the right of the pivot is bigger than the pivot, and then all you have to do to finish up is recurse on both sides. so let's call the elements less than p the first part of the partitioned array, and the elements greater than p the second part of the recursive array.  and now we just call quicksort again to recursively sort the first part, and then the, recursively sort the second part. and that is it. that is the entire quicksort algorithm at the high-level. this is one of the relatively rare recursive, divide- and-conquer algorithms that you're going to see, where you literally do no work after solving the sub-problems. there is no combine step, no merge step. once you've partitioned, you just sort the two sides and you're done. so that's the high- level description of the quicksort algorithm. let me give you a quick tour of what the rest of the video's going to be about. so first of all i owe you details on this partition subroutine. i promise you it can be implemented in linear time with no additional memory. so i'll show you an implementation of that on the next video. we'll have a short video that formally proves correctness of the quicksort algorithm. i think most of you will kinda see intuitively why it's correct. so, that's a video you can skip if you'd want. but if you do want to see what a formal proof of correctness for a divide-and-conquer algorithm looks like, you might want to check out that video. then, we'll be discussing exactly how the pivot is chosen. it turns out the running time of quicksort depends on what pivot you choose. so, we're gonna have to think carefully about that. then, we'll introduce randomized quicksort, which is where you choose a pivot element uniformly at random from the given array, hoping that a random pivot is going to be pretty good, sufficiently often. and then we'll give the mathematical analysis in three parts. we'll prove that the quicksort algorithm runs in n log n time, with small constants, on average, for a randomly chosen pivot. in the first analysis video, i'll introduce a general decomposition principle of how you take a complicated random variable, break it into indicator random variables, and use linearity of expectation to get a relatively simple analysis. that's something we'll use a couple more times in the course. for example, when we study hashing. then, we'll discuss sort of the key insight behind the quicksort analysis, which is about understanding the probability that a given pair of elements gets compared at some point in the algorithm. that'll be the second part. and then there's going to be some mathematical computations just to sort of tie everything together and that will give us the bound the quicksort running time. another video that's available is a review of some basic probability concepts for those of you that are rusty, and they will be using in the analysis of quicksort. okay? so that's it for the overview, let's move on to the details. 
the goal of this video is to provide more details about the implementation of the quicksort algorithm and, in particular, if you're ever going to drill down on the key partition subroutine, just let me remind you what the job of the partition subroutine is in the context of sorting an array. so recall that key idea in quicksort is to partition the input array around a pivot element. so this has two steps. first, you somehow choose a pivot element, and in this video, we're not going to worry about how you choose the pivot element. for concreteness, you might just want to think about you pick the first element in the array to serve as your pivot. so in this example array, the first element happens to be 3, so we can choose 3 as the pivot element. now, there's a key rearrangement step. so you rearrange the array so that it has the following properties. any entries that are to the left of the pivot element should be less than the pivot element. whereas any entries, which are to the right of the pivot element, should be greater than the pivot element. so, for example, in this, version of, the second version of the array, we see to the left of the 3 is the 2 and the 1. they're in reverse order, but that's okay. both the 2 and the 1 are to the left of the 3, and they're both less than 3. and the five elements to the right of the 3, they're jumbled up, but they're all bigger than the pivot element. so, this is a legitimate rearrangement that satisfies the partitioning property. and, again, recall that this definitely makes partial progress toward having a sorted array. the pivot element winds up in its rightful position. it winds up where it's supposed to be in the final sorted array, to the right of everything less than it, to the left of everything bigger than it. moreover, we've correctly bucketed the other n-1 elements to the left and to the right of the pivot according to where they should wind up in the final sorted array. so that's the job, that the partition subroutine is responsible for. now what's cool is we'll be able to implement this partition subroutine in linear time. even better, we'll be able to implement it so that all it does, really, is swaps in the array. that is, it works in-place. it needs no additional, essentially constant additional memory, to rearrange the array according to those properties. and then, as we saw on the high-level description of the quicksort algorithm, what partitioning does is, it enables a divide-and-conquer approach. it reduces the problem size. after you've partitioned the array around the pivot, all you gotta do is recurse on the left side, recurse on the right side, and you're done. so, what i owe you is this implementation. how do you actually satisfy the partitioning property, stuff to the left of the pivot is smaller than it, stuff to the right of the pivot is bigger than it, in linear time, and in- place. well, first, let's observe that, if we didn't care about the in-place requirement, if we were happy to just allocate a second array and copy stuff over, it would actually be pretty easy to implement a partition subroutine in linear time. that is, using o(n) extra memory, it's easy to partition around a pivot element in o(n) time. and as usual, you know, probably i should be more precise and write theta of n, are used in cases that would be the more accurate stronger statement, but i'm going to be sloppy and i'm just going to write the weaker but still correct statement, using big-oh, okay? so o(n) time using linear extra memory. so how would you do this? well let me just sort of illustrate by example. i think you'll get the idea. so let's go back to our running example of an input array. well, if we're allowed to use linear extra space, we can just preallocate another array of length n. then we can just do a simple scan through the input array, bucketing elements according to whether they are bigger than or less than the pivot. and, so for example, we can fill in the additional array both from the left and the right, using elements that are less than or bigger than the pivot respectively. so for example we start with the 8, we know that the 8 is bigger than the pivot, so you put that at the end of the output array. then we get to the 2. the 2 is less than the pivot, so that should go on the left hand side of the output array. when you get to the 5, it should go on the right-hand side, and the 1 should go on the left-hand side, and so on. when we complete our scan through the input array, there'll be one hole left, and that's exactly where the pivot belongs, to the right of everything less than it, to the left of everything bigger than it. so, what's really interesting, then, is to have an implementation of partition, which is not merely linear time, but also uses essentially no additional space. it doesn't re-sort to this cop-out of pre-allocating an extra array of length n. so, let's turn to how that works. first, starting at a high-level, then filling in the details. so i'm gonna describe the partition subroutine only for the case where the pivot is in fact the first element. but really this is without loss of generality. if, instead, you want to use some pivot from the middle of the array, you can just have a preprocessing step that swaps the first element of the array with the given pivot, and then run the subroutine that i'm about to describe, okay. so with constant time preprocessing, the case of a general pivot reduces to the case of when the pivot is the first element. so here's the high-level idea, and it's very cool. the idea is, we're gonna be able to able to get away with just a single linear scan of the input array. so in any given moment in this scan, there's just gonna be a single for-loop, we'll be keeping track of both the part of the array we've looked at so far, and the part that we haven't looked at so far. so there's gonna be two groups, what we've seen, what we haven't seen. then within the group we've seen, we're gonna have definitely split further, according to the elements that are less than the pivot and those that are bigger than the pivot. so we're gonna leave the pivot element just hanging out in the first element of the array until the very end of the algorithm, when we correct its position with a swap. and at any given snapshot of this algorithm, we will have some stuff that we've already looked at, and some stuff that we haven't yet looked at in our linear scan. of course, we have no idea what's up with the elements that we haven't looked at yet, who knows what they are, and whether they're bigger or less than the pivot. but, we're gonna implement the algorithm, so, among the stuff that we've already seen, it will be partitioned, in the sense that all elements less than the pivot come first, all elements bigger than the pivot come last. and, as usual, we don't care about the relative order, amongst elements less than the pivot, or amongst elements bigger than the pivot. so summarizing, we do a single scan through the input array. and the trick will be to maintain the following invariant throughout the linear scan. but basically, everything we have looked at the input array is partitioned. everything less than the pivot comes before everything bigger than the pivot. and, we wanna maintain that invariant, doing only constant work, and no additional storage, with each step of our linear scan. so, here's what i'm gonna do next. i'm gonna go through an example, and execute the partition subroutine on a concrete array, the same input array we've been using as an example, thus far. now, maybe it seems weird to give an example before i've actually given you the algorithm, before i've given you the code. but, doing it this way, i think you'll see the gist of what's going on in the example, and then when i present the code, it'll be very clear what's going on. whereas, if i presented the code first, it may seem a little opaque when i first show you the algorithm. so, let's start with an example. throughout the example, we wanna keep in mind the high-level picture that we discussed in the previous slide. the goal is that, at any time in the partition subroutine, we've got the pivot hanging out in the first entry. then, we've got stuff that we haven't looked at. so, of course, who knows whether those elements are bigger than or less than the pivot? and then, for the stuff we've looked at so far, everything less than the pivot comes before everything bigger than the pivot. this is the picture we wanna retain, as we go through the linear scan. as this high-level picture would suggest, there is two boundaries that we're gonna need to keep track of throughout the algorithm. we're gonna need to keep track of the boundary between what we've looked at so far, and what we haven't looked at yet. so, that's going to be, we're going to use the index  "j" to keep track of that boundary. and then, we also need a second boundary, for amongst the stuff that we've seen, where is the split between those less than the pivot and those bigger than the pivot. so, that's gonna be "i". so, let's use our running example array. >> so stuff is pretty simple when we're starting out. we haven't looked at anything. so all of this stuff is unpartitioned. and "i" and  "j" both point to the boundary between the pivot and all the stuff that we haven't seen yet. now to get a running time reaches linear, we want to make sure that at each step we advance  "j", we look at one new element. that way in a linear number of steps, we'll have looked at everything, and hopefully we'll be done, and we'll have a partitioned array. so, in the next step, we're going to advance  "j". so the region of the array which is, which we haven't looked at, which is unpartitioned, is one smaller than before. we've now looked at the 8, the first element after the pivot. now the 8 itself is indeed a partitioned array. everything less than the pivot comes before, everything after the pivot turns out there's nothing less than the pivot. so vacuously this is indeed partitioned. so  "j" records delineates the boundary between what we've looked at and what we haven't looked at, "i" delineates amongst the stuff we've looked at, where is the boundary between what's bigger than and what's less than the pivot. so the 8 is bigger than the pivot, so "i" should be right here. okay, because we want "i" to be just to the left of all the stuff bigger than the pivot. now, what's gonna happen in the next iteration? this is where things get interesting. suppose we advance  "j" one further. now the part of the array that we've seen is an 8 followed by a 2. now an 8 and a 2 is not a partitioned subarray. remember what it means to be a partitioned subarray? all the stuff less than the pivot, all the stuff less than 3, should come before everything bigger than 3. so (8, 2) obviously fails that property. 2 is less than the pivot, but it comes after the 8, which is bigger than the pivot. so, to correct this, we're going to need to do a swap. we're going to swap the 2 and the 8. that gives us the following version of the original array. so now the stuff that we have not yet looked at is one smaller than before. we've advanced  "j". so all other stuff is unpartitioned. who knows what's going on with that stuff?  "j" is one further entry to the right than it was before, and at least after we have done this swap, we do indeed have a partitioned array. so post-swap, the 2 and the 8, are indeed partitioned. now remember, "i" delineates the boundary between amongst what we've seen so far, the stuff less than the pivot, less than 3 in this case, and that bigger than 3, so "i" is going to be wedged in between the 2 and the 8. in the next iteration, our life is pretty easy. so, in this case, in advancing  "j", we uncover an element which is bigger than the pivot. so, this is what happened in the first iteration, when we uncovered the 8. it's different than what happened in the last iteration when we uncovered the 2. and so, this case, this third iteration is gonna be more similar to the first iteration than the second iteration. in particular, we won't need to swap. we won't need to advance "i". we just advance  "j", and we're done. so, let's see why that's true. so, we've advanced  "j". we've done one more iteration. so, now the stuff we haven't seen yet is only the last four elements. so, who knows what's up with, the stuff we haven't seen yet? but if you look at the stuff we have seen, the 2, the 8, and the 5, this is, in fact, partitioned, right? all the numbers that are bigger than 3 succeed, come after, all the numbers smaller than three. so the "j", the boundary between what we've seen and what we haven't is between the 5 and the 1; and the "i", the boundary between the stuff less than the pivot and bigger than the pivot is between the 2 and the 8, just like it was before. adding a 5 to the end didn't change anything. so let's wrap up this example in the next slide. so first, let's just remember where we left off from the previous slide. so i'm just gonna redraw that same step after three iterations of the algorithm. and notice, in the next generation, we're going to, again, have to make some modifications to the array, if we want preserve our variant. the reason is that when we advance  "j", when we scan this 1, now again we're scanning in a new element which is less than the pivot, and what that means is that, the partitioned region, or the region that we've looked at so far, will not be partitioned. we'll have 2851. remember we need everything less than 3 to precede everything bigger than 3, and this 1 at end is not going to cut it. so we're going to have to make a swap. now what are we going to swap? we're going to swap the 1 and the 8. so, why do we swap the 1 and the 8? well, clearly, we have to swap the 1 with something. and, what makes sense? what makes sense is the left-most array entry, which is currently bigger than the pivot. and, that's exactly the 8. okay, that's the first, left-most entry bigger than 3, so if we swap the 1 with it, then the 1 will become the right-most entry smaller than 3. so after the swap, we're gonna have the following array. the stuff we haven't seen is the 4, the 7, and the 6. so the  "j" will be between the 8 and the 4. the stuff we have seen is the 2, 1, 5, and 8. and notice, that this is indeed partitioned. all the elements, which are less than 3, the 2 and the 1, precede all of the entries, which are bigger than 3, the 5 and the 8. "i", remember, is supposed to split, be the boundary between those less than 3 and those bigger than 3. so, that's gonna lie between the 1 and the 5. that is one further to the right than it was in the previous iteration. okay, so the, because the rest of the unseen elements, the 4, the 7, and the 6, are all bigger than the pivot, the last three iterations are easy. no further swaps are necessary. no increments to "i" are necessary.  "j" is just going to get incremented until we fall off the array. and then, fast forwarding, the partition subroutine, or this main linear scan, will terminate with the following situation. so at this point, all of the elements have been seen, all the elements are partitioned.  "j" in effect has fallen off the end of the array, and "i", the boundary between those less than and bigger than the pivot, still lies between the 1 and the 5. now, we're not quite done, because the pivot element 3 is not in the correct place. remember, what we're aiming for is an array where everything less than the pivot is to the left of it, and everything bigger than the pivot is to the right. but right now, the pivot still is hanging out in the first element. so, we just have to swap that into the correct place. where's the correct place? well, it's going to be the right-most element, which is smaller than the pivot. so, in this case, the 1. so the subroutine will terminate with the following array, 12358476. and, indeed, as desired, everything to the left of the pivot is less than the pivot, and everything to the right of the pivot is bigger than the pivot. the 1 and 2 happen to be in sorted order, but that was just sorta an accident. and the 4, 5, 6 and 7 and 8, you'll notice, are jumbled up. they're not in sorted order. so hopefully from this example you have a gist of how the partition subroutine is going to work in general. but, just to make sure the details are clear, let me now describe the pseudocode for the partition subroutine. so the way i'm going to denote it is, there's going to be an input array a. but rather than being told some explicit link, what's going to be passed to the subroutine are two array indices. the leftmost index, which delineates this part of the separator you're supposed to work on, and the rightmost index. the reason i'm writing it this way is because partition is going to be called recursively from within a quicksort algorithm. so any point in quicksort, we're going to be recursing on some subset, contiguous subset of the original input array. "l(el)" and "r" meant to denote what the left boundary and the right boundary of that subarray are. so, let's not lose sight of the high-level picture of the invariant that the algorithm is meant to maintain. so, as we discussed, we're assuming the pivot element is the first element, although that's really without loss of generality. at any given time, there's gonna be stuff we haven't seen yet. who knows what's up with that? and, amongst the stuff we've seen, we're gonna maintain the invariant that all the stuff less than the pivot comes before all the stuff bigger than the pivot. and  "j" and i denote the boundaries, between the seen and the unseen, and between the small elements and the large elements, respectively. so back to the pseudocode, we initialize the pivot to be the first entry in the array. and again remember, l denotes the leftmost index that we're responsible for looking at. initial value of "i", should be just to the right of the pivot so that's gonna be el+1. that's also the initial value of  "j", which will be assigned in the main for-loop. so this for-loop with "j", taking on all values from el+1 to the rightmost index "r", denotes the linear scan through the input array. and, what we saw in the example is that there were two cases, depending on, for the newly seen element, whether it's bigger than the pivot, or less than the pivot. the easy case is when it's bigger than the pivot. then we essentially don't have to do anything. remember, we didn't do any swaps, we didn't change "i", the boundary didn't change. it was when the new element was less than the pivot that we had to do some work. so, we're gonna check that, is the newly seen element, a[j], less than "p". and if it's not, we actually don't have to do anything. so let me just put as a comment. if the new element is bigger than the pivot, we do nothing. of course at the end of the for-loop, the value of  "j" will get in command so that's the only thing that changes from iteration to iteration, when we're sucking up new elements that happen to be bigger than "p". so what do we do in the example, when we suck up our new element less than p? well we have to do two things. so, in the event that the newly seen element is less than "p", i'll circle that here in pink. we need to do a rearrangement, so we, again, have a partitioned, sub-array amongst those elements we've seen so far. and, the best way to do that is to swap this new element with the left-most element that's bigger than the pivot. and because we have an index "i", which is keeping track of the boundary between the elements less than the pivot and bigger than the pivot, we can immediately access the leftmost element bigger than the pivot. that's just the "i"th entry in the array. now i am doing something a little sneaky here, i should be honest about. which is there is the case where you haven't yet seen any elements bigger than the pivot, and then you don't actually have a leftmost element bigger than the pivot to swap with. turns out this code still works, i'll let you verify that, but it does do some redundant swaps. really, you don't need to do any swaps until you first see some elements bigger than the pivot, and then see some elements less than the pivot. so, you can imagine a different limitation of this, where you actually keep track of whether or not that's happened to avoid the redundant swaps. i'm just gonna give you the simple pseudocode. and again, for intuition, you wanna think about the case just like, in the picture here in blue, where we've already seen some elements that are bigger than the pivot, and the next newly seen element is less than the pivot. that's really sort of the key case here. now the other thing we have to do after one of these swaps is, now the boundary, between where the array elements less than the pivot and those bigger than the pivot, has moved. it's moved one to the right, so we have to increment "i". so, that's the main linear scan. once this concludes,  "j" will have fallen off the end of the array. and, everything that we've seen the final elements, except for the pivot, will be arranged so that those less than "p" are first, those bigger than "p" will be last. the final thing we have to do is just swap the pivot into its rightful position. and, recall for that, we just swap it with the right-most element less than it. so, that is it. that is the partition subroutine. there's a number of variants of partition. this is certainly not the unique implementation. if you look on the web, or if you look in certain textbooks, you'll find some other implementations as well as discussion of the various merits. but, i hope this gives you, i mean, this is a canonical implementation, and i hope it gives you a clear picture of how you rearrange the array using in-place swaps to get the desired property, that all the stuff before the pivot comes first, all the stuff after the pivot comes last. let me just add a few details about why this pseudocode i just gave you does, indeed, have the properties required. the running time is o(n), really theta of n, but again, i'll be sloppy and write o(n). where n is the number of array elements that we have to look at. so, n is r-el+1, which is the length of the sub-array that this partition subroutine is invoked upon. and why is this true? well if you just go inspect the pseudocode, you can just count it up naively and you'll find that this is true. we just do a linear scan through the array and all we do is basically a comparison and possibly a swap and an increment for each array entry that we see. also, if you inspect the code, it is evident that it works in-place. we do not allocate some second copy of an array to populate, like we did in the naive partition subroutine. all we do is repeated swaps. correctness of the subroutine follows by induction, so in particular the best way to argue it is by invariant. so i'll state the invariant here, but mostly leave it for you to check that indeed, every iteration of the for-loop maintains this invariant. so first of all, all of the stuff to the right of the pivot element, to the right of the leftmost entry and up to the index "i", is indeed less than the pivot element, as suggested by the picture. and also suggested by the picture, everything beginning with the "i"th entry, leading just up before the "j"th entry, is bigger than the pivot. and i'll leave it as a good exercise for you to check that this holds by induction. the invariant holds initially, when both "i" and  "j" are equal to el+1, because both of these sets are vacuous, okay? so, there are no such elements, so they're trivially satisfied these properties. and then, every time we advance  "j", well, in one case it's very easy, where the new element is bigger than the pivot. it's clear that, if the invariant held before, it also holds at, at the next iteration. and then, if you think about it carefully, this swap in this increment of "i" that we do, in the case where the new element is less than the pivot. after the swap, once the fold is complete, again if this invariant was true at the beginning of it, it's also true at the end. so what good is that? well, by this claim, at the conclusion of the linear scan at which point "j" has fallen off the end of the array, the array must look like this. at the end of the for-loop, the question mark part of the array has vanished, so everything other than the pivot has been organized so that all this stuff less than the pivot comes before everything after the pivot, and that means once you do the final swap, once you swap the pivot element from its first and left most entry, with the right most entry less than the pivot, you're done. okay? you've got the desired property that everything to the left of the pivot is less than, and everything to the right of the pivot is bigger than. so now that given a pivot element we understand how to very quickly rearrange the array so that it's partitioned around that pivot element, let's move on to understanding how that pivot element should be chosen and how, given suitable choices of that pivot element, we can implement the quicksort algorithm, to run very quickly, in particular, on average in o(n) log time. 
i just got the number of divide and conquer algorithms and, so far, i've been short shrift to proofs of correctness. this has been a conscience decision on my part. coming up with the right divide and conquer algorithm for a problem can definitely be difficult, but once you have that eureka moment and you figure out the right algorithm you tend to, also, have a good understanding of why it's correct, why it actually solves the problem on every possible input. similarly when i present to you a divide and conquer algorithm like, say, merge sort or quicksort, i expect that many of you have a good and accurate intuition about why the algorithm is correct. in contrast the running time of these developed [inaudible] algorithms is often highly non-obvious. so, correctness proofs for divide-and-conquer algorithms tend to simply formalize the intuition that you have via a proof by induction. that's why i haven't been spending much time on them. but nevertheless, i do feel like i owe you at least one rigorous correctness proof for a divide-and-conquer algorithm, and we may as well do it for quicksort. so in this optional video, we'll briefly review proofs by induction, and then we'll show how such a proof can be used to rigorously establish the correctness of quicksort. the correctness proofs for most of the other divide-and-conquer algorithms that we discuss can be formalized in a similar way. so let's begin by reviewing the format for proofs by induction. so, the canonical proofs by induction and the kind that we'll be using here, is when you want to establish an assertion for all of the positive integers in. so now it's some assertion which is parameterized by n, where n is a positive integer. i know this is a little abstract, so let me just be concrete about the assertion that we actually care about for quicksort. so for us, the assertion p(n) is the statement that cor, quicksort is always correct on inputs of length n, arrays that have n elements. so an induction proof has two parts. the first part is a base case and the second part is an inductive step. for the base case you have to get started so you show that at the very least your assertion is true when n equals one. this is often a trivial matter and that'll be the case when we establish the correctness of quick sort. just on our rays with only one element. so, the non-trivial part of a proof by induction is usually the inductive step. and in the inductive step, you look at a value of n not covered by the base case, so a value of n bigger than one. and you show that if the assertion holds for all smaller values, small integers, then it also holds for the integer n. that is, you show that for every positive integer n that's two or greater, you assume that p of k holds for all k strictly less than n. and under that assumption, which is called the inductive hypothesis. under the assumption that p of k holds for all k strictly less than n, you then establish that p of n holds as well. so if you manage to complete both of these steps, if you prove both the base case that p(1) holds, you argue that directly, and then also you argue that assuming the inductive hypothesis, that the assertion holds for all smaller integers, it also holds for an arbitrary integer n. then you're done. then in fact you have proven that the assertion p then holds for every single positive integer n. right? so for any given n that you care about, the way you can derive that from one and two is you just start from the base case, p of one holds. then you apply the inductive step n minus one times. and boom, you've got it. so you know that p holds for the integer n that you care about as well. and that's true for arbitrarily large values of n. so those are proofs by induction in general. now let's instantiate this proof format, this type of proof for establishing the correctness of quicksort. so let me write again what is the assertion we care about. our definition of p(n) is gonna be that quicksort is always correct on arrays of length n. and of course what we want to prove is that quicksort is correct no matter what size array that you give it, that is, we want to prove that p(n) holds for every single n at least one. so this is right in the wheelhouse of proofs by induction. ?kay, so that's how we're going to establish it. now depending on the order in which you're watching the videos, you may or may not have seen our discussion about how you actually choose the pivot, recall that the first thing quick sort does is choose a pivot, then it partitions the array around the pivot. so, we're going establish the correctness of quick sort, no matter how the choose pivot sub-routine gets implemented. okay, so now matter how you choose pivots, you'll always have correctness. as we, as we'll see in a different video, the choice of pivot definitely has an influence on the running of quick sort, the correctness of quick sort, there's no matter how you choose the pivot. so it's perceived by a proof by induction. so for the base case when n equals one, this is a fairly trivial statement. right? so, then we're just talking about inputs that have only one element. every such array is already sorted. quicksorts, in the bai, when n equals one just returns the input array. it doesn't do anything, and that is indeed the sort of array that it returns. so, by the rather trivial argument we had directly proven that p of one holds. we've proven the rather unimpressive statement that quicksort always correctly sorts one element arrays. okay? no big deal. so, let's move on to the inductive step. so in the inductive step we have to fix an arbitrary value of n that's at least two. a value of n not covered by the base case. so let's fix some value of n, that leaves two. now what are we trying to prove? we're trying to prove that quick sort always correctly sorts every input array of length n. so we also have to fix an arbitrary such input. so let's make sure we're all clear on what it is we need to show, what do you show in an inductive step. assuming that pfk holds. for all smaller values, all smaller integers, then p of n holds as well. and remember this is the inductive hypothesis. so in the context of quicksort, we're assuming that quicksort never makes a mistake on any input array that has length strictly smaller than n. and now we just have to show it never makes a mistake on array, input arrays that have size exactly n. so this is the point in the proof where we actually delve into how quick sort is implemented to argue correctness. so recall what the first step of quick sort is, it picks some pivot arbitrarily, we don't know how, we don't care how. and then it partitions the array around this pivot element p. now as we argued in the video where we discussed the partition sub routine, at the conclusion of that sub routine, the array has been rearranged into the following format. the pipit is wherever it is, everything to the left of the pipit is less than the pipit, and everything bigger than the pipit is greater than the pipit. alright, this is where how things stand at the conclusion of the partitioning sub routine. so let's call this stuff less than the pipit the first part of the partition array, and the stuff bigger than the pipit, the second part of the partition array. and recall our observation from the overview video that the pivot winds up in its correct position. right, where would the pivot be? where is any element suppose to be in the final sorted array? what's suppose to be to the right of everything less than it, and to the left of everything bigger than it? and that's exactly where this partitioning subroutine deposits the pivot element peak. so now to imply the inductive hypothesis, which you'll recall is a hypothesis about how quick sort operates on smaller sub arrays. let's call the length of the first part in the second part of the partition [inaudible] k1 and k2 respectively. now, crucially, both k1 and k2 are strictly less than n. both of these two parts have lengths strictly less than that of the given input array a. that's because the pivot in particular is excluded from both of those two parts. so, their gonna have, at most n minus one [inaudible]. that means that we can apply the inductive hypothesis, which says that the quicksort never makes a mistake on an array that has size strictly less than n. that implies that our two recursive calls to quickstart, the one to the first part and the one to the second part don't make mistakes. they're guaranteed to sort those sub arrays correctly by the inductive hypothesis. and to be very precise, what we're using to argue that the [inaudible] are correct, are p of k1 and p of k2. or p is the assertion that [inaudible] is always correct on a [inaudible]. k1 and k2. and we know that both of these statements are true because k1 and k2 are less th, are both less than n and because of the inductive hypothesis. so what's the upshot? the upshot is, quicksort's gonna be correct. and so the first recursive call puts all of the elements that are less than the pivot in the correct relative order. next comes the pivot, which is bigger than all of that stuff in the first part and less than all the stuff in the second part, and then the second recursive call correctly orders all of the elements in the second part. so with those three things pasted together, we have a sorted version of the input array and since this array was an arbitrary one, of link n. that establishes the assertion p of n and since n was arbitrary, that establishes the inductive and completes the proof of correctness of quick sort for an arbitrary method of choosing the pivot element. [sound] 
so let's review the story so far. we've been discussing the quicksort algorithm. here again is its high level description. so in quicksort you call two subroutines first, and then you make two recursive calls. so the first subroutine choosepivot, we haven't discussed yet at all. that'll be one of the main topics of this video. but the job of the choosepivot subroutine is to somehow select one of the n elements in the input array, to act as a pivot element. now what does it mean to be a pivot? well that comes into play in the second subroutine, the partition subroutine, which we did discuss quite a bit in a previous video. so what a partition does is it rearranges the elements in the input array, so that it has the following property, so that the pivot p winds up in its rightful position. that is, it's to the right of all of the elements less than it, and it's to the left of all of the elements bigger than it. the stuff less than it's to the left in some jumbled order. the stuff bigger than it's to the right in some jumbled order. that's what's listed here as the first part and the second part of the partitioned array. now, once you've done this partitioning, you're good to go. you just recursively sort the first part to get them in the right order, you call quicksort again to recursively sort the right part, and bingo, the entire array is sorted. you don't need a combine step, you don't need a merge step. where we'll recall in a previous video, we saw that the partition array can be implemented in linear time. and moreover, it works in place with essentially no additional storage. we also, in an optional video, formally proved the correctness of quicksort, and remember quicksort is independent of how you implement the choosepivot subroutine. so what we're going to do now is discuss the running time of the quicksort algorithm, and this is where the choice of the pivot is very important. so what everybody should be wondering about at this point is, is quicksort a good algorithm? does it run fast? the bar's pretty high. we already have mergesort, which is a very excellent, practical n log n algorithm. the key point to realize at this juncture, is that we are not currently in a position to discuss the running time of the quicksort algorithm. the reason is we do not have enough information. the running time of quicksort depends crucially on how you choose the pivot. it depends crucially on the quality of the pivot chosen. you'd be right to wonder what i mean by a pivot's quality. and basically what i mean, is a pivot is good if it splits the partitioned array into roughly two equal sized subproblems. and it's bad, it's of low quality, if we get very unbalanced subproblems. so to understand both, what i mean, and the ramifications of having good quality and bad quality pivots, let's walk through a couple of quiz questions. this first quiz question is meant to explore a sort of worst case execution of the quicksort algorithm. what happens when you choose pivots that are very poorly suited for the particular input array? let me be more specific. suppose we use the most naive choosepivot implementation, like we were discussing in the partition video. so remember, here we just pluck out the first element of the array and we use that as the pivot. so suppose that's how we implement the choosepivot subroutine, and moreover, suppose that the input array to quicksort is an array that's already in sorted order. so for example, if it just had the numbers one through eight, it would be one, two, three, four, five, six, seven, eight, in order. my question for you is, what is the running time of this recursive quicksort algorithm on an already sorted array, if we always use the first element of a subarray as the pivot? okay, so this is a slightly tricky, but actually a very important question. so the answer is the fourth one. so it turns out, that quicksort, if you pass it an already sorted array and you're using the first element as pivot elements, it runs in quadratic time. and remember for a sorting algorithm, quadratic is bad. it's bad in the sense that we can do better. mergesort runs in time n log n, which is much better than n squared. and if we we're happy with an n squared running time, we wouldn't have to resort to these sort of relatively exotic sorting algorithms. we could just use insertion sort, and we'd be fine. we'd get that same quadratic running time. okay, so now i owe you an explanation. why is it that quicksort can actually run in quadratic time, in this unlucky case, of being passed an already sorted input array? well to understand, let's think about what pivot gets chosen, and what are the ramifications of that pivot choice for how the array gets partitioned, and then what the recursion looks like. so, let's just think of the array as being the numbers 1 through n, in sorted order. what is going to be our pivot? well, by definition we're choosing the first element of the pivot, so the pivot's just going to be 1. now we're going to invoke the partition subroutine. and if you go back to the pseudocode of the partition subroutine, you'll notice that if we pass an already sorted array, it's going to do essentially nothing. okay? so it's just going to advance the index j, until it falls off the end of the array, and it's just going to return back to us, the same array that it was passed as input. so partition subroutine, if given an already sorted array, returns an already sorted array. so we have just a pivot 1, in the first position. and then the numbers 2 through n, in order, in the remainder of the positions. so if we draw our usual picture of what a partitioned array looks like, with everything less than the pivot to the left, everything bigger than the pivot to the right. well, since nothing is less than the pivot, this stuff is going to be empty. this will not exist. and to the right of the pivot, this will have length n- 1, and moreover, it will still be sorted. so once partition completes, we go back to the outer call of quicksort, which then calls itself recursively twice. now in this case, one of the recursive calls is just vacuous. there's just an empty array, there's nothing to do. so really there's only one recursive call, and that happens on a problem of size only one less. so this is about the most unbalanced split we could possibly see, right, where one side has 0 elements, one side's n- 1. splits don't really get any worse than that. and this is going to keep happening over, and over, and over again. we're going to recurse on the numbers 2 through n. we're going to choose the first element, the 2, as the pivot. again, we'll feed it to partition. we'll get back the exact same subarray that we handed it in. we get to the numbers 2 through n, in sorted order. we exclude the pivot 2, we recurse on the numbers 3 through n, a subarray of length n- 2. the next recursion level, we recurse on an array of size of length n- 3, then n- 4, then n- 5, and so on. until finally, after i did recursion depth of n, roughly, we got down to just the last element n, the base case kicks in, and we return that, and quicksort completes. so that's how quicksort is going to execute on this particular input with these particular pivot choices, so what running time does that give to us? well, the first observation is that in each recursive call, we do have to invoke the partition subroutine. and the partition subroutine does look at every element in the array it has passed as input. so if we pass partition in array of length k, it's going to do at least k operations, because it looks at each element at least once. so the runtime is going to be bounded below by the work we do in the outermost call, which is on an array of length n, plus the amount we do in the second level of recursion, which is on a subarray of length (n- 1) + (n- 2) +, blah, blah, blah, blah, blah, all the way down to + 1, for the very last level of the recursion. so this is a lower bound on our running time, and this is already theta of n squared. so, one easy way to see why this sum n + (n- 1) +, etc., etc., leads to a bound of n squared, is to just focus on the first half of the terms. so, the first n over two terms in the sum are all of magnitude at least n over 2, so the sum is at least n squared over 4. it's also evident that this sum is at most, n squared. so, overall, the running time of quicksort on this bad input is going to be quadratic. now having understood what the worst case performance for the quicksort algorithm is, lets move on to discuss it's best case running time. now we don't generally care about the best case performance of algorithms for it's own sake. the reason that we want to think about quicksort in the best case, first of all it'll give us better intuition for how the algorithm works. second of all, it'll draw a line in the sand. its average case running time certainly can't be better than the best case, so this will give us a target for what we're shooting for in our subsequent mathematical analysis. so what were the best case? what was the highest quality pivot we could hope for? well again, we think of the quality of the pivot as the amount of balance that it provides between the two sub problems. so ideally, we choose a pivot which gave us two sub-problems, both of size n over 2 or less. and there's a name for the element that would give us that perfectly balanced split. it's the median element of the array, okay, the element where exactly half of the elements are less than it and half of the elements are bigger than it. that would give us an essentially perfect 50, 50 split of the input array. so, here's the question. suppose we had some input and we ranked quicksort, and everything just worked in our favor, in the magically, in the best possible way. that is, in every single recursive invocation of quicksort, on any sub array of the original input array. suppose, we happen to get, as our pivot the median element. that is, suppose in every single recursive call. we wind up getting a perfect 50/50 split of the input array before we recurse. this question asks you to analyze the running time of this algorithm in this magical best case scenario. so the answer to this question is the third option. it runs (n log n) times. why is that? well, the reason is then the recurrence which governs the running time of quicksort exactly matches the recurrence short running time that we already know is n log n. that is the running time quicksort requires in this magical special case on a array of length n. as usual, you have a recurrence in two parts. there's the work that gets done by the recursive cause and there's the work that gets done now. now by assumption, we wind up picking the median as the pivot. so there's going to be two recursive calls, each of which will be on an input of size at most n over two. and, we can write this, this is because the pivot equals the median. so this is not true for quick sort of general, it's only true in this magical case, where the pivot is the median. so that's what gets done by the two recursive calls. and then how much work do we do outside of the recursive calls? well, we have to do the truth pivot subroutine. and i guess, strictly speaking, i haven't said how that was implemented. but let's assume that choose pivot does only a linear amount of work. and then, as we've seen, the partition subroutine only does a linear amount of work, as well. so let's say o(n), for work outside of the recursive calls. and what do we know? we know this implies, say by using the master method, or just by using the exact same argument as for merge sort, this gives us a running time balunt of (nlogn) and again something i haven't really been emphasizing but which is true is that actually we can write theta (n log n). and that's because in the recurrence, in fact, we know that the work done outside of the recursive calls is exactly theta (n), okay? partition needs really linear time, not just o(n) time. in fact the work done outside of the recursive calls is theta (n). that's because the partition serpentine does indeed look at every entry in the array that it passed, all right, and as a result, we didn't really discuss this so much in the master method. but as i mentioned in passing, if you have recurrences which are tight in this sense, then the result of the master method can also be strengthened to be theta instead of just beta. but those are just some extra details. the upshot of this quiz is that even in the best case, even if we magically get prefect pivots throughout the entire trajectory of quick sort. the best we can hope for is an n log n upper bound, it's not going to get any better than that. so the question is how do we have a principled way of choosing pivots so that we get this best case or something like it that's best case n log n running time. so that's what the problem that we have to solve next. so the last couple quizzes have identified a super important question, as far as the implementation of quicksort, which is how are we going to choose these pivots? we now know that they have a big influence on the running time of our algorithm. it could be as bad as n squared or as good as m log n, and we really want to be on the m log n side. so the key question: how to choose pivots. and quick sort is the first killer application that we're going to see of the idea of randomized algorithms, that is allowing your algorithms to flip coins in the code so that you get some kind of good performance on average. so the big idea is random pivots. by which i mean for every time we recursively call quick sort and we are pass some subarray of length k. among the k candidate pivot elements in the sub-array, we're going to choose each one with probability of one over k. and we're going to make a new random choice every time we have qa recursive call, and we're going to see how the algorithm does. this is our first example of a randomized algorithm. this is an algorithm where, if you feed it exactly the same input, it'll actually run differently, on different execution. and that's because there's randomness internal to the code of the algorithm. now, it's not necessarily intuitive. the randomization should have any purpose in the computation, in software design and algorithm design. but, in fact, and this has been sort of one of the real breakthroughs in algorithm design, mostly in the '70s, in realizing how important this is, that the use of randomization can make algorithms more elegant. simpler, easier to code, faster, or just simply you can solve problems that you could not solve as easily without the use of randomization. it's really one thing that should be in your toolbox as an algorithm designer, randomization. quick sort will be the first [inaudible] application, but we'll see a couple more later in the course. now by the end of this sequence of video's i'll have given you a complete rigorous argument about why this works. why with random pivots, quick sort always runs very quickly, on average. but, you know, before moving into anything to formal let's develop a little bit of intuition or at least kind of a daydream. about why on earth could this possibly work, why on earth could this possibly be a good idea, to have randomness internal to our pro sort implementation. well, so first just very high level, what would be sort of the hope, or the dream. the hope would be, random pivot's are not going to be perfect, i mean you're not going to just sort of guess the median, or you only have a one in chance of figuring out which one the median is, but the hope is that most choices of a pivot will be good enough. so that's pretty fuzzy. let's drill down a little bit and develop this intuition further. let me describe it in two steps. the first claim is that, you know in our last quiz we said suppose we get lucky and we always pick the median in every single recursive call. and we observed we'd do great. we'd get end log in running time. so now let's observe that actually to get the end log in running time, it's not important that we magically get the median every single recursive call. if we get any kind of reasonable pivot, by which a pivot that gives us some kind of approximately balanced with the problems, again, we're going to be good. so the last quiz really wasn't particular to getting the exact median. near medians are also fine. to be concrete, suppose we always pick a pivot which guarantees us a split of 25 to 75, or better. that is, both recursive calls should be called on arrays of size, at most, 75% of the one we started with. so precisely if we always get a 25, 75 split or better in every recursive call i claim that the running time of quick sort in that event will be big o of n log n. just as it was in the last quiz where we're actually assuming something much stronger where we're getting a median. now this is not so obvious, the fact that 25, 75 splits guarantee analog and running time. for those of you that are feeling keen you might want to try to prove this. you can prove this using a recursion tree argument, but because you don't have balanced sub problems you have to work a little bit harder than you do in the cases covered by the master method. so that's the first part of the intuition, and this is what we mean by a pivot being good enough. if we get a 25, 75 split or better, we're good to go, we get our desired, our target analog. the second part of the intuition is to realize that actually we don't have to get all that lucky to just be getting a 25, 75 split. that's actually a pretty modest goal and even this modest goal is enough to get n log n running time, right? so suppose our array contains the numbers, the integers between 1 and 100, so it is an array of length 100. think for a second, which of those elements is going to give us a split that's 25, 75 or better? so, if we pick any element between 26 and 75 inclusive, will be totally good, right? if we pick something that's at least 26, that means the left subproblem is going to have at least the elements 1 through 25. that'll have at least 25% of the elements. if we pick something less than 75 then the right sub-problem will have all of the elements 76 through 100 after we partition, so that'll also have at least 25% of the elements. so anything between 26 and 75 gives us a 75-25 split or better. but that's a full half of the elements, so it's as good as just flipping a fair coin hoping to get heads. so with 50% probability, we get a split that's good enough to get this n log n bound. and so again, the high level hope is that often enough, half of the time, we get these good enough splits, 25-75 split or better, so that would seem to suggest n log n running time on average is a legitimate hope. so that's the high level intuition, but if i were you i would certainly not be content with this somewhat hand wavy explanation that i've given you so far. what i've told you is sort of the hope the dream, why there is at least a chance this might work. but the question remains, and i would encourage such skepticism, which is does this really work? and to answer that we're going to have to do some actual mathematical analysis, and that's what i'm going to show you. i'm going to show you a complete rigorous analysis of the quick sort algorithm with random pivots, and we'll show that yes in fact, it does really work. and this highlights what's going to be a recurring them of this course, and a recurring theme in the study and understanding of algorithms. which is that quite often there's some fundamental problem you're trying to code with a solution, you come up with a novel idea, it might be brilliant, and it might suck. you have no idea. now, obviously, you code up the idea, run it on some concrete instances and get a feel for whether it seems like a good idea or not. but if you really want to know fundamentally what makes the idea good or what makes the idea bad, really, you need to turn to mathematical analysis to give you a complete explanation. and that's exactly what we're going to do with quicksort, and then we'll explain in a very deep way why it works so well. specifically in the next sequence of three videos i'm going to show you an analysis, a proof of the following theorem about quicksort. so under no assumptions about the data, that is, for every input array of a given length, say n, the average running time of quicksort implemented with random pivots is big o of n log n. and again in fact it's theta of n log n but we'll just focus on the big o of n log n part. so this is a very, very cool theorem about this randomized quicksort algorithm. one thing i want to be clear so that you don't under sell this guarantee in your own mind, this is a worst case guarantee with respect to the input. okay so notice at the beginning of this theorem what do we say? for every input array of length n, all right? so, we have absolutely no assumptions about the data. this is a totally general purpose sorting separating which you can use whatever you want even if you have no idea where the data is coming from. and these guarantees are still going to be true. this of course is something i held forth about at some length back in our guiding principles video, when i argued that if you can get away with it, what you really want is general purpose algorithms. which make no data assumption, so they can be used over and over again in all kinds of different contexts and that still have great guarantees, quicksort is one of those. so basically if you have a data set and it fits in the main memory of your machine, again sorting is a four free sub routine in particular quicksort. the quicksort implementation is for free. so this just runs so blazingly fast, doesn't matter what the array is, maybe you don't even know why you want to sort it. but go ahead, why not? maybe it will make your life easier, like it did for example in the closest pair algorithm for those of you who watch those two optional videos. now the word average does appear in this theorem and as i've been harping on, this average is not over any assumptions on the data. we're certainly not assuming the the input array is random in any sense. the input array can be anything, so where is the average then coming from? the averaging is coming only from randomness which is internal to our algorithm. randomness that we put in the code in ourselves, that we're responsible for. so remember, randomized algorithms have the interesting property that even if you run it on the same input over and over again, you're going to get different executions. so the running time of a randomized algorithm can vary as you run it on the same input over and over again. the quizzes have taught us that the running time of quicksort on a given input fluctuates from anywhere between the best case of n log n to the worst case of n squared. so what this theorem is telling us is that for every possible input array, while the running time does indeed fluctuate between an upper bound of n squared and a lower bound of n log n. the best case is dominating. on average it's n log n, on average it's almost as good as the best case. that's what's so amazing about quicksort. those n squared that can pop up once in a while, doesn't matter. you're never going to see it, you're always going to see this n log n like behavior in randomized quicksort. so for some of you i'll see you next in a video on probability review, that's optional. for the rest of you i'll see you in the analysis of this theorem. 
so this is the first video of three in which we'll mathematically analyze the running time of the randomized implementation of quick sort. so in particular we're going to prove that the average running time of quick sort is big o of n log n. now this is the first randomized algorithm that we've seen in the course and therefore in its analysis will be the first time that we're going to need any kind of probability theory. so let me just explain upfront what i'm going to expect you to know. in the following analysis. basically, i need you to know the first few ingredients of discrete probability theory. so i need you to know about sample spaces, that is how to model all of the different things that could happen, all of the ways that random choices could resolve themselves. i need you to know about random variables, functions on sample spaces, which take on real values. i need you to know about expectations that is average values of random variables and very simple but very key propriety we're going to need in the analysis of quick sort is linearity of expectation. so if you haven't seen this before or if you're too rusty definitely you should review this stuff before you watch this video. some places you can go to get that necessary review you can look at the probability review part one video. that's up on the course's website. if you'd prefer to read something, like i said at the beginning of the course, i recommend the free online lecture notes by eric lehman and tom leighton, mathematics for computer science. that covers everything we'll need to know, plus much, much more. there's also a wikibook on discrete probability, which is a perfectly fine, obviously, free source in which you can learn the necessary material. okay? so after you've got that sort of fresh in your mind, then you're ready to watch the rest of this video. and in particular, we're ready to prove the following theorems stated in the previous video. so the quick sort algorithm with a randomized implementation, that is we're in every single recursive subcall, you pick a pivot uniformly at random. we stated the following assertion. but for every single input, so for a worst case input array of length n, the average running time of quicksort with random pivots is o(n log n). and again, to be clear where the randomness is, the randomness is not in the data. we make no assumptions about the data. as per our guiding principles. no matter what the input array is, averaging only over the randomness in our own code, the randomness internal to our algorithm. we get a running time of n log n. we saw in the past that the best case behavior of quicksort is n log n. its worst case behavior is n squared. so this theorem is asserting that no matter what the input array is, the typical behavior of quicksort is far closer to the best case behavior than it is to the worst case behavior. okay. so that's what we're going to prove in the next few videos. so let's go ahead and get started. so first i'm going to set up the necessary notation and be clear about what exactly is the sample space, what is the random variable that we care about, and so on. so we're going to fix an arbitrary array of length n. that's going to be the input to the quick sort algorithm. [sound]. and we'll be working with this fixed but arbitrary input array for the remainder of the analysis. okay. so just fix a single input in your mind. now, what's the relevant sample space? well, recall what a sample space is. it's just all the possible outcomes of the randomness in the world. so it's all the distinct things that could happen. now here, the randomness is of our own devising. it's just the random pivot sequences, the random pivots chosen by quicksort. so omega is just the set of all possible random pivots the quicksort could choose. now the whole point of this theorem proving that the average running time of quick sort is small boils down to computing the expectation of a single random variable. so here's the random variable we're going to care about. for a given pivot sequence remember that random variables are real value functions. defined on the sample space. so for a given point in the sample space or pivot sequence sigma, we're going to define c of sigma as the number of comparisons that quick sort makes. where by comparison, i don't mean something like with an array index in a for-loop. that's not what i mean by comparison. i mean a comparison between two different entries of the input array, by comparing the third entry in the array against the seventh entry in the array, to see whether the third entry or the seventh entry is smaller. notice that this is indeed a random variable that is given knowledge of the pivot sequence sigma, the choices of all pivots. you can think of quick sort at that point as just a deterministic algorithm with all of the pivot choices pre-determined, and so a deterministic version of quicksort make some deterministic member of comparisons so for giving pivot sequence sigma, we're just calling c of sigma to be however many comparisons it makes given those choices of pivots. now with the theorem i stated is not about the number of comparisons of quicksort but rather about the running time of quicksort, but really to think about it kind of the only real work that the quicksort algorithm does, is make comparisons between pairs of elements in the input array. the axis is a little bit of other book keeping but that's all noise that second over stuff. all quicksort really does is compare between pairs of elements in the input array. and if you want to know what i mean by that a little more formally, dominated by comparisons, i mean that there exists a constant c so that the total number of operations of any type that quicksort executes is at most a constant factor larger than the number of comparisons. so lets say that by rt, i mean the number of primitive operations of any form, that quicksort uses. and for every previd sequence, sigma, the total number of operations, is no more than a constant times the total number of comparisons. and if you want a proof of this it's not that interesting so i'm not going to talk about it here. but in the notes posted on the website there is a sketch of why this is true. how you can formally argue that there isn't much work beyond just the comparisons. but i hope most of you find that to be pretty intuitive. so given this, given that the running time that quicksort boils down just to the number of comparisons. we want to prove the running time is n log n. all we gotta do, quote unquote, all we have to do this proves that the average number of comparisons the quicksort mix is all nlogn. and that's what we're going to do. that's what the rest of these lecture is all about. so that's what we got to prove. we got to prove the expectation of this random variable c which counts up the number of comparisons quicksort mix is for arbitrary input array of link n bound by big o of nlogn so the high order bit of this lecture is a decomposition principle. we've identified this random variable, c, the number of comparisons and it's exactly what we care about. it governs the average running time of quicksort. the problem is, it's quite complicated. it's very hard to understand what this capital c is, it's fluctuating between nlogn and then squared. and it's hard to know how to get a handle on it. so how are we going to go about proving this assertion, that the expectant number of comparisons that quicksort makes, is on average just o of nlogn. at this point we've actually have a fair amount of experience with divide and conquer algorithms. you've seen a number of examples. and whenever we had to do a running time analysis of such an algorithm we'd write out a recurrence we applied the master method or in the worst case we'd run our recursion tree to figure out the solution at our recurrence so you'd be very right to expect something similar to happen here. but as we probe deeper and we think about quicksort we quickly realized that the master method just doesn't apply, or at least not in the form that we're used to, the problem is two fold. so first of all the size of the two sub-problems is random, right? as we discuss in the last video, the quality of the pivot is what determines how balanced the split we get into the two sub-problems. it could be as bad as a sub-problem of size 0 and one of size n minus 1. or it could be as good as a perfectly balanced split into two sub problems of equal sizes but we don't know. it's going to depend on the random choice of the pivot. moreover the master method at least as we discussed it required solved subproblems to have the same size and unless you're extremely lucky that's not going to happen. in the quicksort algorithm. it is possible to develop a theory of recurrence relations for randomized algorithms and apply that to quicksort in particular. but i'm not going to go that route for two reasons. the first one is't really quite messy. it get's pretty technical to talk about solutions to recurrences for randomized algorithms. or to thing about random recursion trees, both of those get pretty complicated. the second reason is, i really want to introduce you to what i call a decomposition principle. by which you take a random variable that's complicated, but that you care about a lot. you decompose it into simple random variables, which you don't really care about in their own right, though it's easy analyze. and then you stitch those two things together using linearity and expectation. so that's going to be the workhorse for our analysis of the quicksort algorithm. and it's going to come up again a couple times in the rest of the course, for example, when we study hashing. so to explain how this decomposition principle applies to quicksort in particular. i'm going to need to introduce to you the building blocks, simple random variables. which will make up the complicated random variable that we care about, the number of comparisons. here's some notation. recall that we fixed in the background an arbitrary array of length n and that's denoted by capital a. and some notation which is simple but also quite important. by z sub i, what i mean is the ith smallest element in the input array capital a, also know as the ith order statistic. so let me tell you what zi is not. what zi is not, in general, is the element in the ith position of the input unsorted array. what zi is, is it's the element which is going to wind up in the ith element of the array, once we sort it. okay, so if you fast forward to the end of a sorting algorithm and position i, you're going to find zi. so, let me give you an example. so suppose we had just a simple array here, unsorted with the numbers 6, 8, 10 and 2. then z1, well that's the first smallest, the one smallest, or just the minimum. so z1 would be the 2, z2 would be the 6, z3 would the the 8 and z4 would be the 10, for this particular input array. okay, so zi is just the ith smallest number. whatever it may lie on the original unsorted array, that's what zi refers to. so we already defined the sample space. that's just all possible choices of pivots the quicksort might make. i already described one random variable, the number of comparisons that quicksort makes on a particular choice of pivots. now i'm going to introduce a family of much simpler random variables. which count merely the comparisons involving a given pair of elements in the input array, not all elements, just a given pair. so for a given a choice of pivots, a given sigma, and for given choices of inj, both of which are between 1 and n. and so we only count things once, so i'm going to insist the i is less than j always. and now here's a definition, my xij and this is a random variable, so it's a function of the pivots chosen. this is going to be the number of times that zi and zj are compared in the execution of quicksort. okay, so this is going to be an important definition in our analysis. it's important you understand it. so, for something like the third smallest element and the seventh smallest element. xij is asking, that's when i equals 3 and j equals 7, x37 is asking how many times those two elements get compared as quicksort proceeds. and this is a random variable in the sense that if the pivot choices are all predetermined, if we think of those being chosen in advance. then there's just some fixed deterministic number of times that zi and zj get compared. so it's important you understand these random variables xij, so the next quiz is going to ask a basic question about the range of values that a given xij can take on. so for this quiz we're considering as usual some fixed input array. and now furthermore fixed to specific elements of the input array. for example, the third smallest element, wherever it may lie, and the seventh smallest element, wherever it may lie. think about just these pair of two elements. what is the range of values that the corresponding random variable xij can take on? that is what are the different number of times that a given pair of elements might be conceivably get compared in the execution of the quicksort algorithm? all right, so the correct answer to this quiz is the second option. this is not a trivial quiz. this is a little tricky to see. so the assertion is that a given pair of elements, they might not be compared at all. they might be compared once and they're not going to get compared more than once. so here what i'm going to discuss is why it's not possible for a given pair of elements to be compared twice during the execution of quicksort. it'll be clear later on, if it's not already clear now, that both 0 and 1 are legitimate possibilities. a pair of elements might never get compared and they might get compared once. and again, we'll go into more detail on that in the next video. but why is it impossible to be compared twice? well think about two elements, say the third element and the seventh element. and let's recall how the partition subroutine works. observe that in quicksort, the only place in the code where comparisons between pairs of input array elements happens. it only happens in the partition subroutine, so that's where we have to drill down. so what are the comparisons that get made in the partition subroutine? well, go back and look at that code. the pivot element is compared to each other element in the input array exactly once. so the pivot just hangs up in the first entry of the array. we have this for loop, this index j which marches over the rest of the array. and for each value of j, the jth element of the input array gets compared to the pivot. so summarizing, in an invocation of partition, every single comparison involves the pivot element. so two elements get compared if and only if one is the pivot. all right so let's go back to the question. why can't a given pair of elements of the input array get compared two or more times? well, think about the first time they ever get compared in quicksort. it must be the case, that at that moment we're in a recursive call where either one of those two is the pivot element. so if it's the third smallest element or the seventh smallest element. the first time those two elements are compared to each other, either the third smallest or the seventh smallest is currently the pivot. because all comparisons involve a pivot element. therefore, what's going to happen in the recursion, well the pivot is excluded from both recursive calls. so, for example, if the seventh smallest element is currently the pivot, that's not going to be passed on the recursive call which contains the third smallest element. therefore if you're compared once, one of the elements is the pivot and they'll never be compared again, because the pivot will not even show up in any future recursive calls. so let me just remind you of some terminology. so a random variable which can only take on the values 0 or 1 is often called an indicator random variable, because it's just indicating whether or not a certain things happens. so, in that terminology, each xij is indicating whether or not the ith smallest element in the array and the jth smallest element in the array ever get compared. it can't happen more than once, it may or may not happen, and xij is 1 precisely when it happens. so that's the event that it's indicating. having defined the building blocks i need, these indicator random variables, these xij's. now i can introduce you to the decomposition principle as applied to quicksort. so there's a random variable that we really care about, which is denoted capital c, the number of comparisons the quicksort makes. that's really hard to get a handle on, in and of itself, but we can express c as the sum of indicator random variables, of these xijs. and those we don't care about in their own right, but they're going to be much easier to understand. so let me just rewrite the definitions of c in the xij, so we're all clear on them. so c, recall, counts all of the comparisons between pairs of input elements that quicksort makes, whereas an xij only counts the number. and it's going to be 0 or 1, comparisons that involve the ith smallest and the jth smallest elements in particular. now, since every comparison involves precisely one pair of elements, some i and some j with i less than j, we can write c as the sum of the xijs. so don't get intimidated by this fancy double sum. all this is doing is it's iterating over all of the ordered pairs, so all of the pairs ij, where i and j are both between 1 and n and where i is strictly less than n. this double sum is just a convenient way to do that iteration. and of course, no matter what the pivots chosen are, we have this equality, okay? the comparisons are somehow split up amongst the various pairs of elements, the various is and js. why is it useful to express a complicated random variable as a sum of simple random variables? well, because an equation like this is now right in the wheelhouse of linearity of expectation, so let's just go ahead and apply that. remember, and this is super, super important, linearity of expectation says that the expectation of a sum equals the sum of the expectations. and moreover, this is true whether or not the random variables are independent, okay? and i'm not going to prove it here, but you might want to think about the fact that the xijs are not, in fact, independent. so we're using the fact that linear expectation works even for non-independent random variables. again, why is this interesting? well, the left hand side, this is complicated, right? this is some crazy number of comparisons by some algorithm on some arbitrarily long array. and it fluctuates between two pretty far apart numbers n log n and n squared. on the other hand, this does not seem as intimidating. given xij, it's just 0 or 1, whether or not these two guys get compared or not. so that is the power of this decomposition approach, okay? so, it reduces understanding a complicated random variable to understanding simple random variables. in fact, because these are indicator random variables, we can even clean up this expression some more. so for any given xij being a 0, 1 random variable, if we expand the definition of expectation, just as an average over the various values, what is it? well, it's some probability it takes on the value 0, that's possible, and then some possibility it takes on the value 1. and of course, this 0 part, we can very satisfyingly delete, cancel. and so, the expected value of a given xij is just the probability that xij = 1. and remember, it's an indicator random variable. it's 1 precisely when the ith smallest and the jth smallest elements get compared. so putting it all together, we find that what we care about. the average value of the number of comparisons made by quicksort on this input array is this double sum, which literates over all ordered pairs, where each sum and is the probability that the corresponding xij = 1. that is the probability that zi and zj get compared. and this is essentially the stopping point for this video for the first part of the analysis, so let's call this star and put a nice circle around it. so what's going to happen next is that in the second video for the analysis, we're going to drill down on this probability, probability that a given pair of elements gets compared, and we're going to nail it. we're going to give an exact expression as a function of i and j for exactly what this probability is. then in the third video, we're going to take that exact expression, plug it into the sum, and then evaluate this sum. and it turns out the sum will evaluate to o of n log n. so that's the plan. that's how you'll apply decomposition in terms of 0, 1 or indicator random variables, apply linearity of expectation. in the next video, we'll understand these simple random variables, and then we'll wrap up in the third video. before we move on to the next part of the analysis, i do just want to emphasize that this decomposition principle is relevant not only for quicksort, but it's relevant for the analysis of lots of randomized algorithms. and we will see more applications, at least one more application, later in the course. so just to kind of really hammer the point home, let me spell out the key steps for the general decomposition principle. so first you need to figure out what is it you care about. so in quicksort, we cared about the number of comparisons. we had this lemma that said the running time is dominated by comparisons. so we understood what we wanted to know, the average value for the number of comparisons. the second step is to express this random variable y as a sum of simple random variables, ideally indicator or 0, 1 random variables. now you're in the wheel house of linearity of expectation, you just apply it, and you find that what it is you care about, the average value of the random variable y is just the sum of the probabilities of various events. that given xl, random variable is equal to 1. and so the upshot is to understand the seemingly very complicated left-hand side, all you have to do is understand something, which in many cases, is much simpler, which is understand the probability of these various events. in the next video, i'll show you exactly how that's done in the case of quicksort, where we care about the xijs, the probability that two elements gets compared. so let's move on and get exact expression for that probability. 
this is the second video of three, in which we prove that the average running time of randomized quicksort is big o of n log n. so, to remind you of the formal statements. so again we're thinking about quicksort where we implement the choose pivot sub routine to always choose a pivot uniformly at random from the sub array that it gets passed. and we're proving that for a worst case input array for an arbitrary input array of length n, the average running time of quicksort where the average is over the random pivot choices is big o of n log n. so let me remind you of the story so far. this is where we left things at the previous video. we defined a few random variables. the sample space, recall, is just the, all of the different things that could happen, that is all of the random coin flip outcomes that quicksort could produce. which is equivalent to all of the pivot choices made by quicksort. now, the random variables we care about. so first of all, there is c. which is the number of comparisons between pairs of elements in the input array that quicksort makes for a given pivot sequence sigma. and then there are the xij's. and so that's just meant to count the number of comparisons involving the ith smallest and the jth smallest elements in the input array. where you recall that zi and zj denote the ith smallest and jth smallest entries in the array. now because every comparison involves some zi and some zj we can express c as a sum over the xij's. so we did that in the last video, we applied linearity at expectation, we used the fact that xij are zero one, that is indicator random variables to denote that to write the expectation of an xij just as the probability that it's equal to one and that gave us the following expression. so the key insight and really the heart of the quicksort analysis is to derive an exact expression for this probability as a function of i and j. so for example if the third smallest element in the array, the seventh smallest element in the array. wherever they may be scattered in the input array we want to know exactly what's the probability that they get compared at some point in the execution of quicksort. and we're going to get a extremely precise understanding of this probability in the form of this key claim. so for all pairs of elements, and again, ordered pairs. so we're thinking of i being less than j. the probability that zi and zj get compared at some point in the execution of quicksort is exactly 2 divided by j- i + 1. so for example in this example of the third smallest element and the seventh smallest element, it would be exactly 40% of the time, two over five is how often those two elements would get compared if you ran quicksort with a random choice of pivots, and that's going to be true for every j and i. the proof of this key claim is the purpose of this video. so how do we prove this key claim? how do we prove that the probability that zi, zj get compared is exactly 2 over quantity j- i +s 1. well fix your favorite ordered pair, so fix elements zi, zj with i less than j, for example the third smallest and the seventh smallest element in the array. now, what we want to reason about is the set of all elements in the input array between zi and zj inclusive. and i don't mean between in terms of positions in the array, i mean between in terms of their values. so consider the set between zi and zj + 1 inclusive, so zi, zi + 1,... zj- 1, zj. so for example, the third, fourth, fifth, sixth and seventh smallest elements in the input array. wherever they may be, okay. and of course, the initial array is not sorted, so there's no reason to believe that these j minus i plus 1 elements are contiguous, okay. they're scattered throughout the input array. but we're going to think about them, okay, zi through zj inclusive. now throughout the execution of quicksort, these j minus i plus 1 elements lead parallel lives at least for awhile in the following sense. begin with the outermost call to quicksort and suppose that none of these j minus i plus 1 elements is chosen as a pivot. where then could the pivot lie? well it can only be a pivot that's greater than all of these or it could be less than all of these. for example if this is the third fourth, fifth, sixth and seventh smallest elements in the array, well the pivot is either the minimum or the second minimum in which case it's smaller than all five elements, or it's the eighth or largest, or larger elements in the array in which case it's bigger than all of them. there's no way you're going to have a pivot that somehow is wedged in between this set because this is a contiguous set of order statistics, okay. now what do i mean by these elements leading parallel lives? well, in the case where the pivot is chosen to be smaller than all of these elements, then all of these elements will wind up to the right of the pivot. and they will all be passed to a common recursive call. the second recursive call. if the pivot is chosen to be bigger than all of these elements, then they'll all show up on the left side of the partitioned array. and they'll all be passed to the first recursive call. iterating this or proceeding inductively, we see that as long as the pivot is not drawn from the set of j minus i plus 1 elements. this entire set will get passed on to the same recursive call. so these j minus i plus 1 elements are living blissfully together in harmony until the point in which one of them gets chosen as a pivot. and that, of course, has to happen at some point. the recursion only stops when the array length is equal to zero or one. so, if for no other reason at some point there will be no other elements in a recursive call other than these j minus i plus 1, okay. so at some point, the reverie is interrupted and one of them is chosen as a pivot. so let's pause the quicksort algorithm and think about what things look like at the time when one of these j minus i plus 1 elements is first chosen as a pivot element. there are two cases worth distinguishing between. in the first case the pivot happens to be either zi or zj. now remember what it is we're trying to analyze. we're trying to analyze the frequency, the probability with when zi and zj gets compared. well, if zi and zj are in the same recursive call, and one of them gets chosen as the pivot, then they're definitely going to get compared. remember when you partition an array around this pivot element, the pivot get's compare to everything else. so if zi's chosen as a pivot, it certainly get's compare to zj. if zj gets chosen as a pivot, it gets compared to zi. so either way if one of these two is chosen, they're definitely compared. if, on the other hand, the first of these j minus i plus 1 elements to be chosen as a pivot is not zi or zj. if, instead, it comes from the set zi plus 1, so on, up to zj minus 1. then the opposite is true. then zi and zj are not compared now. nor will they ever be compared in the future. so why is that? well that requires two observations. first recall that when you choose a pivot and you partition an array, all of the comparisons involve the pivot. so two elements neither of which is the pivot do not get compared in a partition sub routine. so they don't get compared right now. moreover, since zi is the smallest of these and zj is the biggest of these, and the pivot comes from somewhere between them. this choice of pivot will split zi and zj into different recursive calls, zi gets passed to the first recursive call, zj gets passed to the second recursive call and they will never meet again. so there's no comparison's in the future, either. so these two observations right here i would say is the key insight in the quicksort analysis. the fact that for a given pair of elements we can very simply characterize exactly when they get compared and when they do not get compared in the quicksort algorithm. that is they get compared exactly when one of them is chosen as the pivot before any of the other elements with value in between those two has had the opportunity to be a pivot. that's exactly when they get compared. so this will allow us to prove this key claim, this exact expression on the comparison probability. that will plug into the formula we had earlier and will give us the desired bound on the average number of comparisons. so let's fill in those details. so first let me rewrite the high order bit from the previous slide. so now at last we will use the fact that our quicksort implementation always chooses a pivot uniformly at random. that each element of a sub array is equally likely to serve as the pivot element in the corresponding partition call. so what does this buy us? this just says all of the elements are symmetric. so each of the elements zi, zi plus 1, all the way to zj, is equally likely to be the first one asked to serve as a pivot element. now the probability that zi and zj get compared is simply the probability that we're in case one, as opposed to in case two. and since each element is equally likely to be the pivot, that just means there's sort of two bad cases, two cases in which one can occur out of the j minus i plus 1 possible different choices of pivot. now we're talking about a set of j minus i plus 1 elements. each of whom is equally likely to be asked to be served first as a pivot element. and the bad case, the case that leads to a comparison, there's two different possibilities for that. it was zi or zj is first. and the other j minus i minus 1 outcomes lead to the good case where zi and zj never get compared. so overall, because everybody's equally likely to be the first pivot, we have that the probability with zi and zj get compared. is exactly the number of pivot choices that lead to comparison, divided by the number of pivot choices overall. and that is exactly the key claim. that is exactly what we asserted was the probability that a given zi and zj get compared for no matter what i and j are. so, wrapping up this video, where does that leave us? we can now plug in this expression for this probability of comparison probabilities. into the double sum that we had before. so putting it all together what we have is that what we really care about the average number of comparisons that quicksort makes on this particular input of array n, of length n is just this double sum which iterates over all possible ordered pairs ij. and what we had here before was the probability of comparing zi and zj we now know exactly what that is so we just substitute. and this is where we're going to stop for this video so this is going to be our key expression star which we still need to evaluate but that's going to be the third video. so essentially we've done all of the conceptual difficulty in understanding where comparisons come from in the quicksort algorithm. all that remains is a little bit of an algebraic manipulation to show that this starred expression really is big o log n. and that's coming up next. 
so we're almost at the finish line of our analysis of quick sort. let me remind you what we're proving. we're proving that for the randomized implementation of quick sort where we always choose the pivot element to partition around uniformly at random, we're showing that for every array, every input of length n, the average running time of quick sort over the random choices of pivots is [inaudible] of n log n. so we've done a lot of work in the last couple of videos. let me just remind you about the stories so far. in the first video what we did is we identified the relevant random variable that we cared about, capital c, the number of comparisons that quicksort makes among the pairs of elements in the input array. then we applied the decomposition approach. we expressed capital c, the overall number of comparisons, as a sum of indicator or 0-1 random variables. for each of those variables xij, just counted the number of comparisons involving the ith smallest and jth smallest entries in the array, and that's gonna be either zero or one. then we applied linearity of expectation to realize, all we really needed to understand was the comparison probabilities for different pairs of elements. [inaudible]. second video we nailed what that comparison probability is, specifically, for the i smallest and the j smallest elements in the array, the probability that quick sort compares them when you always make random [inaudible] choices is exactly. two divided by the quantity j minus i. plus one. so putting that all together, yields the following expression, governing the average number of comparisons made by quick sort. one thing i want you to appreciate is, is in the last couple of videos, we've been sort of amazingly exact as algorithmic analysis goes. specifically we've done nothing sloppy whatsoever. we've done no estimates. the number of comparisons that quick store makes on average is exactly this double sum. now surely we'll do some inequalities to make our lives a little bit easier. but up to this point everything has been completely exact. and this will actually see why there's small constants in the, in the, in quick sort. it's basically going to be this factor two. now the next question to ask is, what are we shooting for? remember the theorem we want to prove is that the expected number of comparisons really the expected run time is all of n log n, so we're already done. well not quite we're gonna have to be a little bit clever, so if we're looking at this double sum, and we ask how big are the sum ends and how many terms are there? well the biggest sum ends we're ever going to see are when i and j are right next to each other when j is one bigger than i, and in that case this fraction is gonna be one half. so the terms can be as big as one half, how many terms are there? well there's a quadratic number of terms. so it would be very easy to derive an upper bound that's quadratic in n, but that's not what we want. we want one that's n log n. so to drive that, we're gonna have to be a little bit more clever about how we evaluate this sum. so, the idea is, what we're going to do, is to think about a fixed value of i in this outermost sum. and then we're gonna ask, how big could the inner sum be? so let's fix some value of i, the value of the index in the outer sum. and then let's look at the inner sum, where j ranges from i plus one up to n, and the value of the sum end is one over the quantity j minus i plus one. so how big can this be? well, let's first understand what the terms actually are. so j starts at i plus one and then it ascends to n. and as j gets bigger the denominator gets bigger. so the sum ends get smaller. so the biggest sum end is gonna be the very first one. and j is as small as possible. namely i plus one. when j is i plus one the sum end is one half. then j gets incremented in the sum. and so that's, we're gonna pick up a one third term followed by one fourth term, and so on. so there's gonna be, for every inner sum is gonna have a this form, one-half plus one-half equals one-fourth. and then it's gonna sort of run out at some point, when j equals n. and the biggest term we're ever going to see is gonna be a one over n, in the case where i equals one. so. let's make our lives easier by taking this expression we started with. star, and instead of having a double sum, let's just upper bound this with a single sum. so what are the ingredients of a single sum? well, there's this two, can't forget the two. then there's n choices for i, actually, there's n minus one choices for i, but let's just be sloppy and say n choices. so that gives us a factor n. and then how big can an inner sum be? well, inner sum is just a bunch of these terms, one-half plus one-third and so on. the biggest of those inner sums is the one occurring when i equals one, at w, at which point the last term is one over n. so, we're gonna just do a change of variable and express the inner [inaudible], upper bound on each inner sum as the sum from k equal two to n of one over k. so that's looking more manageable just having the single sum involving this index k, and life's gonna get really good when we prove the next claim, which is that this sum cannot be very big, it's only logarithmic in n, even though there's a linear number of sum n's, the overall value of the sum is only logarithmic. that, of course, is gonna complete the proof, 'cause that'll give us an overall bound of two times n times the natural log on n. so it's an n login bound with really quite reasonable constants. so, why is this true? why is this sum only logarithmically large? well, let's do a proof by a picture. i'm going to write this sum. in a geometric fashion. so on the x axis, let me mark off points corresponding to the positive integers. and on the y axis, let me mark off points corresponding to fractions of the form, one over k. and what i?m gonna do is gonna draw a bunch of rectangles. of decreasing area, specifically they all have with one, and the heights are gonna be like one over k. so the area of this guy's one, the area of this guy's one half, the area of this guy's one third, and so on. and now i'm going to overlay on this picture the graph of the function, the continuous function, f of x equals one over x. so notice that is going to go through these three points. it's gonna kiss all of these rectangles on their upper right corners. now what is it we're trying to prove? the claim we're trying to prove is that this sum, one half plus one third and so on, is upper bounded by something, so the sum can be just thought of as the areas in these rectangles, the one half, the one third and so on, and we're going to upper bound it by the area under the blue curve, if you notice the area under the blue curve is at least as big as the sum of the areas of the rectangles because the curve hits each of these rectangles in its north east corner. so putting that into mathematics, the sum from k equal two to n of one over k. is met in above by the integral. and we'll start the area of the curve at one. and then we need it to go all the way up to n. of the function one over x. the x, so that's the area under the curve. and if you remember a little bit of calculus the integral of one over x is the natural log of x. so this equals the natural log of x. evaluated at one. also known as login minus log one. and of course log one would be zero, so that gives us our login. so that completes the proof of the claim. that indeed, the sum of these one over k's is bounded above by the natural log of n, and that in fact completes the proof of the theorem. you've got to be the expected number of comparisons, at most two n times this sum, which is at most log n. and altogether, we find that the expected number of comparisons that quick sort makes on an arbitrary input of length n. is two times n times the natural log of n. so that would be big o of n, log n, with quite reasonable constants. now, this is just the number of comparisons, but as we observed earlier, the running time of quicksort on average is not much more than that, the running time is dominated by the number of comparisons that it makes. moreover, as we discussed when we were talking about the details of the implementation, it works in place, essentially no extra storage is necessary. so that is a complete and mathematically rigorous explanation of just why quicksort. is so quick. 
welcome to part one of our probability review. the first time that we need these concepts in the course, is for those of you who want to understand the analysis of quicksort. why it runs in big o of n log n time on average. and these topics will also come up a couple of other times in the course. for example when we study a randomized algorithm for the minimum cut program in graphs and also when we try to understand the performance of hashing. here are the topics we're going to cover. we'll start at the beginning with sample spaces and then we'll discuss events and their probabilities. we'll talk about random variables, which are real valued functions on a sample space. we'll talk about expectation, which is basically the average value of a random variable. we'll identify and prove a very important property, called the linearity of expectation, which will come up over and over again. in our analyses of randomized processes. so that's going to be the topics for part one. then we'll conclude the video with one example tying these concepts together in load balancing. and this video is by no means the only source you can turn to to learn about these concepts. a couple of other sources i recommend are the online lecture notes by eric lehman and tom leighton. also, there's a wikibook on discrete probability, which you could check out. and i want to be clear this is really not meant to be a course or a tutorial on probability concepts, it's really only meant to be a refresher. so i'm going to go at a reasonably fast pace and it's going to be a pretty cursory presentation. and if you want a more thorough review, i would check out one of these other sources. or your favorite book on discrete probability. and along those same lines, i'm thinking that many of you have seen some of this material before. don't feel compelled to watch this video straight from the beginning to the end. feel free to just sort of dip in and review the concepts that you need a refresher on. so, let's start at the beginning with sample spaces. so what is a sample space? well, we're analyzing random processes so any number of things could happen. and in the sample space is just the collection of all of the things that could happen. so this is basically the universe in which we're going to discuss probabilities and average values. so i'll often use the notation big omega to describe the sample space. so one thing we've got going for us in the design of algorithms is typically we can take omega to be a finite set. so that's why we're dealing only with discrete probability which is a very happy thing. because that's much more elementary than more general probability. in addition to defining the outcomes, everything that could possibly happen, we need to define what is the probability of each individual outcome. so of course the probability of each outcome should be at least zero, should be non-negative. and there's also the obvious constraint that the sum of the probabilities should be one. so exactly one thing is going to happen. now i realize this is a super abstract concept and the next few definitions are also a little abstract. so throughout them i'm going to use two really simple, really concrete examples to illustrate what these concepts mean. so the first example is just going to be you take two six sided dice and you roll them. and of course, the same space is just the 36 different outcomes you could have of these two dice. and assuming that each of these two dice is well crafted, then we expect each of these 36 outcomes to be equally likely, to occur with a probability of one over 36. the second running example i'm going to use is more directly related to algorithms, and it's motivated by the quick sort algorithm. recall that we're studying the implementation of quicksort that chooses a pivot, uniformly a random in every recursive call. so, let's just focus on the very first outer most call of quicksort and think about the random choice of the pivot just in that call. so, then in the sample space all of the different things that could happen is just all of the end different choices for a pivot assuming the array has length n. so we can represent the sample space just as the integer is one two all the way up to n corresponding to the array index of the randomly chosen pivot. and again by definition by the def construction of our code each of these things is equal to likely probability of one over n. now let's talk about events. an events is nothing more than a subsets of all of the things that could happen, that is a subset of the sample space omega. the probability of an event isn't exactly what you think it would be, it's just the sum of the probabilities of all of the outcomes contained in that event. right, so an event is just a bunch of stuff that might happen. we know the probability of each individual thing that can happen, we add them up to get the probability of an event. so the next two quizzes are meant to give you some practice with these concepts. and in particular, they'll ask you to compute the probability of events in our two running examples. so on the first quiz, this is our first running example where we think about two dice and we have our 36 possible outcomes. consider the subset of outcomes in which the sum of the two dice equals 7. what is the probability of that event? right so the correct answer is the third one. the probability is 1/ 6. why is that? well first let's be more precise about what this event is. what are the outcomes in which the sum of the dice is equal to 7? well there's exactly six such outcomes. 1,6 2,5 3,4 4,3 5,2 and 6,1. each of the 36 outcomes is equally likely, has the probability of one over 36. so we have six members of the set. each has probability of one over 36. so the probability is 1/6. let's move onto the next quiz, which considers our second running example, namely, the randomly chosen pivot. and the outermost call to quicksort on an input array of length n. so recall that in quick sort, when you choose a pivot, you then partition the array around the pivot. and this splits the input array into two sub-arrays. a left one. elements less than the pivot. and a right one, those bigger than the pivot. and the more balanced the split into theses two sub problems the better. so ideally we'd like a 50 50 split. so what this quiz asked you is what fraction of pivots, that is what's the probability that a randomly chosen pivot will give you a reasonably good split? meaning both of the sub problems have size at least 25%. that is you get a split 25, 75 or better. that's what this quiz asks about. what's the probability that your randomly chosen pivot satisfies that property? so the correct answer to this quiz is again the third option. it's a 50% probability you get a 25-75 split or better. so to see why let's again be precise about what is the event that we're talking about. then we'll compute its probability. so when does a pivot give you a 25-75 split or better? well for concreteness, suppose the array contained just the integers between one and 100. now, what's the property we want? we want that both of the two subarrays have at least 25% of the elements, neither one has more than 75% of the elements. well, if we choose an element that's 26 or bigger in value. then the left sub-problem will have at least 25 elements, the numbers 1 through 25. and if we choose an element that's at most 75, then the right subarray is going to have at least 25 elements, namely the numbers 76 to 100. so anything between 26 and 75, inclusive, is going to give us a 25-75 split. more generally, any pivot from the middle 50% of the quantiles, is going to give us the desired split. so we do badly if we get something within the first quarter, we do badly if we get something within the last quarter. anything in the middle works. so more formally, we can say that the event s that we're analyzing is among the possible pivot choices. we're interested in the ones that is not in the first quarter and not in the last quarter. now the cardinality of this the number of pivots in this set is essentially half of the overall number of pivot choices. i'm ignoring fractions here for simplicity. the probability of this event is the cardinality of this times the probability of each of the individual outcomes. and since we choose the pivot uniformly at random, each one has a probability of one over n. so you get n/2 / n, or 1/2. now that we've explored the concept of events in our one or two examples. we see that the probability that the sum of two dice is equal to 1/6. a useful fact to know if you're ever playing craps. we know that a pivot gives us a 25-75 split or better in a randomized quick sort with 50% probability. a useful fact if you want to develop intuition for why quick sort is, in fact, quick. that's events. let's move on to random variables. random variables are basically some statistic measuring what happens in the random outcome. formally, if we want to define it. it's a real-valued function defined on the sample space omega. given an outcome, given a realization of the randomness. this gives you back a number. the random variable that we most often care about in algorithm design is the running time of a randomized algorithm. that's the case, for example, with the quick sort algorithm. notice, that is, in fact, a random variable. if we know the state of the world. if we know the outcome of all the coin flips that our code's going to make. then there's just some running time of our algorithm. so, in that sense, it's a random variable. given the outcomes of the coin flips, out pops a number. the running time, say, in milliseconds, of the algorithm. here, i'm going to give you a couple more modest examples of random variables in our two running examples. if we're rolling two dice. one very simple random variable takes as input the outcome, the result of the two dice. and spits out the sum. that's certainly a random variable. on any given outcome, it's going to take on some some integer value between 2, at the minimum, and 12, at the maximum. our second running example is the randomly chosen pivot made by the outermost call to quick sort. let's think about the random variable, which is the size. meaning the subarray length, passed to the first recursive call. equivalently, this random variable is the number of elements of the input array smaller than the randomly chosen pivot. this is a random variable that takes on some interval value between zero, at the smallest. that's if we happen to pick the pivot equal to the minimum of the array. and n-1 at the largest. that's if we happen to pick the maximum element as the pivot element. next, let's talk about the expectation of a random variable. this is really nothing more than the average. of course, when you take the average of some statistic. you want to do it weighted by the probability of its various values. let's just make that precise real quick. consider some random variable, x. the expectation, this is also called the expected value. and the notation is capital e, square bracket, then of the random variable. again, in english, the expectation is just the average value. naturally weighted by the probability of the various possible outcomes. or more mathematically, we sum over everything that could happen. so let i denote one possible outcome. we look at the value of this random variable when that outcome occurs. and then we weight up times the probability of that outcome occurring. the next two quizzes ask you to compute the expectation of the two random variables that we identified on the previous slide. the first quiz is about two dice. and the random variable, which is the sum of the values of those two dice. what is the average of that random variable? what is its expectation? the answer to this question is the second option. the average value is 7. there's a bunch of different ways to see that. in my opinion, the best way to compute this is using linearity of expectation. which is the next concept we're going to cover. if you wanted to, you could just compute this by brute force. by which i mean, you could iterate over all 36 possible outcomes. look at the value of the two dice in each. and just evaluate that sum we had in the definition on the last slide. a slightly sneakier way to do it, if you don't know linearity of expectation. would be to pair up the various outcomes. so it's equally likely that the sum of the two dice is 2 or 12. it's equally likely to be 3 or 11, 4 and 10, and so on. each way of pairing up these values of the two dice results in 14. when you average, you get 7. but, again, the right way to do this is linearity of expectation. which we'll cover next. the second quiz covers the second random variable we identified. now we're back to quicksort. and the random pivot chosen in the outermost call. the question is, how big, on average, an expectation is the subarray in the first recursive call? equivalently, on average, how many elements are going to be less than the randomly chosen pivot? the correct answer to this quiz is the third option. in fact, it's actually quantity n-1 / 2, not n/2. but, basically, half the elements. again, this sort of a sneaky way to see this if you want. which is that, clearly, the two recursive calls are symmetric. the expected value of the left recursive call is going to be the same as the expected size of the right recursive call. the two recursive calls always comprise n-1 of the elements. because they're symmetric, you expect half in each. so n-1 / 2 in each. though for this problem, i think it's perfectly fine just to compute this using the definition of expectation. if we let x denote the random variable that we care about, the subarray size. then we can just compute directly by summing over all of the possible outcomes. all of the possible choices of the pivot. with probability 1/n, we choose the minimum of the pivot. resulting in 0 elements being passed to the first recursive call. with probability 1/n, we pick the second smallest element. resulting in 1 element being passed to the first recursive call. with probability 1/n, we pick the third smallest. giving us a subarray size of 2. and so on. with probability 1/n, we pick the maximum element. giving us a subarray size of n-1. if you just compute this sum out, you will get, as expected, n-1 / 2. expectation is the last definition that i'm going to give you in this part one of the probability review. next, is our fifth and final concept for this video. which is linearity of expectation. that's not a definition. that's more of a theorem. what is linearity of expectation? this is a very simple property of random variables that's super-super-important. this comes up all the time when we analyze randomized algorithms. and random processes, more generally. what is linearity of expectation? it's the following, very simple claim. which i'll sometimes denote just by lin exp, for short. suppose you got a bunch of random variables defined on the same sample space. then, if you want to think of the expected value of the sum of these random variables. it doesn't matter if you take the sum first and then take the expectation. or if you take expectations first and then sum. that is, the expected value of a sum of random variables is equal to the sum of the expectations of the individual random variables. one of the reasons linearity of expectations is so ubiquitously useful is because it always works. no matter what these random variables are. in particular, even when the random variables are not independent. now, i haven't defined independent random variables, yet. that will come in part two, the probability review. but hopefully, you have an intuitive sense of what independence means. things are independent if knowing something about one of the random variables doesn't influence what you expect from the other random variable. now i realize the first time you see linearity of expectation it's a little hard to appreciate. so first of all as far as the applications we'll see plenty throughout this course, pretty much every single application of probability that we'll see the analysis will involve linearity of expectation. but it may be hard to appreciate why this is not a tautology. just symbolically, it may look like it has to be true. but to point out that there is content here, if i replace the sums by products, then this equation would in general be false, if the random variables are not independent. so the same thing is not true about products, it's really about sums. so let me just give you a trivial illustration of linearity of expectation, point out how it really easily allows us to evaluate the sum of two dice. so in our first running example let's introduce the random variables x1 and x2 for the results of the first and second die respectively. now computing the expected value of a single die is easy. there's only six outcomes to a enumerate over contrast that with the 36 outcomes to enumerate over when we evaluated the sum of the two dies. so the average value of a single die you won't be surprised to hear is 3.5 right? so it ranges integers between 1 and 6 uniformly so 3.5 on average. and now using linearity of expectation, the sum of two dice is simply double the average value of a single one. so in the next slide i'm going to prove this property, prove linearity of expectation, but frankly the proof is pretty trivial, so if you don't care about the proof that's fine you can skip it without loss i'm inclusing just for completeness. and i got to say i don't know of another mathematical statement which is simultaneously so trivial to prove and so unbelievably useful. it's really something remarkable linearity of expectations. so how's the proof go, well honestly we just write out the sum, the definition of an expectation, then we reverse the sums, and we're done. so let me start with the right hand side of the equation. so that was the sum of expectations of the random variables. so now let's just apply the definition of expectation. so it's just a weighted average over the possible outcomes. in that one, instead of summing first over the random variable j, and then over realized outcome i, i'm going to do it in reverse order. i'm going to sum first over the outcome i and then over the random variable j. now the probability of outcome i is independent of j so we can yank the p(i) outside of that inner sum. but now what have we got? so inside the parentheses we simply have the value of the sum of the xji's, xj's on the outcome i. and then over here, we're just averaging the sum of the xj's with respect to the probabilities, the pi's. so this is just the definition of the expectation of the sum of the random variables. so that's it. so linearity of expectation is really just a reversal of the double sums. now for those of you that are rusty on these kinds of manipulations i just want to point out, this reversal of the double sum itself is there's nothing complicated at all about what's going on. so if you want a really pedestrian way to think about what's happening, just imagine that we take these sum ends, these xji, pi's. and we just write them out in a grid, where one, or let's just say, the columns are indexed by the random variable j, and the rows are indexed by the outcome i. and in a given cell of this grid, we just write the, sum end, xji times pi. so if you get lost in the notation with these double sums, the point is you can just interpret each of them in terms of this grid. both of these double sums are nothing more than the sum of the values in all of the cells of this grid. one order of summation just says you group first according to row sums and then sum those up. that's the first summation. the second summation, you first take column sums and then sum those up. but of course it doesn't matter, you just get the result of everything in the grid. okay, so there's no tricks up my sleeve when i reverse these sums, it's a totally elementary, trivial thing. okay, so again linearity of expectation, trivial to prove, incredibly useful. don't forget it. so i want to conclude this video with one final example in order to tie together all of the concepts that we've just learned, or just reviewed. and that's going to be an example about load balancing, assigning processes to servers. but this in fact is quite important for the analysis of hashing that we're going to see toward the end of the course as well. but for now lets just think about the following simple problem. for some integer n, you have n computer processes that have to be assigned to n servers in some way. now, you're feeling very lazy, okay, so you're just going to take each of these processes and you're just going to assign it to a totally random server, okay with each server equally likely to get a given process. and the question i want to study is does this laziness cost you, at least on average? so if you look at the server, what's the expected load? so let's proceed to the solution, the answer of this question. so before you start talking about expectations one has to be clear about the sample space and what are the probabilities of the various outcomes. so remember the sample space omega just denotes every possible that could happened. so what are we doing for each process we're assigning to a random server, so all of the things that can happen are all of the different assignments of these n processes to these n servers. and if you think about is there are n raised to the n possible outcomes cause you have n choices for each of the n processes. moreover, because each process is assigned to one of the servers uniformly at random, each of these n to the n assignments is equally likely, probability 1 over n to the n. now that we have a sample space, we're in a position to define a random variable. and we already know what random variable we care about, we care about the average load of the server. now all of the servers are exactly the same, so we just have to focus on one server, let's say the first server, and look at the number of processes assigned to it. and if you go back to the problem statement, what we're asked, is to compute the expected value of y, the expected number of processes assigned to a server. now of course, in principle, we could go to the definition of expectation and just compute by brute force the sum over all possible outcomes of the value of y and take the average. unfortunately, there are n to the n different outcomes, and that's a lot. so what could we do other than this brute force computation? well recall our example of linearity of expectation in the sum of two dice. we observe that instead of computing the sum by enumerating over all 36 outcomes, it was much better to just focus on a single die, compute its expectation and then conclude with linearity of expectation. so we'll do the same thing here. instead of focusing on the sum y, we'll focus on constituent parts of y. so whether or not a single process gets assigned to the first server. and then we'll get away with that by linearity of expectation. so more precisely, for a given process j let's define xj to be, whether to be 1, if and only if the jth process gets assigned to the first server 0 otherwise. zero, one random variables like xj are often called indicator random variables. that's because they, in effect, indicate whether or not a certain event occurs. in this case, whether or not the jth process gets assigned to the first server. why did i make this definition? well, observe that the total number of processes that gets assigned to the first server is simply the sum, j equal 1 to n of xj, xj says whether or not a given process, the jth process, is on the first server. the total number is just the sum of these over all j. now, the benefit from this maneuver is we only have to compute the expectation of a extremely simple indicator random variable xj. this is like the win that we got when we were summing up two dice, by instead of having to compute the sum, the expected value of the sum, we just had to focus on the expectation of a single die, that was really easy. similarly here, the expectation of a single xj is really easy. specifically, let's write it out just using the definition of the expectation. so the expected value of an xjis well let's group together all the outcomes in which it takes on the value zero. so the contribution of the expectation is zero for all of those outcomes, and then there's the rest of the outcomes where xj takes on the value one and in those cases it contributes one to the expectation. now obviously we get some happy cancellation happening here with the zero part. and all we have to worry about is the probability that xj takes on the value one. okay what was xj again, how did we define it? remember it's the events that, it's 1 exactly when the jth process gets assigned to the first server. how are processes assigned? well remember the proposed solution assigned to each process to each of the n servers, equally likely with uniform probability. so the probability of the jth process is assigned to the first service is 1 over n. so this leaves us with just the sum from j equal 1 to n of 1 over n. that is we just sum up 1 over n with itself n times, this of course is equal to 1. so at the end of the day what we find is that the expected number of processes assigned to a given server say the first server is just 1. so at least if we only care about averages, we lose very little from this trivial process of randomly spraying the process to the server. on average, any given server has just one process on it. this is characteristic of the role that randomization plays in algorithm design in computer science more generally. often we can get away with really simple heuristics just by making random choices. of course, quicksort is one example of that where we get an extremely, prevalently used practically sorting algorithm just by making it randomly chosen pivets in every recursive call. 
so welcome to part two of our probability review. this video assumes you've already watched part one or at least are familiar with concepts covered in part one. namely sample spaces, events, random variables, expectation and linearity of expectation. in this part of the review we're going to be covering just two topics. conditional probability and the closer related topic of independence. both between events and between random variables. i want to remind you that this is by no means the only source you can or should use to learn this material. a couple of other sources free that i recommend are lecture notes that you can find online by eric. and also there's a wiki book on discrete probability. so, conditional probability, i hope you're not surprised to hear, is fundamental to understanding randomized algorithms. that said, in the five weeks we have here, we'll probably only use it once. and that's in analyzing the correctness of the random contraction algorithm for computing the minimum cut of an undirected graph. so, just to make sure we're all on the same page, here's some stuff you should have learned, from part one of the probability review. you should know what a sample space is. this represents all of the different outcomes of the random coin flips, all of the different things that could happen. often in randomized algorithm analysis, this is just all of the possible random choices that the algorithm might make. each outcome has some known probability [inaudible]. by and, of course, the sum of the probabilities equal one and remember that event is nothing more than a subset of omega. omega is everything that could possibly happen. s is some subset of things that might happen and, of course, the probability of event is just the probability of, of all the outcomes that the event contains. so, let's talk about conditional probability. so one discusses the conditional probability of one event given a second event. so, let x and y denote two events, subsets of the same sample space. you might want to think about these two events x and y in terms of an event diagram. so we could draw a box, representing everything that could conceivably happen. so that's omega. then we can draw a blob corresponding to the event x. so that's some stuff. might or might not happen, who knows. and then the other event y is some other stuff which might or might not happen. and in general these two events could be disjoint, that is they could have no intersection. or they might have a non-trivial intersection. x intersect y. similarly they need not cover omega. it's possible that nothing x nor y happens. so what's we're looking to define is the probability of the event x given the even y. so we write probability of x bar y, phrased x given y. and the definition is, i think, pretty intuitive. so given y means we assume that something in y happened. originally anything in omega could have happened. we didn't know what. now we're being told that whatever happened that lies somewhere in y. so we zoom in on the part of the picture that, in which contains y. so that's gonna be our denominator. so, our new world is the stuff in y. that's what we know happened. and now we're interested in the proportion of y that is filled up with x. so, we're interested in what fraction of y's area is occupied by stuff in x. so x intersect y, divided by the probability of y. that is by definition the conditional probability of x given y. let?s turn to a quiz, using our familiar example of rolling two dice. to make sure that the definition of conditional probability makes sense to you. okay, so the correct answer to this quiz is the third answer. so let's see why that is. so what are the two events that we care about? we want to know the probability of x given y, where x is the event that at least one die is a one. and y is the events that the sum of the two dice is seven. now, the easiest way to explain this is let's zoom in, let's drill down on the y. let's figure out exactly which outcomes y comprises. so the sum of the two dice, being seven, we saw in the first part of the review, there's exactly six outcomes which give rise to the sum seven, namely the ordered pairs one, six. two five, three four, four three, five two, and six one. now, remember that the probability. of x given y is by definition the probability of x intersect y divided by the probability of y. now, what you notice from this formula is we actually don't care about the probability of x per se or even about the event x per se, just about x intersect y. so, let's just fig, so, now we know why there has to be six outcomes. which of those also belong to x? well, x is those where at least one die is one. so, x intersect y is just going to be the one, six and the six, one. now the probability of each of the 36 possible outcomes is equally likely. so each one is one over 36. so since x intersects y, has only two outcomes. that's gonna give us two over 36 in the numerator. since y has six outcomes, that gives us a six over 36 in the denominator. when you cancel everything out, you're left with a one third. so just applying the definition of conditional probability to the correct definition of the two relevant events, we find that indeed a third of the time is when you have a one condition on the sum of the two being seven. let's move on to the independence of two events. so. again we consider two events, x and y. by definition, the events are dependent if and only if the following equation holds. the probability that both of them happen. that is the probability of x intersect y is exactly equal to the probability that x happens times the probability that y happens. so that's a simple innocuous looking definition. let me re phrase it in a way that it's even more intuitive. so i'll you check this, it's just a some trivial algebra. this equation holds, for the events x and y, if and only if, this is just using the definition of conditional probability we had on the last slide, if and only if the probability of x given y, is exactly the same thing as the probability of x. so, intuitively, knowing that y happens, gives you no information about the probability that x happens. that's the sense in which x and y are independent. and, you should also check that this holds if and only if, the probability of y, given x, equals the probability of y. so, symmetrically, knowing that x has occurs gives you no information, no new information about whether or not y has occurred. the probability of y is unaffected by conditioning on x. so at this juncture i feel compelled to issue a warning. which is, you may feel like you have a good grasp of independence. but, in all likelihood, you do not. for example i rarely feel confident that i have a keen grasp on independence. of course i use it all the time in my own research and my own work, but it's a very subtle concept. your intuition about independence is very often wrong, even if you do this for a living. i know of no other source that's created so many bugs in proofs by professional mathematicians and professional computer science researchers as misunderstandings of independence and using intuition instead of the formal definition. so, for those of you without much practice with independence, here's my rule of thumb for whether or not you treat random variables as independent. if things are independent by construction, like, for example, you define it in your algorithm, so the two different things are independent. then you can proceed with the analysis under the assumption that they're independent. if there's any doubt, if it's not obvious the two things are independent, you might want to, as a rule of thumb, assume that they're dependent until further notice. so the slide after next will give you a new example showing you things which are independent and things which are not independent. but before i do that i wanna talk about independence of random variables rather than just independence of events. so you'll recall a random variable is from the first video on probability review. it's just a real value function from the sample space to the real numbers. so once you know what happens you have some number. the random variable evaluates to some real number. now, what does it mean for two random variables to be independent? it means the events of the two variables taking on any given pair of values are independent events. so informally, knowing the value taken on by one of the random variables tells you nothing about what value is taken on by the other random variable. recalling the definition of what it means for two events to be independent, this just means that, the probability that a takes on value little a, b takes on value little b. the probability that both of those happen is just the product of the probabilities that each happens individually. so what's useful about independence of events is that probabilities just multiply. what's useful about independence of random variables is that expectations just multiply. so, we're going to get an analog of linear expectation where we can take, we can interchange an expectation in the product freely, but i want to emphasize this, this interchange of the expectation of the product is valid only for independent random variables and not in general, unlike linear expectations. and we'll see a non example. we'll see how this fails on the next slide for non independent random variables. so, i'll just state it for two random variables, but the same thing holds by induction for any number of random variables. if two random variables are independent, then the expectation of their product. equals the product of their expectations. and again, do not forget that we need a hypothesis. remember, linearity of expectations did not have a hypothesis for this statement about products. we do have a hypothesis of them being independent. so why is this true? well, it's just a straight forward derivation where you follow your nose or write it out here for completeness, but, but i really don't think it's that important. so you start with the expectation of the product. this is just the average value of a times b, of course weighted by the probability of, of any particular value. so the way we're gonna group that sum is we're going to sum over all possible combinations of values, a and b, that capital a and capital b might take on, so that's gonna give us a value of a times b. times the probability of that big a takes on the value of little a and capital b takes on the value of little b. so that's just by definition where this is the value of the random variable, capital a times capital b and this is the probability that it takes on that value with the values a and b. now because a and b are independent, this probability factors into the product of the two probabilities. this would not be true if they were not independent. it's true because they're independent. so same sum where all possible joint values of all a and b. you still have a times b. but now we have times the probability that a takes on the value of a times the probability that b takes on the value of b. so now we just need to regroup these terms. so let's first sum over a. let's yank out all the terms that depend on little a. notice none of those depend on little b. so we can yank it out in front of the sum over little b. so i have an a times the probability that big a takes on the value of little a. and then the stuff that we haven't yanked out is the sum over b, of b times, little b times the probability that capital b takes on the value little b. and what's here inside the quantity? this is just the definition of the expectation of b. and then what remains after we have factored out the expectation of b? just this other sum which is the definition of the expectation of a. so, indeed four independents random variables, the expected value of the product is equal to the product of the expectations. let's now wrap up by tying these concepts together in an example, a simple example that nevertheless illustrates how it can be tricky to figure out what's independent and what's not. so here's the set up. we're going to consider three random variables. x1, x2 and x3. x1 and x2 we choose randomly, so they're equally likely to be zero or one. but x3 is completely determined by x1 and x2. so it's gonna be the xor of x1 and x2. so xor stands for exclusive or. so what that means is that if both of the operands are zero, or if both of them are one, then the output is zero. and if exactly one of them is one, exactly one of them is zero, then the output is one. so it's like the logical or function, except that both of the inputs are true, then you output false, okay? so that's exclusive or. now this is a little hand wavy, when we start talking about probabilities, if we want to be honest about it, we should be explicit about the sample space. so what i mean by this, is that x1 and x2 take on all values, they're equally likely. so we could have a zero, zero or a one zero or a zero one or a one, one and in each of these four cases, x3 is determined by the first two, as the x or, so you get a zero here, a one here, a one here and a zero there. and each of these four outcomes is equally likely. so let me now give you an example of two random variables, which are independent, and a non example. i'll give you two random variables which are not independent. so first, i claim that, if you think that x1 and x3, then they're independent random variables. i'll leave this for you to check [sound]. this may or may not seem counter-intuitive to you. remember x3 is derived in part from x1. never the less, x1 and x3, are indeed independent. and why is that true? well, if you innumerate over the four possible outcomes, you'll notice that all four possible two byte strings occur as values for one and three. so here they're both zero, here they're both one, here you have a zero and one, and here you have a one and zero. so you've got all four of the combinations of probability one over four. so it's just as if x1 and x3 were independent fair coin flips. so that's basically why the claim is true. now. that's a perhaps counterintuitive example of independent random variables. let me give you a perhaps counterintuitive example of dependent random variables. needless to say, this example just scratches the surface and you can find much more devious examples of both independent and non-independents if you look in, say, any good book on discrete probability. so now let?s consider the random variable x1 product x3. and x two and the claim is these are not independent. so this'll give you a formal proof for. the way i'm going to prove this could be slightly sneaky. i'm not going to go back to the definition. i'm not gonna contradict the consequence of the definition. so it's proved that they're not independent all i need to do, is show that the product of the expectations is not the same as the expectations to the products. remember if they were independent, then we would have that equality. [inaudible] product of expectations will equal the expectation to products. so if that's false than there's no way these random variables are independent. so the expectation of the product of these two random variables is just the expected value of the product of all three. and then on the other side, we look at the product of the expected value of x1 and x3. and the expected value of x2. so let's start with the expected value of x2. that's pretty easy to see. that is zero half the time and that is one half the time. so the expected value of x2 is going to be one-half. how about the expected value of x1 and x3? well, from the first claim, we know that x1 and x3 are independent random variables. therefore, the expected value of their product is just the product of their expectations. equal this expectations equal to the expected value of x1 times the expected value of x2, excuse me, of x3. and again, x1 is equally likely to be zero or one. so its expected value is a half. x3 is equally likely to be zero or one so its expected value is a half. so the product of their expectations is one-fourth. so the right-hand side here is one-eighth; one-half times one-fourth, so that's an eighth. what about the left-hand side, the expected value of x1 times x3 times x2? well, let's go back to the sample space. what is the value of the product in the first outcome? zero. what is the value of the product in the second outcome? zero. third outcome? zero. forth outcome? zero. the product of all three random variables is always zero with probability one. therefore, the expected value, of course, is gonna be zero. so indeed, the expected value of the product of x1, x3 and x2 zero does not equal to the product of the corresponding expectations. so this shows that x1, x3 and x2 are not independent. 
i've said pretty much everything i want to say about sorting at this point but i do want to cover one more related topic. namely the selection problem. this is a problem of computing ordered statistics of an array with computing the median of an array being a special case. analogous to our coverage of quick sort the goal is going to be the design and analysis of a super practical randomized algorithm that solves the problem. and this time, we'll even achieve an expected running time that is linear in the length of the input array. that is big o of n for input arrays of length n, as opposed to the o of n log in time that we had for the expected running time of quick sort. like quick sort, the mathematical analysis is also going to be quite elegant. so in addition these two required videos on this very practical algorithm will motivate two optional videos that are on very cool topics but of a similar more theoretical nature. the first optional video is going to be on how you solve the selection problem in deterministic linear time. that is without using randomization. and the second optional video will be a sorting lower bound that is why no comparison based sort can be better than mergeshort. can have better running time than big o of n login. so a few words about what you should have fresh in your mind before you watch this video. i have definitely assuming that you watched quicksort videos. and not just watched them but that you have that material pretty fresh in your mind. so in particular the video of quicksort about the partition subroutine, so this is where you take a input ray and you choose a pivot and you do repeated swaps. you rearrange the array so that everything less then the pivot is to the left of it. everything bigger then the pivot is to the right of it. you should remember that sub routine, you should also remember the previous discussion about pivot choices. the idea that the quality of a pivot depends on how balanced a split into two different sub problems it gives you. those are both going to be important. for the analysis of this randomized linear time selection algorithm i need you to remember the concepts from probability review part one. and particular random variables, their expectation, and linearity of expectation. that said, let's move on and formally define what the selection problem is. the input is the same as for the sorting problem, just you're giving it array of indistinct entries. but in addition, you're told what order statistic you're looking for. so that's going to be a number i, which an integer between 1 and n. and the goal is to output just a single number. namely the ith order statistic, that is the ith smallest entry in this input array. so just to be clear, if you had an array entry of let's just say 4 elements, containing the numbers 10, 8, 2 and 4. and you were looking for, let's say, the 3rd or a statistic that would be this 8. the first order statistic is just the minimum element of the array. that's easier to find with a linear scan. the nth order statistic is just the maximum, again easier, easy to find with a linear scan. the middle element is the median. you should think of that as the canonical version of the selection problem. now when n is odd, it's obvious what the median is, that's just the middle element, so the n plus one over 2th order statistic. if the array has even length, there's two possible medians, so let's just take the smaller of them, that's the n over 2th order statistic. you might wonder why you'd ever want to compute the median of an array rather than the mean, that is the average. it's easy to see you that you can compute the average just with a simple linear scan. and the median you can, one motivation is it's a more robust version of the mean. so if you just have a data entry problem and it corrupts one element of an input array it can totally screw up the average value of the array, but it has generally very little impact on the median. final comment about the problem is that i am going to assume that the array entries are distinct, that is there's no repeated elements. but just like in our discussions of sorting, this is not a big assumption. i can encourage you to think about how to adapt these algorithms to work even if the arrays do have duplicate. you can, indeed, still get the same very practical, very fast algorithms with duplicate elements. now if you think about it, we already have a pretty darn good algorithm that solves the selection problem. here's the algorithm. it's two simple steps and it runs in o of n log n time. step one, sort the input array. we have various subroutines to do that. let's say we pick mergesort. now, what is it we're trying to do? we're trying to the ith smallest element of the input array. well, once we've sorted it we certainly know where the ith smallest element is, it's in the ith position of the sorted array. so that's pretty cool, we've just done what a computer scientist would call a reduction and that's a super useful and super fundamental concept. it's when you realize that you can solve one problem by reducing it to another problem that you already know how to solve. so what we just showed is that the selection problem reduces easily to the sorting problem. we already know how to solve the sorting problem n log n time so that gives an n log n time solution to this selection problem. but again remember the mantra of any algorithm designer worth their salt, is can we do better. we should avoid contentedness. just because we got nlogn we should stop there. maybe can be even faster. now certainly we're going to have to look at all the elements in the input array, in the worst case. you shouldn't expect to do better than linear, but hey, why not linear time? actually if you think about it, we probably should have asked that question back when we were studying the sorting problem. why were we so content with the end login time bound for merch sort. and the o of n login time on average bound, for quick sort. well it turns out, we have a really good reason to be happy with our n login upper bounds for the sorting problem. it turns out and this is not obvious, and will be the subject of the optional video. you actually can't sort an input array of length n better than n log n time. either in the worst case or an average. so another words, if we insist on solving the selection problem via a reduction to the sorting problem then we're stuck with this n log n time bound. okay, strictly speaking that's for something called comparison sorts, see the video for more details but the upshot is if you want a general purpose algorithm. and we want to do better than n log n for selection we have to do it using ingenuity beyond this reduction, we have to prove that selection is a strictly easier problem then sort it. that's the only way we're going to have an algorithm that beats n log n. that's the only way we can conceivably get a linear time algorithm. and that is exactly what is up next on our plates. we're going to show selection is indeed fundamentally easier than sorting. we can have a linear time algorithm for it, even though we can't get a linear time algorithm for sorting. you can think of the algorithm we're going to discuss as a modification of quick sort and in the same spirit of quick sort it will be a randomized algorithm. and the running time will be an expected running time that will hold for any input array. now, for the sorting problem we know that quick sort that's n log in time on average, where the average is over the coin flips done by the code. but we also know that if we wanted to, we could get a sorting algorithm in n log n time that doesn't use randomization. the merge sort algorithm is one such solution. so here, we're giving a linear time solution for selection, for finding order statistics that uses randomization. and it would be natural to wonder, is there an analog to merge sort? is there an algorithm which does not use randomization, and gets this exact same linear time down. in fact there is. the algorithm's a little more complicated, and therefore not quite as practical as this randomized algorithm. but it's still very cool. it's a really fun algorithm to learn and to teach. so i will have an optional video about linear time selection without randomization. so for those of you who aren't going to watch that video or want to know what's the key idea. the idea is to choose the pivot deterministically in a very careful way using a trick called the median of medians. that's all i'm going to say about it now you should watch the optional video if you want more details. i do feel compelled to warn you that if you're going to actually implemented a selection algorithm. you should do the one that we discuss in this video, not the linear time one. because the one we'll discuss in this video has both smaller constants and works in place. so what i want to do next is develop the idea that can modify the quicksort paradigm in order to directly solve the selection problem. so to get an idea of how that works, let me review the partition subroutine. like in quicksort this subroutine will be our workhorse for the selection algorithm. so, what the partition subroutine does, it takes as inputs, some jumbled up array and it's going to solve a problem which is much more modest than sorting. so in partitioning, it's going to first choose a pivot element somehow. we'll have to discuss what's a good strategy for choosing a pivot element. but suppose in this particular input array it chooses the first element, this three, as the pivot element, the responsibility of the partition sub-routine then is to rearrange the elements in this array so that the following properties are satisfied. anything less than the pivot is to the left of it and it can be in jumbled order. but if you're less than pivot you better be to the left like this two and one is less than three. if you're bigger than the pivot than again you can be in jumbled order amongst those elements but all of them have to be to the right of the pivot and that's true for the numbers four through eight. they all are to the right of the pivot three in a jumbled order. so this in particular puts the pivot in its rightful position, where it will belong in the final sorted array. and at least for quicksort, it enabled us to recursively sort to smaller subproblems. so this is where i want you to think a little bit about how we should adapt this paradigm. so, suppose i told you the first step of our selection algorithm is going to be choose a pivot and partition the array. now the question is, how are we going to recurse? we need to understand how to find the ith order statistic of the original input array. it suffices to recurse on just one sub problem of smaller size, and find a suitable or a statistic in it. so how should we do that? let me ask you that with some very concrete examples. about what pivot we choose and what order statistic we're looking for and see what you think. so the correct action to this quiz is the second answer. so we can get away with recursing just once, and then this particular example, we're going to recurse on the right side of the array. and instead of looking for the fifth order statistic like we would originally, we're going to recursively search for the second order statistic. so why is that? well first why do we recurse on the right side of the array? so by assumption we have this array of ten elements, we choose the pivot, we do partitioning, remember the pivot winds up in its rightful position. that's what partitioning does. so in the bid it winds up in the third position, we know it's the third smallest element in the array. now that's not what we were looking for. we were looking for the fifth smallest element in the array. that, of course, is bigger than the third smallest element of the array. so by partitioning, where is the fifth element going to be? it's gotta be to the right of this third smallest element, to the right of the pivot. so we know for sure that the fifth order statistic of the original array lies to the right of the pivot. that is guaranteed. so we know where to recurse on the right hand side. now, what are we looking for? we are no longer looking for the fifth order statistic, the fifth smallest element. why? well we've thrown out both the pivot and everything smaller than it. remember we're only recursing on the right hand side. so we've thrown out the pivot, the hird element, and everything less than it, the minimum and the second minimum. having deleted the three smallest elements and originally looking for the fifth smallest of what remains, of what we're recursing on. we're looking for the second smallest element. so the selection algorithm in general, is just the generalization of this idea. so arbitrary arrays and arbitrary situations of whether the pivot comes back equal to less or bigger than the element you are looking for. so let me be more precise, i am going to call this algorithm r select for randomized selection, and according to the problem definition it takes as input, as usual an array a of some length n. then also the order statistic that we are looking for, so we are going to call that i, and of course we assume that i is some integer between one and inclusive. so for the base case, that is going to be if the array has size one, then the only element we could be looking for is the oneth order statistic and we just return the sole element of the array. now we have to partition the array around the pivot element. and just like in quick sort, we're going to very lazy about choosing the pivot. we're going to choose it uniformly at random from the n possibilities, and hope things work out. and that will be the crux of the analysis, proving that random pivots are good enough sufficiently often. having chosen the pivot, we now just invoke the standard partitioning and subroutine. as usual, that's going to give us the partitioned array. you'll have the pivot element, you'll have everything less in the pivot to the left, everything bigger, to the right. as usual, i'll call everything to the left, the first parts of the partitioned array. and everything bigger, the second part. now we have a couple of cases, depending on whether the pivot is bigger or less then the element we are looking for. so i need a little notation to talk about that. so let's let j be the order statistic that p is. so if p winds up being the third smallest element like in the quiz, then j's going to be equal to three. equivalently we can think of j as defined as the physician of the pivot in the partition version of the array. now there's one case, which is very unlikely to occur, but we should include it just for completeness. if we're really lucky, then, in fact, a random pivot just happens to be the order statistic we were looking for. that's when i equals j. we're looking for the ith smallest element. if by dumb luck the pivot winds up being the ith smallest element, we're done. we can just return it. we don't have to recurse. now in general of course, we don't randomly choose the element we are looking for. we choose something that, that could be bigger or could be smaller then it. in the quiz we chose a pivot that was smaller then what we were looking for. actually, that's the harder case. so, let's first start with a case, where the pivot winds up being bigger then the element we were looking for. so that means that j is bigger than i. we're looking for the i smallest. we randomly chose the j smallest for j bigger than i. so this is the opposite case of the quiz. this is where we know what we're looking for has to be to the left of the pivot. the pivot's the j smallest everything less than is to the left. we're looking for the i smallest, i is less than j, so that's got to be on the left. that's where it recurs. moreover it clear we're looking for exactly the same order statistic. if we're looking for the third smallest element, we're only throwing out stuff which is bigger than something even bigger hthan the third smallest element so we're still looking for the third smallest of what remains. and naturally the new array size is j minus 1 because that's what's to the left of the pivot. and then finally, the final case is when the random element that we choose is less than what we're looking for and then we're just like the quiz. namely what we're looking for is bigger than the pivot. it's got to be in the right-hand side. we know we've got a recurse in the right-hand side. whenever the right-hand side has n minus j elements, we throw out everything up to the pivot. so we throw out j things. there's n minus j left. all of those j things we threw out are less than what we're looking for. so if we used to be looking for the i smallest element now we're looking for the i minus j smallest element. so that is the whole algorithm. that is how we adopt the approach we took to the sorting problem in quick sort and adapt it to the problem of selection. so, is this algorithm any good? let's start studying its properties and understand how well it works. so let's begin with correctness. so the claim is that, no matter how the algorithm's coin flips come up, no matter what random pivots we choose, the algorithm is correct. in the sense that it's guaranteed to output the ith order statistic. the proof is by induction. it proceeds very similarly to quick sort. so i'm not going to give it here. if you're curious about how these proofs go, there's an optional video about the correctness of quick sort. if you watch that and understand it, it should be clear how to adapt that inductive argument to apply to this select algorithm as well. so as usual for divide and conquer algorithms, the interesting part is not so much knowing, understanding why the algorithm works, but rather understanding how fast it runs. so the big question is, what is the running time of this selection algorithm? now, to understand this we have to understand the ramifications of pivot choices on the running time. so you've seen the quicksort videos they're fresh in your mind so what should be clear is that just like in quicksort how fast this algorithm runs is going to depend on how good the pivots are and what good pivots means is pivots that guarantee a balanced split. so, the next quiz, we'll make sure that you understand this point and ask you to think about just how bad the running time of the selection algorithm could be if you get extremely unlucky in your pivot choices. so the correct answer to this question is exactly the same as the answer for quicksort. the worst case running time, if the pivots are chosen just in a really unlucky way. is actually quadratic in the array length. remember, we're shooting for linear time. so this quadratic is a total disaster. so how could this happen? well suppose you're looking for the median, and suppose you choose the minimum element as the pivot every single time. so if this is what happens, if every time you choose a pivot to be the minimum, just like in quicksort, this means every time you recurse, all you succeed in doing is peeling off a single element from the input array. now, you're not going to find the median element until you've roughly n over 2 recursive cause, each on an array that has size at least a constant fraction of the original one. so it's a linear number of recursive calls, each on an array of size at least some constant times n. so that gives you a total running time of quadratic overall. of course, this is an absurdly unlikely event. frankly, your computer is more likely to be struck by a meteor than it is for the pivot to be chosen as the minimum element in every recursive call. but if you really have an absolutely worst case choice of pivots, it would give this quadratic run time down. so the upshot then is that the running time of this randomized selection algorithm depends on how good our pivots are. and for a worse case chose of pivots the running time can be as large as m squared. now hopefully most of the time we're going to have much better pivots. so the analysis receives by making that idea precise. so the key to a fast running time is going to be the usual property that we want to see in the divide and conquer algorithms, namely every time that recurse the problem size better not just be smaller but it better be smaller by a significant factor. how would that happen in the selection approach based on the partition subroutine? well if both of the sub-problems are not too big, then we're guaranteed that when we recurse we make a lot of progress. so let's think about what the best possible pivot would be in the sense of giving a "balanced" split, right, so of course in some sense the best pivot is you just choose your statistic group you're looking for. then you're done in constant time. but that's extremely unlikely, and it's not worth worrying about. so ignore the fact that we might guess the pivot. what's the best pivot if we want to guarantee an aggressive decrease in the input side for the next iteration. well, the best pivot is the one that gives as most balanced split as possible. so what's the pivot that gives us the most balanced split? a 50/50 split. if you think about it it's exactly the median. of course, this is not super-helpful, because the median might well be what we're looking for in the first place. so this is sort of a circular idea. but for intuition, it's still worth exploring what kind of running time we would get in the best case, right? if we're not going to get linear time even in this magical best case, we certainly wouldn't expect to get it on average over random choices of the pivots. so what would happen if we actually did luckily choose the median as the pivot every single time? well we get the recurrence that the running time that the algorithm requires at a rate of length n. well, there's only going to be one recursive call. so this is the big difference from quicksort where we had to recurse on both sides and we had two recursive calls. so here, we're only going to have one recursive call. in the magical case where our pivots are always equal to the median, both sub-problem sizes are only half as large as the original one. so when we recurse, it's on a problem size guaranteed. could be at most n over two and then outside the recursive call pretty much all we do is a partitioning invocation, and we know that that is linear time. so the recurrence we get is t of n is the most t of n over two plus big o of n. this is totally ready to get plugged into the master method. it winds up being two of the master method and indeed we get exactly what we wanted, linear time. to reiterate this is not interesting in its own right. this is just for intuition. this was a sanity check that at least for a best case choice of pivots we'd get what we want, the linear time algorithm and we do. now, the question is how well do we do with random pivots? now the intuition, the hope is exactly as it was for quicksort which is the random pivots are perfectly good surrogate for the median, for the perfect pivot. so having the analysis of quicksort under our belt where indeed random pivots do approximate very closely to the performance you get with best case pivots maybe now we have reason to believe that this is hopefully true. that said, as a mathematical statement this is totally not obvious and it's going to take a proof. that's the subject for the next video. let me just be clear exactly what we're claiming. here is the running time guarantee the random rselection provide. for an arbitrary input array of input length n, the average running time of this randomized selection is linear. big o of n. let me reiterate a couple of points i made for the analogous guarantee for the quicksort algorithm. the first is that we're making no assumptions for data whatsoever. in particular we're not assuming that the data is random. this guarantee holds, no matter what input array you feed into this randomized algorithm. in that sense, this is a totally general purpose subroutine. so where then does this averaging come from? where does the expectation come from? the randomness is not in the data, rather, the randomness is in the code. and we put it there ourselves. now let's proceed to the analysis. 
in this video i'll explain the mathematical analysis of the randomized linear time selection algorithm that we studied in the previous video. specifically, i'm going to prove to you the following guarantee for that algorithm. for every single input array of length n the running time of this randomized selection algorithm on average will be linear. pretty amazing if you think about it because that's barely more what the time it takes just to read the input. and in particular this linear time algorithm is even faster than sorting. so this shows that selection is a fundamentally easier problem than sorting. you don't need to reduce to sorting. you can solve it directly in o(n) time. i want to reiterate the same points i made about quick sort. the guarantee is the same. it is a general purpose subroutine. we make no assumptions about data. this theorem holds no matter what the input array is. the expectation, the average that's in the theorem statement is only over the coin flips made by the algorithm made inside it's code of our own devising. before we plunge into the analysis, let me just make sure you remember what the algorithm is. so it's like quick sort. we partition around a pivot except we only recurse once, not twice. so we're given an array with some length n. we're looking for the ith order statistic, the ith smallest element. the base case is obvious. you're not in the base case; you choose a pivot p, uniformly at random from the input array just like we did in quick sort. we partition around the pivot just like we did in pic, in quick sort. that splits the array into a first part of those elements less than the pivot and the second part of those elements which are bigger than the pivot. now, we have a couple of cases. the case which is very unlikely so we don't really worry about, if we're lucky enough to guess the pivot as the ith order statistic what we're looking for. that's when the new position j. of the pivot element happens to equal i. what we're looking for. then, of course, we just return it. that was exactly what we wanted. in the general case, the pivot is going to be in the position j, which is either bigger than what we're looking for i, that's when the pivot is too big or j. it's position will be less than the ith order statistic we're looking for. that's when the pivot is too small. so if the pivot's too big, if j is bigger than i  that when we're looking for is on the left hand side amongst the elements less than the pivot. so that's where we recurse. we've thrown out both the pivot and everything to the right of it. that leaves us with an array of j minus i elements and we're still looking for the ith smallest among these j minus1 smallest elements. and then the final case, this is what we went through in the quiz and last video, is if we choose a pivot who's smaller than what we're looking for, that's when j is less than i, then it means we're safe to throw out the pivot and everything less than it. we're safe recursing on the second part of those elements bigger than the pivot. having thrown out the j's smallest elements, we're recursing on an element of length of n-j and we're looking for the i-j smallest element in those that remain, having already thrown out the j smallest from the input array. so that's randomized selection. let's discuss why it's linear time on average. the first thought that you might have, and this would be a good thought, would be that we should proceed exactly the same way that we did in quick sort. you recall that when we analyzed quick sort, we set up these indicator random variables, x, i ,j determining whether or not a given, pair of elements got compared at any point in the algorithm. and then we just realized the sum of the comparisons is just the sum, overall, of these x,i, js. we applied linearity of expectation and it boiled down to just figuring out the probability that a given pair of elements gets compared. you can analyze this randomized selection algorithm in exactly the same way. and it does give you a linear time bound on average. but it's a little messy. it winds up being not quite as clean as in the quick sort analysis. moreover, because of the special structure of the selection problem, we can proceed in an even more slick way here than the way we did with quick sort. so, again we'll have some constituent random variables. we'll again apply linearity of expectation but the definition of those random variables is going to be a little bit different than it was in quick sort. so, first a preliminary observation. which is that the workhorse for this randomized selection procedure is exactly the same as it was in quick sort. namely it's the partition subroutine. essentially all of the work that gets done outside of the recursive call just partitions the input array around some pivot element as we discussed in detail in a separate video that takes linear time. so usually when we say something's linear time we just use big o notation. i'm gonna go ahead and explicitly use a constant c here for the operations outside the recursive call. that'll make it clear that i'm not hiding anything up my sleeves when we do the rest of the analysis. now what i wanna do on this slide is introduce some vocabulary, some notation which will allow us to cleanly track the progress of this recursive selection algorithm. and by progress i mean. the length of the array on which is currently operating. remember we're hoping for a big win over quick sort, cuz here we only do one recursive call, not two. we don't have to recurse on both sides of the pivot just on one of them. so it stands to reason, that we can think about the argument making more and more progress as a single recursive calls operating on arrays of smaller and smaller length. so the notion that will be important for this proof is that of a phase. this quantifies how much progress we've made so far, with higher numbered phases corresponding to more and more progress. we'll say that the r select algorithm at some midpoint of its execution is in the middle of phase j. if the array size that the current recursive call is working on has length between 3/4th raised to the j times n and the smaller number 3/4th j+1 times n. for example think about the case where j equals zero. that says phase zero recursive calls, operate on arrays with size of n and 75 percent of n. so, certainly, the outermost recursive call is going to be in phase zero. because the input array has size n. and then, depending on the choice of the pivot, you may or may not get out of phase zero in the next recursive call. if you choose a good pivot, and you wind up recursing on something, that has, at most, 75 percent of the original elements, you will no longer be in phase zero. if you recurse on something that has more than 75 percent of what you started with, of the. input array, then you're still gonna be in phase zero even in the second recursive call. so overall the phase number j, quantifies the number of times we've made 75 percent progress, relative to the original input array. and the other piece of notation that's going to be important is what i'm going to call xj. so for a phase j, xj simply counts the number of recursive calls in which a randomized selection algorithm is in phase j. so this is gonna be some integer. it could be as small as zero, if you think about it, for some of the phases. or it could be larger. so why am i doing this? why am i making these definitions of phases and of these xjs? what's the point? we're gonna remember the point was we wanna be able to cleanly talk about the progress that the randomized selection algorithm makes through its recursion, and what i wanna now show you is that in terms of these xjs, counting the number of iterations in each phase, we can derive a relatively simple upper bound on the number of operations that our algorithm requires. specifically the running time of our algorithm, can be bounded above by the running time in a given phase, and then summing those quantities over all of the possible phases. so we're gonna start with a big sum, over all the phases j. we want to look at the number of recursive calls that we have to endure in phase j, so that's xj by definition. and then we look at the work that we do outside of the recursive calls in each recursive call during phase j. now, in a given recursive call, outside of its recursive call, we do c times m operations where m is the length of the input array and during phase j we have an upper bound on the link of the input array. by definition it's at most three quarters raised to the j times n. so that is, we multiply the running time times this constant c this, we inherit from the partition subroutine and then we can, for the input length, we can put an upper bound of three quarters raised to the j times n. so just to review where all of these terms come from, there's three quarters j times n is an upper bound on the array size. during phase j, this by the definition of the phase. then, if we multiply that times c, that's the amount of work that we do on each phase j sub-problem. how much work do we do in phase j overall or we just take the work per sub problem that's what's circled in yellow and we multiply it times the number of such sub problems we have. and, of course, we don't wanna forget any of our sub problems so we just make sure we sum all of our phases, j, to insure that at every point we count the work done in each of the sub problems. okay? so, that's the upshot of this slide. we can upper bound the running time of our randomized algorithm very simply in terms of phases and the xj's, the number of sub problems that we have to endure during phase j. so, this upper bound on our running time is important enough to give it notation, we'll call this star, this will be the starting point of our final derivation when we complete the proof of this theorem. now don't forget, we're analyzing a randomized algorithm so therefore the left hand side of this inequality the running time of r select, that's a random variable. so that's a different number depending on the outcome of the random coin flips of the algorithm. depending on the random pick it has chosen, you will get different random running times. similarly the right hand side of this inequality. is also a random variable. that's because the x j's are random variables. the number of sub problems in phase j depends on which pivots get chosen. so. to analyze, what we care about is the expectations of these quantities, their average values. so we're gonna start modestly and as usual, this will extend our modest accomplishments to much more impressive ones using linearity of expectation, but our first modest goal is just to, to understand the average value. of an xj, the expected value of xj. we're gonna do that in two steps. on the next slide, i'm going to argue that to analyze the expectation of xj, it's sufficient to understand the expectation of a very simple coin flipping experiment. then, we'll analyze that coin flipping experiment. then we'll have the dominos all set up in a row. and on the final slide, we'll knock'em down and finish the proof. so let's try to understand the average number of recursive calls we expect to see in a given phase. so, again, just so we don't forget. xj is defined as the number of recursive calls during phase j. where a recursive call is in phase j, if and only if the current sub array length lies between three-fourths raised to the j+1 times n. and then, the larger number of three-fourths raised to the j times n. so again, for example, phase zero is just the recursive calls under which the array length is between 75 percent of the original element and 100 percent of the original elements. so what i wanna do next is point out that a very simple sufficient condition guarantees that we'll proceed from a given phase onto the next phase. so it's a condition guaranteeing termination of the current phase. and it's an event that we've discussed in previous videos. mainly that the pivot that we choose gives a reasonably balanced split. 25-75 or better. so recall how partitioning works, we choose a pivot p. it winds up wherever it winds up. and the stuff to the left of it's less than p. the stuff to the right of it is bigger than p. so 25 to 75 split or better, what i mean is that each of these, each, the first part and the second part has, at most, 75 percent of the elements in the input array. both have twen-, both have at least 25%, and, at most, 65%. and the key point is, that if we wind up choosing a pivot that gives us a split that's at least as good the current phase must end. why must the current phase end? well, to get a 25, 75 split or better than no matter which case we wind up in, in the algorithm we're guaranteed to recurse on a sub problem that has at most 75 percent of what we started with. that guarantees that whatever phase we're in now, we're going to be in an even bigger phase when we recursed. now, i want you to remember something that we talked about before, which is that you've got a decent chance when you pick a random pivot of getting something that gives you a 25, 75 split or better. in fact, the probability is 50 percent. right? if you have an array that has the integers from one to 100 inclusive, anything from 76 to s, 26 to 75 will do the trick. that'll insure that at least the first 25 elements are excluded from the rightmost call and at least rightmost 25 elements are excluded from the left recursive call. so this is why we can reduce our analysis of the number of recursive calls during a given phase, to a simple experiment involving flipping coins. specifically, the expected number of recursive calls. now we are gonna see in a given phase j, is no more than the expected number of coin flips in the following experiment. okay, so you've got a fair coin, 50 percent heads, 50 percent tails. you commit to flipping it until you see the head and the question is, how many coin flips does it take up to and including the first head that you see? so the minimum it's gonna be one coin flip if you hit a head the first time it's one. if you get a tails and then a head, then it's two. if it's tails, tails, head it's three and so on, and you always stop when you hit that first head. so what's the correspondence? well, think of heads as being you're in phase j, and if you get a good pivot, it gives you a 25/75 split. call that heads. and it guarantees that you exit this phase j. just like it guarantees that you get to terminate the coin flipping experience, experiment. now, if you get a pivot which doesn't give you a 25/75 split, you may or may not pass to a higher phase j, but in the worst case, you don't. you stick to phase j is you get a bad split, and that's like getting a tails in the coin flipping experiment, and you have to try again. this correspondence give us a very elementary way to think about the progress that, that our randomized selection algorithm is making. so, there's one recursive call in every step in our algorithm, and each time we either choose a good pivot or a bad pivot, both could happen, 50-50 probability. a good pivot means we get a 75-25 split or better. a bad pivot means, by definition, we get a split worse than 25-75. so what have we accomplished? we've reduced the task of upper bounding the expected number of recursive calls in a phase j to understanding the expected number of times you have to flip a fair coin before you get one hit. so on the next slide we'll give you the classical and precise answer to this coin flipping experiment. so, let me use capital n to denote the random variable, which we were just talking about, the number of coin flips you need to do before you see the first heads. and, it's not very important, but you should know that these random variables have their own name. this would be a geometric random variable with parameter one-half. so you can use a few different methods to compute the expected value of a geometric random variable such as this, and brute force using the definition of expectation works fine as long as you know how to manipulate infinite sums. but for the sake of variety, let me give you a very sneaky proof of what it's expectation is. so the sneaky approach is to write to the expected value of this random variable in terms of itself and then solve for the unknown, solve for the expectation. so let's think about it. so how many coins flips do you need? well for sure you're gonna need one. that's the best case scenario. and now two things can happen, either you get heads and that has 50 percent probability you stop or you get tails that happens with 50 percent probability and now you start all over again. again you just put points until you get first heads. on average how many times does that take. well by the definition of capital n you expect. the expectation of n coin flips, in the case where you get tails, and you have to start all over. so this one represents the first coin flip, the one-half is the probability that you can't stop, that you have to start all over probability of tails, and then because it's a memory less process, because when you start anew on the second coin flip having gotten the tails, it's as if you're back at time one all over again. so now we have a trivial equation, in terms of the unknown expected value of n and the unique solution, the unique value, that the expected value of capital n could have, in light of this equation, is two. so, on average if you flip a fair coin and stop when you get heads, you're going to see two coin flips on average. to make sure you haven't sort of lost the forest for the trees, let me remind you why we were talking about this coin flipping analysis in the first place. so recall in the previous slide we showed that xj, and remember xj is the number of recursive calls you'd expect to see in a given phase j, and we argued that the number of recursive calls you're gonna see is bounded above. by the expected number of coin flips until the heads. so this exact calculation of two for the coin flips gives us an upper bound of two for the number of recursive calls on average in any given phase j. so now that we've got all our ducks lined up in a row, let's wrap up the proof on this final slide. so, inherited from part one of the proof, we have an upper bound. on the expected running time. of the r select algorithm. this is what we were calling star on the first brief slide in star, it looked a little messy, but we had the sum over the phases j. but we had two things that were independent of j: the constant c and the original input length n, so let me just yank the c and the n out front. and then we have this residual sum over the phases j. of three quarters raised to the j remember that comes from our upper bound on the sub problem size during phase j and then of course we have to keep track of how many phase j sub problems we have solved that by definition is xj. now star was written as a rand in accordance terms to the random variables. now we're gonna go ahead and take the expectations and again i have said this over and over but don't forget where's the expectation come from. this is over the random pivot choices that our code makes. so the expected running time of the algorithm is most the expectation of this start quantity. so like i said earlier, pretty much every time we're gonna do any analysis of [inaudible] process, we're gonna wind up using linearity of expectation at some point. here is where we do it. linear expectation says the expectation of a sum is just the sum of the expectations. so we yank the c and the n outside of the expectation. we yank this sum over phases. outside of the expectation. we yank this three-fourths raised to the j outside of the expectation and then we just have the expected value of xj, the average number of recursive calls we expect to see in hj. now on the previous two slides, we figured out an upper bound on how many recursive calls we expect to see in each phase. so first by the coin flip analysis, by the reduction of the coin flip analysis, this is the most expected number of coin flips n, which on the previous slide, we argued was exactly two. so bringing that two out in front of the sum, that no longer depends on j. so we get a most 2cn. times the sum over phases j, of three quarters raised to the j. now this kind of sum we have seen previously in the course. it came up when we were analyzing the master method and we summed up our running time upper bounds over the levels of our recursin tree. and if we're not in case one if we're in case two or three we had geometric sums that were nontrivial. they require a certain formula to calculate, so let me remind you of that formula here, when the three quarters are being powered up to the j. so this has value at most, one over one minus, the number that's getting powered, so in this case it's three quarters, one minus three quarters is a quarter check reciprocal, you got four. and the upshot is that the expected number of operations that this randomized selection algorithm uses to find the [inaudible] ordered statistic in a given input array, is eight times c times n. where c is the, hidden constant in the linear running time of partition. and so that completes the proof. the input array was arbitrary. we showed the expected running time over the random choices of the algorithm is linear in n. that is, only a constant factor larger than what is required to read the input. pretty amazing. 
previous videos covered an outstanding algorithm for the selection problem, the problem of computing the ith statistic of a given array. that algorithm which we called the r select algorithm was excellent in two senses. first its super practical, runs blazingly fast in practice. but also it enjoys a satisfying theoretical guarantee. for every input array of length n at the expected running time of r select is big o of n, where the expectation is over the random choices of the pivots that r select makes during execution, now in this optional video i'm going to teach you yet another algorithm for the selection problem. well why bother given that our select is so good? well frankly, i just can't help myself. the ideas of this algorithm are just too cool not to tell you about, at least in optional video like this one. the selection algorithm , we cover here is deterministic. that is, it uses no randomization whatsoever. and it's still gonna run in linear time, big o of n time. but now, in the worst case for every single input array. thus, the same way merge short gets the same asymptotic running time, big o of n log n, as quick sorts gets on average. this deterministic algorithm will get the same running time o of n, as the r select algorithm does on average. that said, the algorithm we're gonna cover here, well, it's not slow. it's not as fast as r select in practice, both because the hidden constants in it are larger. and also because it doesn't' operate in place. for those of you who are feeling keen, you might wanna try coding up both the randomized and the deterministic selection algorithms, and make your own measurements about how much better the randomized one seems to be. but if you have an appreciation for boolean algorithms, i think you'll enjoy these lectures nonetheless. so let me remind of the problem. this is the i-th order statistic problem. so we're given an array, it has n distinct entries. again, the distinctness is for simplicity. and you're given a number i between one and n. you're responsible for finding the i-th smallest number, which we call the i-th order statistic. for example, if i is something like n over two, then we're looking for the median. so let's briefly review the randomized selection algorithm. we can think of the deterministic algorithm covered here as a modification of the randomized algorithm, the r select algorithm. so when that algorithm is passed in array with length n, and when you're looking for the i-th order statistic, as usual, there's a trivial base case. but when you're not in the base case, just like in quick sort, what you do is you're gonna partition the array around pivot element p. now, how are you gonna choose p? well, just like quick sort, in the randomized algorithm, you choose it uniformly at random. so each of the n elements of the input array are equally likely to be chosen. as the pivot. so, call that pivot p. now, do the partitioning. remember partitioning puts all of the elements less than the pivot to the left of the pivot. we call that the first part of the partitioned array. anything big, bigger than the pivot gets moved to the right of the pivot. we call that the second part of the array. and let j denote the position of the pivot in this partitioned array. equivalently, let j be what order statistic that the pivot winds up happening to be. right? so, we happen to choose the minimum element then j's gonna be equal to one. if we happen to choose the maximum element, j's gonna be equal to n. and so on. so, there's always the lucky case, chance one in n, that we happen to choose the ith order statistic as our pivot. so, we're going to find that out when we notice that j equals i. in that super lucky case, we just return the pivot and we're done. that's what we're looking for in the first place. of course, that's so rare it's not worth worrying about, so really the two main cases depend on whether the pivot that we randomly choose is bigger than what we are looking for or if it's less than what we are looking for. so, if it's bigger than what we are looking for, that means j is bigger than i, we're looking for the ith smallest, we randomly chose the j'th smallest. then remember we know that the ith smallest element has to lie to the left of the pivot. good element in that first part of the partition array. so we recurs there. it's an array that has j-1 elements in it, everything less than the pivot. and we're still looking for the ith smallest among them. in the other case, this was the case covered in a quiz a couple videos back, if we guess a pivot element that is less than what we're looking for, well then we should discard everything less than the pivot and the pivot itself. so we should recurs on the second part of a, stuff bigger than the pivot. we know that's where what we're looking for lies. and having thrown away j elements, the smallest ones at that. we're rehearsing on a ray of [inaudible] and minus j, i'm looking for the [inaudible] smallest element in that second part. so, that was the randomized selection algorithm, and you'll recall the intuition for why this works is random pivot should usually give pretty good splits. so the way the analysis went is we should. each iteration, each recursive call, with 50 percent probability, we get a 25/75 split or better. therefore, on average, every two recursive calls, we are pretty aggressively shrinking the size of the recursive call. and for that reason, we should get, something like a linear time bound. we do almost as well as if we picked the median in every single call, just because random pivots are a good enough proxy of best case pivots, of. the median. so now the big question is: suppose we weren't permitted to make use of randomizations. suppose this choose-a-random-pivot trick was not in our tool box. what could we do? how are we going to deterministically choose a good pivot? let's just remember quickly what it means to be a good pivot. a good pivot is one that gives us a balanced split, after we do the partitioning of the array. that is, we want as close to a 50/50 split between the first and the second parts of the partitioned array as possible. now, what pivot would give us the perfect 50/50 split? well, in fact, that would be the median. well, that seems like a totally ridiculous observation, because we canonically, are trying to find the median. so previously we were able to be lazy, and we just picked a random pivot, and used that as a pretty good proxy for the best case pivot. but now, we have to have some subroutine that deterministically finds us a pretty good approximation of the median. and the big idea in this linear time selection algorithm, is to use what's called the median of medians as a proxy for the true meaning of the input array. so when i say median of medians, you're not supposed to know what i'm talking about. you're just supposed to be intrigued. now, let me explain a little bit further. here's the plan, we're gonna have our new implementation of chose pivot and it's gonna be deterministic. you will see no randomization on this slide, i promise. so the high-level strategy is gonna be we're gonna think about the elements of this array like sports teams, and we're gonna run a tournament, a 2-round. knockout tournament, and the winner of this tournament is going to be who we return as the proposed pivot element. then we'll have to prove that this is a pretty good pivot element. so there's gonna be two rounds in this tournament. each element, each team is gonna first participate in a world group, if you will. so they'll be, small groups of five teams each, five elements each. and to win your first round, you have to be the middle element out of those five. so that'll give us n over five first round winners. and then the winner of that second round is going to the med-, be the median of those n over five winners from the first round. here are the details. the first step isn't really something you actually do in the program, it's just conceptually. so logically, we're going to take this array capital a, which has n elements, and we're gonna think of it as comprising n over five groups with five elements each. so if n is not a multiple of five, obviously, there'll be one extra group that has size between one and four. now for each of these groups of five, we're going to compute the median, so the middle element of those five. now for five elements, we may as well just invoke a reduction to sorting; we're just gonna sort each group separately, and then use the middle element, which is the median. it doesn't really how you do the sorting. because after all, there's only five elements. but you know, let's use [inaudible] sort, what the heck. now what we're going to do is we're going to take our first round winners and we're gonna copy them over into their own private array. now this next step is the one that's going to seem dangerously like cheating, dangerously like i'm doing something circular and not actually defining a proper algorithm, so c you'll notice has linked over n over five. we started with an array of link n. this is a smaller input. so let's recursively compute the median of this array capital c. that is the second round of our tournament amongst the n over five first-round winners, the n over five middle elements of the sorted groups. we recursively compute the median, that's our final winner, and that's what we return as the pivot element from this subroutine. now i realize it's very hard to keep track of both what's happening internal to this juice pivot subroutine and what's happening in the calling function of our deterministic selection algorithm. so let me put them both together and show them to you, cleaned up, on a single slide. so, here is the proposed deterministic, selection algorithm. so, this algorithm uses no randomization. previously, the only randomization was in choosing the pivot. now we have a deterministic subroutine for choosing the pivot, and so there's no randomization at all. i've taken the liberty of in-lining true's pivot subroutine. so that is exactly what lines one, two, and three are. i haven't written down the base case just to save space i'm sure you can remember it, so if you're not in the base case. what did we do before? the first thing we do is choose a random pivot. what do we do now? well, we have steps one through three. we do something much more clever to choose a pivot. and this is exactly what we said on the last slide. we break the array into groups of five. we sort each group, for example, using merge sort. we copy over the middle element of each of the n over five groups into their own array capital c. and, then, we recursively compute the median of c. so, when we recurs on select that we pass the input c. c has n over five elements so that's the new link. that's a smaller link than what we start with so it's a legitimate recursive call refining the median of n over five element. so, that's gonna be the n over tenth order statistic. as usual. well to keep things clear i'm ignoring stuff like fractions, in your real implementation you'd just round it up or down. as appropriate. so steps one through three are the new [inaudible] step routine that replaces the randomized selection that we had before. steps four through seven are exactly the same as before. we've changed nothing. all we have done is ripped out that one line where we chose the pivot randomly and pasted in these lines one through three. that is the only change to the randomized selection algorithm. so, the next quiz is a standard check that you understand this algorithm, at least, not necessarily why it?s fast; but, at least, just how it actually works. and i only ask you to identify how many recursive calls there are, each time. so, for example in [inaudible] there's two recursive calls, in quick-sort there's two recursive calls, in r-select there's one recursive call. how many recursive calls do you have each time, outside of the base case in the d-select algorithm? all right, so the correct answer is two. there are two recursive calls in deselect, and maybe the easiest way to answer this question is not to think too hard about it and literally just inspect the code and count, right namely there's one recursive call in line three, and there's one recursive call in either six or seven, so quite literally, you know there's seven lines of code, and two of the ones that get executed have a recursive call so the answer is two. now what's confusing is that in the random, a couple things, first in the randomized selection algorithm, we only have one recursive call. we have the recursive call. in line six or seven, we didn't have this in line three. that one in line three is new compared to the randomized procedure. so we're kind of used to thinking of one recursive call using the divide and conquer approach to selection, here we have two. moreover. conceptually. the roll of these two recursive calls are different. so the one in line six or seven is the one we're used to. that's after you've done the partitioning so you have a smaller sub-problem and then you just recursively find the residual or statistic in the residual array. that's sort of the standard divide and conquer approach. what's sort all crazy. is this second use of a recursive call which is part of identifying a good pivot element for this outer recursive call and this is so counter-intuitive, many students in my experience don't even think that this algorithm will hold, sort of, they sort of expect it to go into an infinite loop. but again, that sort of over thinking it. so let's just compare this to an algorithm like merge sort. what does merge sort do? well it does two recursive calls and it does some other stuff. okay. it does linear work. that's what it does to merge. and then there are two recursive calls on smaller sub problems, right? no issue. we definitely feel confident that merge [inaudible] is gonna terminate because the sub problems keep getting smaller. what does deselect do, if you squint? so don't think about the details just [inaudible] high level. what is the work done in deselect? well. there are two recursive calls, there's [inaudible] one's in line three, one's in line six or seven, but there's two recursive calls on sm, smaller sub problem sizes. and there's some other stuff. there's some stuff in steps one and two and four, but whatever. those are recursive calls. it does some work. two recurs have caused the smaller sub-problems, ti's got to terminate. we don't know what the run time is, but it's got to terminate, okay? so if you're worried about this terminating, forget about the fact that the two recurs of cause have different semantics and just remember, if ever, you only recurs on smaller sub-problems, you're definitely going to terminate. now, of course who knows what the running time is? i owe you an argument on why it would be anything reasonable, that's going to come later. in fact what i'm gonna prove to you is not only does this selection algorithm terminate, run in finite time, it actually runs in linear time. no matter what the input array is. so where as with r select, we could only discuss its expected running time being linear. we showed that with disastrously bad choices for pivots, r selects can actually take quadratic time. under no circumstances will deselect ever take, ever take quadratic time. so for every input array it's big o of n time. there's no randomization because we don't randomly do anything in choose pivot, so there's no need to talk about average running time; just the worst case running time over all inputs is o of n. that said, i want to reiterate the warning i gave you at the very beginning of this video which is, if you actually need to implement a selection algorithm, you know, this one wouldn't be a disaster. but it is not the method of choice, so i don't want you to be misled. as i said there are two reasons for this. the first is that the constants hidden in the begon notation are larger for v select than for r select. that will be somewhat evident from the analyses that we give for the two algorithms. the second reason is, recall we made a big deal about how partitioning works in place and therefore quicksort and r select also work in place, that is, with no real additional memory storage. but in this deselect algorithm we do need this extra array c to copy over the middle elements, the first round winners. and so the extra memory, as usual, slows down the practical performance. one final comment. so for many of the algorithms that we cover, i hope i explain them clearly enough that their elegance shines through and that for many of them you feel like you could have up with it yourself, if only you'd been in the right place at the right time. i think that's a great way to feel and a great way to appreciate some of these very cool algorithms. that said, linear time selection, i don't blame you if it, if you feel like you never might have come up with this algorithm. i think that's a totally reasonable way to feel after you see this code. if it makes you feel better, let me tell you about who came up with this algorithm. it's quite old at this point, about 40 years, from 1973. and the authors, there are five of them and at the time this was very unusual. so, manuel blum. bob floyd. vaughn pratt. ron rivest. and bob targen. and this is a pretty heavy weight line up, so as we've discussed in the past, the highest award in computer science is the acm turing award given once each year. and i like to ask my algorithms classes how many of these authors do they think, have been awarded a turing award. i've asked him many times. the favorite answer anyone's ever given me has been. six, which i think is in spirit should be correct. strictly speaking the answer is four. so, the only one of these five authors that doesn't have a touring award is von pratt, although he's done remarkable things spanning the gambit from co-founding sun systems to having very famous theorems about, for example, testing primality. but the other four have all been awarded the touring award at some point. so in chronological order, so the late bob floyd who was a professor here at stanford. was awarded the 1978 [inaudible] award, both for contributions to algorithms but also to program languages and compilers. so bob targen who, as we speak, is here as a visitor at stanford and has spent his ph. d here and has been here as a faculty at times, was awarded it for contributions to graph algorithms and data structures. we'll talk some more about some of his other contributions in future courses. manuel blum was awarded the turing award in'95 largely for contributions in cryptography, and many of you probably know ron rivest as the r in the rsa cryptosystem. so he, won the, turing award along with shamir and adleman back in'02. so in summary, if this algorithm seems like one that might have alluded you even on your most creative days, i wouldn't feel bad about it. this is a, this is a quite clever algorithm. so let's now turn to the analysis and explain why it runs in linear time in the worst 
now let's turn to the analysis of the deterministic selection algorithm that we discussed in the last slide by blum, floyd, pratt, rivest, and tarjan. in particular, let's prove that it runs in linear time on every possible input. let's, remind you what the algorithm is. so the idea is, we just take the r select algorithm. but instead of choosing a pivot at random, we do quite a bit more work to choose what we hope is going to be a guaranteed pretty good pivot. so again, lines one through three are the new choose pivot subroutine. and it's essentially implementing a two round knockout tournament. so first, we do the first round matches. so what does that mean? that means we take a we think of it as comprising these groups of five elements. so the first five elements one through five and the elements six through ten and points eleven through fifteen and there again and so on. if we sort each of those five using, let's say, merge sort although it doesn't matter much, then the winner in each of these five first round matches is the median of those five. that is the third highest element, third largest element out of the five. so we take those in over five first round winners the middle element of each of the five and the sorted groups, we copy those over into a new array of capital [inaudible] and [inaudible] in it for five. and then we. second round of our tournament at which we elect the medium of these n over five, first round winners as our final pivot, as our final winner. so, we do that by recursively calling deselect on c. it has a length n over five [inaudible] for the medium. so that's the n over tenth [inaudible] statistic in that array. so, we call the pivot p and then we just proceed exactly like we did. and in the randomized case. that is, we partition a around the pivot, we get a first part, a second part, and we recurs on the left side or the right side as appropriate, depending on whether the pivot is less than or bigger than the element that we're looking for. so the claim is, believe it or not, that this algorithm runs in linear time. now, you'd be right to be a little skeptical of that claim. certainly, you should be demanding from me some kind of mathematical argument about this linear time claim. it's not at all clear that that's true. one reason for skepticism is that this is an unusually extravagant algorithm. in two senses for something that's gotta run in linear time. first is, first is it's extravagant use of recursion. there are two different recursive calls, as discussed in the previous video. we have not yet seen any algorithm that makes two recursive calls and runs in linear time. the best case scenario was always [inaudible] for our two recursive call algorithms like merge sort or quick sort. the second reason is that, outside the recursive calls, it seems like it?s just kind of a lot of work, as well. so, to drill down on that point, and get a better understanding for how much work this algorithm is doing, the next quiz asks you to focus just on line one. so when we sort groups of five in the input array how long does that take. so the correct answer to this quiz is the third answer. maybe you would have guessed that given that i'm claiming that the whole algorithm takes linear time, you could have guessed that this sub-routine is going to be worse than linear time. but you should also be wondering you know, isn't sorting always n log n so, aren't we doing sorting here. why isn't the n log n thing kicking in? the reason is we're doing something much, much more modest than sorting the linked n input array, all we're sorting are these puny little sub-arrays that have only five elements and that's just not that hard, that can be done in constant time so let me be a little more precise about it. the claim is that sorting an element, an array with five elements takes only some constant number of operations. let's say 120. where did this number, 120 come from? well, you know, for example, suppose we used merge sort. if you go back to those very early lectures, we actually counted up the number of operations that merge sort needs to sort an array length of m. for some generic m, here m is five, so we can just plug five into our previous formula that we computed from merge sort. right if we plug amicle five into this formula, what do we get, we get six times five times log base 205+1. who knows what log base 205 is, that's some weird member but it's gonna be a most three right. so that's the most three of three+1 is four multiply that by five and again time six and will get you 120. so it's constant time to sort just one of these groups of five. now of course, we have to do a bunch of groups of five because there's only a linear number of groups. constant for each, so it's gonna be linear time overall. so to be really pedantic. we do 120 operations at most per group. there's n over five different groups. we multiply those, we get 24 n operations. so do all the sorting and that's obviously a big o event. so linear time for step one. so having warmed up with step one. let's look now at the whole seven line algorithm, and see what's going on. now i hope you haven't forgotten the paradigm that we discussed for analyzing the running time of deterministic divide and conquer algorithms like this one. so namely we're gonna develop a recurrence and remember a recurrence expresses the running time, the number of operations performed, in two parts. first of all, there's the work done by the recursive calls on smaller sub-problems. and secondly, there's the work done locally, not in the recursive calls. so let's just go through these lines one at a time, and just do a running tally of how much work is done by this algorithm, both locally and by the recursive calls. so the quiz was about, step number one. we just argued that since it's constant time to sort each group, and there's a linear number of groups, we're gonna do linear work, theta of n. for step one. so copying these first round winners over in to their special array c is obviously linear time. now, when we get to the third line, we have a recursive call, but it's a quite easy recursive call to understand. it's just, recursing on a, a ray that has size twenty percent as large as the one we started with, on the n over five elements. so this, remember the notation we used for recurrences. generally, we denote by capital t the running time of an algorithm on [inaudible] of a given length. so this is going to be the running time that our algorithm has in the worst case on inputs of length n over five. cuz n over five is the length of the array that we're passing to this recursive call. good. step four, partition. well we had. videos about how they were going to partition the y to linear time. we knew that all the way back from quick sort, so that's definitely theta of n. step five is constant time, i'm not going to worry about it. and finally we get to lines six and seven so at most one of these will execute so in either case there's one recursive call. so that's fine, we know in recurrences when there's recursive call we'll just write capital t of whatever the input length is. so we just have to figure out what the input length here is. it was n over five in step, in line three so we just have to figure out what it is in line six or seven. oh yeah, now we're remembering why we didn't use recurrences when we discussed randomized quick sort and. the randomized selection algorithm. it's because we don't actually know how big the recursive call is, how big the input passed to this recursive call in line six or seven is. line three, no problem. it's guaranteed to be twenty percent of the input array cuz that's how we defined it. but for line six or seven, the size of the input array that gets passed to the, to the recursive call depends on how good the pivot is. it depends on the splitting of the array a into two parts, which depends on the choice of the pivot p. so at the moment all we can write is t. of question mark. we don't know. we don't know how much work gets done in that recursion, cause we don't know what the input size is. let me summarize the results of this discussion. so write down a recurrence for the d select algorithms. so with t of n denote the maximum number of operations the d select ever requires to terminate an array of input [inaudible]. it's just the usual definition of t of n when using recurrences. what we established in our tally on the last slide is that deselects does linear stuff outside the recursive calls. it does the sorting of groups of five. it does the copying, and it does the partitioning. each of those is linear, so all of them together is also linear. and then it does two recursive calls. one whose size we understand, one whose size we don't understand. so, for once i'm not going to be sloppy and i'm going to write out an explicit constant about the work done outside of the recursive cause. i'm going to write [inaudible], i'm going to actually write c times n for some constant c. so of course no one ever cares about base cases, but for completing this let me write it down anyways. when d select gets an input of only one element it returns it, what's called that one operation for simplicity. and then in the generals cases and this is what's interesting. when you're not in the base case and you have to recurs, what happens? well you do a linear work outside of the recursive call. so that's c times n for some constant c. c is just the [inaudible] constant on all of our big thetas on the previous slide. plus the recursive call in line three, and we know that happens on an array of size [inaudible] five. as usual, i'm not gonna worry about rounding up or rounding down, it doesn't matter. plus our mystery recursive call on an array of unknown size. so that's where we stand and we seem stuck because of this pesky question-mark. so, let's prove lemma which is gonna replace this question-mark with something we can reason with, with an actual number that we can then analyze. so the upshot of this key lemma is that all of our hard work in our choose pivot subroutine in lines one through three bears fruit in the sense that we're guaranteed to have a pretty good pivot. it may not be the median, it may not give us a 50/50 split. then we could replace the question mark with, one-half times n. but it's gonna let us replace the question mark by seven-tenths times n. now, i don't wanna lie to you, i'm gonna be honest, it's not quite 7/10n, it's more like 7/10n minus five, there's a little bit of additive error, so, taking care of the additive error adds nothing to your conceptual understanding of this algorithm or why it works. for those of you who want a truly rigorous proof, there are some posted lecture notes which go through all the gory details. but in lecture i'm just gonna tell you what's sort of morally true and ignore the fact that we're gonna be off by three here and four there. and then we'll be clear when i show you the proof of this limit, where i'm being a little bit sloppy and why it really shouldn't matter, and it doesn't. so to explain why this key limit is true why we get a 30 70 split or better guaranteed, let me set up a little notation. i'm getting sick of writing n over five over and over again, so let's just give that a synonym, let's say, k. so this is the number of different sort of first round matches that we have, the number of groups. i also want some notation to talk about the first round winners, that is the medians of these groups of five, the k first round winners. so, were gonna call xi the [inaudible] smallest of those who win their first round match and make it to the second round. so just to make sure the notation is clear, we can express the pivot element in terms of these x?s. remember, the pivot is the final winner. it wins not only its first round tournament, but it also the second round tournament. it's not only the middle element of the first group of five. it's actually the median of the n over five middle element. it's the median of the medians. that is, of the k middle elements, it's the k over two order statistic, [inaudible] k over two smallest. i'm saying this, assuming that k is even. if k was odd, it would be some slightly different formula as you know. so let's remember what we're trying to prove. we're trying to prove that for our proposed pivot, which is exactly this element x sub k over two, it's exactly the winner of this 2-round knockout tournament. we're trying to argue that for this proposed pivot, we definitely get a 30-70 split or better. so what that means is, there better be at least 30 percent of the elements that are bigger than the pivot. that way if you recurs on the left side on the first part, we don't have to deal with more that more than 70 percent of the original elements. similarly, there better be at least 30 percent of the elements that are smaller than the pivot. that way if we recurs on the right hand side we know we don't have to deal with more than 70 percent of the original input elements. so if we achieve this goal, we prove that there's at least 30 percent on each side of xk over two, then we're done. that proves the key lemma that would get a 30/70 split or better. so i'm gonna show you why this goal is true. i'm gonna introduce a thought experiment. and i'm gonna lay out it abstractly. then we'll sorta do an example to make it more clear. and then we'll go back to the general discussion and finish the proof. so what we're gonna do is a thought experiment, for the purposes of counting how many elements of the input array are bigger than our pivot choice, and how many are smaller. so in our minds we're going to imagine that we're taking elements in a and rearrange them in a 2d grid. so here are the semantics of this grid. each column will have exactly five elements that will correspond to one of the groups of five. so we'll have n over five columns corresponding to our n over five groups in our first round of our tournament. [inaudible] is not a multiple of five then one of these groups has size between one and four but i'm just not gonna worry about it, that some of the additive loss, which i'm ignoring. moreover were going to arrange each column in a certain way so that going from bottom to top the entries of that go from smallest to largest. so this means that in this grid we have five rows. and the middle row, the third row, corresponds exactly to the middle elements, to the winners of the first round matches. so because these middle elements these first round winners are treated specially, i'm going to denote them with big squares, the other four elements of the group two of which are smaller two of which are bigger are just going to be little circles. furthermore, in this thought experiment, in our mind, we're going to arrange the columns from left to right in order of increasing value of the middle element. now remember, i introduced this notation x of i is the [inaudible] smallest amongst the middle elements. so a different way of what i'm trying to say is that the leftmost column is the group that has x1 as its middle element. so among the n over five middle elements, one of the groups has the smallest middle elements. we put that all the way on the left. so this is gonna be x1 in the first column, the smallest of the first round winners. x2 is the second smallest of the first round winners, x3 is the third smallest and so on. at some point we get to the median of the first round winners, xk over two. and then, way at the rights is the largest of the first round winners. and i'm sure that you remember that the median of medians which is xk over two is exactly our pivot. so this is our lucky winner. i know this is a lot to absorb, so i'm gonna go ahead and go through an example. if what i've said so far makes perfect sense, you should feel free to skip the following example. but if there's still some details you're wondering about, and hoping this example will make everything crystal clear. so let's suppose we have an input array. i need a, a slightly big one to [inaudible] grid make sense. let's say there's an input array of twenty elements. so there's going to be the input array, which is in a totally arbitrary order. there's gonna be the vertical [inaudible] after we sort each group of five. and then i'm gonna show you the grid. so this is the input we're all gonna use. let's now go ahead and delineate the various groups of five. so after sorting this group, you get the following. from each group there's a single winner mainly the middle element so that would be the twelve, and the six, and the nine, and the fourteen, those are the four survivors from the first round of the tournament. and the median of these four elements which, at the end of the day is gonna to be our pivot is the second smallest of the four, that's how we define the median from an even number of elements, so that's gonna be the nine. so, this first transformation from the input array, to this vaguely mini sorted version of the input array with the groups of five sorted, this we actually do in the code. this happens in the algorithm. now, this grid we're just doing in our minds. okay? we're just in the middle of proving why the algorithm is fast. why the fit bits guaranteed to give us close to a, a 30 70 split or better. so, let me show you an example of this grid in our mind, what it looks like for this particular input. so the grid always has five rows. the columns always have five elements cause the columns correspond to the groups. here because n equals twenty and over five is four. so there's gonna be, four columns and five rows. and moreover we arrange the columns from left to right so that these middle elements go from smallest, to largest. so the middle elements are six nine twelve and fourteen and we're gonna draw the columns in that order from left to right. so first we'll write down the middle elements, the middle row from decreasing to increasing, six, nine, twelve, fourteen. again the median of this is our pivot, which is the nine. and then each column is just the other four elements that goes along with this middle element from decreasing to increasing as we go from bottom to top. so this is the grid that we're been talking about on the other slide, in this particular example. so i hope that makes what we're talking about clear, what these x?s mean, and what worry we have amongst the rows, amongst the columns and so on. so let?s go back to the general argument. here is the key point, here is why were doing this entire thought experiment, it?s going to let us prove our key limit. we're going to get a 30/70 split or better. 30 percent of the stuff at least is less than the pivot; 30 percent at least is bigger than the pivot. so why is there at least 30 percent of the stuff below the pivot? why is the pivot bigger then at least 30%? well, it's bigger then everything to the left and everything below the stuff to the left. that is we know that xk over two is bigger than the k over two minus one elements. that is to the left of it, those other middle elements that it's bigger then. that's because it's the median of the medians. >> so, if we just go straight west from the pivot we only see stuff which is less. furthermore, these columns are arranged from decreasing to increasing order as we go from south to north, from bottom to top. so if you travel south from any of these smaller xmi we only see stuff which is still smaller. so all we're using in here is transitivity of the less than relation. if you go straight west you see stuff which is only smaller from any of those points if you go southward you'll see stuff which is even smaller than that. so this entire yellow region, everything southwest of the pivot element, is smaller than it. and that's a good chunk of the grid. right? so for all of these columns, it's basically three out of the five, or 60 percent of them are smaller than the pivot, and half of the columns, essentially, are in this part of the grid. so if the pivots bigger than 60 percent of the stuff in 50 percent of the groups that means it's bigger than 30 percent of the elements overall. and if we reason in an exactly symmetric way, we find that the pivot is also smaller than at least 30 percent of the array. so to find things bigger than the pivot, what do we do? first we travel eastward. that gives us middle elements that are only bigger than it and then we stop wherever you want on our eastward journey and we head north, and we're gonna see stuff which is still bigger. so this entire north eastern corner. is bigger than the pivot element, and again that's 50%, that's at 60 percent of roughly 50 percent of the groups. returning to our example, the southwest region of the nine. is this stuff, one, three, four, five, six. certainly, all of that is smaller than the nine. you'll notice there's other things smaller than the nine as well. there's the eight, there's the two, there's the seven, which we're not counting. but it depends on the exact array. whether or not, in those positions, you're gonna have stuff smaller than the pivot or not. so it's this yellow region we're guaranteed to be smaller than the pivot. similarly, everything northeast of the pivot is bigger than it. those are all double digit numbers and our pivot is nine. again there's some other stuff in other regions bigger than the pivot, the twenty, the ten, the eleven, but again those are positions where we can't be guaranteed that it will be bigger than the pivot. so it's the yellow regions that are guaranteed to be bigger and smaller than the pivot, and that gives us the guaranteed 30 70 split. okay, so that proof was hard work, showing that this deterministic choose pivot subroutine guarantees a 30-70 split or better. and you probably feel a little exhausted and like we deserve a qed at this point. but we haven't earned it. we have not at all proved that this deterministic selection algorithm runs in linear time. why doesn't a guaranteed 30-70 split guarantee us linear time automatically? well, we had to work pretty hard to figure out this element guaranteeing this 30-70 split. in particular we had to invoke another recursive call. so maybe this was a pyrrhic victory. maybe we had to work so hard to compute the pivot that it outweighs the benefit we'd get from this guarantee. 30 70 split. so, we still have to prove that's not the case even in conjunction doing both of these things, we still have our linear time bound. we'll finish the analysis in the next video. [sound]. 
so the time has arrived for us to finish the proof of the fact that this deterministic algorithm based on the median of median ideas, does indeed run in linear time. we've done really all the [inaudible] difficult work. we've discussed the algorithmic ingenuity required. to choose a pivot deterministically that's guaranteed to be pretty good. so remember the idea was you take the input array, you logically break it into groups of five, you sort each group. that's like the first round of a two round knockout tournament. the winners of the first round are the middle elements of each group of five. that's the initial set of medians. and then the second around we take a median of these n over five first round winners, and that's what we return as the pivot. and we proved this key lemma which is the 30/70 lemma, which says that if you choose the pivot by this two round knockout tournament, you're guaranteed to get a 30/70 split or better. so your recursive call in line six or seven. of having a de-select is guaranteed to be on an array that has at most 70 percent of the elements that you started with. in other words you're guaranteed to prune at least 30 percent of the array before you recurs again. but what remains to understand is whether or not we've done a sensible trade off. have we kept the work required to compute this 30/70 split small enough. that we get the desired linear running time. or have we, is the cost of finding a pretty good pivot outweighing the benefit of having guaranteed good splits? that's what we gotta prove. that's the next subject. here's the story so far. you'll recall that, as usual, we define t of n to be the worst case running time of an algorithm. in this case, d select on inputs of array length. i didn't put arrays of length n. and we discussed, okay, there's the base case as usual. but in the general case, we discussed how, outside of the two recursive calls. the deselect algorithm, there's a linear number of operations. what does it have to do? it has to do the sorting, but each sorting is on a group of sized constants, size five, so it takes constant time for a group. there's a linear number of groups, so step one takes linear time, the copying takes linear time, and the partitioning takes linear time. so, there's some constant c, which is gonna be bigger than one, but it's gonna be constant. so, then outside of a recursive cause. deselect always does at most c times n operations. now what's up with the recursive calls. well, remember there's two of them. first, there's one on line three that's just responsible for helping choose the pivot. this one we understand. it's always on twenty percent of the imputed rate of like the first round winners, so we can very safely write t of n over five for the work done, in the worst case, by that first recursive call. what we didn't understand until we proved the key lemma was what's up with the second recursive call, which happens on either line six or line seven. the size of the imputed rate on which we recursed depends on the quality of the pivot, and it was only when we proved the key lemma that we had a guarantee on the. [inaudible] 30-70 split or better what does that mean? that means the largest sub-array we could possibly recurs on has seven-tenths n elements. so what remains is to find the solution for this recurrence and hopefully prove that it is indeed big o event. so i'm going to go ahead and rewrite the occurrence at the to of the slide. we're not really going to worry about the t to one equal one. what we're interested in is the fact that the running time on an input of length n is at most c times n. where again c is some constant which is gonna have to be at least one, given all the work that we do outside of the recursive calls. plus the recursive call on line three on an array of size n over five. plus the second recursive call, which is on some array that has size in the worst case seven-tenths n. so that's cool. this is exactly how we handle the over deterministic divide and conquer algorithms that we studied in earlier videos. we just wrote down a recurrence and then we solve the recurrence, but now, here's the trick. and all of the other recurrences that came up. for example, merge short, strassner's matrix multiplication algorithm, [inaudible] multiplication, you name it. we just plug the parameters into the masters method. and because of the power of the master method, boom! out popped up an answer. it just told us what the recurrence evaluated to. now, the master method, as powerful as it is, it did have an assumption, you might recall. the assumption was that every sub-problem solved had the same size. and that assumption is violated by this linear time selection algorithm. there are two recursive calls. one of 'ems on twenty percent of the original array. the other one is probably on much more than twenty percent of the original array. it could be as much as 70 percent of the original array. so because we have two recursive calls, and sub problems of different size, this does not fit into the situations that the master method covers. it's a very rare algorithm in that regard. now, there are more general versions of the master method, of the master theorem which can accommodate a wider class of recurrences including this one here. alternatively we could push the recursion tree proof so that we could get a solution for this recurrence. some of you might want to try that at home. but i want to highlight a different way you can solve recurrences just for variety, just to give you yet another tool. now the good news of the, about this approach that i'm gonna show you is that it's very flexible. it can be used to solve sort of arbitrarily crazy recurrences. it's certainly going to be powerful enough to evaluate this one. the bad news is that it's very out of hock. it's not very necessarily very easy to use. it's kind of a dark art figuring out how to apply it. so it's often just guess and check, is what it's called. you guess what the answer to the recurrence is and then you verify it by induction. here, because we have such a specific target in mind, the whole point of this exercise is to prove a linear is not bound, i'm gonna call it just hope and check. so we're gonna hope there's linear of time and then we're gonna try to produce a proof of that just that verifies the linear time bound using induction. specifically what are we hoping for, we're crossing our fingers that there's a constant, i'm going to call it a, a can be big but it's got to be constant, and again remember constant means it does not depend on n in any way. such that our recurrence at the top of this slide t-n is bound above by a times n for all and at least one. why is this what we're hoping? well suppose this were true. by definition t of n is a upper bound of the running time of our algorithm. so it's bound and [inaudible] by a times n then it's obviously an o event. it's obviously a linear time algorithm. it's obviously a gets that gets suppressed in the big rotation. so that's the hope, now let's check it. and again, check mean just verify by induction on n. so the precise claim that i'm going to prove is the following. i'm gonna go ahead and choose the constant a. remember all we need is some constant a, no matter how big as long as it's independent of n. that'll give us the big o of n time. so i'm actually gonna tell you what a i'm gonna use for convenience. i'm gonna choose a to be 10c. now what is c? c is just a constant that we inherit from the recurrence that we're given. now remember what this recurrence means is this is what the running time is of the deselect algorithm and the c times n represents the work that's outside of the recursive calls. so this is just a constant multiple on the amount of linear work that deselect does for sorting the groups, for doing the partitioning and for doing the copying. and so there's gonna be some small task at a reasonable cost and, and for the proof i'm just gonna multiply that by ten and use that as my a. and the claim is if i define a in that way then indeed, it is true that for all and at least one, t of n is banded above by a times n. now, i realized i just, i pulled this constant a out of nowhere, right? y10 times c. well, if you recall our discussion when we proved that things were big o of something else, there again, there was some constant. so to formally prove that something is big o of something else, you have to say what the constant is. and in the proof, you always wonder how do you know what constant to use? so, in practice, when you're actually, if you have to actually do one of these proofs yourself, you reverse engineer what kind of constant would work. so you just go through the argument with a generic constant. and then you're like, oh, well, if i set the constant to be this, i can complete the proof. so we'll see, that's exactly what's gonna happen in the proof of this claim. it'll be obvious. the very last line you'll see why it shows a equals 10c. so i just reverse engineered what i needed for the proof. but to keep the proof easy to follow line by line i decided to just full disclosure tell you the cost right at the beginning. now no prizes for guessing that the way this proof proceeds is by induction on n. induction's the obvious thing to use, we're trying to prove an assertion for every single positive number n and moreover we're given this recurrence which relates solutions of smaller sub-problems to that of bigger problems. so that sets things up for use of the inductive hypothesis. if you want a longer review of what proofs by induction are, i suggest that you go back and re-watch the optional video where we prove the correctness of quicksort. that is, is a fairly formal discussion of what the template is like for a proof by induction. and that's the one we're gonna apply here. so, there's two ingredients in any proof by induction is, is a usually trivial one in the form of a base case. that's also gonna be trivial here. in the base case you just directly establish the assertion when n equals one. so, we're trying to prove that t of n is the most a times n for every n when n equals one we could just substitute. but what we're trying to prove is that t of one is at most a time one also known as a. and we're given that t of one is one. right that's the base case of the recurrence that we're given. so that's what we're using here. what we want to be true is that this isn't the most a times one, but it is. so the constant c we're assuming is at least one, so it certainly can multiply c times ten to get a. it's definitely at least one. so the right hand side here is unquestionably bigger than the left hand side. a in fact is bigger than ten, let alone bigger than ten. so the interesting ingredient is generally the inductive step so remember what you do is here is you assume you've already proven the assertion that, in this case the t of n is at most an for all smaller integers, and now you just merely have to prove it again for the current integer. so we're now interested in the case where n is bigger than one and the assumption that we've already [sound] proven to everything smaller is called inductive hypotheses. so what does it mean that we already proved it for all smaller numbers, that means we can use in the proof of our inductive step the fact that p of k is the most a times k for all k strictly less than n. all we gotta do is enlarge the range of n's to which this holds to one more to the current value n. and all we have to do is follow our nose. so pretty much, we, we have to start on the left hand side with t of n, and we have to wind up on the right hand side with a times n. and pretty much, at every step of the proof, there's just gonna be one conceivable thing we could do. so we just follow our nose. we start with what we wanna upper bound, t of n. well, what do we got going for us? the only thing we can do at this point is invoke the recurrence that we were given up here. so we have an upper bound on t of n in terms of the t value of smaller integers. so we are given that t of n is at most c times n, plus t of n over five, plus t of seven-tenths n. of course ignoring fractions, you would round up or round down, if you wanted to be precise, and the auxiliary lecture notes are more precise, if you want to see what the gory details look like. but it's really just exactly the same argument. one just has to be a little bit more anal about it. so now that we've invoked the recurrence, what can we possibly do, right? we can't really do any direct manipulation on any of these three terms. but fortunately, we have this inductive hypothesis. that applies to any value, any integer which is less than n. so we have her, n/5, that's certainly less than n. we have 70 percent of n. that's certainly less than n. so we can apply the inductive hypothesis twice. we already know that these t values are bounded above by a times their arguments. so t of n over 5's at most a, times n over five. t of seven-tenths n is at most a, times seven-tenths n. now we can group terms together, not we're comparing apples to apples. so we have n, times quantity c, plus a/5, plus seven-tenths a. let me just go ahead and group the two a turns together. and that's gonna be nine-tenths a. no, don't forget where we're going, what the end goal is. we want a upper bound t of n by an. so we wanna write that this is bounded above by a times n. and now you see exactly how i reverse engineered our choice of a, as a function of the given constant c. since a is ten times as big as c, if i take 90 percent of a and add c, i just get a back. so by our choice of a. this equals an. and that pretty much wraps things up. so don't forget what all this stuff stands for. so what did we just prove? what did we just prove by induction? we proved t of n is, at most, a constant times n for every n. that is, t of n is big o of n. what was t of n? that was the running time of our algorithm. that's all we cared about. so because t of n is big o of n, indeed, deselect runs in o of n time. 
this optional video will be, more or less, the last word that we have on sorting for the purposes of this course. and it'll answer the question, can we do better? remember, that's the mantra of any good algorithm designer. i've shown you n log n algorithms for sorting, merge short in the worst case, quick sort, on average. can we do better than n log n? indeed, for the selection problem, we saw we could do better than n log n. we could linear time. maybe we can do linear time for sorting as well. the purpose of this video is to explain to you why we cannot, do sorting, in linear time. so this is a rare problem where we understand quite precisely how well it can be solved at least for a particular class of [inaudible] called comparison based sorts which i'll explain in a moment. so here's the form of the theorem, i want to give you the gist of in this video. so in addition to restricting to comparison based sorts which is necessary as we'll see in a second, i'm going to make a second assumption which is not necessary but is convenient for the lecture which is that i'm only going to think about deterministic algorithms for the moment. i encourage you to think about why the same style of arguments gives an n log and lower bound on the expected running time of any randomized algorithm. maybe i'll put that on the course site as an optional theory problem. so, in particular, a quick sort is optimal in the randomized sense. it have average and long end time and then again claims that no comparison based sort can be better than that, even on average. so, i need to tell you what i mean by a comparison based sorting algorithm. what it means, it's a sorting algorithm that accesses the elements of the input array. only via comparisons, it does not do any kind of direct manipulation on a single array element. all it does, is it picks pairs of elements and asks the question is the left one bigger or is the right one bigger. i like to think of comparison based sorts as general purpose sorting routines. they make no assumptions about what the data is other than that it's from some totally ordered set. i like to think of it really as a function that takes as an argument a function pointer that allows it to do comparisons between abstract data types. there's no way to access the guts of the elements. all you can do is go through this api, which allows you to make comparisons. and indeed if you look at the sorting routine and say the unit's operating system, that's exactly how it's set up. you just patch in a function pointer to a comparison operator. i know this sounds super abstract so, i think it becomes clear once we talk about some examples. there's famous examples of comparison based sort including everything we've discussed in the class so far. there's also famous examples of non comparison based sort which we're not gonna cover, but perhaps some of you have heard of or at the very least they're very easy to look up on wikipedia or wherever. so examples include the two sorting algorithms we discussed so far, mergesort. the only way that mergesort interacts with the elements in the input array is by comparing them and by copying them. similarly, the only think quick sort does with the input array elements is compare them and swap them in place. for those of you that know about the heap data structure which we'll be reviewing later in the class. heap sort. where you just, heapify a bunch of elements, and then extract the minimum n times. that also uses only comparisons. so what are some famous non examples? i think this will make it even more clear what we're talking about. so bucket sort is one very useful one. so, bucket sort's used most frequently when you have some kind of distributional assumption on the data that you're sorting. remember that's exactly what i'm focusing on avoiding in this class. i'm focusing on general purpose subroutines where you don't know anything about the data. if you do know stuff about the data, bucket sorting can sometimes be a really useful method. for example, suppose you can model your data as i-i-d samples from the uniform distribution on zero one. so they're all rational numbers, bigger than zero, less than one, and you expect them to be evenly spread through that interval. then what you can do in bucket sort is you can just. preallocate end buckets where you're gonna collect these elements. each one is gonna have the same width, width one over n. the first bucket you just do linear pass with the input array. everything that's between zero and one over n you stick in the first bucket. everything in between one over n and two over n you stick in the second bucket. two over end and three over n you sick in the third bucket and so on. so with the single pass. you've classified the input elements according to which bucket they belong in, now because the data is assumed to be uniform at random, that means you expect each of the buckets to have a very small population, just a few elements in it. so remember if it. elements are drawing uniform from the interval zero one, then it's equally likely to be in each of the n available buckets. and since there's n elements that means you only expect one element per bucket. so that each one is gonna have a very small population. having bucketed the data, you can now just use, say, insertion sort on each bucket independently. you're gonna be doing insertion sort on a tiny number of elements, so that'll run in constant time, and then there's gonna be linear number of buckets, so it's linear time overall. so the upshot is. if you're willing to make really strong assumptions about your data like it's drawn uniformly at random from the interval zero one then there's not an n log in lower bound in fact you can allude the lower bound and sort them in your time. so, just to be clear. in what sense is bucket sort not comparison based? in what sense does it look at the guts of its elements and do something other than access them by pairs of comparisons? well, it actually looks at an element at input array and it says what is its value, and it checks if its value is.17 versus.27 versus.77, and according to what value it sees inside this element, it makes the decision of which bucket to allocate it to. so, it actually stares at the guts of an element to decide how, what to do next. another non-example, which eh, can be quite useful is count and sort. so this sorting algorithm is good when your data again we're gonna make an assumption on the data, when their integers, and their small integers, so they're between zero and k where k is say ideally at most linear in n. so then what you do, is you do a single pass through the input array. again, you just bucket the elements according to what their value is. it's somewhere between zero and k, and it's an integer by assumption. so you need k buckets. and then you do a pass, and you sort of depopulate the buckets and copy them into an output array. and that gives you a, a sorting algorithm which runs in time, o of n plus k. where k is the size of the biggest integer. so the upshot with counting sort is that, if you're willing to assume that datas are integers bounded above by some factor linear in n, proportional to n, then you can sort them in linear time. again county sort does not access the rail and it's merely through comparisons. it actually stares at an element, figures out what it's value is, and uses that value to determine what bucket to put the element in. so in that sense it's not a comparison case sort and it can under compare it's assumptions to beat the end log and lower it down. so a final example is the one that would [inaudible] them rated sort. i think that this is sort of an extension of counting sort, although you don't have to use counting sort as the interloop you can use other so called stable sorts as well. it's the stuff you can read about in many programming books or on the web. and up shot at rated sort. [inaudible]. you, you again you assume that the date are integers. you think of them in digit representation, say binary representation. and now you just sort one bit at time, starting from the least significant bits and going all the way out to the most significant bits. and so the upside of rating sort, it's an extension of counting sort is the sense that if your data is integers that are not too big, polynomially bounded in n. then it lets you sort in linear time. so, summarizing, a comparison based sort is one that can only access the input array through this api, that lets you do comparisons between two elements. you cannot access the value of an element, so in particular you cannot do any kind of bucketing technique. bucket sort, counting sort, and rating sort all fundamentally are doing some kind of bucketing and that's why when you're willing to make assumptions about what the data is and how you are permitted to access that data, that's when you can bypass in all of those cases, this analog and lower value. but if you're stuck with a comparison based sort, if you wanna have something. general purpose. you're gonna be doing n log n comparisons in the worst case. let's see why. so we have to prove a lower band for every single comparison based sorting method, so a fixed one. and let's focus on a particular input length. call it n. okay, so now, let's simplify our lives. now that we're focused on a comparison based sorting method, one that doesn't look at the values of the array elements just in the relative order. we may as well think of the array as just containing the elements... one, two, three, all the way up to n, in some jumbled order. now, some other algorithm could make use of the fact that everything is small integers. but a comparison based sorting method cannot. so there's no loss in just thinking about an unsorted array containing the integers [inaudible] n inclusive. now, depsite seemingly restricting the space of inputs that we're thinking about, even here, there's kind of a lot of different inputs we've gotta worry about, right? so n elements can, can show up, and n factorial different orderings, right? there's n choices for who the first element is, then n-1 choices for the second element, m minus two choices for the third element, and so on. so, there's n factorial for how these elements are, are arranged in the input array. so i don't wanna prove this super formally, but i wanna give you, the gist, i think, the good intuition. now, we're interested in lower bounding the number of comparisons that, this method makes in the worst case. so let's introduce a parameter k, which is its worst case number of comparisons. that is, for every input, each of these end factorial inputs, by assumption, this method makes no more than k comparisons. the idea behind the proof is that, because we have n factorial fundamentally different inputs, the sorting method has to execute in a fundamentally different way on each of those inputs. but since the only thing that causes a branch in the execution of the sorting method is the resolution of the comparison, and we have only [inaudible] comparisons, it can only have two to the k different execution paths. so that forces two to the k to be at least n factorial. and a calculation then shows that, that forces k to be at least omega n log n. so let me just quickly fill in the details. so cross all in-factorial possible inputs just as a thought experiment. we can imagine running this method in factorial times just looking at the pattern of how the comparison is resolved. right? for each of these in-factorial inputs, we run it through this sorting method, it makes comparison number one, then comparison number two, then comparison number three, then comparison number four, then comparison number five, and you know it gets back a zero, then a one, then a one, then a zero. give in some other input and it gets back a one, then a one, then a zero, then a zero and so on. the point is, for each of these in-factorial inputs, it makes at most k comparisons, we can associate that with a k bit string, and because it. is there's only k bits we're only going to see two to the k different k-bit strings two to the k different ways that a sequence of comparisons results. now to finish the proof we are gonna apply something which i don't get to use as much as i'd like in an evident class but it's always fun when it comes up, which is the pigeon-hole principle. the [inaudible] principle you recall is the essentially obvious fact that if you try to stuff k plus one pigeons into just k cubby holes, one of those k cubby holes has got to get two of the pigeons. okay at least one of the cubby holes gets at least two pigeons. so for us what are the pigeons and what are the holes? so our pigeons are these in factorial different inputs. the different ways you can scramble the images one through. and, what are our holes? those are the two indicate different executions that the sorting method can possibly take on. now if. the number of comparisons k used is so small, that two to the k, the number of distinct execution, number of distinct ways comparisons can resolve themselves, is less than the number of different inputs that have to be correctly sorted. then by the pivotal principal. one color [inaudible] gets two holes. that is, two different inputs get treated in exactly the same way, by the sorting method. they are asked, exactly the same k comparisons and the comparisons resolve identically. [inaudible] one. jumbling of one through n, then you get a 01101 then it's a totally different jumbling of n and then again you get a 01101 and if this happens the algorithm is toast, in the sense that it's definitely not correct, right, cuz we've fed it two different inputs. and it is unable to resolve which of the two it is. right? so, it may be one premutation of one through n, or this totally different premutation of one through n. the algorithm has tried to learn about what the input is through these k comparisons, but it has exactly the same data about the input in two, the two cases. so, if it outputs the correct sorted version in one case, it's gonna get the other one wrong. so, you can't have a common execution of a sorting algorithm unscramble totally different premutations. it can't be done. so what have we learned? we've learned that by correctness, two to the k is in fact at least in the factorial. so how does that help us? well, we wanna lower bound k. k is the number of comparisons this arbitrary storing method is using. they wanna show that's at least n log n. so we, to lower bound k, we better lower bound n factorial. so, you know, you could use stirling's approximation or something fancy. but we don't need anything fancy here. we'll just do something super crude. we'll just say, well, look. this is the product of n things, right? n times n minus one time n minus two, blah, blah, blah, blah. and the largest of those, the n over two largest of those n terms are all at least n over two. the rest we'll just ignore. pretty sloppy, but it gives us a lower bound of n divided by two raised to the n divided by two. now we'll just take log base two of both sides, and we get the k is at least n over two, log base two of n over two, also known as omega of n log n. and that my friends is why a heretics deterministic sorting algorithm that's comparison based has gotta use n log n comparisons in the worst case. 
so, in this set of lectures we'll be discussing the minimum cut problem and graphs and we'll be discussing the randomized contraction algorithm. randomize algorithm which is so simple and elegant and it's almost impossible to believe that it can possibly work but that's exactly at what we'll be approving. so one way you can think about these set of lectures, as a segue of sorts, between our discussion of randomization and our discussion of graphs. so we just finished talking about randomization in the context of sorting and searching. we'll pick it up again toward the end of the class when we discuss hashing. but while we're in the middle of randomization probability review, i'm going to give you another application of randomization in a totally different domain. in particular to the domain of graphs, rather than to sorting and searching. so that's one high level goal of these lectures. the second one, is we'll get our feet wet talking about graphs, and a lot of the next couple weeks, that's what we're going to be talking about, fundamental graph primitives. so this will give us an excuse to start warming up with the vocabulary, some of the basic concepts of the graphs and what a graph algorithm looks like. another perk, although it's not one of the main goals, but i want to do, i do want to point this fact, compared to most of this stuff that we're discussing in this class, this is a relatively recent algorithm, the contraction algorithm. by relatively recent i mean, it's 20 years old. but at least that means most of us, i know not all of us, but most of us at least were born at the time that this algorithm was invented. and so just one quick digression. in an intro course like this, most of what we're going to cover are oldies but goodies, stuff from as much as 50 years ago. and while it's kind of amazing, given how much the world and how much technology has changed over the past 50 years, that ideas in computer science from that long ago are still useful, they are. it's just sort of an amazing thing about the stuff that the first generation of computer scientists figured out. it's still relevant to this day. that said, algorithms is still a vibrant field with lots of open questions. and when i have an opportunity, i'll try and give you glimpses of that fact. so i do want to point out here that this is a somewhat more recent algorithm than most of the other ones we'll see, which dates back to the 90s. so let's talk about graphs. fundamentally, what a graph does is represent pair-wise relationships among a set of objects. so, as such, a graph is going to have two ingredients. so first of all, there's the objects that you're talking about. and these have two very common names and you're just going to have to know both of the names, even though they're completely synonymous. the first name is vertices. so vertex is the singular, vertices is the plural. also known interchangeably as nodes. i'll be using the notation v for the set up of vertices. so those are the objects, now i want to represent pair wise relationships so these pairs are going to be called edges. will be noted by, denoted by e. and there's two flavors of graphs and both are really important. both come up all the time in applications, so you should be aware of both kinds. so there's undirected graphs and directed graphs and that just depends on whether the edges themselves are undirected or directed. so edges can be undirected by which i mean this pair is unordered. there are just two vertices of an edge the two endpoints, say u and v, and you don't distinguish one as the first and one as the second. or edges can be directed, in which case you have a directed graph. and here, a pair is ordered, so you do have a notion of a first vertex, or a first end point. and the second vertex or second end point of an edge. those are often called the tail and the head respectively. and once in a while, although i will try to not use this terminology, you hear directed edges called arcs. now i think all of this is much clearer if i just draw some pictures. indeed one use to call graphs, dots and lines. the dots would refer to the vertices, so here's four dots, or four vertices. and the edges would be lines, so the way you denote one of these edges is you just draw a line between the two end points of that edge, the two vertices that it corresponds to. so this is undirected graph with four vertices and five edges. we can equally we'll have a directed version of this graph. so let's still have four vertices and five edges, but to indicate that this is directed graph and then each edge was first vertex and the second vertex, were going to add arrows to the line. so the arrow points to the second vertex, or to the head of the edge. so, the first vertex is often called the tail of the edge. so, graphs are completely fundamental, they show up not just in computer science but in all kinds of different disciplines, social sciences and biology being two prominent ones. so, let me just mention a couple of reasons you might use them just off the top of my head but literally there's hundreds or thousands of others, so a very literal example would be road networks. so imagine you type in asking for your driving directions from point a to point b in some web application or software, or whatever, it computes a route for you. what it's doing, is it's manipulating some representation of a road network, which inevitably is going to be stored as a graph, where the vertices corresponds to intersections and the edges correspond to individual roads. the web is often fruitfully thought of as a directed graph, so here the vertices are the individual web pages, and edges correspond to hyperlinks. so the first vertex in an edge detail is going to be the page that contains the hyperlink. the second vertex, or the head of the edge, is going to be what the hyperlink points to. so that's the web as a directed graph. social networks are quite naturally represented as graphs. so here the vertices correspond to the individuals in the social network. and the edges correspond to relationships. they have friendship links. i encourage you to think about among the popular social networks these days, which ones are undirected graphs and which ones are directed graphs, we have some interesting examples of each of those.. and often graphs are useful even when there isn't such an obvious network structure. so just to mention one example. let me just write down precedence constraints. so to say what i mean, you might think about, let's say you're a freshman in college and you're looking at your majors, you're a science major and you want to know what courses to take in what order. and you can think about the following graph where each of the courses in your major corresponds to a vertex and you draw a directed edge from course a to course b, if course a is a prerequisite for course b. that is, it has to be completed before you begin course b. okay, so that's a way to represent dependencies, sort of a temporal ordering, between pairs of objects using a directed graph. so that's the basic language of graphs. let me now talk about cuts in graphs. because again, this set of lectures is going to be about so called minimum cut problem. so, the definition of a cut of a graph is very simple, it's just a grouping, a partition of the vertices of the graph into two groups, a and b, and both of those two groups should be non-empty. so, to describe this in pictures, let me give you a cartoon of the cut in both the undirected and directed cases. so for an undirected graph, you can imagine drawing your two sets, a and b. and once you've defined the two sets a and b, the edges then fall into one of three categories. you've got edges with both of the endpoints in a. you've got edges with both of the endpoints in b. and then, you've got edges with one endpoint in a, and one endpoint in b. so that's generically what the picture of the graph looks like viewed through the lens of a particular cut, a b. the picture for directed graphs is similar. you would again have an a, and you'd again have a b, you have directed edges with both endpoints in a, directed edges with both endpoints in b. and now you should have two further categories, so you have edges who cross the cut from left to right, that is tail vertex is in a and the head vertex is in b and you can also have edges which cross the cut in the opposite direction, that is their tail is in b and their head is in a. usually when we talk about cuts, we're going to be concerned with how many edges cross the given cuts. and by that i mean the following, the crossing edges of a cut (a,b) are those that satisfy the following property. so in the undirected case, it's exactly what you think it would be, one of the endpoints is an a, the other endpoint is in b, that's what it means to cross the cut. now in the directed case, there's a number of reasonable definitions you could propose, about which edges crossed the cut. typically and in this course, we're going to focus on the case where we only think about edges that cross the cut from the left to the right, and we ignore edges which cross from the right to the left. so that is the edges that cross the cut are those with tail in a and head in b. so referring to our two pictures, our two corrections of cuts for the underrated one all three of these blue edges would be the edges crossing the cut ab. because they're the ones that have one end point on the left side and one end point on the right side. now for the directed one, we only have two crossing edges. so the two that cross from left to right. we have tail in a and head in b. the one that's crossing backwards does not contribute. we don't count it as a crossing edge of the cut. so the next quiz is just a sanity check that you've absorbed the definition of a cut of a graph. all right, so the answer to this quiz is the third option. recall what is the definition of a cut, it's just a way to group the vertices into two sets a and b, both should also be not empty. so we have n vertices and essentially we have one binary degree of freedom for each, for each vertex, we can decide whether or not it goes in set a or it goes in set b, so two choices for each of the n vertices, that gives us a two to the n possible choices, two to the n possible cuts overall. now that's slightly incorrect because we call that a cut. you can't have a non empty set a or a non empty set b, so those are two of the two to the n options which are disallowed. so strictly speaking the number is two to the n minus two, but two to the n is certainly the closest answer of the four provided. now, the minimum cut problem is exactly what you'd think it would be. i give you as input a graph and among these exponentially, many cuts, i want you to identify one for me with the fewest number of crossing edges. so a few quick comments, so first of all the name for this cut is a min cut. a min cut is one with the fewest number of crossing edges. secondly, to clarify, i am going to allow in the input what's called parallel edges. there will be lots of applications where parallel edges are sort of pointless, but for minimum cut actually it's natural to allow parallel edges. and that means you have two edges that correspond to exactly the same pair of vertices. finally, the more seasoned programmers among you are probably wondering what i mean by, you're given the graph as input. you might be wondering about how exactly that's represented, so the next video's going to discuss exactly that, the popular ways of representing graphs and how you're usually going to do it in this course, specifically via what's called an adjacency list. okay, so i want to make sure that everybody understands exactly what the minimum problem is asking. so, let me draw for you a particular graph with eight vertices and quite a few edges. and what i want you to answer is what is the min cut value in this graph? that is, how many edges cross the minimum cut, the cut with the fewest number of crossing edges? all right, so the correct answer is the second choice. the min cut value is 2 and the cut which shows that, is just to break it basically in half. and there were two halves. in this case, there are only two crossing edges, this one and this one. and i'll leave it for you to check that there's no other edge that has as few as two edges. now in this case, we got a very balanced split when we took the minimum cut. in general, that need not be true. sometimes even a single vertex can define the minimum cut of a graph, and i encourage you to think about a concrete example that proves that. so why should you care about computing the minimum cut? well, this is one problem among a genre called graph partitioning, where you're given a graph and you want to break it into two or more pieces. and these kinds of graph partitioning problem comes up all the time, in a surprisingly diverse array of applications. so let me just mention a couple at a high level. so one very obvious one when your graph is representing its physical network, when identifying something like a min cut allows you to do, is identify weaknesses in your network. perhaps it's your own network, and you want to understand where you soup of the infrastructure because it's, in some sense, a hot spot of your network or a weak point. or, maybe there's someone else's network and you want to know where the weak spot in their network. in fact, there are some declassified documents about 15 years ago or so. which showed that the united states and soviet union militaries, back during the cold war, were actually quite interested in computing minimum cuts, because they were looking for things like, for example, what's the most efficient way to disrupt the other country's transportation network? another application, which is a big deal in social network analysis these days, is the idea of community detection. so the question is among the huge graph, say the graph of everybody who is on facebook or something like that. how can you identify small pockets of people that seem tightly knit, that seem closely related, from which you like to infer that there are community of some sort? maybe they all go to the same school, maybe they all have the same interest, maybe they're part of the same biological family whatever. now, it's to some degree still an open question how to best define communities and social networks. but as a quick and dirty sort of first order heuristic, you can imagine looking for small regions, which on the one hand, are highly interconnected among themselves, but quite weakly connected to the rest of the graph. so sub-routines like the minimum cut problem, can be used for identifying these small densely interconnected, but then weakly connected to everybody else, pockets of a graph. finally, cut problems are also used a lot in vision. so for example, one way you can use them in what's called image segmentation. so here what's going on is you're given as input a 2d array where each entry is a pixel from some image. and there's a graph, which is very natural to define, given a 2d array of pixels. namely, you have an edge between two pixels if they are neighboring. so for two pixels that are immediately next to each other left and right or top to bottom, you put an edge there. so that gives you what's called a grid graph. and now unlike the basic minimum cut problem that we're talking about here, in image segmentation it's most natural to use edge weights. where the weight of an edge is basically how likely you expect those two pixels to be coming from a common object. why might you're expect to enabling pixels to come from the same object, well perhaps their color maps were almost exactly the same and you just expected that they're part of the same thing. so once you've defined the screen graph which suitable edge ways now you run a graph partitioning or maybe cut type separate team, and the hope is that the cut that it identifies rips off one of the contiguous objects in the picture. and then you do that a few times and you get the major objects in the given picture. so this list is by no means exhaustive of the applications of min cut and graph partitioning server teams, but i hope it serves as sufficient motivation to watch the rest of the lectures in this sequence. 
okay, so this video's not about any particular graph problem, not about a, any particular graph algorithm. just, sort of, the preliminaries we need to discuss algorithms on graphs. how do we measure their size? how do we represent them, and so on. remember what a graph is, it really has two ingredients. first of all, there's this set of objects we're talking about. those might be called vertices. synonymously, we might call them nodes. we represent pair wise relationships using edges. these can be either un-directed in which case, they're ordered pairs or an edge can be directed from 1 to another. in that case, they're ordered pairs, and we have a directed graph. now, when we talk about say, the size of a graph, or the running time of an algorithm that operates on a graph. we need to think about what we mean by input size. in particular, for a graph, there's really two different parameters that control how big it is, unlike an array. for arrays, we just had a single number, the length. for graphs, we have the number of vertices, and we have the number of edges. usually we'll use the notation n for the number vertices, m for the number of edges. so the next quiz will ask you to think about how the number of edges m, can depend on the number of vertices, n. so, in particular, i want you to think about in this quiz, an un-directed graph it has n vertices. there's no parallel edges. 'kay, so for a given pair of vertices, there's either zero or one edge between them. moreover, let's assume that the graph is unconnected. 'kay? so i don't want you to think about graphs that have zero edges. now, i haven't defined what a graph is. what it means for a graph to be connected formally, yet, but i hope you get the idea. it means it's in one piece, you can't break it into two parts that have no edges crossing between them. so, for such a graph, no parallel edges, in one piece, n vertices, think about what is the minimum number of edges it could possibly have, and what is the maximum number of edg es, as a function of n, that it could possibly have. all right, so the correct option is the first one the fewest number of edges that a connected undirected graph we can have is n minus 1, and the maximum number of edges that an undirected graph with no parallel edges can have is n times n minus 1 over 2, better known as n choose 2. so why does it need at least n minus 1 edges, if it's going to be in one piece. well think about at, adding the edges one at a time. okay, on each of the edges of the graph. now, initially, you just have a graph with zero edges, the graph has indifferent pieces and isolated vertices has no edges at all. now each time you add one edge, what you do is you take two of the existing pieces, at best, and fuse them into one. so, the maximum decrease you can have in the number of different pieces of a graph is it can decrease by 1 each time you add an edge. so from a starting point of n different pieces, you've got to get down to 1 piece. so that requires the addition of n minus 1 edges. you can also convince yourself of this best, by drawing a few pictures and noticing that trees achieve this bound exactly, so for example here is a 4 vertex tree that has 3 edges. so this is a case where m is indeed, n minus 1. now, for the upper bound, why can't you have more than n choose 2? well, it's clear that the largest number of edges you can have is for the complete graph. where every single pair of edges has 1 between them. again, there's no parallel arcs and edges are unordered. so, there's at most, n choose 2 possibilities of where to put an edge. so again, if n equals 4, here would be an example with a maximum possible number, 6 edges. so, now that i've got you thinking about how the number of edges can vary with the number of vertices. let me talk about the distinction between sparse and dense graphs. it's important to distinguish between these two concepts because some data structures and algorithms are better suited for sparse graphs. other data structures and algorithms are better suited for dense graphs. so, to make this precise, let me just put down this very common notation n is the number of vertices of the graph under discussion, m is the number of inches. this is quite standard notation. please get used to it and use it yourself. if you reverse these, you will confuse a lot of people who have familiarity with graph algorithms and data structures. now one thing we learned from the previous quiz is the following. so in most applications, not all applications, but most applications, m is at least linear in n. remember in the quiz we saw is at least n minus 1 if you wanted the graph to be connected, and it's also big o of n squared. this is under the assumption that there's no parallel arcs. now, there are cases where we want to allow parallel arcs. in fact we'll do that in the contraction algorithm for the min cut problem. there are cases where we want to allow the number of edges to drop so low, that the graph breaks into multiple pieces. for example, when we talk about connected components but more often than not, we're thinking about a connected graph with no parallel edges. and then we can pin down the number of edges m to be somewhere between the linear and the number of nodes, linear and n and quadratic in it. now i'm not going to give you a super formal definition of what a sparse or a dense graph is, and people are a little loose with this, this terminology in practice. but basically, sparse means you're closer to the lower bound, closer to linear. dense means, you're closer to the upper bound, closer to quadratic. now i know this leaves ambiguity when the number of edges is something you know like n to the 3 halves. usually in that case you'd think of that as a dense graph. so usually anything which is more than n times logarythmic terms, you'd think of that as a dense graph. but again, people are a little bit sloppy with this when they talk about graphs. next i want to discuss two representations of graphs and we're mostly going to be using the s econd one in this course, but this first one, the adjacency matrix, i do want to mention just briefly, just on this slide. this is the supernatural idea where you represent the edges in a graph using a matrix. let me describe it first for undirected graphs. so, the matrix is going to be denoted by capital a, and it says square n by n matrix where n is the number of vertices of the graph. and the semantics are the i-jth entry of the matrix is 1. if and only if there's an edge between the vertices i and j in the graph. i'm assuming here that the vertices are named 1, 2, 3, 4, et cetera all the way up to n. it's easy to add bells and whistles to the adjacency matrix to accommodate parallel edges to accommodate edge weights, which is accommodate directed arcs, directed edges. if you wanted to have parallel arcs, you could just have aij denote the number of arcs that are between i and j. if edges have different weights, you could just have aij be the weight of the ij edge. and for the directed graph you could use plus ones and minus ones. so if the arc is directed from i to j, you'd set i, aij to be plus 1. if the arc is directed from j to i, you'd set aij to minus 1. there are many metrics by which you can evaluate a data structure, or a representation. two important ones i want to discuss here. first of all, the number of resources it requires and in this context, that's the amount of space that the data structure needs. the second thing is what are the operations of the data structure supports. so let's just begin with space requirements. what are they for the adjacency matrix? alright, so the answer at least with the sort of straight forward way of storing a matrix is n squared. and this is n dependent of the number of edges. so you could try to beat this down for sparse graphs using sparse matrix tricks. but for the basic idea of just actually representing an n by n matrix, you got n squared entries, you gotta store one bit in each whether the edge is there or not. so that's going to give yo u n squared space. the constants are, of course, very small, because you're just storing one bit per entry. but nonetheless this is quadratic in the number of vertices. now that's going to be fine if you have a dense graph, the number of edges is as high as n squared, then you're not really wasting anything in this representation. but in a sparse graph, if m is much closer to linear, then this is a super wasteful representation. let's talk about the ajacently list representation, this is the, the dominant one we'll be using in this class. this has several ingredients. so, first you keep track of both the vertices and the edges as independent entities. so you're going to have an array, or a list of each. and then we want these two arrays to cross-reference each other in the obvious way. so given a vertex, you want to know which edges it's involved in. given an edge, you want to know what its endpoints are. so, let's say first, most simply, each edge is going to have two pointers, one for each of the two endpoints. and in directed graph, of course, it would keep track of which one is the head and which one is the tail. now, each vertex is going to point to all of the edges of which it's a member. now in an undirected graph, it's clear what i mean by that. in a directed graph, you could do it in a couple ways. generally you'd have a vertex, keep track of all of the edges, for which it is the tail. that is, all of the edges which you can follow one hop out from the edge. if you wanted to, you can also have a second array, at a more expense of storage, where the vertex also keeps track of the edges pointing to it. the edges for which it's the head. so, let me ask you the same question i did with an adjacency matrix. what is the space required of an adjacency list, as a function of the number of edges m, and the number of vertices n, of the graph? so, the correct answer to this question is the third option, theta of m plus n, which we're going to think of as linear space in the size of the gra ph. so, this quiz is, is a little tricky. so, it's explain the answer when we return to the slide with the ingredients of adjacency lists. and let's compute the space for each of these four ingredients separately. most of them are straightforward. for example, consider the first ingredient. this is just an array, or a list of the n vertices. and we just need constant space per vertex to keep track of its existence. so this is going to be theta of n, linear in the number of vertices. similarly, for the m edges, we just need linear space in the number of edges to remember their existence. so that's going to be theta of m. now, each edge has to keep track of both of its endpoints. so that's two pointers, but two is a constant. for each of the m edges, we have a constant space to keep track of endpoints. so that's going to give us another theta of m constant per edge. now, this fourth case, you might be feeling kind of nervous, because a vertex, in principle could have edges involving all n minus 1 of the vertices. so the number of point or is it a single vertex could be theta of n. also you could have you know, you do have n vertices that could be theta of n squared. and certainly in something like a complete graph you really would have that function. but the point is in sparse graphs n, n squared is way overkill to the space needed by this fourth set of pointers. actually, if you think about it for each pointer in the fourth category, a vertex pointing to a given edge, there is a pointer in the third category pointing in the opposite direction, from that edge back to that vertex. so, there's actually a one to one correspondence. between pointers in the third category, and pointers in the fourth category. since the third category has space theta of m, so does all of the pointers in the fourth category. so adding up over the four ingredients, we have one theta of n, and three theta of ms, so that's going to give us overall a theta of m plus n. if you prefer, another way you could think about this would be theta of the max of m and n. these are the same up to a constant factor. now, as we discussed in a previous slide. often, m is going to be bigger than n, but i wanted to do a generic analysis here, which applies even if the graph is not connected, even, even if it is in multiple pieces. so the space of the adjacency list is within a constant factor the same as the number of ingredients in the graph, the number of vertices plus the number of edges. so in that sense, that's exactly what you want. now being confronted with these two graph representations that i've shown you i'm sure you're asking, well, which one should you remember? which one should you use? and the answer, as it so often is, is it depends. it depends on two things. it depends on the density of your graph. it depends on how m compares to n. and it also depends on what kind of operations that you support, want to support. now given what we're covering in this class, and also the motivating applications i have in mind i can give you basically a clean answer to this question for the purposes of these five weeks. which is we're going to be focusing on adjacency lists. the reason we're going to focus on adjacency lists in this class, is both, is for both of these reasons, both because of the operations we want and both because of the graph density and motivating applications. so, first of all, most of the graph primitives, not all, but most, will be dealing with graph search and adjacency lists are perfect for doing graph search. you get to a node. you follow an outgoing arc. you go to another node. you follow an outgoing arc and so on. and so, adjacency lists are the perfect thing to do graph search. adjacency matrices are definitely good for certain kinds of graph operations. but they're not things we're really going to be covering in this class. so that's reason one. reason two is, a lot of the motivations for graph primitives these days comes from massive, massive networks. i mentioned earlier how the web ca n be fruitfully thought of as a directed graph. where the vertices are individual web pages. and directed arcs correspond to hyperlinks, going from the page with the hyperlink, pointing to the one that the hyperlink goes to. now, it's hard to get an exact measurement of the web graph, but a conservative lower bound on the number of vertices is something like 10 billion. so that's 10 to the 10. now that's pushing the limits of what computers can do, but it's within the limits. so if you work hard, you can actually operate on graphs with 10 to the 10 nodes. now, suppose we use an adjacency matrix representation. so if n is 10 to the 10, then n squared is going to be like 10 to the 20. and now we're getting close to the estimated number of atoms in the known universe. so that is clearly not feasible now and it's not going to be feasible ever. so the adjacency matrix representation is totally out for, huge sparse graphs like the web graph. adjacency lists, well, the degree, on average, in the web, is thought to be something like 10. so, the number of edges is only going to be something like 10 to the 11. and then the adjacency of this representation will be proportional to that. and again, that's really pushing what we can do with current technology, but it is within the limits, so using that representation we can do non-trivial computations on graphs, even at the scale of the web graph. 
so now i get to tell you about the very cool randomized contraction algorithm for computing the minimum cut of a graph. let's just recall what the minimum cut problem is. we're given as input an undirected graph. and the parallel edges are allowed. in fact, they will arise naturally throughout the course of the algorithm. that is, we're given pair of vertices, which have multiple edges which have that pair as endpoints. now, i do sort of assume you've watched the other video on how graphs are actually represented, although that's not going to play a major role in the description of this particular algorithm. and, again, the goal is to compute the cut. so, a cut is a partition of the graph vertices into two groups, a and b. the number of edges crossing the cut is simply those that have one endpoint on each side. and amongst all the exponentially possible cuts, we want to identify one that has the fewest number of crossing edges, or a "min cut". >>so, here's the random contraction algorithm. so, this algorithm was devised by david karger back when he was an early ph.d student here at stanford, and this was in the early 90s. so like i said, quote unquote only about twenty years ago. and the basic idea is to use random sampling. now, we'd known forever, right, ever since quicksort, that random sampling could be a good idea in certain context, in particular when you're sorting and searching. now one of the things that was such a breakthrough about karger's contraction algorithm is, it showed that random sampling can be extremely effective for fundamental graph problems. >>so here's how it works. we're just gonna have one main loop. each iteration of this while-loop is going to decrease the number of vertices in the graph by 1, and we're gonna terminate when we get down to just two vertices remaining. now, in a given iteration, here's the random sampling: amongst all of the edges that remain in the graph to this point, we're going to choose one of those edges uniformly at random. each edge is equally likely. once you've chosen an edge, that's when we do the contraction. so we take the two endpoints of the edge, call them the vertex u and the vertex v, and we fuse them into a single vertex that represents both of them. this may become more clear when i go through a couple examples on the next couple of slides. this merging may create parallel edges, even if you didn't have them before. that's okay. we're gonna leave the parallel edges. and it may create a self-loop edge pointer that both of the endpoints is the same. and self-loops are stupid, so we're just gonna delete as they arise. each generation decreases the number of vertices that remain. we start with n vertices. we end up with 2. so after n-2 generations, that's when we stop and at that point we return the cuts represented by those two final vertices. you might well be wondering what i mean by the cut represented by the final two vertices. but i think that will become clear in the examples, which i'll proceed to now. >>so suppose the input graph is the following four node, four edge graph. there's a square plus one diagonal. so, how would the contraction algorithm work on this graph? well, of course, it's a randomized algorithm so it could work in different ways. and so, we're gonna look at two different trajectories. in the first iteration each of these five edges is equally likely. each is chosen for contraction with twenty percent probability. for concreteness, let's say that the algorithm happens to choose this edge to contract, to fuse the two endpoints. after the fusion these two vertices on the left have become one, whereas the two vertices on the right are still hanging around like they always were. so, the edge between the two original vertices is unchanged. the contracted edge between the two vertices on the left has gotten sucked up, so that's gone. and so what remains are these two edges here. the edge on top, and the diagonal. and those are now parallel edges, between the fused node and the upper right node. and then i also shouldn't forget the bottom edge, which is edge from the lower right node to the super node. so that's what we mean by taking a pair of the vertices and contracting them. the edge that was previously connected with them vanishes, and then all the other edges just get pulled into the fusion. >>so that's the first iteration of karger's algorithm of one possible execution. so now we proceed to the second iteration of the contraction algorithm, and the same thing happens all over again. we pick an edge, uniformly at random. now there's only four edges that remain, each of which is equally likely to be chosen, so the 25% probability. for concreteness, let's say that in the second iteration, we wind up choosing one of the two parallel edges, say this one here. so what happens? well, now, instead of three vertices, we go down to 2. we have the original bottom right vertex that hasn't participated in any contractions at all, so that's as it was. and then we have the second vertex, which actually represents diffusion of all of the other three vertices. so two of them were fused, the leftmost vertices were fused in iteration 1. and now the upper right vertex got fused into with them to create this super node representing three original vertices. so, what happens to the four edges? well, the contracted one disappears. that just gets sucked into the super node, and we never see it again. again, and then the other three go, and where there's, go where they're supposed to go. so there's the edge that used to be the right most edge. that has no hash mark. there's the edge with two hash marks. that goes between the, the same two nodes that it did before. just the super node is now an even bigger node representing three nodes. and then the edge which was parallel to the one that we contracted, the other one with a hash mark becomes a self-loop. and remember what the, what the algorithm does is, whenever self loops like this appear, they get deleted automatically. and now that we've done our n-2 iterations, we're down to just two nodes. we return the corresponding cut. by corresponding cut, what i mean is, one group of the cut is the vertices that got fused into each other, and wound up corresponding to the super node. in this case, everything but the bottom right node, and then the other group is the original nodes corresponding to the other super node of the contracted graphs, which, in this case, in just the bottom right node by itself. so this set a is going to be these three nodes here, which all got fused into each other, contracted into each other. and b is going to be this node over here which never participated in any contractions at all. and what's cool is, you'll notice, this does, in fact, define a min cut. there are two edges crossing this cut. this one, the rightmost one and the bottommost one. and i'll leave it for you to check that there is no cut in this graph with fewer than two crossing edges, so this is in fact a min cut. >>of course, this is a randomized algorithm, and randomized algorithms can behave differently on different executions. so let's look at a second possible execution of the contraction algorithm on this exact same input. let's even suppose the first iteration goes about in exactly the same way. so, in particular, this leftmost edge is gonna get chosen in the first iteration. then instead of choosing one of the two parallel edges, which suppose that we choose the rightmost edge to contract in the second iteration. totally possible, 25% chance that it's gonna happen. now what happens after the contraction? well, again, we're gonna be left with two nodes, no surprise there. the contracted node gets sucked into oblivion and vanishes. but the other three edges, the ones with the hash marks, all stick around, and become parallel edges between these two final nodes. this, again, corresponds to a cut (a, b), where a is the left two vertices, and b is the right two vertices. now, this cut you'll notice has three crossing edges, and we've already seen that there is a cut with two crossing edges. therefore, this is <i>not</i> a min cut. >>so what have we learned? we've learned that, the contractual algorithm sometimes identifies the min cut, and sometimes it does not. it depends on the random choices that it makes. it depends on which edges it chooses to randomly contract. so the obvious question is, you know, is this a useful algorithm. so in particular, what is the probability that it gets the right answer? we know it's bigger than 0, and we know it's less than 1. is it close to 1, or is it close to 0? so we find ourselves in a familiar position. we have what seems like a quite sweet algorithm, this random contraction algorithm. and we don't really know if it's good or not. we don't really know how often it works, and we're going to need to do a little bit of math to answer that question. so in particular, we'll need some conditional probability. so for those of you, who need a refresher, go to your favorite source, or you can watch the probability review part ii, to get a refresher on conditional probability and independence. once you have that in your mathematical toolbox, we'll be able to totally nail this question. get a very precise answer to exactly how frequently the contraction algorithm successfully computes the minimum cut. 
so in the last video i left you with a cliffhanger. i introduced you to the minimum cut problem. i introduced you to a very simple algorithm, randomized algorithm, in the form of contraction algorithm. we observed that sometimes it finds the main cut and sometimes it doesn't. and so the $64000 question is just how frequently does it succeed and just how frequently does it fail. so now that i hope you've brushed up the conditional probability and independence, we are gonna give a very precise answer to that question in this lecture. >>so recalling this problem we are given as input in undirected graph, possibly with parallel edges, and that the goal is to compute among the exponential number of possible different cuts, that's with the fewest number of crossing edges. so, for example in this graph here, which you've seen in a previous video, the goal is to compute the cut (a, b). here, cuz there are only two crossing edges, and that's as small as it gets. that's the minimum cut problem and karger proposed the following random contraction algorithm based on random sampling, so we have n-2 iterations, and the number of vertices gets decremented by 1 in each iteration. so we start with n vertices, we get down to 2. and how do we decrease the number of vertices? we do it by contracting or fusing two vertices together. how do we pick which pair of edges, which pair of vertices to fuse? well we pick one of the remaining edges, uniformly at random. so there's [inaudible] many edges there are remaining. we pick each one, equally likely. what, if the endpoints of that edge are (u, v), then we collapse u and v together into a single super node. so that's what we mean by contracting two nodes into a single vertex and then if that causes any self-loops, and as we saw the examples, we will in general have self-loops, then we delete them before proceeding. after the n-2 generations, only two vertices remain. you'll recall that two vertices naturally correspond to a cut. the first group of the cut a corresponds to the vertices that were fused into one of the super vertices remaining at the end. the other super vertex corresponds to the set b the other original vertices of the graph. >>so the goal of this lec, of this video is to give an answer to the following question: what is the probability of success? where by success, we mean outputs a particular min cut (a, b). so let's set up the basic notation. we're gonna fix any with input graph, undirected graph. as usual we use n to denote the number of vertices and m to denote the number of edges. we're also going to fix a minimum cuts (a, b). if a graph has only one minimum cut, then it's clear what i'm talking about here. if a graph has multiple minimum cuts, i'm actually selecting just one of them. because i'm gonna focus on a distinguished minimum cut (a, b), and we're only gonna define the algorithm as successful if it outputs this particular minimum cut (a, b). if it outputs some other minimum cut, we don't count it. we don't count it as successful. okay. so, we really want this distinguished minimum cut (a, b). in addition to n and m, we're gonna have a parameter k, which is the size of the minimum cut. that is, it's the number of crossing edges of a minimum cut. for example, that cross (a, b). the k edges that cross the minimum cut (a, b); we're going to call capital f. so the picture you wanna have in mind is, there is, out there in the world, this minimum cut (a, b). there's lots of edges with both end points in a, lots of edges possibly with both endpoints in b. but, there's not a whole lot of edges with one endpoint in a and one in endpoint in b. so the edges f, would be precisely, these three crossing edges here. >>so our next step is to get a very clear understanding of exactly when the execution of the contraction algorithm can go wrong, and exactly when it's gonna work, exactly when we're going to succeed. so let me redraw the same picture from the previous slide. so given they were hoping that the contraction algorithm outputs this cut (a, b) at the end of the day, what could possibly go wrong? well, to see what could go wrong, suppose,, at some iteration, one of the edges in capital f, remember f are the edges crossing the min cut (a, b), so it's these three magenta edges in the picture. suppose at some iteration one of the edges of f gets chosen for contraction. well because this edge of f has one endpoint in a and one endpoint in b, when it gets chosen for contraction, it causes this node from a and this node from b to be fused together. what does that mean? that means, in the cut that the contraction algorithm finally outputs, this node from a and this node from b will be on the same side of the output cut. okay, so the cut output by the contraction algorithm will have on one side both the node from a and the node from b. therefore, the output of the contraction algorithm if this happens will be a different cut than (a, b), okay? it will not output (a, b) if some edge of f is contracted. >>and if you think about it, the converse is also true. so let's assume now, that in each of the n-2 iterations, the contraction algorithm never contracts an edge from capital f. remember capital f are exactly the edges with one endpoint in a and one endpoint in b. so if it never contracts any edge of f, then it only contracts edges where both endpoints lie in capital a or both endpoints lie in capital b. well, if this is the case then, vertices from a always stick together in the fused nodes, and vertices from b always stick together in the fused nodes. there is never a iteration where a node from a and a node from b are fused together. what does that mean? that means that when the algorithm outputs <i>cuts</i> all of the nodes in a have been grouped together, all of the nodes in b have been grouped together, in each of the two super nodes, which means that the output of the algorithm is indeed the desired cut (a, b). summarizing, the contraction algorithm will do what we want. it will succeed and output the cut (a, b), if and only if it never chooses an edge from capital f for contraction. therefore, the probability of success, that is, the probability that the output is the distinguished min cut (a, b), is exactly the probability that never contracts an edge of capital f. >>so, this is what we're gonna be interested in here. this really is the object of our mathematical analysis, the probability that in all of the n-2 iterations we never contact an edge of capital f. so, to think about that, let's think about each iteration in isolation, and actually define some events describing that. so for an iteration i, let si denote the event, that we screw up an iteration i. with this notation, we can succinctly say what our goal is, so, to compute the probability of success. what we wanna do is we wanna compute the probability that <i>none</i> of the events, s1, s2 up to n minus, s(n-2) never occur. so, i'm gonna use this not() symbol to say that s1 does not happen. so we don't screw up in iteration 1, we don't screw up in iteration 2, we don't screw up in iteration 3, and so on. all the way up to, we don't screw up, we don't contract anything from capital f, in the final iteration, either. so summarizing, analyzing the success probability of the contraction algorithm boils down to analyzing the probability of this event, the intersection of the not sis with i ranging from iteration 1 to iteration n-2. >>so we're gonna take this in baby steps, and the next quiz will lead you through the first one, which is, let's have a more modest goal. let's just think about iteration 1. let's try and understand, what's the chance we screw up, what's the chance we don't screw up, just in the first iteration? so the answer to this quiz is the second option. the probability is k/m, where k is the number edges crossing the cut (a, b), and m is the total number of edges. and that's just because the probability of s1, the probability we screw up, is just the number of crossing edges. that's the number of outcomes which are bad, which cause which trigger s1, divided by the number of edges. that's the total number of things that could happen. and since all edges are equally likely, it just boils down to this. and by the definition of our notation, this is exactly k/m. so this gives us an exact calculation of the failure probability in the first iteration, as a function of the number of crossing edges, and the number of overall edges. now, it turns out it's gonna be more useful for us to have a bound not quite as exact, an inequality. that's in terms of the number of vertices n, rather than the number of edges, m. the reason for that is, it's a little hard to understand how the number of edges is changing in each iteration. it's certainly going down by 1 in each iteration, because we contract that in each iteration, but it might go down by more than 1 when we delete self-loops. by contrast the number of vertices is this very steady obvious process. one less vertex with each successive iteration. >>so, let's rewrite this bound in terms of the number of vertices n. to do that in a useful way, we make the following key observation. i claim that, in the original graph g, we are given as input, every vertex has at least k edges incident on it, that is in graph theoretic terminology, every edge has degree at least k. where, recall, k is the number of edges crossing our favorite min cut (a, b). so why is that true? why must every vertex have a decent number of neighbors, a decent number of edges incident to it. well, it's because, if you think about it, each vertex defines a cut by itself. remember, a cut is just any grouping into other vertices into two groups, that are not empty, that don't overlap. so one cut is to take a single vertex, and make that the first group, a, and take the other n-1 vertices, and make that the second group, capital b. so how many edges cross this cut? well, it's exactly the edges that are incident on the first note, on the note on the left side. so every single cut, fall exponentially many cuts, have at least k crossing edges, then certainly the n cuts defined by single vertices have at least k crossing edges, so therefore, the degree of a vertex is at least k. so our assumption that every single cut in the graph has at least k crossing edges because it's a lower bound on the number edges incident on each possible vertex. >>so, why is that usual? well let's recall the following general facts about any graph; which is that if you sum up over the degrees of the nodes, so if you go node by node, look at how many edges are insident on that node, that's the degree of v, and then sum them up over all vertices. what will you get? you'll get exactly twice the number of edges, okay? so this is true for any undirected graph, that the sum of the degrees of the vertices is exactly double- the number of edges. to see this, you might think about taking a graph, starting with the empty set of edges, and then adding the edges of the graph one at a time. each time you add a new edge to a graph, obviously the number of edges goes up by 1, and the degree of each of the endpoints of that edge also go up by 1, and there are, of course, two endpoints. so every time you add an edge, the number of edges goes up by 1, the sum of those degrees goes up by 2. therefore, when you've added all the edges, the sum of the degrees is double the number of edges that you've added. that's why this is true. now, in this graph, at that we have a hand here, every degree is at least k, and there's n nodes. so this left hand side, of course, is at least kn for us. so therefore if we just divide through by 2, and flip the inequality around, we have the number of edges has to be at least the size of the crossing cut, so the degrees of every vertex times the number of vertices divided by 2. so this is just the primitive inequality rearranging. putting this together with your answer on the quiz, since the probability of s1 is exactly k/m, and m is at least kn/2, if we substitute, we get that the probability of s1 is at worst 2/n, 2 over the number of vertices, and the k cancels out. so that's, sort of, our first milestone. we've figured out the chance that we screw up in the first iteration, that we pick some edge from the crosses the cut (a, b). and things look good. this is a, this is a small number, right? so, in general, the number of vertices might be quite big. and this says that the probability we screw up is only 2 over the number of vertices. so, so far, so good. of course, this was only the first iteration. who knows what happens later? >>so now that we understand the chances of screwing up in the first iteration, let's take our next baby step, and understand the probability that we don't screw up in either of the first two iterations. that is, we're gonna be interested. and the following probability. the probability that we don't screw up in the first iteration nor in the second iteration. now, as you go back to the definition of a conditional probability, to realize that we can rewrite an intersection like this in terms of conditional probabilities. namely, as the probability that we don't screw up in the second iteration, given that we didn't do it already, times the probability that we didn't screw up in the first iteration. okay? so the probability that we miss all of these k vulnerable edges and in the second iteration given that we didn't contract any of them in the first iteration. now notice this, we already have a good understanding on the previous slide. we are given a nice lower bound of this. we say there's a good chance that we don't screw up, probably at least 1-2/n. and in some sense we also have a very good understanding of this probability. we know this is 1 minus the chance that we do screw up. and what's the chance that we do screw up? well, these k edges are still hanging out in the graph. remember we didn't contract any, in the first iteration that's what's given. so there are k ways to screw up, and we choose an edge to contract uniformly at random, so the total number of choices is the number of remaining edges. >>now the problem is, what's nice is we have an exact understanding of this probability. this is an equality. the problem is we don't have a good understanding of this denominator. how many remaining edges are there? we have an upper bound on this. we know this is at most n-1, assuming we got rid of one edge in the previous iteration, but actually what, if you think about it, what we need of this quantity is a lower bound and that's a little unclear because in addition to contracting the one edge and getting that out of the graph, we might have created a bunch of self loops and deleted all events. so it's hard to understand exactly what this quantity is. so instead we're gonna rewrite this bound in terms of the numbers of the remaining vertices, and of course we know it's exactly n-1 vertices remaining. we took two of the last iterations and contracted down to 1. so how do we relate the number of edges to the number of vertices? well we do it just in exactly the same way as in the first iteration. we'll make some more general observation. in the first iteration, we observed that every node in the original graph induces a cut. okay, with that node was on one side, the other n-1 edges were on the other side. but the fact that's a more general statement, even after we've done a bunch of contractions, any single node in the contracted graph, even if it represents a union of a bunch of nodes in the original graph, we can still think of that as a cut in the original graph. right? so if there's some super node in the contracted graph, which is the result of fusing twelve different things together, that corresponds to a cut where those twelve nodes in the original graph are on the one side a, and the other n-12 vertices are on the other side of the cut, b. so, even after contractions, as long as we have at least two nodes in our contracted graph, you can take any node and think of it as half of a cut, one side of a cut in the original graph. >>now remember, k is the number of edges crossing our minimum cut (a, b), so any cuts in the original graph g has to have k crossing edges. so, since every node in the contracted graph naturally maps over to a cut in the original graph with at least k edges crossing it, that means, in the contracted graph, all of the degrees have to be at least k. if you ever had a node in the contracted graph that had only say k-1 incident edges, well then you'd have a cut in the original graph with only k-1 edges contradiction. so just like in the first iteration, now that we have a lower bound on the degree of every single vertex, we can derive a lower bound on the number of edges that remain in the graph. the number of remaining edges is at least 1/2, that's because when you sum over the degrees of the vertices, you double count the edges, times the degree of each vertex, that we just argued that that's at least k in this contracted graph, times the number of vertices, that we know there's exactly n-1 vertices left in the graph at this point. so now what we do is to plug this inequality, to plug this lower bound of the number of remaining edges, on, as we'll substitute that for this denominator, so in lower bounding the denominator, we upper bound this fraction, which gives us a lower bound on 1 minus that fraction, and that's what we want. so what we find is that the probability that we don't screw up in the second iteration given that we didn't screw up in the first iteration. where again, by screwing up means picking one of these k edges crossing (a, b) to contract is at least 1-(2/(n-1)). so, that's pretty cool. we took the first iteration, we analyzed it, we showed the probability that we screw up is pretty low, we succeed with probability of at least 1-(2/n). in the second iteration, our success probability has dropped a little bit, but it's still looking pretty good for reasonable values of n, 1-(2/(n-1)). >>now, as i hope you've picked up, we can generalize this pattern to any number of iterations, so that the degree of every node of the contracted graph remains at least k. the only thing which is changing is the number of vertices is dropping by 1. so, extending this pattern to its logical conclusion, we get the following lower bound on the probability that the contraction algorithm succeeds. the probability that the contraction algorithm outputs the cut (a, b), you recall we argued, is exactly the same thing as the probability that it doesn't contract anything, any of the k crossing edges, any of the set f in the first iteration, nor in the second iteration, nor in the third iteration, and then so on, all the way up to the final (n-2)th iteration. using the definition of conditional probability, this is just the probability that we don't screw up in the first iteration, times the probability that we don't screw up in the second iteration given that we didn't screw up in the first iteration, and so on. in the previous two slides, we showed that, we don't screw up in the first iteration, with probability of at least 1-(2/n). in the second iteration, with probability at least 1-(2/(n-1)). and of course, you can guess what that pattern looks like. and that results in the following product. now, because we stop when we get down to two nodes remaining, the last iteration in which we actually make a contraction, there are three nodes. and then, the second to last iteration in which we make a contraction, there are four nodes. so that's where these last two terms come from. rewriting, this is just (n-2)/n times (n-3)/(n-1), and so on. and now something very cool happens, which is massive cancellation, and to this day, this is always just incredibly satisfying to be able to cross out so many terms. so you get n-2, cross it out here and now here, there's going to be a pair of n-3s that get crossed out, and n-4s, and so on. on the other side, there's going to be a pair of 4s that get crossed out, and a pair of 3s that get crossed out. and we'll be left with only the two largest terms on the denominator, and the two smallest terms in the numerator, which is exactly 2/n(n-1). and to keep things simple among friends, let's just be sloppy and lower bound this by 1/(n^2). so that's it. that's our analysis of the success probability of karger's contraction algorithm. pretty cool, pretty slick, huh? >>okay, i'll concede, probably you're thinking "hey, wait a minute. we're analyzing the probability that the algorithm succeeds, and we're thinking of the number of vertices n as being big, so we'll see here as a success probability of only 1/(n^2), and that kinda sucks." so that's a good point. let me address that problem. this is a low success probability. so that's disappointing. so why are we talking about this algorithm, or this analysis? well, here's something i want to point out. maybe this is not so good, 1/(n^2) you're going to succeed, but this is still actually shockingly high for an brute-forth algorithm which honestly seems to be doing almost nothing. this is a nontrivial lower bound and non trivial success probability, because don't forget, there's an exponential number of cuts in the graph. so if you try to just pick a random cut i.e you put every vertex 50:50 left or right, you'll be doing way worse than this. you'll have a success probability of like 1/(2^n). so this is way, way better than that. and the fact that its inverse polynomial means is that using repeated trials, we can turn a success probability that's incredibly small into a failure probability that's incredibly small. so lemme show you how to do that next. >>so, we're gonna boost the success probability of the contraction algorithm in, if you think about it a totally obvious way. we're gonna run it a bunch of times, each one independently using a fresh batch of random coins. and we're just going to remember the smallest cut that we ever see, and that's what we're gonna return, the best of a bunch of repeated trials. now the question is, how many trials are we gonna need before we're pretty confident that we actually find the meant cut that we're looking for? to answer this question vigorously, let's define some suitable events. so by ti, i mean the event at the ith trail succeeds, that is the ith time we run the contraction algorithm which does output that desired meant cut (a, b). for those of you that watched the part ii of the probability review, i said a rule of thumb for dealing with independents is that, you should maybe, as a working hypothesis, assume granted variables are dependent, unless they're explicitly constructed to be independent. so here's a case where we're just gonna define the random variables to be independent. we're just gonna say that we run [inaudible] the contraction algorithm over and over again with fresh randomness so that they're gonna be independent trials. now, we know that the, probability that a single trial fails can be pretty big, could be as big as 1-1/(n^2). but, here, now, with repeated trials, we're only in trouble if every single trial fails. if even one succeeds, then we find the meant cut. so a different way of saying that is we're interested in the intersection of t1 and t2 and so on, that's the event that every single trial fails. and now we use the fact that the trials are independent. so, the probability that all of these things happen is just the product of the relevant probabilities. so, the product from i=1 to capital n of the probability of not ti. recall that we argued that the success probability of a single trial was bounded below by 1/(n^2). so the failure probability is bounded above by 1-1/(n^2). so since that's true for each of the capital n terms, you get an overall failure probability for all capital n trials of 1 minus 1/(n^2) raised to the capital of n. alright, so that's a little calculation. don't lose sight of why we're doing the calculation. we want to answer this question, how many trials do we need? how big does capital n need to be before are confident that we get the answer that we want? >>okay, so to answer that question i need a quick calculus fact, which is both very simple and very useful. so for all real numbers x, we have the following inequality, 1+x is bound above by e^x. so i'll just give you a quick proof via picture. so first think about the line 1+x. what does that cross through? well, that crosses through the points when x is -1, y is 0, and when x is 0, y is 1. and it's a line, so this looks like this blue line here. what does e^x look like? well, if you substitute x = 0, it's gonna be 1. so in fact two curves kiss each other at x = 0. but exponentials grow really quickly, so as you jack up x to higher positive numbers, it becomes very, very steep. and for x negative numbers it stays non-negative the whole way. so this sort of flattens out for the negative numbers. so, pictorially, and i encourage you to, you know, type this into your own favorite graphing program. you see the e^x bounds above everywhere, the line, the 1+x. for those of you who want something more rigorous, there's a bunch of ways to do it. for example, you can look at the [inaudible] expansion of e^x at the point 0. >>what's the point? the point is this allows us to do some very simple calculations on our previous upper bound on the failure probability by working with exponentials instead of working with these ugly one minus whatevers raised to the whatever term. so, let's combine our upper bound from the previous slide with the upper bound provided by the calculus fact. and to be concrete, let's substitute some particular number of capital n. so, let's use little n^2 trials, where little n is the number of vertices of the graph. in which case, the probability that every single trial fails to recover the cut (a, b) is bounded above by e to the -1/(n^2). that's using the calculus fact applied with x = -1/(n^2). and then we inherit the capital n and the exponent which we just substantiated to little n^2. so of course the n^2 are gonna cancel, this is gonna give us e^(-1), also known as 1/e. so if we're willing to do little n^2 trials, then our failure probability has gone from something very close to 1, to something which is more like, say, 30 some more percent. now, once you get to a constant success probability, it's very easy to boost it further by just doing a few more trials. so if we just add a natural log factor, so instead of a little n^2 trials, we do little n^2 times the natural log of the little n. now, the probability that everything fails is bound and above by the 1/e that we had last time, but still with the residual natural log of n up top. and this is now, merely 1/n. so i hope it's clear what happened. we took a very simple, very elegant algorithm, that almost always didn't do what we want. it almost always failed to output the cut (a, b). it did it with only probability 1/(n^2). but, 1/(n^2) is still big enough that we can boost it, so that it almost always succeeds just by doing repeated trials. and the number of repeated trials that we need is the reciprocal of its original success probability boosted by, for the logarithmic factor. so that transformed this almost always failing algorithm into an almost always succeeding algorithm. and that's a more general less, more general algorithm technique, which is certainly worth remembering. >>let me conclude with a couple comments about the running time. this is probably the first algorithm of a course, of the course where we haven't obsessed over just what the running time is. and i said, it's simple enough. it's not hard to figure out what it is, but it's actually not that impressive. and that's why i haven't been obsessing over it. this is not almost linear. this is not a for free primitive as i've described it here. so it's certainly a polynomial-time algorithm; its running time is bounded above by some polynomial in n and m. so it's way better than the exponential time you get from brute-force search through all 2^n possible cuts. but it is certainly, the way i've described it, we gotta to n^2 trials, plus a log factor, which i'm not even going to bother writing down. and also, each trial, while at the very least, you look at all the edges, so that's going to be another factor of m. so this is a bigger polynomial than in any, almost any of the algorithms that we're going to see. now, i don't wanna undersell this application of random sampling in computing cuts because i've just shown you the simplest, most elegant, most basic, but therefore also the slowest implementation of using contractions to compute cuts. there's been follow-up work with a lot of extra optimizations, in particular, doing stuff much more clever than just repeated trials, so basically using work that you did in previous trials to inform how you look for cuts in subsequent trials. and you can shave large factors off of the running time. so there are much better implementations of this randomized contraction algorithm than what i'm showing you here. those are, however, outside the course, scope of this course. 
so this is short optional video, really just for fun, i want to point out an interesting consequence tracking algorithm has about a problem that is in pure graph theory. so, to motivate the question, i want to remind you of something we discussed in passing, which is that a graph may have more than one minimum cut. so, there may be distinct cuts which are tied for the fewest number of crossing edges. for a concrete example, you could think about a tree. so, if you just look at a star graph, that is hubs and spokes, it's evident that if you isolate any leaf by itself, then you get a minimum cut with exactly one crossing edge. in fact, if you think about it for a little while, you'll see that in any tree you'll have n-1 different minimum cuts, each with exactly one crossing edge. >>the question concerns counting the number of minimum cuts. namely, given that a graph may have more than one minimum cut, what is the largest number of minimum cuts that a graph with n vertices can have? we know the answer is at least n-1. we already discussed how trees have n-1 distinct minimum cuts. we know the answer at most something like 2^n, because a graph only has roughly 2^n cuts. in fact, the answer is both very nice and wedged in between. so the answer is exactly n choose 2, where n is the number of vertices. this is also known as n(n-1)/2. so it can be bigger than it is in trees, but not a lot bigger. in particular, graphs have only; undirected graphs have only polynomially many minimum cuts. and that's been a useful fact in a number of different applications. so, i'm going to prove this back to you. all i need is one short slide on the lower bound and then one slide for the upper bound, which follows from properties of the random contraction algorithm. >>so for the lower bound, we don't have to look much beyond our trees example. we're just gonna look at cycles. so for any value of n, consider the n cycle. so here, for example, is the n cycle with n = 8. that would be an octagon. and the key observation is that, just like in the tree, how moving each of the n-1 edges breaks the tree into two pieces and defines the cut. with a cycle, if you remove just one edge, you don't get a cut. the thing remains connected, but if you remove any pair of edges, then that induces a cut of the graph, corresponding to the two pieces that will remain. no matter which pair of edges you remove, you get a distinct pair of groups, distinct cuts. so ranging overall n choose 2 choices of pairs of edges, you generate n choose 2 different cuts. each of these cuts has exactly two crossing edges, and it's easy to see that's the fewest possible. >>so that's the lower bound, which was simple enough. let's now move on to the upper bound, which, a purely count-all fact will follow from an algorithm. so consider any graph that has n vertices, and let's think about the different minimum cuts of that graph. what we're going to use is that the analysis of the contraction algorithm proceeded in a fairly curious way. so remember how we define the success probability of a contraction algorithm. we fixed up front, some min cut (a, b). and we defined the contraction algorithm, the basic contraction algorithm, before the repeated trials. we defined the contraction algorithm as successful, if and only if it output the minimum cut (a, b) that we designated upfront. if it output some other minimum cut, we didn't count it. we said nope, that's a failure. so we actually analyzed a stronger property than what we were trying to solve, which is outputting a given min cut (a, b) rather than just any/all min cut. so how is that useful? well, let's apply it here. for each of these t minimum cuts of this graph, we can think about the probability that the contraction algorithm outputs that particular min cut. so we're gonna instantiate the analysis with a particular minimum cut (ai, bi). and what we proved in the analysis is that the probability that the algorithm outputs the cut (ai, bi), not just any/all min cut. but, in fact, this exact cut (ai, bi) is bounded below by. we, in the end, we made a sloppy inequality. we said it's at least 1/(n^2). but if you go back to the analysis, you'll see that it was, in fact, 2/n(n-1), also known as 1/(n choose 2). so instantiating the contraction algorithm success probability analysis without all of the repeated trials business, we show that for each of these t cuts, for each fixed cut (ai, bi), the probability that this algorithm outputs that particular cut is at least 1/(n choose 2). >>let's introduce a name for this event, the event that the contraction algorithm outputs the ith min cut. let's call this si. the key observation is that the sis are disjoint events. remember an event is just a subset of stuff that could happen. so one thing that could happen is that the algorithm outputs the ith main cuts, and by this joint, we just mean that there is no outcome that in a given pair of events. and that's because the contraction algorithm at n to the [inaudible], once it makes its conflicts, it outputs a single cut is a distinct cut. it can only output a dest one of them. >>why is it important that these sis are disjoint events? well, with disjoint events, the probabilities <i>add</i> the probability of the union of a bunch of disjoint events is the sum of the probabilities of constituent events. if you want to think about this pictorially, and just draw a big box, denoting everything that could happen omega, and then these sis just these [inaudible] that don't overlap. so s1, s2, s3, and so on. now the sum or probabilities of this joint events can sum to, at most, 1, right? the probability of all of omega is 1, and these sis have not overlap and are packed into omega, so the sum of their probabilities is gonna be smaller. >>we're adding up formally. we have that the sum of the probabilities. which we can lower bound by the number of different events. and remember there are t different min cuts for some parameter t. for each min cut (ai, bi), a lower bound of the probability that, that could spit out as output is 1/(n choose 2). so a lower bound on the sum of all of these probabilities is the number of them, t times the probability lower bound, 1/(n choose 2), and this has got to be at most 1. rearranging, what do we find? t, the number of different mid-cuts, is bounded above by n choose 2. exactly the lower bound provided by the n cycle. the n cycle has n choose 2 distinct minimum cuts. no other graph has more. every graph has only a polynomial number indeed, at most a quadratic number of minimum cuts. 
so let's talk about the absolutely fundamental problem of searching a graph, and the very related problem of finding paths through graphs. so why would one be interested in searching a graph, or figuring out if there's a path from point a to point b? well there's many, many reasons. i'm going to give you a highly non-exhaustive list on this slide. >>so let me begin with a very sorta obvious and literal example, which is if you have a physical network, then often you want to make sure that the network is fully connected in the sense that you can get from any starting point to any other point. so, for example, think back to the phone network. it would've been a disaster if callers from california could only reach callers in nevada, but not their family members in utah. so obviously a minimal condition for functionality in something like a phone network is that you can get from any one place to any other place, similarly for road networks within a given country, and so on. it can also be fun to think about other non-physical networks and ask if they're connected. so one network that's fun to play around with is the movie network. so this is the graph where the nodes correspond to actors and actresses, and you have an edge between two nodes, if they played a role in a common movie. so this is going to be an undirected graph, where the edges correspond to, not necessarily co-starring, but both the actors appearing at least some point in the same movie. so versions of this movie network you should be able to find publicly available on the web, and there's lots of fun questions you can ask about the movie network. like, for example, what's the minimum number of hops, where a hop here again is the movie that two people both played a role in? the minimum number of hops or edges from one actor to another actor, so perhaps the most famous statistic that's been thought about with the movie is the bacon number. so this refers to the fairly ubiquitous actor kevin bacon, and the question the bacon number of an actor is defined as the minimum number of hops you need in this movie graph to get to kevin bacon. so, for example, you could ask about jon hamm, also known as don draper from "mad men". and you could ask how many edges do you need on a path through the movie graph to get to kevin bacon? and it turns out that the answer is 1, excuse me, 2 edges. you need one intermediate point, namely colin firth. and that's became, that's because colin firth and kevin bacon both starred in atom egoyan's movie, "where "the truth lies", and jon hamm and colin firth were both in the movie "a single man". so that would give jon hamm a bacon number of 2. so, these are the kind of questions you're gonna ask about connectivity. not just physical networks, like telephone and telecommunication networks, but also logical networks about parallel relationships between objects in general. so the bacon number is fundamentally not just about any path, but actually shortest paths, the minimum number of edges you need to traverse to get from one actor to kevin bacon. and shortest paths are also have a very practical use, that you might use yourself in the driving directions. so when you use a website or a phone app and you ask for the best way to get from where you are now to say some restaurant where you're gonna have dinner, obviously you're trying to find some kind of path through a network through a graph, and indeed often you want the, the shortest path, perhaps in mileage or perhaps in anticipated travel time. now i realize that when you are thinking about paths and graphs, it's natural to focus on sort of very literal paths and quite literal physical networks. things like routes through a road network or paths through the internet and so on. you should really think more abstractly as a path as just a sequence of decisions, taking you from some initial state to some final state. and it's this abstract mentality which is what makes graph search so ubiquitous, it feels like artificial intelligence, where you want to formulate a plan of how to get from an initial state to some goal state. so, to give a simple recreational example, you can imagine just trying to understand how to compute automatically a way to fill in a sudoku puzzle so that you get to, so that you solve the puzzle correctly. so you might ask, you know, what is the graph that we're talking about, when we wanna solve a sudoku puzzle. well this is gonna be a directed graph, where here the nodes correspond to partially completed puzzles. so, for example, at one node of this extremely large graph, perhaps 40 out of the 81 cells are filled in with some kind of number, and now, again, remember a path is supposed to correspond to a sequence of decisions. so, what are the actions that you take in solving sudoku? well, you fill in a number into a square. so, an edge which here is going to be directed, is going to move you from one partially completed puzzle to another, where one previously empty square gets filled in with one number. and of course then the path is that you're interested in computing, or what your searching for when you search this graph. you begin with the initial state of the sudoku puzzle and you want to reach some goal state where the sudoku puzzle is completely filled in without any violations of the rules of sudoku. and of course it's easy to imagine millions of other situations where you wanna formulate some kind of plan like this, for example if you have a robotic hand and you wanna grasp some object, you need to think about exactly how to approach the object with this robotic hand, so that you can grab it without, for example, first knocking it over, and you can think of millions of other examples. another thing which turns out to be closely related to graph search, as we'll see, it has many applications in its own right, is that of computing connectivity information about graphs, in particular the connected components. so this, especially for undirected graphs, corresponds to the pieces of the graph. we'll talk about these topics in their own right, and i'll give you applications for them later. so for undirected graphs i'll briefly mention an easy clustering heuristic you can derive out of computing connected components. for directed graphs where the very definition of computing components is a bit more subtle, i'll show you applications to understanding the structure of the web. so these are a few of the reasons why it's important for you understand how to efficiently search graphs. it's a, a fundamental and widely applicable graph primitive. and i'm happy to report that in this section of the course, pretty much anything, any questions we wanna answer about graph search, computing connected components, and so on, there's gonna be really fast algorithms to do it. so, this will be the part of the course where there's lots of what i call for free primitives, processing steps, subroutines you can run without even thinking about it. all of these algorithms we're gonna discuss in the next several lectures, are gonna run in linear time, and they're gonna have quite reasonable constants. so, they're really barely slower than reading the input. so, if you have a graph and you're trying to reason about it and you're trying to make sense about it, you should in some sense feel free to apply any of these subroutines we're gonna discuss to try and glean some more information about what they look like, how you might use the network data. there's a lot of different approaches to systematically searching a graph. so, there's many methods. in this class we're gonna focus on two very important ones, mainly breadth first search and depth first search. but all of the graph search methods share some things in common. so, in this slide let me just tell you the high order bits of really any graph search algorithm. so graph search subroutines generally are passed as input a starting search vertex from which the search originates. so that's often called source vertex. and your goal then is to find everything findable from the search vertex and obviously you're not gonna find anything that you can't find that is not findable. what i mean by findable, i mean, there's a path from the starting point to this other node. so any other node to which you can get along on a path from the starting point you should discover. so, for example, if you're given an undirected graph that has three different pieces, like this one i'm drawing on the right, then perhaps s is this left most node here, then the findable vertices starting from s, i.e. the ones which you can reach from a path to s, is clearly precisely these four vertices. so, you would want graph search to automatically discover and efficiently discover these four vertices if you started at s. you can also think about a directed version of the exact same graph, where i'm gonna direct the vertices like so. so now the definition of the findable nodes is a little bit different. we're only expecting to follow arcs forward, along the forward direction. so we should only expect at best to find all of the nodes that you can reach, by following a succession of forward arcs, that is, any node that there's a path to from s. so in this case, these three nodes would be the ones we'd be hoping to find. this blue node to the right, we would no longer expect to find, because the only way to get there from s, is by going backward along arcs. and that's not what we're going to be thinking about in our graph searches. so we want to find everything findable, i.e. that we can get to along paths, and we want to do it efficiently. efficiently means we don't want to explore anything twice. right, so the graph has m arcs, m edges and n nodes or n vertices and really we wanna just look at either each piece of the graph only once for a small cost number of times. so looking for running time which is linear on the size of the graph that is big o of m plus n. now when we were talking about representing graphs, i said that in many applications, it's natural to focus on connected graphs, in which case m is gonna dominate n, and you're gonna have at least as many edges as nodes, essentially. but connectivity is the classic case where you might have the number of edges of being much smaller than the number of nodes. there might be many pieces of the whole point of what you're trying to do is discover them. so, for this sequence of lectures where we talk about graph search and connectivity, we will usually write m plus n. we'll think that either one can be bigger or smaller than the other. so let me now give you a generic approach to graph search. it's gonna be under-specified, there'll be many different ways to instantiate it. two particular instantiations will give us breadth first search and depth first search but here is just a general systematic method to finding everything findable without exploring anything more than once. so motivated by the second goal, the fact that we don't want to explore anything twice, with each node, with each vertex, we're gonna remember whether or not we explored it before. so we just need one boolean per node and we will initialize it by having everything unexplored except s, our starting point we'll have it start off as explored. and it's useful to think of the nodes thus far as being in some sense territory conquered by the algorithm. and then there's going to be a frontier in between the conquered and unconquered territory. and the goal of the generic outcome is that each step we supplement the conquered territory by one new node, assuming that there is one adjacent to the territory you've already conquered. so for example in this top example with the undirected network, initially the only thing we've explored is the starting point s. so that's sort of our home base. that's all that we have conquered so far. and then in our main while loop, which we iterate as many times as we can until we don't have any edges meeting the following criterion, we look for an edge with one end point that we've already explored. one end point inside the conquered territory and then the other end point outside. so this is how we can in one hop supplement the number of nodes we've seen by one new one. if we can't find such an edge then this is where the search stops. if we can find such an edge, well then we suck v into the conquered territory. we think of it being explored. and we return to the main while loop. so, for example, in this example on the right, we start with the only explored node being s. now, there's actually two edges that cross the frontier, in the sense one of the endpoints is explored, namely one of the endpoints is s, and the other one is some other vertex. right? there's this there's these two vert, two edges to the left, two vertices adjacent to s. so, in this algorithm we pick either one. it's un, under-specified which one we pick. maybe we pick the top one. and then all of the sudden, this second top vertex is also explored so the conquered territory is a union of them, and so now we have a new frontier. so now again we have two edges that cross from the explored nodes to the unexplored nodes. these are the edges that are in some sense going from northwest to southeast. again, we pick one of them. it's not clear how. the algorithm doesn't tell us, we just pick any of them. so, maybe for example we pick this right most edge crossing the frontier. now the right most edge of these-- right most vertex of these four is explored so our conquered territory is the top three vertices. and now again we have two edges crossing the frontier. the two edges that are incident to the bottom node, we pick one of them, not clear which one, maybe this one. and now the bottom node is also explored. and now there are no edges crossing the frontier. so there are no edges who, on the one hand, have one end-point being explored, and the other end-point being unexplored. so these will be the four vertices, as one would hope, that the search will explore started from s. well generally the claim is that this generic graph search algorithm does what we wanted. it finds everything findable from the starting point and moreover it doesn't explore anything twice. i think that's fairly clear that it doesn't explore anything twice. right? as soon as you look at a node for the first time, you suck it into the conquered territory never to look at it again. similarly as soon as you look at an edge, you suck them in. but when we explore breadth and depth first search, i'll be more precise about the running time and exactly what i mean by you don't explore something twice. so, at this level of generality, i just wanna focus on the first point, that any way you instantiate this search algorithm, it's going to find everything findable. so, what do i really mean by that? the formal claim is that at the termination of this algorithm, the nodes that we've marked exp-, explored, are precisely the ones that can be reached via a path from s. that's the sense in which the algorithm explores everything that could potentially be findable from the starting point s. and one thing i wanna mention is that this claim and the proof i'm going to give of it, it holds whether or not g is an undirected graph or a directed graph. in fact, almost all of the things that i'm gonna say about graph search, and i'm talking about breadth first search and depth first search, work in essentially the same way, either in undirected graphs or directed graphs. the obvious difference being in an undirected graph you can traverse an edge in either direction. in a directed graph, we're only supposed to traverse it in a forward direction from the tail to the head. the one big difference between undirected and directed graphs is when we connectivity computations and i'll remind you when we get to that point which one we're talking about. okay? but for the most part, when we just talk about basic graph search it works essentially the same way whether it's undirected or directed. so keep that in mind. alright, so why is this true? why are the nodes that get explored precisely the nodes for which there's a path to them from s? well, one direction is easy. which is, you can't find anything which is not findable, that is, if you wind up exploring a node, the only reason that can happen is because you traversed a sequence of edges that got you there. and that sequence of edges obviously defines a path from s to v. if you really want to be pedantic about the forward direction that explored nodes have to have paths from s. then you can just do an easy induction. and i'll leave this for you to check, if you want, in the privacy of your own home. so the important direction of this claim is really the opposite. why is it that no matter how we instantiate this generic graph search procedure, it's impossible for us to miss anything. that's the crucial point, we don't miss anything that we could, in principle, find via a path. but we're gonna proceed by contradiction. so, what does that mean, we're going to assume that, the statement that we want to prove is true, is not true. which means that, it's possible that, g has a path from s to v and yet, somehow our algorithm misses it, doesn't mark it as explored. alright, that's the thing we're really hoping doesn't happen. so let's suppose it does happen and then derive a contradiction. so suppose g does have a path from s to some vertex v. call the path p. i'm gonna draw the picture for an undirected graph but the situation would be same in the, in the directed case. so there's a bunch of hops, there's a bunch of edges and then eventually this path ends at v. now the bad situation, the situation from which we want to derive a contradiction is that v is unexplored at the end of this algorithm. so let's take stock of what we know. s for sure is explored, right. we initialized this search procedure so that s is marked as explored. v by hypothesis in this proof by contradiction is unexplored. so s is explored, v is unexplored. so now imagine we, just in our heads as a thought experiment which traverse this path p. we start at s and we know it's explored. we go the next vertex, it may or may not have been explored, we're not sure. we go to the third vertex, again who knows. might be explored, might be unexplored and so on, but by time we get to v, we know it's unexplored. so we start at s, it's been explored, we get to v it's been unexplored. so at some point there's some hop, along this path p, from which we move from an explored vertex, to an unexplored vertex. there has to be a switch, at some point, cuz the end of the day at the end of the path we're at an unexplored node. so consider the first edge, and there must be one that we switch from being at an explore node to being at an unexplored node. so, i'm going to call the end points of this purported edge u and w. where u is the explored one and w is the unexplored one. now, for all we know u could be the same as s, that's a possibility, or for all we know, w could be same as v. that's also a possibility. in the picture, i'll draw it as if this edge ux was somewhere in the middle of this path. but, again it may be at one of the ends. that's totally fine. but now in this case, there's something i need you to explain to me. how is it possible that, on the one hand, our algorithm terminated. and on the other hand, there's this edge u comma x. where u has been explored and x has not been explored. that, my friends, is impossible. our generic search algorithm does not give up. it does not terminate, unless there are no edges where the one end point is explored and the other end point is unexplored. as long as there's such an edge, it has, is gonna suck in that unexplored vertex into the conquered territory, it's gonna keep going. so the upshot is there's no way that our algorithm terminated with this picture. with there being an edge u x, u explored, x unexplored. so, that's the contradiction. this contradicts the fact that our algorithm terminated with v unexplored. so that is a general approach to graph search. so that i hope gives you the flavor of how this is going to work. but now there's two particular instantiations of this generic method that are really important and have their own suites of applications. so we're gonna focus on breadth-first search and depth-first search. we'll cover them in detail in the next couple of videos. i wanna give you the highlights to conclude this video. now let me just make sure it's clear where the ambiguity in our generic method is. why we can have different instantiations of it that potentially have different properties and different applications. the question is at a given iteration of this while loop, what do you got? you've got your nodes that you've already explored, so that includes s plus probably some other stuff, and then you've got your nodes that are unexplored, and then you have your crossing edges. right? so, there are edges with one point in each side. and for an undirected graph, there's no orientation to worry about. these crossing edges just have one endpoint on the explored side, one endpoint on the unexplored side. in the directed case, you focus on edges where the tail of the edge is on the explored side and the head of the edge is on the unexplored side. so, they go from the explored side to the unexplored side. and the question is, in general, in an iteration of this while loop there's gonna be many such crossing edges. there are many different unexplored nodes we could go to next, and different strategies for picking the unexplored node to explore next leads us to different graph search algorithms with different properties. so the first specific search strategy we're gonna study is breadth first search, colloquially known as bfs. so let me tell you sort of the high level idea and applications of bread first search. so, the goal is going to be to explore the nodes in what i call, layers. so, the starting point s will be in its own layer, layer-0. the neighbors of s will constitute layer-1, and then layer-2 will be the nodes that are neighbors of layer-1 but that are not already in layer zero or layer one, and so on. so layer i plus one, is the stuff next to layer i that you haven't already seen yet. you can think of this as a fairly cautious and tentative exploration of the graph. and it's gonna turn out that there's a close correspondence between these layers and shortest path distances. so if you wanna know the minimal number of hops, the minimal number of edges you need in a path to get from point a to point b in a graph. the way we wanted to know the fewest number of edges in the movie graph necessary to connect to john hamm to kevin bacon. that corresponds directly to these layers. so if a node is in layer i, then you need i edges to get from s to i in the graph. once we discuss breadth-first search, we'll also discuss how to compute the connected components, or the different pieces, of an undirected graph. turns out this isn't that special to breadth-first search, you can use any number of graph search strategies to compute connected components in undirected graphs. but i'll show you how to do it using a simple looped version of breadth-first search. and we'll be able to do this stuff in the linear time that we want. the very ambitious goal of getting linear time. to get the linear time implementation, you do wanna use the right data structure, but it's a simple, simple data structure, something probably you've seen in the past. namely a queue. so, something's that first in and first out. so, the second search strategy that's super important to know is depth first search, also known as dfs to its friends. depth first search has a rather different feel than breadth first search. it's a much more aggressive search where you immediately try to plunge as deep as you can. it's very much in the spirit of how you might explore a maze, where you go as deeply as you can only backtracking when absolutely necessary. depth first search will also have its own set of applications. it's not, for example, very useful for computing shortest path information, but especially in directed graphs it's going to do some remarkable things for us. so, in directed acyclic graphs, so a directed graph with no directed cycles it will give us what's called the topological ordering. so it'll sequence the nodes in a linear ordering from the first to the last, so that all of the arcs of the directed graph go forward. so this is useful for example if you have a number of tasks that need to get completed with certain precedence constraints. like for example you have to take all of the classes in your undergraduate major, and there was certain prerequisites, topological ordering will give you a way in which to do it, respecting all of the prerequisites. and finally where for undirected graphs it doesn't really matter whether you use bfs or dfs to connect the components, in the directed graphs where even defining connected components is a little tricky it turns out depth first search is exactly what you want. that's what you're going to get a linear time implementation for computing the right notion of connected components in the directed graph case. time-wise, both of these are superb strategies for exploring a graph. they're both linear time with very good constants. so depth-first search again, we're gonna get o of m plus n time in a graph with m edges and n vertices. you do wanna use a different data structure reflecting the different search strategy. so, here because you're exploring aggressively, as soon as you get to a node you'll meet and you start exploring its neighbors, you wanna last-in first-out data structure, also known as a stack. depth first search also admits a very elegant recursive formulation, and in that formulation, you don't even need to maintain a stack data structure explicitly, the stack is implicitly taken care of in the recursion. so that concludes this overview of graph search. both what it is, what our goals are, what kind of applications they have and two of the most common strategies. the next couple videos are going to explore these search strategies, as well as a couple of these applications in greater depth. 
so in this lecture we're going to drill down into our first specific, search strategy for graphs and also explore some applications. namely, breadth first search. so let me remind you the intuition and applications of breath first search. the plan is to systematically explore the nodes of this graph beginning with the given starting vertex in layers. so let's think about the following example graph. where s is the starting point for our breadth first search. so to start vertex s will constitute the first layer. so we'll call that l zero. and then the neighbors of s are going to be the first layer. and so those are the vertices that we explore just after s. so those are l one. now the second layer is going to be the vertices that are neighboring vertices of l one but are not themselves in l one or for that matter l zero. so that's going to be c and d. that's going to be the second layer. now you'll notice for example s is itself a neighbor of these nodes in layer one, but we've already counted that in a previous layer so we don't count s toward l two. and then finally the neighbors of l two, which are not already put in some layer is e. that will be layer three. again notice c and d are neighbors of each other but, they've already been classified in layer two. so, that's where they belong, not in layer three. so that's the high level picture of breadth first search you should have. we'll talk about how to actually precisely implement it on the next slide. again just a couple other things that you can do with breadth first search which we'll explore in this video is computing shortest paths. so it turns out shortest path distances correspond precisely to these layers. so, for example if you had that as s, you had that as the kevin bacon node in the movie graph, then jon hamm would pop up in the second layer from the breadth first search from kevin bacon. i'm also going to show you how to compute the connected components of an undirected graph. that is to compute its pieces. we'll do that in linear time. and for this entire sequence of videos on graph primitives, we will be satisfied with nothing less than the holy grail of linear time. and again, remember in a graph you have two different size parameters, the number of edges m and the number of nodes n. for these videos i'm not going to assume any relationship between m and n. either one could be bigger. so linear time's gonna mean o of m plus n. so let's talk about how you'd actually implement breadth first search in linear time. so the sub routine is given as input both the graph g. i'm gonna explain this as if it's undirected, but this entire procedure will work in exactly the same way for a directed graph. again, obviously in an undirected graph you can traverse an edge in either direction. for a directed graph, you have to be careful only to traverse arcs in the intended direction from the tail to the head, that is traverse them forward. so as we discussed when we talked about just generic strategies for graph search, we don't want to explore anything twice, that would certainly be inefficient. so we're going to keep a boolean at each node, marking whether we've already explored it or not. and by default, i'm just, we're just going to assume that nodes are unexplored. they're only explored if we explicitly mark them as such. so we're going to initialize the search with the starting vertex s. so we mark s as explored and then we're gonna put that in what i was previously calling conquered territory the nodes we have already started to explore. so to get linear time we are gonna have to manage those in a slightly non naive but, but pretty straightforward way namely via a queue, which is a first in first out data structure that i assume you have seen. if you have never seen a queue before, please look it up in a programming textbook or on the web. basically a queue is just something where you can add stuff to the back in constant time and you can take stuff from the front in constant time. you can implement these, for example, using a doubly linked list. now recall that in the general systematic approach to graph search, the trick was to, in each iteration of some while loop, to add one new vertex to the conquered territory. to identify one unexplored node that is now going to be explored. so that while loop's gonna translate into one in which we just check if the queue is non-empty. so we're assuming that the queue data structure supports that query in constant time which is easy to implement. and if the queue is not empty we remove a node from it. and because it's a queue, removing nodes from the front is what you can do in constant time. so call the node that you get out of the queue v. so, now we're going to look at v's neighbors, vertices with which it shares edges, and we're gonna see if any of them have not already been explored. so, if w's something we haven't seen before, that's unexplored, that means it's in the unconquered territory, which is great. so, we have a new victim. we can mark w as explored. we can put it in our queue and we've advanced the frontier and now we have one more explored node than we did previously. and again, a queue by construction, it supports adding constant time additions at the end of the queue, so it's where we put w. so, let's see how this code actually executes in this same graph that we were looking at in the previous slide. and what i'm gonna do is i'm gonna number the nodes in the order in which they are explored. so, obviously the first node to get explored is s. that's where the queue starts. so now, when we follow the code, what happens? well in the first iteration of the while loop we ask is the queue empty? no it's not, because s is in it. so we remove in this case the only node of the queue. it's s. and then we iterate over the edges incident to s. now there are two of them. there's the edge between s and a and there's the edge between s and b. and again this is still a little under specified. in the sense that the algorithm doesn't tell us which of those two edges we should look at. turns out it doesn't matter. each of those is a valid execution of breadth first search. but for concreteness, let's suppose that of the two possible edges, we look at the edge s comma a. so, then we ask, has a already been explored? no, it hasn't. this is the first time we've seen it, so we say, oh goody. this is sort of new grist for the mill. so, we can add a to the queue at the end and we mark w as, sorry mark a as explored. so, a is gonna be the second vertex that we mark. so, after marking a as explored and adding it to the queue, so now we go back to the for loop, and so now we move on to the second edge. it's into s, that's the edge between s and b. so, we ask, have we already explored b? nope, this is the first time we've seen it. so, now we have the same thing with b. so, b gets marked as explored and gets added to the queue at the end. so the queue at this juncture has first a record for a, cause that was the first one we put in it after we took s out. and then b follows a in the queue. again, depending on the execution this could go either way. but for concreteness, i've done it so that a got added before b. so at this point, this is what the queue looks like. so now we go back up to the while loop, we say is the queue empty? certainly not. there's actually two elements. now we remove the first node from queue, in this case, that's the node a that was the one we put in before the node b. and so now we say, well, let's look at all the edges incident to a. and in this case a has two two incident edges. it has one that it shares with s and it has one that it shares with c. and so, if we look at the edge between a and s, then we'd be asking an if statement. has s already been explored? yes it has, that's where we started. so, there's no reason to do anything with s. that's already been taken out of the queue. so, in this for loop for a, there's two iterations. one involves the edge with s, and that one we completely ignore. but then there's the other edge that a shares with c, and c we haven't seen yet. so, at that part of the for loop, we say ahah. c is a new thing, new node we can mark as explored and put in the queue. so, that's gonna be our number four. so now how has the queue changed. well, we got rid of a. and so now b is in the front and we added c at the end. and so now the same thing happens. we go back to the while loop, the queue is not empty, we take off the first vertex, in this case that's gonna be b. b has three incident edges, it has one incident s but that's irrelevant, we've already seen s. it has one incident to c, that's also irrelevant, that's also irrelevant, because we've already seen c. true, we just saw it very recently, but we've already seen it. but the edge between b and d is new, and so that means we can take the node d, mark it as explored and add it to the queue. so d is going to be the fifth one that we see. and now the queue has the element c followed by d. so now we go back to the while loop and we take c off of the queue. it again has four now edges. the one with a is irrelevant, we've already seen a. the one with b is irrelevant, we've already seen b. the one with d is irrelevant, we've already seen d. but we haven't seen e yet. so, when we get to the part of the for loop, or the edge between c and e, we say, aha, e is new. so e will be the sixth and final vertex to be marked as explored. and that will get added at the end of the queue. so then in the final two iterations of the while loop the d is going to be removed, we'll iterate through its three edges, none of those will be relevant because we've seen the three endpoints. and then we'll go back to the while loop and we'll get rid of the e. e is irrelevant cause it has two edges we've already seen the other endpoints. now we go back to the while loop. the queue is empty. and we stop. that is breadth-first search. and to see how this simulates the notion of the layers that we were discussing in the previous slide notice that the nodes are numbered according to the layer that they're in, so s was layer zero. and then the two nodes that s caused to get added to the queue, the a and the b, are number two and three, and the edges of layer three are precisely the ones, sorry the edges of layer two are precisely the ones that got added to the queue, while we were processing the nodes from layer one. that is, c and d are precisely the nodes that got added to the queue while we were processing a and b. so, this is level zero, level one, and level two. e is the only node that got added to the queue while we were processing level, layer two. the vertices c and d. so e will be the third layer. so, in that sense, by using a first in first out data structure, this queue, we do wind up kinda processing the nodes according to the layers that we discussed earlier. so, the claim that breadth first search is a good way to explore a graph, in the sense that it meets the two high level goals that we delineated in the previous video. first of all it finds everything findable, and obviously nothing else, and second of all, it does it without redundancy. it does it without exploring anything twice, which is the key to its linear time implementation. so a little bit more formally, claim number one. at the end of the algorithm, the vertices that we've explored are precisely the ones such that there was a path from s to that vertex. again this claim is equally valid, whether you're running bfs in an undirected graph or a directed graph. of course in an undirected graph, meaning an undirected path from s to v, whereas a directed graph in a directed path from s to v. that means a path where every arc in the path gets traversed in the forward direction. so, why is this true? well, this is true, we basically proved this more generally for any graph search strategy of a certain form of which breadth first search is one. if it's hard for you to see the right way to interpret breadth-first search as a special case of our generic search algorithm, you can also just look at our proof for the generic search algorithm and copy it down for breadth-first search. so it's clear that you're only gonna, again, the forward direction of this claim is clear. if you actually find something, if something's marked as explored, it's only because you found a sequence of edges that led you there. so the only way you mark something as explored is if there's a path from s to v. conversely, to prove that anything with an s to v, for with a path from v will be found, you can proceed by contradiction: you can look at the part of the path from s to v that, that bfs does successfully explore, and then you gotta ask, why didn't it go one more hop? it never would've terminated before reaching all the way to v. so, you can also just copy that same proof that we had for the generic search strategy in the previous video. okay? so, again, the upshot. breadth first search finds everything you'd wanna find. okay? so, it only traverses paths, so you're not gonna find anything where there isn't a path to it. but it never misses out. okay? anything where there's a path, bfs, guaranteed to find it. no problem. claim number two is that the running time is exactly what we want and i am gonna state it in a form that will be useful later when we talk about connected components. so the running time of the main while loop, ignoring any kind of pre processing or initialization is proportional to what i am gonna call ns and ms which is the number of nodes that can be reached from s and number of edges that can be reached from s. and the reason for this claim it just becomes clear if you inspect the code which we'll do in a second. so let's return to the code and just tally up all the work that gets done. so i'm gonna ignore this initialization. i'm just gonna focus on the main while loop. so we can summarize the total work done in this while loop as follows. first we just think about the vertices so in this search we're only gonna deal, ever deal with the vertices that are findable from s, so that's ns. and what do we do for the given node, well we insert it into the queue and we delete it from the queue. alright? so we're never gonna deal with a single node more than once. so that's constant time overhead per vertex that we ever see, so that's the proportion of the ns part. now, a given edge, we might look at it twice. so, for an edge v w, we might consider it once when we first look at the vertex v, and we might consider it again when we look at the vertex w. each time we look at an edge we do constant work. so that means we're only gonna do constant work per edge. okay. so we look at each vertex at most once. we look at each edge findable from s at most twice. we do constant time, constant work when we look at something. so the overall running time is going to be proportional to the number of vertices findable from s plus the number of edges findable from s. so, that's really cool. we have a linear time of implementation of a really nice graph search strategy. moreover we just need very basic data structures, a queue, to make it run fast with small constants. but it gets even better. we can use breadth first search as a work horse for some interesting applications. so, that's what we'll talk about in the rest of this video. 
and let's begin with the idea of shortest paths. so, again i'll give you the movie graph. i'll give you kevin bacon as a starting point. what's the fewest number of hops, the fewest number of edges on a path that leads to, say, jon hamm? so some notation, i'm going to use dist of v, to denote this shortest path distance. so with respect to a starting node s, the fewest number of hops or the fewest number of edges on a path that starts at s, and goes to v. and again you can define this in the same way for undirected graphs or directed graphs. in a directed graph, you always want to traverse arcs in the forward direction, in the correct direction. and to do this we just have to add a very small amount of extra code to the bfs code that i showed you earlier. it's just gonna be a very small constant overhead, and basically it just keeps track of what layer each node belongs to, and the layers are exactly tracking shortest path distances away from the starting point s. so what's the extra code. well first in the initialization step, you set your preliminary estimate of the distance, the number of the shortest path distance from s to vertex v as well if v equals s, you know you can get from s to s on a path of length zero, the empty path. and if it's any other vertex all bets are off, you have no idea if there's a path to v at all. so let's just initially put plus infinity for all vertices other than the starting point. this is something we will of course revise once we actually discover a path to vertex v. and the only other extra code you have to add is, when you're considering, so when you take a vertex off of the front of the queue and then you iterate through its edges and you're considering one of those edges v, w, so your v would be the vertex that you just removed from the front of the queue. and as usual if the other end of the edge w has already been dealt with then, you know, you just throw it out. that would be redundant work to look at it again. but if this is the first time you've seen the vertex w. then, in addition to what we did previously, in addition to marking it as explored and putting it in the queue at the back, we also compute its distance, and its distance is just going to be one more than the distance of the vertex v, responsible for w's addition to the queue, responsible for first discovering this vertex w. so, returning to our running example of breadth first search, let's see what happens. so, again, remember the way this worked is we start out with from the vertex s, and we set the distance, you know in our initialization equal to zero. we don't know what the distance is of anything else. so, then how did breadth first search work? so, we, in the initial step we put s in the queue. we go to the main while loop, and then the queue's not empty. we extract s from the queue. we look at its neighbors. those neighbors are a and b. we handle them in some order. let's again think of that we first handle the edge between s and a. so, then what do we do? we say we haven't seen a yet. so we mark a as explored. we put a in the queue at the front, and now we have this extra step. it's the first time we're seeing a, so we wanna compute its distance. and we compute its distance as one more than the vertex responsible for discovering a. and so in this case s was the vertex whose exploration unveiled the existence of the vertex a to us. s's distance is zero so we set a's distance to one. and that's tantamount to being a member of the ith layer. so what happens in the next iteration of the while loop. so now the queue contains sorry, the next iteration of the for loop, excuse me. so after we've handled the edge s comma a, we're still dealing with s's edges, now we handle the edge s comma b. we put, it is the first time we've seen b. we put b at the end of the queue, we mark it as explored, and then we also execute this new step. we set b's distance to one more than the vertex responsible for discovering it. that would again be the vertex s. s led to b's discovery. and so we set b's distance to be one more than s's distance, also known as one. and that corresponds to being the other node in layer one. now having handled all of s's adjacent arcs we go back to the while loop. we ask if the queue is empty. certainly not. it takes two vertices, first a then b. we extract the first vertex cuz it's fifo, that would be the vertex a. now we look at a's incident edges. there's s comma a, which we ignore. there's a comma c. this is the first time we've seen c. so as before we mark c as explored. we add c to the end of the queue and now again we have this additional line. we set c's distance to be one more than the vertex responsible for its discovery. in this case it's a. that first discovered c. so we're gonna set c's distance to be one more than a's distance also known as two. so then having handled a we move on to the next vertex in the queue, which in this case is b. again we can forget about the edge between s and v. we've already seen s, we can forget about the edge between b and c. we've already seen c but d is now discovered for the first time via b. it gets more as explored, it goes to the end of the queue and its distance is set equal to one more than b's distances which is two. so, then we deal with c. again it has four arcs, four edges, three of them are irrelevant. the one to e is not irrelevant, cause this is the first time we've seen e. so, e's distance is computed as one more than c, cause c was the one who first found e, and so e gets a distance of three, and then the rest of the algorithm proceeds as before. and you will notice that the labelings, the shortest path labels, are exactly the layers as promised. i hope you find it very easy to believe at this point that, that claim is true in general. that the distance computed by breadth-first search for an arbitrary vertex v, that's reachable from s is, that's gonna be equal to i if and only if v is in the ith layer as we've been defining it previously. and what does it really mean to be in the ith layer? it means that the shortest path distance between v and s has i hops, i edges. so i don't wanna spend time giving a super rigorous proof of this claim but let me just give you the gist, the basic idea, and i encourage you to produce some formal proof at home if that is something that interests you. so one way to do it is you can do it by induction on the layer i. and so what you want to prove is that all of the nodes that belong to a given layer i do indeed, breadth first search does indeed compute the distance of i for them. so what does it mean to be a node in layer i? well, first of all, you can't have been seen in either of the, any of the previous layers; you weren't a member of layer zero through i minus one. and furthermore, you're a, a neighbor of somebody who's in layer i minus one. right? you're seen for the first time once all of the layer i minus one nodes are processed. so the inductive hypothesis tells you that distances were correctly computed for everybody from the lower l, from the lower layers. so in particular, whoever this node v was from layer i minus one was responsible for discovering u, in layer i. it has a distance computed as i minus one. yours is assigned to be one more than its, namely i. so that pushes through the inductive step everything in layer i indeed gets the correct label of a shortest path distance i away from s. so before we wrap up with this application, i do wanna emphasize, it is only breadth first search that gives us this guarantee of shortest paths. so, we have a wide family of graph search strategies, all of which find everything findable. breadth-first search is one of those, but this is a special additional property that breadth-first search has: you get shortest path distances from it. so in particular depth-first search does not in general compute shortest path distances. this is really a special property of breadth-first search. by contrast in this next application, which is going to be computing the connected components of an undirected graph, this is not really fundamental to breadth first search. for example, you could use depth first search instead and that would work just as well. 
so what's the problem? well, i did say most of the stuff about graph search really doesn't matter, undirected or directed, it's pretty much cosmetic changes. but the big exception is when you're computing connectivity, when you're computing the pieces of the graph. so right now, i'm only going to talk about undirected graphs. the directed case, we can get very efficient algorithms for, but it's quite a bit harder work, so that's going to be covered in detail in a separate video. so for now, focus just on an undirected graph, g. and we're certainly not going to assume that g is connected, and the part of the point here is to figure out whether or not it's connected, i.e., in one piece. so maybe the graph looks like this. so for example maybe the graph has ten vertices and looks like this on the right. and intuitively, especially given that i've drawn it in such a clean way, it's clear that this graph has three pieces. and those are the things that we want to call the connected components. but we do want to sum up more formal definitions, something which is actually in math that we could say is true or false about a given graph. and roughly, we define the connected components of an undirected graph as the maximal regions that are connected. in the sense you can get from any vertex in the region from any other vertex in the region using a path. so, maximal connected regions in that sense. now the slick way to do this is using an equivalence relation. and i'm going to do this here in part because it's really the right way to think about the directed graph case, which we'll talk about in some detail later. so from your [inaudible] graphs, so this isn't super important, but let me go ahead and state the formal definition just for completeness about what is a connecting component. what do i mean by a maximal region that's mutually connected. so a good formal definition is as the equivalence classes of the relation on vertices where we define by u being related to v if and only if there's a path between u and v in the graph. so i'll leave it for you to do the simple check that this squiggle is indeed an equivalence relation. i'm going to remind you what equivalence relations are. this is something you generally learn in your first class on proofs or your first class in discrete math. so it's just something which may or may not be true about pairs of objects. in an equivalence relation, you have to satisfy three properties. so first you have to be reflexive, meaning everything has to be related to itself, and indeed in a graph there is a path from any node to itself, namely the empty path. also a couple of these relations have to be symmetric, meaning that if u and v are related then v and u are related. because this is an undirected graph it's clear that this is symmetric. if there's a path from u to v in the graph there's also a path from v to u so no problem there. finally equivalence classes have got to be transitive. so that means if u and v are related and so are v and w and so are u and wo. that is if u and v have a path, v and w have a path, then so does u and w and you can prove transtivity just by pasting the two paths together. and so the upshot is, when you want to say something like the maximal subset of something where everything is the same. the right way to make that mathematical is using equivalence relations. so over in this blue graph we want to say one, three, five, seven, and nine, which are all the same in the sense that they're mutually connected and so that's exactly what this relation is making precise. all five of those nodes are related to each other. 2 and 4 are related to each other. 6, 8, and 10, all pairs of them are related. so equivalence relations always have equivalence classes, the maximal mutual related stuff. and in this graph context, it's exactly these connected components, exactly what you want. so what i want to show you is you can use wrapped in an outer of the vertices to compute, to identify all of the connected components of a graph. in time linear in the graph in n plus n time. now you might be wondering why do you want to do that. well there's a lot of reasons, so an obvious one which is relevant for physical networks is to check if a network has broken into two pieces. so certainly if you're an internet service provider, you want to make sure that from any point in your network, you can reach any other point in the network. and that boils down to just understanding whether the graph that represents you network, is a connected graph, that is if it's in one piece, or not in one piece. so obviously, you can ask this same question about recreational examples. so if you return to the movie graph, maybe you're wondering, can you get from every single actor in the imdb database to kevin bacon? or are there actors for which you cannot reach kevin bacon? via a sequence of edges, a sequences of movies in which two actors have both played a role. so that's something that boils down to a connectivity computation. if you have networked data and you want to display it, you want to visualize it and show it to a group of people so that they can interpret it. obviously one thing you want to do is you want to know if there's multiple pieces, and then you want to display the different pieces separately. so let me make sure that one probably a little less obvious application of undirected connectivity is that it gives us a nice quick and dirty heuristic for doing clustering if you have paralyzed information about objects. let me be a little more concrete. suppose you have a set of objects that you really care about. this could be documents, maybe web pages that you crawl, something like that. it could be a set of images, either your own or drawn from some data base. or it could be, for example, a set of genomes. suppose further, that you have a pairwise function, which for each pair of objects tells you whether they are very much like each other or very much different. so let's suppose that two objects are very similar to each other, like the two web pages that are almost the same. or there are two genomes where you can get from one to the other with a small number of mutations. then, they have a low score. so low numbers, close to zero, indicates that the objects are very similar to each other. high numbers, let's say, they can go up to a thousand or something, indicate that they are very different objects. two webpages that have nothing to do with each other, two genomes for totally unrelated parts, or two images that seem to be of completely different people or even completely different objects. now here's a graph you can construct using these objects and the similarity data that you have about them. so you can have a graph where the nodes are the objects. okay, so for each image, for each document, whatever, you have a single node and then for a given pair of nodes, you put in an edge if and only if the two objects are very similar. so for example, you could put in an edge between two objects if and only if the score is at most ten. so remember, the more similar two objects are, the closer there score is to zero. so you're going to get an edge between very similar documents, very similar genomes, very similar images. now in this graph you've constructed, you can find the connected components. so each of these connected components will be a group of objects, which more or less are all very similar to each other. so this would be a cluster of closely related objects in your database. and you can imagine a lot of reasons why, given a large set of unstructured data, just a bunch of pictures, a bunch of documents or whatever, you might want to find clusters of highly related objects. so we'll probably see more sophisticated heuristics for clustering in some sql course. but already undirected connectivity gives you a super fast linear time quick and dirty heuristic for identifying clusters of similar objects, given pairwise data about similarity. so that's some reasons you might want to do it. now let's actually talk about how to compute the components in the near time using just a simple for loop and breadth-first search as it's inner work horse. so here's the code to compute all the connected components of an undirected graph. so first we initialize all nodes as being unexplored. i'm also going to assume that the nodes have names. let's say that the names are from 1 to n. so these names could just be the position in the node array that these nodes occupy. so this is going to be an outer for loop, which walks through the nodes in an arbitrary order, let's say from 1 to n. this outer for loop is to ensure that every single node of the graph will be inspected for sure at some point in the algorithm. now, again, one of maxims is that we should never do redundant work, so before we start exploring from some node, we check if they've already been there. and if we haven't seen i before, then we invoke the breath-first search, a term we were talking about previously in the lecture, in the graph g, starting from the node i. so to make sure this is clear, let's just run this algorithm on this blue graph to the right. so we start in the outer for loop and we said i equal to 1. and we say have we explored node number 1 yet. and of course not, we haven't explored anything yet. so the first thing we're going to do is we're going to invoke bfs on node number 1 here. so now we start running the usual breadth for search subroutine starting from this node one and so we explore layer one here is going to be nodes 3 and 5. so we explore them in some order for example maybe node number 3 is what we explore second. then node number five is what we explore third and then the second layer in this component is going to be the nodes 7 and 9. so we'll explore them in some order as well and say 7 first followed by 9. so after this bfs initiated from node number one completes, of course it will have found everything it could possibly find, namely the five nodes in the same connected component as node number 1. and of course, all of the five of these nodes will be marked as explored. so now we return once that exits we return to the outer for loop we increment i we go to i equal 2, and we say we already explored node number 2 no we have not. and so now we invoke bfs again from node number 2. so that will be the sixth node we explore. there's not much to do from two, all we can do is go to 4, so that's the seventh node we explore. that bfs terminates at finding the nodes in this connected component, then we go back to the outer for loop. we increment i to 3, we say we've already seen node number three. yes we have, we saw that in the first breadth first search. so we certainly don't bother to bfs from node 3, then we increment item four. have we seen 4? yes we have, in the second called to bfs. have we seen node 5? yes we have, in the first call to bfs. have we seen node 6? no, we have not. so the final indication of breadth-first search begins from node number 6. that's going to be the eighth node overall that we see. and then we're going to see the notes 8 and 10 in some order, so for example maybe we first explore note number 8. that's one of the first layer in this component, and then note number 10 is the other note of the first layer in this component. so in general, what's going on, well what we know will happen when we invoked that first search from a given node i. we're going to discover exactly the nodes in i's connected component. right, anything where there's a path from i to that node, we'll find it. that's the bfs guarantee, that's also the definition of a connected component. all the other nodes which have a path to i. another thing that i hope was clear from the example, but just to reiterate it, is every search call, when you explore a node, you remember that through the entire for loop. so when we invoke breadth-first search from node number 1, we explore nodes 1, 3, 5, 7 and 9, and we keep those marked as explored for the rest of this algorithm. and that's so we don't do redundant work when we get to later stages of the for loop. so as far as what does this algorithm accomplish, well, it certainly finds every connected component. there is absolutely no way it can miss a node because this for loop literally walks through the nodes, all of them, one at a time. so you're not going to miss a node. moreover, we know that as soon as you hit a connected component for the first time, and you do breadth-first search from that node, you're going to find the whole thing. that the breadth-first a search guarantee. as far as what's the running time, well it's going to be exactly what we want. it's going to be linear time, which again means proportional to the number of edges plus the number of vertices. and again depending on the graph, one of these might be bigger that the other. so why is it o of m plus n? well as far as the nodes, we have to do this initialization there where we mark them all as unexplored, so that takes constant time per node. we have just the basic overhead of a for loop, so that's constant time per node. and then we have this check, constant time per node, so that's o of n. and then recall we proved that within breath for research, you do a amount of work proportional. you do constant time for each node in that connected component. now, each of the nodes of the graph is in exactly one of the connected components. so you'll do constant time for each node in the bfs in which you discover that node. so that's again, o of n over all of the connected components. and as far as the edges, note we don't even bother to look at edges until we're inside one of these breadth first search calls. they played no role in the outer for loop or in the pre-processing. and remember what we proved about an indication of breadth first search. the running time, you only do constant amount of work per edge in the connected component that you're exploring. in the worst case, you look at an edge once from either endpoint and each of that triggers a constant amount of work. so when you discover a given connected component, the edge work is proportional to the number of edges in that kind of component. each edge of the graph is only in exactly one of the connect components, so over this entire for loop, over all of these bfs calls. for each edge of the graph, you'll only be responsible for a constant amount of work of the algorithm. so summarizing because breadth-first search from a given starting node. that is, works in time proportional to the size of that component, piggybacking on that sub routine and looping over all of the nodes of the graph. we find all of the connecting components in time proportional to the amount of edges and nodes in the entire graph. 
let's explore our second strategy for graph search, namely depth-first search. and again, like with breadth-first search, i'll open by just reminding you what depth-first search is good for and we'll trace through it in a particular example, and then we'll tell you what the actual code is. so if breadth-first search is the cautious and tentative exploration strategy, then depth-first search or dfs for short is its more aggressive cousin. so the plan is to explore aggressively and only back track when necessary. and this is very much the strategy one often uses when trying to solve a maze. to explain what i mean let me show you how this would work in the same running example we used when we discussed breadth-first search. so here if we invoke depth-first search from the node number s, here's what's going to happen. so obviously we start at s and obviously there's two places where we can go next. we can go to a or to b. and depth-first search is under determine like breadth-first search we can pick either one. select with a breadth-first search example let's go to a first. so a will be the second one that we explore. but now, unlike breadth first search where we automatically went to node b next, since that was the other layer one node. here, the only rule is that we have to go next to one of a's immediate neighbors. we might go to b, but we're not going to b because it is one of the neighbor's of s, we go because it is one of the neighbors of a. and actually to make sure the difference is clear let's assume that we aggressively pursue deeper when we go from a to c and now the depth first search strategy is again to just pursue deeper, so you go to one of c's immediate neighbors, so maybe we go to e next, so e is going to be the fourth one visited. now from e there's only one neighbor not counting the one that we came in on so from e we go to d. and d is the fifth one we see. now from d we have a choice, we can either go to b or we could go to c. so let's suppose we go to c from d. well then we get to a node number three where we've been before. and as usual we're going to keep track of where we've already been. so at this point we have to back track from c back to d. we retreat to d. now, there's still another outgoing edge from d to explore and then they'll be the one to b. and so what happens is we actually wind up wrapping all the way around this outer cycle and we could b sixth. and now, of course, anywhere we try to explore we see somewhere we've already been. so, from b we try to go to s, but we've been there so we retreat to b. we can try to go to a but we've been there so we retreat to b. now we've explored all of the options out of b. so we have to retreat from b, we have to go back to d. now from d we've explored both b and c so we have to retreat back to e. we've explored the only outgoing arc d, so we have to retreat to c. we retreat to a. from a we actually haven't yet looked along this arc, but that just sends us to b where we have been before. so then we retreat back to a. finally we retreat back to s and s, even at s there's still an extra edge to explore. at s we say, we haven't tried this as b-edge yet. but of course, when we look across we get the b where we've been before and then we backtrack to s. then we've looked at every edge once, and so we stop. so that's how depth-first search works. you just pursue your path, you go to an immediate neighbor as long as you can until you hit somewhere you've been before. and then you retreat. so you might be wondering why bother with another graph search strategy? after all we have breadth-first search, which seemed pretty awesome, right. it runs in linear time. it's guaranteed to find everything you might want to find, it computes shortest paths, it computes connected components if you embed it in a foreloop. it kind of seems like, what else would you want? well, it turns out, depth-first search is going to have its own impressive catalogue of applications, which you can't necessarily replicate with breadth-first search. and i'm going to focus on applications in directed graphs. so there's going to be a simple one that we discuss in this video, and then there's going to be a more complicated one that has a separate video devoted to it. so in this video we're going to be discussing, computing topological orderings of directed acyclic graphs. that is, directed graphs that have no directed cycle. the more complicated application is computing strongly connected components in directed graphs. the run time will be, essentially, the same as it was for breadth-first search, and the best we could hope for, which is linear time. and again, we're not assuming that there's necessarily that many edges. there may be much fewer edges than vertices. so linear time and these connectivity applications means o of m plus n. so let's not talk about the actual code of depth for search. there's a couple of ways to do it. one way to do it is to just make some minor modifications to the code for breadth for a search. the primary difference being instead of using a queue in its first in first out behavior, you swap in a stack with its last in first out behavior. again, if you don't know what a stack is you should read about that in the program textbook or on the web. it's something that supports constant time insertions to the front and constant time deletions from the front, unlike a queue which is meant to support constant time deletions to the back. okay so stack that operates just like those cafeteria trays that you know where you put in a tray and the last one that got pushed in when you take the first one out that's the last one that got put in. so these are called push and pop, in a stack context both are constant time. so if you swap out the queue you swap in the stack, make a couple other minor modifications. breadth-first search turns into depth-first search. for the sake of both variety and elegance, i'm instead going to show you a recursive version. so depth-first search is very naturally phrased as a recursive algorithm, and that's what we'll discuss here. so, depth-first search, of course, takes as input a graph g and again it could be undirected or directed. it doesn't matter, just with a directed graph be sure that you only follow arcs in the appropriate direction, which should be automatically handled in the adjacency lists of your graph data structure anyways. so as always we keep a boolean local to each vertex of the graph, remembering whether we've been there before or not. and of course, as soon as we start exploring from s we better make a note that now we have been there. we better plant a flag, as it were. and remember, depth-first search is an aggressive search, so we immediately try to recursively search from any of s's neighbors that we haven't already been to. and now, if we find such a vertex, if we find somewhere we've never been, we recursively call depth-first search from that node. the basic guarantees of depth-first search are exactly the same as they were for breath-first search. we find everything we could possibly hope to find and we do it in linear time. and once again, the reason is this is simply a special case of the generic stretch procedure that we started this sequence of videos about. so it just corresponds to a particular way of choosing amongst multiple crossing edges between the region of explored nodes, and between the region of unexplored nodes. essentially always being biased toward the most recently discovered explored nodes. and just like, breadth for search, the running time is going to be proportional to the size of the component that you're discovering. and the basic reason is that each node is looked at only once, right? this boolean makes sure that we don't ever explore a node more than once, and then for each edge, we look at it at most twice, once from each endpoint. and given that these exact same two claims hold for depth-first search as for breadth-first search, that means if we wanted to compute connected components in an undirected graph, we could equally well use an outer for loop with depth-first search as our workhorse in the inner loop. it wouldn't matter. either of those for undirected graphs, depth-first search, breadth-first search, is going to find all the connected components in o of n plus m time, in linear time. so instead, i want to focus on an application in particular to depth-first search, and this is about finding a topological ordering of a directed acyclic graph. 
let me begin by telling you what a topological ordering of a directed graph is. essentially, it's an ordering of the vertices of a graph so that all of the arcs, the directed edges of the graph, only go forward in the ordering. so let me encode an ordering by a labeling of the vertices with the numbers one through n. this is just to encode the position of each vertex in this ordering. so formally there's going to be a function which takes vertices of g and maps them to integers between 1 and n. each of the numbers 1 through n should be taken on by exactly one vertex. here n is the number of vertices of g. so that's just a way to encode an ordering, and then here's really the important property that every directed edge of g goes forward in the ordering. that is if u v is directed edge of the directed graph g, then it should be that the f value of the tail is less than the f value of the head. that is this directed edge has the higher f value as traverse in the correct direction. let me give you an example just to make this more clear. so suppose we have this very simple directed graph, with four vertices. let me show you two different, totally legitimate topological orderings of this graph. so the first thing you could do, is you could label s1, v2, w3 and t4. another option would be to label them the same way, except you can swap the labels of v and w. so if you want, you can label v 3 and w 2. so again, what these labelings are really meant to encode is an ordering of the vertices. so the blue labeling, you can think of as encoding the ordering in which we put s first then v then w and then t. whereas the green labeling can be thought of as the same ordering of the nodes except with w coming before v. what's important is that the pattern of the edge is exactly the same in both cases, and in particular all of the edges go forward in this ordering. so in either case we have s with edges from s to v, and s to w. so that looks the same way pictorially, whichever order v or w are in, and then symmetrically there are edges from v and w to t. so you'll notice that no matter which order that we put v and w in, all four of these edges go forward in each of these orderings. now if you try to put v before s it wouldn't work because the edge from s to v would be going backward if v preceded s. similarly, if you put t anywhere other than the final position, you would not have a topological ordering. so in fact, these are the only two topological orderings of this directed graph. i encourage you to convince yourself of that. now, who cares about topological orderings? well, this is actually a very useful subroutine. this has been come up in all kinds of applications. really, whenever you want to sequence a bunch of tasks when there's precedent constraints among them. by precedence constraint i mean one task has to be finished before another. you can think, for example, about the courses in some kind of undergraduate major like a computer science major. here the vertices are going to correspond to all of the course and there's a directed edge from course a to course b if course a is a prerequisite for course b, if you have to take it first. so then of course, you'd like to know a sequence in which you can take these courses so that you always take a course after you've taken its pre-requisite. and that's exactly what a topological ordering will accomplish. so it's reasonable to ask the question when does a directed graph have a topological ordering and when a graph does have such an ordering, how do we get our grubby little hands on it? well there's a very clear necessary condition for a graph to have a topological ordering, which is it had better be a cyclic. put differently, if a directed graph has a directed cycle then there is certainly no way there is going to be a topological ordering. so i hope the reason for this is fairly clear. consider any directed graph which does have a directed cycle and consider any purported way of ordering the vertices. well, now just traverse the edges of the cycle, one by one. so you start somewhere of the cycle, and if the first edge goes backward, well, you're already screwed. you already know that this ordering is not topological. no edges can go backward. so evidently, the first edge of this cycle has to go forward. but now, you have to traverse the rest of the edges on this cycle, and eventually you come back to where you started. so if you started out by going forward, at some point you have to go backward. so that edge goes backward in the ordering, violating the property of the topological ordering. that's true for every ordering, so directed cycles exclude the possibility of topological ordering. now the question is what if you don't have a cycle? is that a strong enough condition that you're guaranteed to have a topological ordering? is the only obstruction to sequencing jobs without conflicts the obvious one of having circular precedence constraints? so it turns out not only is the answer yes, as long as you don't have any directed cycles, you're guaranteed a topological ordering. but we can even compute on in linear time no less via depth-first search. so before i show you the super slick and super efficient reduction of computing topological orderings of depth-first search. let me first show over a pretty good but slightly less slick and slightly less efficient solution to help build up your intuition about directed acyclic graphs and their topological orderings. so for the straightforward solution, we're going to begin with a simple observation. every directed acyclic graph has what i'm going to call a sink vertex. that is a vertex without any outgoing arcs. so in the forenode, directed acyclic graph we were exploring on the last slide there's exactly one source of sink vertex, and that's this right-most vertex here. that has no outgoing arcs, the other three vertices all have at least one outgoing arc. now why is it the case that a directed acyclic graph has to have a sink vertex? well, suppose it didn't, suppose it had no sink vertex. that would mean every single vertex has at least one outgoing arc. so what could we do if every vertex has one outgoing arc? well, we can start in an arbitrary node. we know it's not a sink vertex, because we're assuming there aren't any. so there's an outgoing arc, so let's follow it. we get to some other node. by assumption there's no sink vertex, so this isn't a sink vertex, so there's an outgoing arc, so let's follow it. we get to some other node. that also has an outgoing arc, let's follow that, and so on. so we just keep following outgoing arcs, and we do this as long as we want because every vertex has at least one outgoing arc. well there's a finite number of vertices, right this graph has say n vertices. so if we follow n arcs, we are going to see n+1 vertices. so by the pigeon-hole principle, we're going to have to see a repeat. right so, if n+1 vertices, is only n distinct vertices, we're going to see some vertex twice. so for example, maybe after i take the outgoing arc from this vertex, i get back to this one that i saw previously. well, what have we done? what happens when we get a repeated vertex? by tracing these outgoing arcs and repeating a vertex, we have exhibited a directed cycle. and that's exactly what we're assuming doesn't exist. we're talking about directed acyclic graphs. so, put differently, we just prove that a vertex with no sink vertex has to have a directed cycle. so a directed acyclic graph therefore has to have at least one sink vertex. so here's how we use this one very simple observation now to compute a topological ordering of a directed acyclic graph. well let's do a little thought experiment. suppose in fact this graph did have a topological order. let's think about the vertex which goes last in this topological ordering. remember, any arc which goes backward in the ordering is a violation. so we have to avoid that. we have to make sure every arc goes forward in the ordering. now any vertex which has an outgoing arc, we better put somewhere other than in the final position, right? so the node that we put in the final position, all of its outgoing arcs are going to wind up going backward in the topological ordering. there's no where else they can go, this vertex is last. so in other words, if we plan to successfully compute a topological ordering, the only candidate vertices for that final position in the ordering are the sink vertices. that's all that's going to work. we put a non-sink vertex there, we're toast, it's not going to happen. fortunately, if it's directed acyclic, we know there is a sink vertex. so let v be a sink vertex of g, if there's many sink vertices, we pick one arbitrarily. we set v's label to be the maximum possible, so if there's n vertices we're going to put that in the nth position. and now we just recurse on the rest of the graph, which has only n-1 vertices. so how would this work in the example on the right? well in the first iteration, or the first outermost recursive call, the only sink vertex is this right most one, circled in green. so there's four vertices, so we're going to give that the label 4. so, then having labeled that 4, we delete that vertex and all the edges incident to it. and we recurse on what's left of the graph, so that would be the left-most three vertices plus the left-most two edges. now, this graph has two sink vertices, after we've deleted 4 and everything from it. so both this top vertex and this bottom vertex are sinks in the residual graph. so now in the next recursive call, we can choose either of those as our sink vertex. because we have two choices, that generates two topological orderings. those are exactly the ones that we saw in the example. but if, for example, we choose this one to be our sink vertex, then that gets the label 3. then we recurse just on the northwestern most two edges. this vertex is the unique sink in that graph, that gets the label 2. and then it recurs on the one node that we graph, and that gets the label 1. so, why is this algorithm work? well, there's just two quick observations we need. so, first of all, we need to argue that it makes sense that in every iteration or in every recursive call, we can indeed find the sink vertex, that we can assign in the final position that's still unfilled. and the reason for that is just if you take a directed acyclic graph and you delete one or more vertices from it, you're still going to have a directed acyclic graph, right? you can't create cycles by just getting rid of stuff. you can only destroy cycles, and we started with no cycles. so through all the intermediate recursive calls we have no cycles by our first observation is always the sink. so the second that that we have to argue is that we really do produce a topological ordering. so remember what that means, that means for every edge of the graph, it goes forward in the ordering. that is the head of the arc is given a position later than the tail of the arc. and this simply follows because we always use sink vertices. so consider the vertex v which is assigned to the position i. this means then, that when we're down to a graph that only has i vertices remaining, v is the sink vertex. if v is the sink vertex when only the first i vertices remain, what property does it have in the original graph? well, it means all of outgoing arcs that it has have to go to vertices that were already deleted and assigned higher positions. so for every vertex, by the time it actually gets assigned a position, it's a sink and it only has incoming arcs from the as yet unsigned vertices. it's outgoing arcs all go forward to vertices that were already assigned higher positions, and got deleted previously from the graph. so now we have under our belt a pretty reasonable solution for computing a topological ordering of a directed acyclic graph. in particular, remember we observed that if a graph does have a directed cycle, then of course there's no way there's a topological ordering. however, you order the vertices summage of the side that's going to have to go backward. and the solution on the previous slide shows that, as long as you don't have a cycle, it guarantees the topological ordering does indeed exist. and in fact, it's a constructive proof, constructive argument, that gives an algorithm. what you do is you would keep plucking off sinks, sink vertices one at a time, and populating the ordering from right to left, as you keep peeling off these sinks. so that's a pretty good algorithm, it's not too slow, and actually if you implement it just so, you can even get it to run in linear time. but i want to conclude this video with an application of depth first search, which is a very slick, very efficient computation of a topological ordering of a directed acyclic graph. so, we're just going to make two really quite minor modifications to our previous depth first search subroutine. the first thing is we have to embed it in a for loop, just like we did with breadth first search when we were computing the connected components of an undirected graph. that's because, in completing a topological ordering, we better give every single vertex a label, we'd better look at every vertex at least once. so to do that, we'll just make sure there's an outer for loop and then if we have multiple components, we'll just make sure to invoke dfs as often as we need to. the second thing we'll do is we'll add a little bit of bookkeeping, and this will make sure that every node gets a label. and in fact, these labels will define a topological order. so let's not forget the code for depth first search. this is where you're given a graph, g, in this case we're interested in a directed acyclic graph, and you're given a start vertex, s. and what you do is, as soon as you get to s you very aggressively start trying to explore its neighbors. of course, you don't visit any vertex you've already been to. you keep track of who you visited. and if you find any vertex that you haven't seen before, you immediately start recursing on that node. so i said the first modification we need is to embed this into an outer for loop to ensure that every single node gets labeled. so i'm going to call that subroutine dfs-loop. it does not take a start vertex. initialization, all nodes start out on an explorative course. and we're also going to keep track of a global variable, which i'll call current_label. this is going to be initialized to n, and we're going to count down each time we finish exploring a new node. and these will be precisely the f values. these will be exactly the positions of the vertices in the topological ordering that we output. in the main loop we're going to iterate over all of the nodes of the graph. so for example, we just do a scan through the node array. as usual, we don't want to do any work twice, so if a vertex has already been explored in some previous invocation of dfs, we don't search from it. this should all be familiar from our embedding of breadth first search in a for loop when we computed the connected components of an undirected graph. and if we get to a vertex v of the graph, that we haven't explored yet, then we just invoke dfs in the graph, with that vertex as the starting point. so, the final thing i need to add is i need to tell you what the f values are, what the actual assignments of vertices to positions are. and as i foreshadowed, we're going to use this global current_label variable. and that'll have us assign vertices to positions from right to the left. very much mimicking what was going on in our recursive solution, where we plucked off sink vertices one at a time. so, when's the right time to assign a vertex its position? well, it turns out, the right time is when we've completely finished with that vertex. so we're about to pop the recursive call from the stack corresponding to that vertex. so, after we've gone through the for loop of all the edges outgoing from a given vertex, we set f (s) = to whatever the current_label is and then we decrement the current_label. and that's it, that is the entire algorithm. so, the claim is going to be that the f values produced, which you'll notice are going to be the integers between n through 1, because dfs will be called eventually once on every vertex, and it will get some integer assignment at the end. and everybody is going to get a distinct value, and the largest one is n and the smallest one is 1. the claim is that is a topological ordering. clearly this algorithm is just as blazingly fast as dfs itself, with just a trivial amount of extra bookkeeping. lets see how it works on our running example. so lets just say we have this four node directed graph that we're getting quite used to. so this has four vertices, so we initialize the current label variable to be equal to 4. so let's say that in the outer dfs loop, let's say we start somewhere like the vertex v. so notice in this outer for loop, we wind up considering the vertices in a totally arbitrary order. so let's say we first call dfs from this vertex v. so what happens, well, the only place you can go from v is to t, and then at t, there's nowhere to go. so we recursively call dfs at t, there's no edges to go through, we finish the for loop, and so t is going to be assigned an f value equal to the current label, which is n, and here, n is the number of vertices, which is 4. so f(t) is going to get, so our t is going to get the assignment, the label 4. so then now we're done with t, we backtrack back to v. we decrement the current label as we finish up with t, we get to v, and now there's no more outgoing arcs to explore, so for loops finish. so we're done with it in depth-first search. so it gets what's the new current label, which is now 3, and again, having finished with v, we decrement the current label, which is now down to 2. so now we go back to the outer for loop, maybe the next vertex we consider is the vertex t. but we've already been there, so we don't bother to dfs on t. and then maybe after that, we try it on s. so maybe s is the third vertex that the for loop considers. we haven't seen s yet, so we invoke dfs, starting from the vertex s. from s, there's two arcs to explore, the one with v, v we've already seen, so nothing's going to happen with the arc sv. but on the other hand, the arc sw will cause us to recursively call dfs on w. from w, we try to look at the arc from w to t, but we've already been to t, so we don't do anything. that finishes up with w, so depth-first search then finishes up at the vertex w, w gets the assignment of the current label. so f(w) = 2. we decrement current label, now its value is 1. now we backtrack to s, so we've already considered all of s's outgoing arcs, so we're done with s. it gets the current label, which is 1. and this is indeed one of the two topological orderings of this graph that we exhibited a couple slides ago. so that's the full description of the algorithm and how it works in a concrete example. let's just discuss what are its key properties, its running time and its correctness. so, as far as the running time of this algorithm the running time is linear. it's exactly what you'd want it to be. and the reason the running time is linear is for the usual reasons that these search algorithms run in linear time. you're explicitly keeping track of which nodes you've been to so that you don't visit them twice, so you only do a constant amount of work for each of the n nodes. and each edge, in a directed graph, you actually only look at each edge once, when you visit the tail of that edge. so you only do a constant amount of work per edge as well. of course, the other key property is correctness. that is, we need to show that you are guaranteed to get a topological ordering. so what does that mean? that means every arc travels forward in the ordering. so if (u,v) is an edge, then f(u), the label assigned to u in this algorithm is less than the label assigned to v. the proof of correctness splits into two cases, depending on which of the vertices u or v is visited first by depth-first search. because of our for loop, which iterates over all of the vertices of the graph g, depth-first search is going to be invoked exactly once from each of the vertices. either u or v could be first, both are possible. so first let's assume that u was visited by dfs before v, so then what happens? well, remember what depth first search does, when you invoke it from a node, it's going to find everything findable from that node. so if u is visited before v, that means v isn't getting explored, so it's a candidate for being discovered. moreover, there's a an arc straight from u to v, so certainly dfs invoked at u is going to discover v. furthermore, the recursive call corresponding to the node v is going to finish, it's going to get popped off the program stack before that of u. the easiest way to see this is just to think about the recursive structure of depth-first search. so when you call depth-first search from u, that recursive call, that's going to make further recursive calls to all of the relevant neighbors including v, and u's call is not going to get popped off the stack until v's does beforehand. that's because of the last in, first out nature of a stack or of a recursive algorithm. so because v's recursive call finishes before that of u, that means it will be assigned a larger label than u. remember, the labels keep decreasing as more and more recursive calls get popped off the stack. so that's exactly what we wanted. now, what's up in the second case, case two? so this is where v is visited before u. and here's where we use the fact that the graph has no cycles. so there's a direct arc from u to v. that means there cannot be any directed path from v all the way back to u. that would create a directed cycle. therefore, dfs invoked from v is not going to discover u. there's no directed path from v to u, again, if there was, there'd be a directed cycle. so it doesn't find u at all. so the recursive call of v again is going to get popped before u's is even pushed onto the stack. so we're totally done with v before we even start to consider u. so therefore, for the same reasons, since v's recursive call finishes first, its label is going to be larger, which is exactly what we wanted to prove. so that concludes the first quite interesting application of depth-first search. in the next video, we'll look at an even more interesting one which computes the strongly connected components of a directed graph. this time, we can't do it in one depth-first search, we'll need two. 
having mastered computing the connecting components of an undirected graph in linear time, let's now turn our attention to directed graphs, which also arise in all kinds of different applications. now the good news is, is we'll be able to get just a blazingly fast primitive for computing connectivity information for directed graphs. the bad news, if you want to call it hat, is we'll have to think a little bit harder. so it won't be as obvious how to do it. but by the end of this lecture will you know linear time algorithms, a very good concept. it's really just based on depth first search for computing all of the pieces of a directed graph. in fact, it's not even so obvious how to define pieces, how to define connected components in a directed graph. certainly not as obvious as it was with undirected graphs. so, see what i mean, let's consider the following four node directed graph. so on the one hand, this graph in some sense in one piece. if this was an actual physical object, made say of a bunch of strings connected to each other and we picked it up with our hands, it wouldn't fall apart into two pieces, it would hang together in one piece. on the other hand, when you think about moving around this network, it's not connected in the sense that we might think about. you cannot get from any one point a, to any other point b. for example, if you started the right-most node in this graph, certainly there's no directed path that will get you to the left-most node. so what's typically studied and most useful for directed graphs is what you call strong connectivity. and a graph is strongly connected if you can get from any one point to any other point and vice versa. and the components then informally are the maximal portions of this graph, the maximal regions, which are internally strongly connected. so the maximum regions from within which you can get from any one ,point a to any other point b along a directed graph. for the record, let me go ahead and give you a formal definition, although this intuition is perfectly valid. just regions where you can get from anywhere to anywhere else. so we say that the strongly connected components of directed graph, or the sccs for short. and as in the undirected case, we're going to give a somewhat slick definition rather than talking about maximal region satisfying some property. we're going to talk about the equivalence classes of a particular equivalence relation. but really it means the same thing. this is just sort of the more mathematically more mature way of writing it down. so the equivalence relation we're going to use, if it's on nodes of the graph, and we'll say that u and node is related to a node v if you can get from u to v via directed path, and also from v to u in some other directed path. i'm not going to bore you with the verification that this is an equivalence relation. that's something you can work out in the privacy of your home. so remember what it means to be an equivalence relation, that's reflexive, that is everybody's related to itself. but of course there is a path from every node to itself. it also means that it's symmetric, so if u is related to v, then v is related to u. well again, by definition we're saying that the vertices are mutually reachable from each other. so that's clearly symmetric by definition. and then it has to be transitive. and the way you prove it's transitive is you just paste paths together, and it just works the way you'd expect it to. so let's illustrate this concretely with a somewhat more complicated directed graph. so here's a directed graph, and i claim that it has exactly four strongly connecting components. there's a triangle on the left. a triangle on the right. it has this single node on top. and then it has this directed forecycle with a diagonal at the bottom. so what i hope is pretty clear is that each of these circled regions is indeed strongly connected. that is, if you start from one node in one of these circled regions, you can have a directed path to any other node. so that's symmetric, because on a directed cycle you can get from any one starting point to anyother place. and all of these have directed cycles. and then there's the one strong component that just has the one node, which is obviously strongly connected. what i also claim is true is that all of these regions are maximal. subject to being strongly connected. that's why they're the strongly connected components. that's why they're the equivalence classes of this equivalence relation we just defined. so if you take any two pairs of nodes which lie in two different circles, you either won't have a path from the first one to the second, or you won't have a directed path from the second one back to the first one. in fact, the structure of the strong components in this black graph exactly mirrors the directed acyclic graph that we started in red. so in the same way in the red four-node graph, you can't move from right to left. here in this bigger graph, you can't move from any of the circled sccs to the right to any of the circled sccs to the left. so for example, from the right-most nodes, there are no directed paths to the left-most nodes. so that's a recap of the definition of the strongly connected components. i've motivated in a separate video some reasons why you might care about computing strongly connected components. and in particular, on extremely large graphs which motivates the need for blazingly fast sub routines, so four free primitives that will let you compute connectivity information. so you'd love to be able to just know the pieces of a directed graph. maybe you don't even know why they're going to be useful but you just compute them because why not? it's four free primitive. so, that's what i'm going to give you on this lecture. so the algorithm that we're going to present is based on depth-first search. and that's going to be the main reason why it's going to be so blazingly fast, is because depth-first search is blazingly fast. now, you might be wondering, what on earth does graph search have to do with computing components? they don't seem obviously related. so let's return to the same directed graph that i shows you on the previous slide. and to see why something like depth-first search might conceivably have some use for computing strong components. suppose we called depth-first search starting from this red node as a starting point. what would it would explore? so remember what the guarantee of something like depth-first search or breath first search for that matter is. you find everything that findable, but naturally nothing else. so what is findable from this red node? where by findable i mean, you can reach it from a directed path emanating from this red node. well, there's not much you can do. so from here you can explore this arc. and you can explore this arc. and then you can go backward. and so, if you do dfs or bfs from this node, you're going to find precisely the nodes in this triangle. all of the other arcs involved go the wrong direction and they won't be traversed by, say, a depth-first search call. so, why is that interesting? what's interesting is that if we invoke dfs from this red node, or any of the three nodes from this triangle, then it's going to discover precisely this strongly connected component, precisely the three nodes in this circled scc. so that seems really cool, seems like maybe we just do a dfs, and boom we get an scc. so maybe if we can do that over and over again we'll get all the sccs. so that's a good initial intuition, but something can go wrong. suppose that instead of initiating dfs from one of these three nodes on the triangle, we say, initiated from this bottom most node in green. so remember, what is the guarantee of a graph search subroutine like dfs? it will find everything findable but of course, nothing more. so what's findable from this green node? well, naturally everything in its own scc, right? so the four nodes here, it'll certainly find those four nodes. on the other hand, if we start from this green node, since there are arcs that go from this bottom-most scc to the right-most the scc. not only will this dfs call find the four nodes in the green node's strong component, but it will also traverse these blue arcs and discover the three nodes in the red triangle. so, if we call dfs from this green node, we'll capture all seven of these. so the point is, if we call dfs, it looks like we're going to get a union of possibly multiple sccs. in fact, in the worst case, if we invoke dfs from the leftmost node, what's it going to discover? it's going to discover the entire graph. and that didn't give us any insight into the strong component structure at all. so, what's the takeaway point is, the takeaway point is if you call dfs from just the right place, you'll actually uncover an scc. if you call it from the wrong place, it will give you no information at all. so the magic of the algorithm that we're going to discuss next is we'll show having this super slick pre-processing step which ironically is itself is called a depth-first search. we can in linear time compute exactly where we want to start the subsequent depth-first searches from, so that each indication gets us exactly one strongly connected component and nothing more. so the algorithm that i'm going to show you is due to kosaraju and it will show the following theorem, that the strongly connected components of a directed graph can be computed in linear time. and as we'll see, the constants are also very small. it's really just going to be two passes of depth first search. and again i'm going to remind you that for many problems, it's natural to assume that the number of edges is at least the number of nodes because you're only interested in cases where the graph is connected. of course when you're computing connected components, that's one of the most natural cases where you might have a super sparse broken up graph. so we're not going to assume m is at least n so that's why linear time is going to be m plus n because that's the size of the input. and we don't know either m could be bigger than n or n could be bigger than m. we have no idea. kosaraju's algorithm is shocking in its simplicity. it has three steps. let me tell you what they are. first very mysteriously we are going to reverse all of the archs of the given graph. totally not clear why that would be an interesting thing to do yet. then we're going to do our first pass, our first depth first search, and we're going to do it on the reverse graph. now the naive way to implement this would be to literally construct a new copy of the input graph with all the the arcs in the reverse direction, and then just run depth first search on it. of course, the sophisticated, the sort of obvious optimization would be to just run dfs on the original graph, but going across arcs backwards. so i'll let you think through the details of how you'd do that, but that just works. you run dfs, and instead of going forward along edges, you go backward along edges, that simulates depth first search on the reverse graph. now i've written here dfs loop and that just means the user will check more to make sure that you see all of the nodes of the graph even if it's disconnected you have an outer loop where you just try each starting point separately. if you haven't already seen it then you run dfs from that given node. i'll be more detailed on the next slide. and the third step is just you run depth-first search again, but this time on the original graph. now at this point you should be thinking that i'm totally crazy, right, so what are we trying to do? we're trying to compute these strongly connected components. we're trying to actually compute real objects, these maximal regions and all i'm doing is searching the graph. i do it once forward. i do it once backward. i mean it doesn't seem like i'm computing anything. so here's the catch and it's a very minor catch. so we're going to get you to do a little bit of bookkeeping, it's going to be very little overhead so we'll still have a blazingly fast algorithm. so but with a little bit of bookkeeping, here's what's going to happen. the second depth first search. which searches the graph, will in it's search process discover the components one at a time in a very natural way. and that will be really obvious when we do an example which we'll do in just a second. now, for the second depth first search to work as magical way where it just discovers the connective component one at a time, it's really important that executes the depth first searches in a particular order, that it goes to the nodes of the graph in a particular order. and that is exactly the job of the first pass. the depth first search on reverse graph is going to compute an ordering of the nodes which, when the second depth first search goes through them in that order, it will just discover the sccs one at a time. in linear time. so let me say a little bit more about the form of the bookkeeping and then i'll show you how that bookkeeping is kept in as we do depth-first search. so we're going to have a notion of a finishing time of a vertex. and that's going to be computed in the first pass when we do depth-first search in the reverse graph. and we're going to make use of this data in the second pass. so rather than just going through the nodes of the graph in an arbitrary order, like we usually do when we sort of have a loop to depth first search. we're going to make sure that we go through the vertices in decreasing order of these finishing times. now there's still the question of what sense doe this second depth first search discover and report to the strong connected components that it finds? so we're going to label each node in the second pass with what we call a leader. and the idea is that the nodes in the same strong connected component will be labeled with exactly the same leader node. and again, all of this will be much more clear once we do a concrete example, but i want to have it down for the record right now. so that's the algorithm at a high level. it's really just two passes of dfs with some bookkeeping, but this is under specify. you really shouldn't understand how to implement the algorithm just yet. so what do i owe you? i owe you exactly what i mean by the dfs-loop, although this is seen more or less in the past. it's just a loop over all the vertices of the graph, and if you haven't seen something yet in dfs from that starting point, i need to tell you what finishing times are and how they get computed. they're just going to be integers 1 to n, which is basically when depth first search gets finished with one of the nodes, and then i need to tell you how you compute these leaders. so, let me tell you all three of those things on the next slot. so the work course for kosaraju's strongly connected components algorithm is this dfs-loop subroutine, and it takes, as input, a graph. so it does not take as input a starting node, it's going to loop over possible starting nodes. now for the bookkeeping to compute finishing nodes, we're going to keep track of a global variable of it all called t. which we initialize to zero. the point of t is to count how many nodes we've totally finished exploring at this point. so this is the variable we use to compute those finishing times in the first pass, that magical ordering that i was talking about. now we're also going to have a second global variable to compute these things i was calling leaders, and these are only relevant for the second pass. so what s is going to keep track of is the most recent vertex from which a dfs was initiated. so to keep the code simple, i'm just going to do all of the bookkeeping in the dfs-loop, so really dfs-loop gets called twice, once in a reverse graph, once in the forward graph. and we only need to compute these finishing times in the first pass on the reverse graph and we only need to compute these leaders on the second pass for the forward graph. but let's just keep them both in there just for kicks. now we're going to need to loop through the vertices. and so the question is in what order are we going to loop through the vertices? and that's going to happen differently in the two different passes, but let me just use some common notation. let's just assume, in this sub-routine, that the nodes are somehow labeled from 1 to n. in our first depth first search it's going to be labeled totally arbitrary, so these are basically just the names of the node or their position in the node array, whatever, you just do it in some arbitrary order. now the second time we run dfs loop, as indicated on the previous slide, we're going to use the finishing times as the labeling. as we'll see, the finishing times are indeed numbers between 1 in it, so now what do we do is we just iterate through the nodes in decreasing order. and if we haven't already seen node i, then we initiate a dfs from it, so as usual we're going to be maintaining a local boolean to keep track of what we had already seen a node yet in one of the dfs passes. now remember, the global variable s is responsible for keeping track of the most recent node from which depth first search had been initiated, so if i's not explored and we initiate a depth first search from it, we better reset s. and then we do the usual dfs ng starting from the source node i. so for completeness let me just remind you what the depth first search sub-routine looks like, so now we're given a graph and a starting node. so the first time we see a node we mark it as explored. and just a side note that once a node is marked explored, it's explored for this entire indication of dfs loop. okay so even if this dfs from a given node i finishes, and then the outer for loop marches on, and encounters i again, it's still going to be marked as explored. now one of our bookkeeping jobs is to keep track of from which vertex did the dfs that discovered i get called. so when i is first encountered, we remember that s was the node from which this dfs originated. and that by definition is the leader of i. and then we do what we always do with depth first search, we immediately look at the arcs going our of i and we try recursively dfs on any of those. although we don't bother to do it if we've already seen those nodes. now once this for loop has completed, once we've examined every outgoing arc from i and for each node j. either we already saw it in the past or we've recursively explored from j and have returned. at the point, we call ourselves done with node i, there's no more outgoing arc to explore. we think of it being finished, remember t is the global variable that's keeping track of how many nodes we're done with, so we increment t because now we're done with i. and we also remember that i was the t-th vertex with which we finished. that is, we said i's finishing time to be t. because depth first search is guaranteed to visit every node exactly once, and that therefore finish with every node exactly once. this global counter t, well when the first node is finished it'll be value 1, then when the next node gets finished i'll have value 2, then it'll have value 3 and so on. when the final node gets finished with it'll have value n. so the finishing times of the nodes are going to be exactly the integers from 1 to n. let's make this much more concrete by going through a careful example. in fact, i think it'll be better for everybody if you, yourself, traced through part of this algorithm on a concrete example. so let me draw a nine node graph for you. so to be clear, let's assume that we've already executed step one of the algorithm, and we've already reversed the graph. so that is, this blue graph that i've drawn on the slide, this is the reversal. we've already reversed the arcs. moreover the nodes are labeled in some arbitrary way from 1 to 9. just assume these are how they show up in the node array for example and remember in the dfs loop routine you're supposed to process the nodes from top to bottom from n down to 1. so my question for you then is in the second step of the algorithm when we run dfs-loop, and we process the nodes from the highest name 9 in order down to the lowest name 1. what are the finishing times that we're going to compute as we run dfs-loop? now, it is true that you can get different finishing times depending on the different choices that the dfs-loop has to make about which outgoing arc to explore next. but i've given you four options for what the finishing times of the nodes 1,2,3, all the way up to 9, respectively, might be. and only one of these four could conceivably be an output of the finishing time of dfs loop on this graph, so which one is it? all right so the answer is the fourth option, that is the only one of these four sets of finishing times that you might see being computed by dfs-loop on this blue graph. so let's go ahead and trace through dfs-loop and see how we might get this set of finishing times. so remember in the main loop we start from the highest node 9 and then we descend down to the lowest node 1. so we start by invoking dfs from the node 9. so now from here there's only one outgoing arc, we have to go to so we mark 9 as explored. and then there's only one place we can go, we can go to 6. so we mark 6 as explored. now there's two places we can go next, we can either go to 3 or we can go to 8 and in general dfs could do either one. now to generate this fourth set of finishing times i'm going to need to assume that i go to 3 first okay? so again, what dfs does, what we're assuming it does, it starts at 9, and it has to go to 6, it marks those as explored, then it goes to 3. it does not go to 8 first it goes to 3 first. now, from 3, there's only one outgoing arc which goes to 9, but 9, we've already marked as explored. so it's not going to re-explore 9, it's going to skip that arc. since that's 3's only outgoing arc, then that for loop completes, and then 3 is the first node to finish. so when we finish with 3, we increment t, it started at 0, now it's 1, and we set the finishing time of 3 to be 1. just like we said it was in the example. so, now we backtrack to 6. now we have another outgoing arc from 6 to explore, so now we go to 8. from 8 we have to go to 2, from 2 we have to go to 5, from 5 we have to go 8. 8 we've already seen, so then we're going to be done with 5, because that was its only outgoing arc. so then we increment t, now it's 2, and the finishing time of 5 is going to be 2 as promised. so now we've backtracked to 2, there's no more outgoing arcs from 2. so 2 is going to be the third one that we finish as promised. then we finish with 8, so the finishing time for 8 is going to be the fourth node to be done as promised. and now i back track back to 6, now at 6, that's the fifth node to be completed as promised. and finally we got all the way back to where we started at 9 and 9 is the sixth node to be completed as promised. now if we were computing those leaders all of these nodes would get the leader 9, but again the leaders are only relevant for the second pass. so we're just going to ignore the leaders as we're doing this tree so we're just going to keep track of finishing times. so now we're not done so all we did is we finished with the dfs that is invoked from the node 9 and we found 6 of the nodes total in that depth first search. so now we return to the outer for loop and we decrement i. so it started at 9, we're done with that, now we go down to 8. we say, have we already seen 8, yes 8's already explored so we skip it. we go, we decrement i down to 7, we say have we already seen node 7? no we have not okay? 7 is not yet explored. so we invoke dfs now from node sever 7 has two outgoing arcs, it can either go to 4 or it can go to 9. let's say it checks the outgoing arc to 9 first. now 9 we already explored. granted, that was an earlier part of the for loop, but we remember that. we're going to keep track of who got explored on previous iterations of the for loop so we don't bother to re-explore 9, so we skip that. so now from 7 we have to go to 4, from 4 we have to go to 1, from 1 we have to go back to 7. 7's already been exploratory backtrack and now we're done with 1. so 1 is the next one we're completed with and the finishing count of 1 is going to be 7 as promised. we backtrack to 4, there's no more outgoing arcs from 4 to explore, so that's going to be the eighth one to finish. as promised, and the last one to finish is poor node 7. it is last. so that would be an example of how the dfs-loop subroutine computes finishing times on a reversed graph. so now, let's work through the second pass on the forward version of the graph using the same example. now remember, the point of the first pass is to compute a magical ordering, and the magical ordering is these finishing times. so now we're going to throw out the original node names, and we're going to replace the node names in blue by the finishing times in red. we're also going to work with the original graphs, which means we have to reverse the arcs back to where they were originally. so those are the two changes you're going to see when i redraw this graph. first of all, all the arcs were reverse orientation. second of all, all of nodes will change names from their original ones to the finishing times that we just computed. so here's our new graph with the new node names and all of the arcs with their orientation reversed. and now we run dfs again on this graph. and again we're going to process the nodes in order from the highest label 9 down to the lowest label 1. moreover, we don't need to compute finishing times in the second pass, we only need to do that in the first pass. in the second pass we have to keep track of the leaders, and remember the leader of a vertex is the vertex from which dfs was called that first discovered that node. all right, so what's going to happen? well, in the outer for loop, again, we start with i equal to nine, and we invoke dfs from the node 9. so, that's going to be the current leader because that's where the current dfs got initiated. now, from 9, there's only one choice. we have to go to 7. from 7, there's only one choice, we have to go to 8. from 8, there's only one choice, we have to go back to 9. and then, 9's already been seen, so we backtrack. we go back to 8, we go back to 7, we go back to 9, and that's it. so, when we invoke dfs for node 9, the only things that we encounter are, the nodes 7, 8, and 9. and these are all going to be given the leader vertex 9. you will notice that this is indeed one of the strongly connected components of the graph. we just sort of found it with this indication of dfs from the node 9. so, now we go back to the outer for loop. and we say, okay, let's go to node 8, have we already seen 8? yes. what about 7, have we already seen 7? yes. what about 6? have we have already seen 6? we have not, we have not yet discovered 6, so we invoke dfs from node 6, we reset the global source vortex s to 6. from 6, we can go to 9, we can go to 1. so, let's say we explore 9 first. well, we already saw 9 in an earlier iteration in the for loop, so we don't explore it again, so, we don't discover 9 now, so we backtrack to 6. we go to 1, from 1, we have to go to 5, and 5 we have to go to 6, and then we start backtracking again. so, the only new nodes that we encounter when we invoke dfs from the node 6 are the vertices 6, 1, and 5. and all of these will have a leader vertex of 6 because that's where we called dfs from when we first discovered these 3 nodes. and you'll notice, this is another fcc of this directed graph. so, we invoke dfs again, not from a new node, the new node 6. and what it discovered, the new nodes it discovered was exactly an fcc of the graph. nothing more, nothing less. so, now we return to the outer for loop, we go to node 5, have we already seen 5? yes. have we already seen 4? no, we haven't seen 4 yet. so, now we invoke dfs from 4. again, we could try to explore 5, but we've seen that before, we're not going to explore it again. so from 4 then, we have to go to 2, from 2 we have to go to 3, from 3 have to go back to 4, and then, after all the backtracking, we're done. so, the final call to dfs will be from the node 4. and, that dfs will discover precisely, newly discover, precisely the nodes 2, 3 and 4. they will all have the leader vertex 4 because that was where this dfs was called from. it's true we'll go back to the for loop and we'll check have we seen 3 yet? yes. have we seen 2 yet? yes. have we seen 1 yet? yes, and then the whole thing completes. and, what we see is that, using the finishing times computed from that first depth first search pass, somehow the strongly connected components of this graph just showed up and presented themselves to us and one at a time on a silver platter. every time we invoke dfs, the nodes we discovered newly were precisely one of the fccs, nothing more, nothing less. and, that's really what's going on in this algorithm, turns out this was true in general. the first pass, dfs on the reverse graph, computes finishing times so that if you then process nodes according to decreased order in finishing times. in the second pass, each invocation to dfs will discover one new fcc and exactly one fcc. so, they'll just present themselves to you, one per dfs call in that second pass' for loop. this is, of course, merely an example. you should not just take a single example as proof that this algorithm always works. i will give you a general argument in the next video. but hopefully there is at least a plausibility arcing. no longer does this three-step algorithm seem totally insane. and maybe you could imagine, perhaps it works. at least there's some principles going on. where you first compute the right ordering to process the nodes, and then the second pass peels off fccs one at a time like layers from an onion. one thing that i hope is pretty clear is that this algorithm, correct or not, is blazingly fast. pretty much all you do is two depth per searches. and, since depth per search as we've seen in the past, runs in time linear in the size of the graph, so does kosaraju's two-pass algorithm. there are a couple subtleties and i encourage you to think about this and you'll be forced to think about this in the program and project for week four. so, for example, in the second pass, how do you process the nodes in decreasing order of finishing time? you don't want to sort the nodes by their finishing time, because that would take n log and time. so, you need to make sure that you remember in the first pass, that you sort of remember the nodes in a way that you can just do a linear scan through them in a second pass. so, there are some details, but, if your intuition is that this is really just double dfs properly implemented, that's pretty much exactly right. so, having spelled out the full implementation, argued that it's definitely a linear time algorithm and given at least a plausibility argument via an example that it might conceivably be correct, let's now turn to the general argument. 
so the goal of this video is to prove the correctness of kasaraju two-pass, depth-first-search based, linear time algorithm that computes the strongly connected components of a directed graph. so i've given you the full specification of the algorithm. i've also given you a plausibility argument of why it might work, in that at least it does something sensible on an example. namely, it first does a pass of depth first search on the reverse graph. it computes this magical ordering. and what's so special about this ordering is then when we do a depth first search using this ordering on the forward graph; it seems to do exactly what we want. every indication of depth first search to some new node discovers exactly the nodes of the strong component and no extra stuff. remember that was our first observation, but that was unclear whether depth for search would be useful or not for computing strong components. if you call depth first search from just the right place, you're gonna get exactly the nodes of an scc and nothing more. if you call it from the wrong place, you might get all of the nodes of the graph, and get no information at all about the structure of the strong components and at least in this example this first pass with the finishing time seems to be accomplishing seems to be leading us to invoking dfs from exactly the right places. so remember how this worked in the example so in the top graph, i have shown you the graph with the arch reversed. this is where we first invoked dfs loop with the loop over the nodes going from the highest node name nine all the way down to the node name one. and here we compute finishing time that's the bookkeeping that we do in the first pass so we just keep the running count of how many nodes we've finished processing. that is how many we've both explored that node as well as explore all of the outgoing arches and so that gives us these numbers in the red, these finishing times between one and nine for the various nodes. those became the new node names in the second graph and then we reverse the arches again and get the original graphs back and then we saw that every time we invoked dfs in our second pass we uncovered exactly the nodes of an scc. so when we invoked it from the node 9 we discovered that 9, 8 and 7 those have a leader vortex 9. then when we next invoked dfs from 6, we discovered 6, 5, and 1, and nothing else. and then finally we invoked it from 4, and we discovered 2, 3, and 4, and nothing else. and those are exactly the three, sccs of this graph. so let's now understand why this works in any directed graph, not just this, in this one example. so let's begin with a simple observation about directed graphs, which is actually interesting in its own right. the claim is that every directed graph has two levels of granularity. if you squint, if you sort of zoom out, then what you see is a directed acyclic graph, of course comprising its strongly connective components. and if you want, you can zoom in and focus on the fine grain structure with one scc. a little bit more precisely. the claim is of the strongly connected components of a directed graph induce in a natural way an acyclic metagraph. so what is this metagraph? what are the nodes and what are the arcs, what are the edges? well, the metanodes are just the sccs, so we think of every strong connected component as being a single node in this metagraph. so call them say 'c1' up to 'ck'. so what are the arcs in this metagraph? well, they're basically just the ones corresponding to the arcs between sccs in the original graph. that is, we include in the meta graph an arc from the strong component 'c' to 'c-hat' if and only if there's an arc from a node in 'c' to a node in 'c-hat' in the original graph 'g'. so for example if this is your 'c' and so the triangle is your 'c-hat' and you have one or maybe multiple edges going from 'c' to 'c-hat', then in the corresponding metagraph your just gonna have a node for 'c', a node for 'c-hat' and the directed arch from 'c' to 'c-hat'. so if we go back to some of the directed graphs that we've used as running examples so we go back to the beginning of the previous video and it's look maybe something like this the corresponding directed acyclic graph has four nodes and four arches. and for the running example we used to illustrate kosaraju's algorithm with the three triangles, the corresponding metagraph would just be a path with three nodes. so why is this meta-graph guaranteed to be acyclic? well, remember metanodes correspond to strong components, and in a strong component you can get from anywhere to anywhere else. so, if you had a cycle that involved two different metanodes, that is two different strong connected components, remember on a directed cycle you can also get from anywhere to anywhere else. so if you had two supposedly distinct sccs, that you could get from the one to the other and vice versa, they would collapse into a single scc. you can get from anywhere to anywhere in one, anywhere from anywhere in the other one, and you can also go between them at will, so you can get from anywhere in this union to anywhere in the union. so not just in this context of competing strong components but also just more generally, this is a useful fact to know about directed graphs. on the one hand, they can have very complex structure within a strong components. you have paths going from everywhere to everywhere else, and it may be sort of complicated looking. but at a higher level, if you abstract out to the level of sccs, you are guaranteed to have this simple dag, this simple directed acyclic graph structure. so, to reinforce these concepts, and also segue into thinking about kosaraju's algorithm in particular, let me ask you a question about how reversing arcs affects the strong components of a directed graph. so the correct answer to this quiz is the fourth one. the strong components are exactly the same as they were before, in fact the relation that we described is exactly the same as it was before so therefore the equivalence classes of the strong components is exactly the same. so if two nodes were related in the original graph, that is a path from u to v and a path from v to u, that's still true after you reverse all the arcs, you just use the reversal of the two paths that you had before. similarly if the two nodes weren't related before, for example because you could not get from u to v, well that after you reverse everything, then you can't get from v to u, so again you don't have this relation holding, so the sccs are exactly the same in the forward or the backward graph. so in particular in kazarogi's algorithm, the strong component structure is exactly the same in the first pass of dfs and in the second pass of dfs. so now that we understand how every directed graph has a meta graph with the nodes correspond to a strong connected components, and you have an arch from one scc to another if there's any arch from any node in that scc to the other scc in the original graph, i'm in a position to state what's the key lemma. that drives the correctness of kosaraju's two pass algorithm for computing the strong connected component of a directed graph. so here's the lemma statement. it considers two strongly connecting components that are adjacent, in the sense that there's an arc from one node in one of them to one node in the other one. so let's say we have one scc - 'c1', with a node i, and another, scc 'c2' with a node j, and that in g, in the graph, there's an arc directly from i to j. so in this sense we say that these sccs are adjacent, with the second one being in some sense after the first one. now let's suppose we've already run the first pass of the dfs loop subroutine. and remember that works on the reverse graph. so we've invoked it on the reverse graph. we've computed these finishing times. as usual we'll let f(v) denote the finishing times computed in that depth first search subroutine on the reverse graph. the lemma then asserts the following. it says first, amongst all the nodes in 'c1' look at the one with the largest finishing time. similarly amongst all nodes in 'c2' look at the one with the biggest finishing time. amongst all of these the claim is that the biggest finishing time will be in 'c2' not in 'c1'. so what i wanna do next is i wanna assume that this lemma is true temporarily. i wanna explore the consequences of that assumption and in particular what i wanna show you is that if this lemma holds, then we can complete the proof of correctness of kosaraju's two-pass scc computation algorithm. okay, so if the lemma is true then after... i'll give you the argument about why we're done. about why we just peel off the scc one at a time with the second pass of depth first search. now of course a proof with a hole in it, isn't a proof. so at the end of the lecture i'm gonna fill in the hole. that is, i'm gonna supply a proof of this key lemma. but for now, as a working hypothesis, let's assume that it's true. let's begin with a corollary, that is a statement which follows essentially immediately, from the statement of a lema. so for the corollary, let's forget about just trying to find the maximum maximum finishing time in a single scc. let's think about the maximum finishing time in the entire graph. now, why do we care about the maximum finishing time in the entire graph? well, notice that's exactly where the second pass of dfs is going to begin. right, so it processes nodes in order from largest finishing time to smallest finishing time. so equivalently, let's think about the node at which the second pass of depth first search is going to begin, i.e., the node with the maximum finishing time. where could it be? well, the corollary is that it has to be in what i'm gonna call a sink, a strongly connected component, that is a strongly connected component without any outgoing arcs. so for example let's go back to the, meta graph of sccs for the very first directed graph we looked at. you recall in the very first direc ted graph we looked at in when we started talking about this algorithm there were four sccs. so there was a 'c1', a 'c2', a 'c3', and a 'c4'. and of course within each of these components, there could be multiple nodes but they are all strongly connected to each other. now, let's use f1, f2, f3 and f4 to denote the maximum finishing time in each of these sccs. so we have f1, f2, f3 and f4. so, now we have four different opportunities to apply this lemma. right? those four different pairs of adjacent sccs. and so, what do we find? we find that well, comparing f1 and f2, because c2 comes after c1, that is there's an arc from c1 to c2, the max finishing time in c2 has to be bigger than that in c1. that is f2 is bigger than f1. for the same reasoning f3 has to be bigger than f1. symmetrically we can apply the limit to the pair c2, c4 and c3, c4 and we get that f4 has to dominate both of them. now notice we actually have no idea whether f2 or f3 is bigger. so that pair we can't resolver. but we do know these relationships. okay f1 is the smallest and f4 is the smallest [biggest!!]. and you also notice that c4 is a sink scc and the sink has no outgoing arches and you think about it that's a totally general consequence of this lema. so in a simple group of contradiction will go as follows. consider this scc with the maximum f value. suppose it was not a sink scc that it has an outgoing arch, follow that outgoing arch to get some other scc by the lema the scc you've got into has even bigger maximum finishing time. so that contradicts the fact that you started in the scc with a maximum finishing time. okay. so just like in this cartoon, where the unique sink scc has to have the largest finishing time, that's totally general. as another sanity check, we might return to the nine node graph, where we actually ran kasaraja's algorithm and looking at the ford version of the graph, which is the one on the bottom, we see that the maximum finishing times in the three fcc are 4,6 and 9. and it turns out that the same as the leader nodes which is not an accident if you think about it for a little while and again you'll observe the maximum finishing time in this graph namely 9 is indeed in the left most scc which is the only scc with no outgoing arks. okay but it's totally general basically you can keep following arks and you keep seeing bigger and bigger finishing times so the biggest one of all it has to be somewhere where you get stuck where you can't go forward but there's no outgoing arks and that's what i'm calling a sink scc. okay. so assuming the lemma is true we know that the corollary is true. now using this corollary let's finish the proof of correctness, of kasaraja's algorithm, module over proof of the key lima. so i'm not going to do this super rigorously, although everything i say is correct and can be made, made rigorous. and if you want a more rigorous version i'll post some notes on the course website which you can consult for more details. so what the previous corollary accomplished, it allows us to locate, the node with maximum finishing time. we can locate it in somewhere in some sink scc. let me remind you about the discussion we had at the very beginning of talking about computing strong components. we're tryna understand depth-first search would be a useful workhorse for finding the strong components. and the key observation was that it depends, where you begin that depth-first search. so for example in this, graph with four scc's shown in blue on the right. a really bad place to start. dfs called depth for search would be somewhere in c1. somewhere in this source scc, so this is a bad dfs. why is it bad? well remember what depth for search does; it finds everything findable from its starting point. and from c1 you can get to the entire world, you can get to all the nodes in the entire graph. so you can discover everything. and this is totally useless because we wanted to discover much more fine-grain structure. we wanted to discover c1, c2, c3 and c4 individually. so that would be an disaster if we invoked depth first search somewhere from c1. fortunately that's not what's going to happen, right? we computed this magical ordering in the first pass to insure that we look at the node with the maximum finishing time and, by the corollary, the maximum finishing time is going to be somewhere in c4. that's gonna be a good dfs, in the sense that, when we start exploring from anywhere in c4, there's no outgoing arcs. so, of course, we're gonna find everything in c4. everything in c4's strongly connected to each other. but we can't get out. we will not have the option of trespassing on other strong components, and we're not gonna find'em. so we're only gonna find c4, nothing more. now, here's where i'm gonna be a little informal. although, again, everything i'm gonna say is gonna be correct. so what happens now, once we've discovered everything in c4? well, all the nodes in c4 get marked as explored, as we're doing depth first search. and then they're basically dead to us, right? the rest of our depth first search loop will never explore them again. they're already marked as explored. if we ever see'em, we don't even go there. so the way to think about that is when we proceed with the rest of our for loop in dfs loop it's as if we're starting afresh. we're doing depth first search from scratch on a smaller graph, on the residual graph. the graph g with this newly discovered strong component 'c<i>'</i> deleted. so in this example on the right, all of the nodes in c4 are dead to us and it's as if we run dfs anew, just on the graph containing the strong components c1, c2 and c3. so in particular, where is the next indication of depth first search going to come from? it's going to come from some sink scc in the residual graph, right? it's going to start at the node that remains and that has the largest finishing time left. so there's some ambiguity in this picture. again recall we don't know whether f2 is bigger or f3 is bigger. it could be either one. maybe f2 is the largest remaining finishing time in which case the next dfs indication's gonna begin somewhere more from c2. again, the only things outgoing from c2 are these already explored nodes. their effectively deleted. we're not gonna go there again. so this is essentially a sink fcc. we discover, we newly discover the nodes in c2 and nothing else. those are now effectively deleted. now, the next indication of dfs will come from somewhere in f3, somewhere in c3. that's the only remaining sink scc in the residual graph. so the third call, the dfs will discover this stuff. and now, of course, we're left only with c1. and so the final indication of dfs will emerge from and discover the nodes in c1. and in this sense because we've ordered the nodes by finishing times when dfs was reverse graph, that ordering has this incredible property that when you process the nodes in the second pass we'll just peel off the strongly connected components one at a time. if you think about it, it's in reverse topological order with respect to the directed asypric graph of the strongly connected components. so we've constructed a proof of correctness of kosaraju's, algorithm for computing strongly connected components. but again, there's a hole in it. so we completed the argument assuming a statement that we haven't proved. so let's fill in that last gap in the proof, and we'll we done. so what we need to do is prove the key lemma. let me remind you what it says. it says if you have two adjacent sccs, c1 and c2 and is an arc from a node in c1, call it 'i' to a node in c2, say j. then the max finishing time in c2 is bigger than the max finishing time in c1. where, as always, these finishing times are computed in that first pass of depth-first search loop in the reversed graph. all right, now the finishing times are computed in the reversed graph, so let's actually reverse all the arcs and reason about what's happening there. we still have c1. it still contains the node i. we still have c2, which still contains the node j. but now of course the orientation of the arc has reversed. so the arc now points from j to i. recall we had a quiz which said, asked you to understand the effect of reversing all arcs on the scc's and in particular there is no effect. so the scc's in the reverse graph are exactly the same as in the forward graph. so now we're going to have two cases in this proof and the cases correspond to where we first encounter a node of c1 and union c2. now remember, when we do this dfs loop, this second pass, because we have this outer four loop that iterates over all of the nodes we're guaranteed to explore every single node of the graph at some point. so in particular we're gonna have to explore at some point every node in c1 and c2. what i want you to do is pause the algorithm. when it first, for the first time, explores some node that's in either c1 or c2. there's going to be two cases, of course, because that node might be in c1, you might see that first. or it might be in c2, you might see something from c2 first. so our case one is going to be when the first node that we see from either one happens to lie in c1. and the second case is where the first node v that we see happens to lie in c2. so clearly exactly one of these will occur. so let's think about case one. when we see a node of c1 before we see any nodes of c2. so in this case where we encounter a node in c1 before we encounter any node in c2, the claim is that we're going to explore everything in c1 before we ever see anything in c2. why is that true? the reason is there cannot be a path that starts somewhere in c1, for example, like the vertex v, and reaches c2. this is where we are using the fact that the meta-graph on scc is a cyclic. right c1 is strong connected, c2 is strong connected, you can get from c2 to c1 and, if you can also get from c1 back to c2 this all collapses into a single strongly connected component. but that would be a contraction, we're assuming c1 and c2 are distinct strongly connected components, therefore you can't have paths in both directions. we already have a path from right to left, via ji, so there's no path from left to right. that's why if you originate a depth first search from somewhere inside c1 like this vertex v, you would finish exploring all of c1 before you ever are going to see c2, you're. only gonna see c2 at some later point in the outer for loop. so, what's the consequence that you completely finish with c1 before you ever see c2? well it means every single finishing time in c1 is going to be smaller than every single finishing time in c2. so that's even stronger that what we're claiming, we're just claiming that the biggest thing in c2 is bigger than the biggest of c1. but actually finishing times in c2 totally dominate those in c1, because you finish c1 before you ever see c2. so let's now have a look at case one actually in action. let's return to the nine-node graph, the one that we actually ran kosaraju's algorithm to completion. so if we go back to this graph which has the three connected components, then remember that the bottom version is the forward version, the top version is the reversed version. so if, if you think about the middle scc as being c1, pulling the row of c1 and the left most. scc playing the role of c2, then what we have exactly is case one of the key lemma. so, which was the first of these six vertices visited during the dfs loop in the reversed graph? well that would just be the node with the highest name, so the node nine. so this was the first of these six vertices that depth first search looked at in the first pass, that lies in what we're calling c1. and indeed everything in c1 was discovered in that pass before anything in c2 and that's why all of the finishing times in c2, the 7,8,9 are bigger than all of the finishing times in c1 - the 1,5, and 6. so we're good to go in case two. we've proven sorry, in case one, we've proven the lemma. when it's the case that, amongst the vertices in c1 and c2, depth first search in the first pass sees something from c1 first. so now, let's look at this other case, this grey case, which could also happen, totally possible. well, the first thing we see when depth first searching in the first pass, is something from c2. and here now is where we truly use the fact that we're using a depth first search rather than some other graph search algorithm like breadth first search. there's a lot of places in this algorithm you could swap in breadth first search but in this case two, you'll see why it's important we're using depth first search to compute the finishing times. and what's the key point? the key point is that, when we invoke depth first search beginning from this node v, which is now assuming the line c2. remember depth first search will not complete. we won't be done with v until we've found everything there is to find from it, right? so we recursively explore all of the outgoing arcs. they recursively explore all of the outgoing arcs, and so on. it's only when all paths going out of v have been totally explored and exhausted, that we finally backtrack all the way to v, and we consider ourselves done with it. that is, depth first search. in the reverse graph initiated at v. won't finish until everything findable has been completely explored. because there's an arc from c2 to c1, obviously everything to c2 is findable from v, that's strongly connected. we can from c2 to c1 just using this arc from j to i. c1 being strongly connected we can then find all of that. maybe we can find other strongly connected components, as well, but for sure depth-first search starting from v will find everything in c1 to c2, maybe some other things. and we won't finish with v until we finish with everything else, that's the depth-first search property. for that reason the finishing time of this vertex v will be the largest of anything reachable from it. so in particular it'll be larger than everything in c two but more to the point, it'll be larger than everything in c1 which is what we are trying to prove. again let's just see this quickly in action in the nine node network on which we traced through kosaraju's algorithm. so to show the rule that case two is playing in this concrete example let's think of the right most strongly connected component as being c1. and let's think of the middle scc as being c.2. now the last time. we called the middle one c1 and the leftmost one c2. now we're calling the rightmost one c1 and the middle one c2. so again, we have to ask the question, you know, of the six nodes in c1 and in c2, what is the first one encountered in the depth first search that we do in the first pass. and then again, is the node nine? the, the node which is originally labeled not. so it's the same node that was relevant in the previous case, but now with this relabeling of the components, nine appears in the strongly connected component c-2, not in the one labeled c-1. so that's the reason now we're in case two, not in case one. and what you'll see is, what is the finishing time that this originally labeled nine node gets. it gets the finishing time six. and you'll notice six is bigger than any of the other finishing times of any of the other nodes in c1 or c2. all, the other five nodes have the finishing times one through five. and that's exactly because when we ran depth first search in the first pass, and we started it at the node originally labeled nine, it discovered these other five nodes and finished exploring them first before finally back tracking all the way back to nine, and deeming nine fully explored. and it was only at that point that nine got its finishing time after everything reachable from it had gotten there lower finishing times. so that wraps it up. we had two cases depending on whether in these two adjacent scc's, the first vertex encountered was in the c1, or in c2. either way it doesn't matter, the largest finishing time has to be in c2. sometimes it's bigger than everything, sometimes it's just bigger than the biggest in c-1, but it's all the same to us. and to re cap how the rest of the proof goes, we have a corollary based on this lemma, which is maximum finishing time have to lie in sink scc and that's exactly where we want our depth first search to initiate. if you're initiated in a strong component with no outgoing arcs, you do dfs. the stuff you find is just the stuff and that strongly connected component. you do not have any avenues by which to trespass on other strong components. so you find exactly one scc. in effect, you can peel that off and recurse on the rest of the graph. and our slick way of implementing this recursion is to just do the single second, dfs pass, where you just treat the nodes in decreasing order of finishing times, that in effect, unveil, unveils all of the sccs in reverse topological ordering. so that's it, kosaraju's algorithm, and the complete proof of correctness. a blazingly fast graph primitive that, in any directed graph, will tell you its strong components. 
so, we've now put in a lot of work designing and analyzing super fast algorithms for reasoning about graphs. so in this optional video, what i want to do is show you why you might want such a primitive, especially for computation on extremely large graphs. specifically, we're going to look at the results of a famous study that computes the strongly connected components of the web graph. so what is the web graph? well it's the graph in which the vertices correspond to webpages. so for example i have my own webpage where i list my research papers and also links to courses, such as this one. and the edges are going to be directed and they correspond precisely to hyperlinks. so the links that bring you from one web page to another. note, of course, these are directed edges, where the tail is the page that contains the hyperlink. and the head is the page that you go to if you click the hyperlink. and so, this is a directed graph. so from my home page you can get to my papers, you can get to my courses. sometimes i have random links up to things i like, say, my favorite record store. and of course for many of these webpages, there are additional links going out or going in. so for example, from my papers i might link to some of my co-authors. some of my co-authors might be linking from their homepages to me. or of course, there's webpages out there which list the currently available free online courses and so on. so obviously, this is just part of a massive web graph, just a tiny, tiny piece of it. so the origins of the web were probably around 1990 or so, but it started to really explode in the mid 90s. and by the year 2000, it was sort of already beyond comprehension. even though, in internet years, the year 2000 is sort of the stone age, relative to right now, relative to 2012. but still, even by 2000 people were so overwhelmed with the massive scale of the web graph, they wanted to understand anything about it, even the most basic things. now of course, one issue with understanding what the graph looks like is you don't even have it locally, right? it's distributed over all of these different servers over the entire world. so the first thing people really focused on, when the wanted to answer this question, was on techniques for crawling. so having software which just follows lots of hyperlinks, reports back to the home base, from which you can assemble at least some kind of sketch of what this graph actually is. but then the question is, even once you have this crawled information, even once you've accessed a good chunk of the nodes and the edges of this network, what does it look like? so what makes this a difficult question, more difficult than, say, for any other directed graph you might encounter? well, it's simply the massive scale of the web graph, it's just so big. so for the graph used in the particular study i'm going to discuss, like we said, it was in the year 2000, which is sort of the stone age compared to 2012. so the graph was small, relatively, but still the graph was really, really big. so it was something like 200 million nodes and one billion edges, really one and a half billion edges. so the reference for the work i'm going to discuss is a paper by a number of authors. the first author is andre broder and then he has many co-authors and this was a paper that appeared in the www conference of the year 2000, that's the world wide web conference. and i encourage to those of you who are interested to go track down the paper online and read the original source. so andre broder, the lead author, at this time he was at a company that was called alta vista. so how many of you remember a company called alta vista? well, some of you, especially the youngest ones among you maybe have never heard of alta vista. and the youngest ones among you maybe can't even conceive of a world in which we didn't have google. but in fact, there was a time when we had web search, but google did not yet exist. that was sort of in the maybe '97 or so, and so this is in the very embryonic years of google, and this data set actually came out of alta vista instead. so broder et al wanted to get some answers to this question, what does this web graph look like? and they approached it in a few ways. but the one i'm going to focus on here is, they asked, well, what's the most detailed structure we can get about this web graph, without doing an infeasible amount of computation? really just sticking to linear time algorithms, at the worst. and, what have we seen? we've seen that in a directed graph you can get full connectivity information just really using depth first search. that you can compute strongly connected components in linear time with small constants. and that's one of the major things that they did in this study. now if you wanted to do the same computation today, you'd have one thing going against you and one thing going for you. the obvious thing that you'd have going against you is that the web is still very much bigger than it was here, certainly by an order of magnitude. the thing that you'd have going for you is now there's specialized systems which are meant to operate on massive data sets. and in particular, they can do things like compute connectivity information on graph data. so what you have to remember, for those of you who are aware of these terms, in 2000 there was no mapreduce, there was no hadoop. there were no tools for automated processing large data sets, these guys really had to do it from scratch. so let me tell you about what broder et al found when they did strong connectivity computations on the web graph. they explain their results in what they called the bow tie picture of the web. so let's begin with the center, or the knot of the bow tie. so in the middle we have what we're going to call a giant strongly connected component. with the interpretation being this is the core of the web in some sense. all right, so all of you know what an scc is at this point. a strongly connected component is a region from which you can get from any point to any other point along a directed path. so in the context of the web graph, with this giant scc, what this means is that from any webpage inside this blob, you can get to any other webpage inside this blob, just by traversing a sequence of hyperlinks. and hopefully it doesn't strike you as too surprising that a big chunk of the web is strongly connected, is well-connected in this sense, right? so if you think about all the different universities in the world, probably all of the webpages corresponding to all of the different universities, you can get from any one place to any other place. for example, from the homepage on which i put my papers, i often include links to my co-authors, which very commonly are at other universities. so that already provides a web link from some stanford page to some page at say, berkeley or cornell or whatever. and of course, i'm just one person, i'm just one of many faculty members at stanford. so you put all of these together, you would expect all of the different sccs corresponding to different universities to collapse into a single one. and so on for other sectors as well. and then of course, if you knew that a huge chunk of the web was in the same strongly connected component, so let's say 10% of the web, which would be tens of millions of webpages. you wouldn't expect there to be a second one, right? it would be super weird if there were two different blobs, 10 million web pages each that somehow were not mutually reachable from each other. all it takes to collapse two sccs into one is a lone arc going from one to the other and then a lone arc going in the reverse direction. and then those two sccs collapse into one. so we do expect a giant scc, just sort of thinking anecdotally about what the web looks like. and then once we realize there's one giant scc, we don't expect there to be more than one. all right, so is that the whole story? is the web graph just one big scc? well, one of the perhaps interesting findings of this broder et al paper is that there is a giant scc, but it doesn't actually take up the whole web, or anything really that close to the entire web. so what else would there be in such a picture? well, there's the other two ends of the bow tie, which are called the in and the out regions. in the out regions you have a bunch of strongly connected components, not giant sccs. we've established their shouldn't be any other giant sccs. but small sccs, which you can reach from the giant strongly connected component. but from which you cannot go back to the giant strongly connected component. i encourage you to think about what types of websites you would expect to see in this out part of the bow tie. i'll give you one example. very often if you look at a corporate site, including those of well known corporations, which you would definitely expect to be reachable from the giant scc. it's actually a corporate policy that no hyperlinks can go from something in the corporate site to something outside the corporate site. so that means the corporate site is going to be a collection of webpages, which is certainly strongly connected. because it's a major corporation, you can certainly get there from the giant scc. but because of its corporate policy, you can't get back out. symmetrically, in the in part of the bow tie, you have strongly connected components, generally small ones. from which you can reach the giant scc, but you cannot get to them from the giant scc. again, i encourage you to think about all the different types of webpages you might expect to see in this in part of the bow tie. certainly i think one really obvious example would be new webpages. so if you just create something, and then if i just created a webpage and pointed it to stanford university, that would immediately be in this in component or this in collection of components. now, if you think about it, this does not exhaust all of the possibilities of where nodes can lie. there's a few other cases that frankly are pretty weird, but they're there. you can have passive hyperlinks, which bypass the giant scc and go straight from the in part of the bow tie to the out part. so broder et al suggested calling these tubes. and then there's also a kind of very curious outgrowths going out of the in component, but which don't make it all the way to the giant scc. and similarly, there's stuff which goes into the out component. and broder et al recommended calling these strange creatures tendrils. and then, in fact, you can just have some weird isolated islands of sccs that are not connected, even weakly, to the giant scc. so this is the picture that emerged from broder et al's strong component computation on the web graph. and here's, qualitatively, some of the main findings that they came up with. so first of all, that picture on the previous slide i drew roughly to scale. in the sense that all four parts, so the giant scc, the in part, the out part, and then the residual stuff, the tubes and tendrils, have roughly the same size, more or less 25% of the nodes in the graph. i think this is surprising people. i think some people might have thought that the core, that the giant scc might have been a little bit bigger than just 25 or 28%. but it turns out there's a lot of other stuff outside of this strongly connected core. you might wonder if this is just an artifact of this data set being from the stone age, being from 2000 or so. but people have rerun this experiment on the web graph again in later years. and of course the numbers are changing because the graph is growing rapidly. but these qualitative findings have seemed pretty stable throughout subsequent reevaluations of the structure of the web. on the other hand, while the core of the web is not as big as you might have expected, it's extremely well connected. perhaps better connected then you might have expected. now you'd be right to ask the question, what could i mean by unusually well connected? we've already established that this giant core of the web is strongly connected. you can get from any one place to any other place via a sequence of hyperlinks. what else could you want? well in fact, it has a very richer notion of connectivity called the small world property. so let me tell you about the small world property or the phenomenon colloquially known as six degrees of separation. so this is an idea that had been in the air at least since the early 20th century, but really kind of was studied in a major way and popularized by stanley milgram, who's a social scientist, back in 1967. so, milgram was interested in understanding, are people at great distance, in fact, connected by a short change of intermediaries? so, the way he evaluated this, he ran the following experiment. he identified a friend in boston, massachusetts, a doctor, i believe. and so this was going to be the target. and then he identified a bunch of people who were thought to be far away, both culturally and geographically, specifically omaha. so for those of you who don't live in the us, just take it on faith that many people in the us would regard boston and omaha as being fairly far apart, geographically and otherwise. and what did is he wrote each of these residents of omaha the following letter. he said, look, here is the name and address of this doctor who lives in boston. your job is to get this letter to this doctor in boston. now, you're not allowed to mail the letter directly to the doctor. instead, you need to mail it to an intermediary, someone who you know on a first name basis. so of course, if you knew the doctor on a first name basis, you could mail it straight to them, but that was very unlikely. so what people would do in omaha is they'd say, well, i don't know any doctors or i don't know anyone in boston, but at least i know somebody in pittsburgh. and at least that's closer to boston than omaha, that's further eastward. or maybe someone would say, well, i don't really know anyone on the east coast but at least i do know some doctors here in omaha. and so they'd give the letter to somebody that they knew on a first name basis in omaha. and then the situation would repeat. whoever got the letter, again they'd be given the same instructions. if you know this doctor in boston on a first name basis, send him the letter. otherwise, pass the letter on to somebody who seems more likely closer to them than you are. now, of course, many of these letters never reach their destination, but shocking, at least to me, is that a lot of them did. so something like 25%, at least, of the letters that they started with made it all the way to boston. which i think says something about people in the late 60s just having more free time on their hands than they do in the early 21st century. i find this hard to imagine, but it's a fact. so you had dozens and dozens of letters reaching this doctor in boston. and they were able to trace exactly which path of individuals the letter went along before it eventually reached this doctor in boston. and so then what they did is they looked at the distribution of chain links. so how many intermediaries were required to get from some random person in omaha to this doctor in boston? some were as few as two, some were as big as nine but the average number of hops, the average number of intermediaries, was in the range of five and a half or six. and so this is what has given rise to the colloquialism, even the name of a popular play, the six degrees of separation. so that's the origin myth, that's where this phrase comes from, these sort of experiments with physical letters. but now in network science, the small world property is meant to be a network. which on the one hand is richly connected, but also in some sense there are enough cues about which links are likely to get closer to some target. so that if you need to route information from point a to point b, not only is there a short path, but if you in some sense follow your nose, then you'll actually exhibit a short path. so in some sense, routing information is easy in small world networks. now this is exactly the property that broder et al identified within this giant scc. very rich with short paths, and if you want to get from point a to point b, just follow your nose and you'll do great. you don't need a very sophisticated shortest path algorithm to find a short path. some of you may have heard of sammy milgram, not for the small world experiment, but for another famous, or maybe infamous experiment he did earlier in the 60s. which consisted into tricking volunteers into thinking they are subjecting other human beings to massive doses of electric shocks. so that wound up causing a rewrite to certain standards of ethics in experimental psychology. you don't hear about that so much when people are talking about networks, but that's another reason why milgram's work is well known. and just as a point of contrast, outside of this giant strongly connected component, which has this rich small world structure, very poor connectivity in the other parts of the web graph. so there's lots of cool research going on these days about the study of information networks like the web graph. so i don't want you to get the impression that the entire interaction between algorithms and thinking about information networks has just been this one strongly connected component computation in 2000. of course, there's all kinds of interactions. i've just singled one out that was easy to explain and also highly intellectual and interesting back in the day. but these days, lots of stuff's going on. people are thinking about information networks in all kinds of different ways and of course algorithms, like in almost everything, is playing a very fundamental role. so let me just dash off sort of a few examples, maybe to whet your appetite. maybe you want to go explore this topic in greater depth outside of this course. so one super interesting question is, rather than looking at a static snapshot of the web, like we were doing so far in this video, the web's changing all the time. new pages are getting created, new links are getting created and destroyed and so on. and how does this evolution proceed? can we have a mathematical model, which faithfully reproduces the most important first order properties of this evolutionary process? so a second issue is to think not just about the dynamics of the graph itself, but the dynamics of information that gets carried by the graph. and you could ask this both about the web graph and about other social networks, like say, facebook or twitter. another really important topic, which there's been a lot of work on, but we still don't fully understand by any means, is getting at the finer grain structure in networks including the web graph. in particular, what we'd really like to do is have foolproof methods for identifying communities. so groups of nodes, this could either be webpages in the web graph or individuals in a social network, which we should think of as grouped together. we discussed this a little bit when we talked about applications of cuts. one motivation for cuts is to identify communities, if you think of communities as being relatively densely connected inside and sparsely connected outside. but that's just a baby step. really, we need much better techniques for both defining and computing communities in these kinds of networks. so i think these questions are super interesting, both from a mathematical/technical level, but also they're very timely. answering them really helps us understand our world better. unfortunately, these are going to be well outside the course of just the bread-and-butter design analysis of algorithms, which is what i'm tasked with covering here. but i will leave you with a reference book that i recommend if you want to read more about these topics. namely, the quite recent book by david easley and john kleinberg called networks, crowds, and markets. 
we've arrived at another one of computer science's greatest hits, namely djikstra's shortest path algorithm. so let me tell you about the problem, it's a problem called single source shortest paths. basically what we want to do is compute something like driving directions. so we're given as input a graph, in this lecture i'm going to work with directed graphs, although the same algorithm would work for undirected graphs with cosmetic changes. as usual, we'll use m to denote the number of edges and n to denote the number of vertices. the input also includes two extra ingredients. first of all, for each edge e we're given as input a non-negative link which i'll denote as l sub e. in the context of a driving directions application l sub e could denote the mileage how long this particular road is, or it could also denote the expected travel time along the edge. the second ingredient is a vertex from which we are looking for paths. this is exactly the same as we had in breadth first search. in that first search we have an originating vertex which we'll call here the source. our responsibility then is to, given this input, to compute for every other vertex, v, in this network the length of the shortest path from the source vertex, s, to that destination vertex, v. and so just to be clear, what is the length of the path that has say three edges in it? well it's just the sum of the link of the first edge in the path plus the length of the second edge in the path plus the length of the third edge in the path. so if you had a path like this with three edges and length one, two and three, then the length of the path would just be six. and then we define the shortest sv path in the natural way, so amongst all of the paths directed from s to v, each one has its own respective path length and then the minimum overall sv paths is the shortest path distance in the graph g. so i'm going to make two assumptions for these lectures. one is just really for convenience, the other is really important. the other assumption, without which, dijkstra's algorithm is not correct, as we'll see. so for convenience we'll assume that there is a directed path from s to every other vertex v in the graph, otherwise the shortest path distance is something we define to be plus infinity. and the reason this is not a big assumption is, if you think about it, you could detect which vertices are not reachable from s just in a preprocessing step using, say, breadth-first or depth-first search. and then you could delete the irrelevant part of the graph, and run dijkstra's algorithm as we'll describe it on what remains. alternatively, dijkstra's algorithm will quite naturally figure out what vertices there are paths to from s and which ones there are not, so this won't really come up. so to keep it simple, just think about we have an input graph where you can get from s to v, for every different vertex v. and the challenge then is amongst all the ways to get from s to v, what is the shortest way to do it? so the second assumption already appears in the problem statement, but i want to reiterate it just so it's really clear. when we analyze jackson's algorithm, we always focus on graphs where every length is non-negative, no negative edge lengths are allowed. and we'll see why a little bit later in the video. now in the context of a driving directions application it's natural to ask the question why would you ever care about negative edge lengths. until we invent a time machine that doesn't seem like negative edge lengths are going to be relevant when you are computing literal paths through literal networks. but again remember that paths can be thought of as more abstractly as a just sequence of decisions. and some of the most powerful applications of shortest paths are coming up with optimal weight such sequences. so, for example, maybe you're engaging in financial transactions and you have the option of both buying and selling assets at different times. if you sell then you get some kind of profit and that would correspond to a negative edge length. so there are quite interesting applications in which negative edge lengths are relevant. if you are dealing with such an application, dijkstra's algorithm is not the algorithm to use. there's a different shortest path algorithm, a couple other ones. but the most well-known one is called bellman-ford. that's something based on dynamic programming, which we may well cover in a sql course. okay, so for dijkstra's algorithm, we always focus on graphs. that'll have only non-negative edge lengths. so, with the next quiz, i just want to make sure that you understand the single source shortest path problem. let me draw for you here a simple four node network, and ask you for, what are the four shortest path lengths. so from the source vertex s, to each of the four vertices in the network. all right, so the answer to this quiz is the final option, 0,1,3,6. to see why that's true, well, all of the options had 0 as the shortest-path distance from s to itself. so that just seemed kind of obvious. so the empty path will get you from s to itself and have 0 length. no suppose you wanted to get from s to v, well there's actually only one way to do that, you have to go along this one hop path. so the only path has length of one, so the shortest path distance from s to v is one. now w's more interesting, there's a direct one hop path, sw, that has a length of four, but that is not the shortest path from s to w inf act to two-hop path that goes through v as an intermediary has total path length three which is less than the length of the direct arc from s to w. so therefore the shortest distance from s to w is going to be 3. and finally for the vertex t there's three different paths going from s to t. there's the two-hop path that goes through v. there's the two hop path which goes through w, both of those have path length 7, and then there's the three hop path which goes through both v and w. and that actually has a path length of one plus two plus three equals six. so despite having a largest number of edges, the zigzag path is, in fact, the shortest path from s to t and it has length 6. all right, so before i tell you how dijstrka's algorthin works, i feel like i should justify the existence of this video a little bit. all right? because this is not the first time we've seen shortest paths. you might be thinking rightfully so. we already know how to compute shortest paths. that was one of the applications of breadth first search. so the answer to this question is both yes and no. breadth first search does indeed compute shortest paths. we had an entire video about that. but it works only in the special case where the length of every edge of the graph is one. at the moment we're trying to solve a more general problem. we're trying to solve shortest paths, when edges can have arbitrary non-negative edge lengths. so for example, in the graph that we've explored in the previous quiz, if we ran breadth first search, starting from the vertex s, it would say that the shortest path distance from s to t is 2 and that's because there's a path with two hops going from s to t. put differently, t is in the second layer emanating from s. but as we saw in the quiz, there's not in fact a shortest two hop path from s to t if you care about the edge lengths. rather the minimum length path, the shortest path, with respect to the edge weights, is this three hop path which gives us a total length of 6. so breadth first search is not going to give us what we want when the edge lengths are not all the same. and if you think about an application like driving directions, then needless to say, it's not the case that every edge in the network is the same. some roads are much longer than others, some roads will have much larger travel times than others, so we really do need to solve this more general shortest path problem. similarly, if you're thinking more abstractly about a sequence of decisions like financial transactions, in general different transactions will have different values. so you really want to solve general shortest paths, you're not in the special case that breadth-first search solves. now, if you're feeling particularly sharp today, you might have the following objection to what i've just said. you might say, eh, big deal. general edge weights, unit edge weights, it's basically the same. say you have an edge that has length three. how is that fundamentally different than having a path with three edges, each of which has length one? so why not just replace all the edges with a path of edges of the appropriate length? now we have a network in which every edge has unit length and now we can just run breadth-first search. so put succinctly, isn't it the case that computing shortest paths with general edge weights reduces to computing shortest paths with unit edge weights? well, the first comment i want to make is i think this would be an excellent objection to raise. and indeed, as programmers, as computer scientists, this is the way you should be thinking. if you see a problem that seems superficially harder than another one, you always want to ask, well, maybe just with a clever trick i can reduce it to a problem i already know how to solve. that's a great attitude in general for problem solving. and indeed, if all of the edge lengths were just small numbers, like 1, 2, and 3 and so on, this trick would work fine. the issue is when you have a network where the different edges can have very different lengths. and that's certainly the case in many applications. definitely road networks would be one where you have both sort of long highways and you have neighborhood streets. and potentially in financial transaction based networks you would also have a wide variance between the value of different transactions. and the problem then is some of these edge lengths might be really big. they might be 100, they might be 1,000. it's very hard to put operating bounds on how large these edge weights could be. so if you start wantonly replacing single edges with these really long paths of like 1,000, you've blown up the size of your graph way too much. so you do have a faithful representation of your old network, but it's too wasteful. so even though breadth-first search runs in linear time, it's now on this much larger graph. and we'd much prefer something which is linear time or almost linear time that works directly on the original graph. and that is exactly what dijkstra's shortest-path algorithm is going to accomplish. let's now move on to the pseudocode for dijkstra's shortest path algorithm. so this is another one of those algorithms where no matter how many times i explain it, it's always just super fun to teach. and the main reason is because it exposes the beauty that pops up in good algorithm design. so the pseudocode, as you'll see in a second, is itself very elegant. we're just going to have one loop, and in each iteration of the loop we will compute the shortest path distance to one additional vertex. and by the end of the loop we'll compute shortest path distances to everybody. the proof of correctness, which we'll do in the next video, is a little bit subtle, but also quite natural, quite pretty. and then finally, dijkstra's algorithm will give us our first opportunity to see the interplay between good algorithm design and good data structure design. so with a suitable application of the heap data structure, we'll be able to implement dijkstra's algorithm so it runs blazingly fast, almost linear time, namely m times log n. but i'm getting little ahead of myself. let me actually show you this pseudocode. at a high level, you really should think of dijkstra's algorithm as being a close cousin of breadth-first search. and indeed, if all of the edge lengths are equal to one, dijkstra's algorithm becomes breadth-first search. so this is sort of a slick generalization of breadth-first search when edges can have different lengths. so like our generic graph search procedures, we're going to start at the source vertex s, and in each iteration we're going to conquer one new vertex. and we'll do that once each iteration after m minus1 iteration, we'll be done. and in each iteration will correctly compute the shortest path distance to one new possible destination vertex v. so let me just start by initializing some notation. so capital x is going to denote the vertices that we've dealt with so far. and by dealt with, i mean we've correctly computed shortest path distance from the source vertex to every vertex in x. we're going to augment x by one new vertex in each iteration of the main loop. remember that we're responsible for outputting n numbers, one for each vertex. we're not just computing one thing, we're computing the shortest path distance from the source vertex s to every other vertex. so i'm going to frame the output in terms of this array capital a. so for each vertex, we're going to have an entry in the array a, and the goal is at the end of the algorithm, a will be populated with the correct shortest path distances. now to help you understand dijkstra's algorithm, i'm going to do some additional bookkeeping which you would not do in a real implementation of dijkstra's algorithm. specifically, in addition to this array capital a in which we compute shortest path distances from the source vertex to every other destination, there's going to be an array capital b in which we'll keep track of the actual shortest path itself from the source vertex s to each destination v. so the arrays a and b will be indexed in the same way. there'll be one entry for each possible destination vertex v. capital a will store just a number for each destination, shortest path distance. the array b in each position will store an actual path, so the shortest path from s to v. but again, you would not include this in an actual implementation. i just find in my experience it's easier for students to understand this algorithm if we think of the paths being carried along as well. so now that i've told you the semantics of these two arrays, i hope it's no surprise how we initialize them for the source vertex itself, s. the shortest path distance from s to itself is 0. the empty path gets you from s to s with length 0. there's no negative edges by assumption, so there's no way you can get from s back to s with a non-positive length, so this is definitely the shortest path distance for s. by the same reasoning, the shortest path from s to s is just the empty path, the path with no edges in it. so now let's proceed to the main while loop. so the plan is we want to grow this set capital x like a mold until it covers the entire graph. so in each iteration, it's going to grow and cover up one new vertex and that vertex will then be processed. and at the time of processing, we're responsible for computing the shortest path distance from s to this vertex and also figuring out what the actual shortest path from s to this vertex is. so in each iteration, we need to grow x by one node to ensure that we make progress. so the obvious question is, which node should we pick? which one do we add to x next? so there's going to be two ideas here. the first one we've already seen in terms of all of these generic graph search procedures, which is we're going to look at the edges and the vertices which are on the frontier. so we're going to look at the vertices that are just one hop away from vertices we've already put into x. so that motivates that a given iteration of the while loop to look at the stuff we've already process, that's x, and the stuff we haven't already processed, that's v minus x. s, of course, starts in x and we never take anything out of x, so s is still there. in some generic iteration of the while loop, we might have some other vertices that are in x. and in a generic iteration of this while loop, there might be multiple vertices which are not in x. and now, as we've seen in our graph search procedures, there are general or edges crossing this cut. so there are edges which have one endpoint in each side, one endpoint in x and one endpoint outside of x. this is a directed graph so they can cross in two directions. they can cross from left to right or they can cross from right to left. so you might have some edges internal to x. those are things we don't care about at this point. you might have edges which are internal to v- x. we also don't care about those, at least not quite yet. and then you got edges which can cross from x to v-x, as well as edges that can cross in the reverse direction, from v -x back to x. and the ones we're going to be interested in, just like when we did graph search and directed graphs, are the edges crossing from left to right, the edges whose tail is amongst the vertices we've already seen and whose head is some not yet explored vertex. so the first idea is that in each iteration of the while loop we scan, or we examine, all of the edges with tail in x and head outside of x. one of those is going to lead us to the vertex that we pick next. so that's the first idea, but now we need a second idea, because this is again quite underdetermined. there could be multiple such vertices which meet this criterion. so for example, in the cartoon in the bottom left part of this slide, you'll notice that there's one vertex here. which is the head of an arc that crosses from left to right. and there's yet another vertex down here in v minus x, which again is the head of an arc which crosses from left to right. there are two options of which of those two to suck into our set x and we might want some guidance about which one to pick next. the key idea in dijkstra is to give each vertex a score corresponding to how close that vertex seems to the source vertex s, and then to pick among all candidate vertices, the one that has the minimum score. let me be more precise. among all crossing edges, with tail on the left side and head on the right side, we pick the edge that minimizes the following criterion. the shortest path distance that we've previously computed from s to the vertex v, plus the length of the edge that connects v to w. this is quite an important expression, so i will call this dijkstra's greedy criterion. this is a very good idea to use this method to choose which vertex to add to the set x, as we'll see. i need to give a name to this edge which minimizes this quantity over all crossing edges. let's call it v star w star. for example, in the cartoon in the bottom left, maybe of the two edges crossing from left to right, maybe the top one is the one that has a smaller value of dijkstra's greedy criterion. in that case, this would be the vertex v star and the other end of the edge would be the vertex w star. this edge, v star, w star is going to do wonders for us. it will both guide us to the vertex that we should add the x next, that's going to be w star. it's going to tell us how we should compute the shortest path distance to w star, as well as what the actual shortest path from s to w star is. specifically, in this iteration of the wild loop, after we've chosen this edge v star w star, we add w star to x. remember, by definition, w star was previously not in capitol x. we're making progress by adding it to x, that's one more vertex in x. now x is supposed to represent all of the nodes that we've already processed. so an environ of this algorithm is that we've computed shortest path distances for everybody in x as well as the actual shortest paths. now that we're putting w star in x, we're responsible for all this information, the shortest path information. what we're going to do is we're going to set the r estimate of w star's shortest path distance from s to be equal to the value of this dijkstra's greedy criterion for this edge. that is, whatever our previously computed shortest path distance from s to v star was plus the length of the direct edge from v star to w star. now a key point is to realize that this code does make sense. by which i mean, if you think about this quantity a(v), this is been previously computed. and that's because environ of this algorithm is we've always computed shortest path distances to everything that is in capital x. and of course, the same thing holds when we need to assign w star shortest path distance because v star was a member of capital x, we had already computed its shortest path distance. so we can just look up the v star entry position in the array a. over in our picture on our left, we would just say, what did we compute the shortest path distance to v star previously? maybe it's something like 17. and then we'd say, what is the length of this direct edge from v star to w star? maybe that's 6. then we would just add 17 and 6 and we would put 23 as our estimate of the shortest path distance from s to w star. we do something analogous with the shortest path itself in the array b. that is, again, we're responsible, since we just added w star to capital x, we're responsible for suggesting a path from s to w star in the b array. what we're going to do is we're just going to inherit the previously computed path to v star and we're just going to tack on the end one extra hop, namely the direct edge from v star to w star. that will give us a path from s all the way to w star via v star as an intermediate pit stop and that is the entirety of dijkstra's algorithm. i've explained all of the ingredients about how it works at a conceptual level. the two things i argue is, why is it correct? why does it actually compute shortest paths directly to all of the different vertices, and then secondly, how fast can we implement it? the next two videos are going to answer both of those questions but before we do that, let's go through an example to get a better feel for how this algorithm actually works. i also want to go through a non example so that you can appreciate how it breaks down when there are negative edges, and that'll make it clear why do we need a proof of correctness because it's not correct without any assumptions about the edge lengths. 
so let's just see how it works in the same example we traced through earlier. so we start out just by initializing things in the obvious way, so the shortest path distance from s to itself we say is 0. and the shortest path from s to itself is just the empty path. and initially, our x is going to be just the source for text itself. so, now we enter the main while loop and so, remember in the while loop, we say well, let's scan all of the edges whose tail is in the vertices we've already looked at, whose tail is in x and whose head is outside of x. now, in this first iteration, there are two such edges, there's the edge (s,v) and the edge (s,w). so how do we know which of these two to choose? well, we evaluate dijkstra's greedy criterion. and so remember what that is, dijkstra's greedy score for a given edge (v,w) that's crossing the frontier is just the previously computed shortest path distance for the tail of the arc plus the length of the arc itself. so at this point, (s,v) has a greedy score of 0 + 1, which is 1, and the arc (s,w) has a greedy score of 0 + 4, which is 4. so obviously (s,v) is going to be the shorter of those two. so we use the edge (s,v), this is playing the role of v*w* on the previous slide. and the algorithm then suggests that we should add v to our set x. so we suck in v, and our new x consists of s, n, v, and it also tells us how to compute the shortest path distance and the shortest path from s to v. namely, in the a array, we just write down what was the dijkstra's greedy score for this particular edge, and that was 0 + 1, or 1. it also tells how to compute the shortest path for v. namely, we just inherit the shortest path to the tail of the arc, which in this case, is the empty path from s to itself and then we tack on the end, we append the arc we used to get here, the arc s(v). so now we go to the next iteration of the while loop, so with our new set capital x consisting of s and v. and now again, we want to look at all edges which are crossing the frontier, edges that have tail in x and head outside x. and now we see there's three such crossing edges. there's (s,w), there's (v,w), and there's (v,t). all of those have the tail in x and the head outside of x. so we need to compute dijksta's greedy score for each of those three and then pick the minimum. so let's go from bottom to top. so first of all, we can look at the arc (s,w). and the greedy score here is the shortest path distance for the tail, so it's 0 plus the length of the arc, which is 4. so here we get a 4 in this iteration. then if we do this cross-bar edge, this (v,w) edge, the dijkstra's greedy score is the a value, or the shortest path distance value of the tail. and we computed that last iteration, that a(v) value is 1, we add to that the length of the arc, which in this case is 2. so this edge (v,w) has a score of 3. finally there's the arc (v,t) and here we're going to add 1, which is the shortest path distance of the tail of the arc plus the edge length, which is 6. so that has the worst score. so, since the edge (v,w) has the smallest score, that's the one that guides how we supplement x and how we compute the shortest path distances and the shortest path for the newly acquired vertex w. so the changes are, first of all, we enlarge x. so x is now everything, but t. and then how do we compute things for w? well, the shortest path, so the r entry in the a array is just going to be dijkstra's greedy score in the previous iteration. so that was 1+2, so it's going to be equal to 3. and then what is the shortest path? how do we fill up the array b? well we inherit the shortest path to the tail of the arc, which in this case, is the arc (s,v) and then we append the arc that we used to choose this new vertex w, so that's the arc (v,w). so the new path is just the (s,v,w) path, okay, so it's what we compute is the shortest path from s to w in this graph. so now we proceed to the final iteration of dijkstra's algorithm. we know what vertex we're going to bring in to x, it's going to be the vertex t, that's the only one left. but we still have to compute by which edge we discovered t and bring it in to the set x. so we have to compute the greedy score for each of the two crossing arcs, (v,t) and (w,t). and then this final iteration, the score for the arc (v,t) is unchanged. so this is still going to be the a value of its tail 1, plus the length of the arc, 6. so the score here is still 7, and now for the first time, (w,t) is a crossing edge of the frontier. and when we compute its score, it's the a value of its tail w, which is 3, plus the length of this arc which is 3, so i get a greedy score of 6. so by dijkstra's greedy criterion, we picked the edge (w,t) instead of the edge (v,t), and of course, that doesn't matter who gets brought into x, but it does matter how we compute the a and b values for t. so in the final iteration, we compute (a,t) to be the dijkstra's greedy score of the edge that we picked, which is the edge (w,t), and the score was 6. so we compute the shortest path distance from s to t to be 6. and then what is the path itself? well, we inherit the shortest path to the tail of the arc that we used to discover t, so that's the shortest path to w, which we previously computed as being the path through v. and then we append the edge we used to discover t, so we append the edge (w,t). so the shortest path from s to t, we're going to compute as the zigzag path, s goes to v goes to w goes to t. and then now, dx is all the vertices, we've computed it for everything, this is our final output. the contents of the, especially the a array is the final output, shortest path distances from s to all of the four possible destinations. and if you go back and compare this to the example you went through the quiz, you will see at least on this example, indeed dijkstra's algorithm corrects the shortest path distances. now i've said it before, i'm going to say it again. someone shows you their algorithm works just on some examples, especially a pretty simple four note example, you should not jump to the conclusion that this algorithm always works. sometimes algorithms work fine on small examples, but break down once you go to more interesting complicated examples. so i definitely owe you a proof. the dijkstra's algorithm works not only in this network, but in any network. and actually it doesn't work in any network, it's only going to work in any network with non-negative edge lengths. so to help you appreciate that, let's conclude this video with a non example, showing what goes wrong in dijkstra's algorithm when you have networks with negative edge lengths. so before i actually give you a real non-example, let me just answer a preliminary question, which you might have, and this would be a very good question if it's something that has occurred to you. the question would be well, why are these negative edge links such a big deal? why can't we just reduce shortest path computation with negative edge links to the problem of computing shortest paths with non-negative edge links, right? so why don't we just sort of clear things out? we just add a big number to all the edges, that makes them all non-negative and then we just run dijkstra's algorithm and we're good to go. so this is exactly the sort of question you should be looking to ask if as a computer scientist, as a serious programmer. when confronted with a problem, you always want to look for ways to reduce it to simpler problems that you already know how to solve. and this is a very natural idea of how to reduce a seemingly harder shortest path problem to one we already know how to solve using dijkstra's algorithm. the only problem is, it doesn't quite work. why doesn't it work? well, let's say you have a graph, and the most negative edge is minus ten. so, all the other edge links are minus 10 and above. so then, what you want to do is add 10 to every single edge in the network, and that ensures that all the lengths are non-negative. run dijkstra's algorithm, get your shortest path. the issue is that different paths between a common origin and destination have differing numbers of edges. so, some might have five edges, some might have two edges. now, if you add 10 to every single edge in the graph, you're going to change path lengths by different amounts. if a path has five edges, it's going to go up by 50 when you add 10 to every edge. if a path has only two edges, it's only going to go up by 20 when you add 10 to every edge. so as soon as you start changing the path lengths of different paths by different amounts, you might actually screw up which path is the shortest. the path which is shortest of the new edge lengths need not be the one the shortest under the old edge lengths. so that's why this reduction doesn't work. to be concrete, let's look at this very simple three vertex graph where vertices s, v, and t and edge lengths as shown 1, -5 and -2. now what i hope is clear is that in this graph, the shortest path, the one with the minimum length, is the too hot path, s, v, t. that has length minus 4. the direct s,t arc has length minus 2, which is bigger than minus 4. so the upper path is the shortest path. now suppose we try to massage this by adding a constant to every edge so that all edge links were non-negative. we have to add 5 to every edge, because that's the biggest negative number, the (v,t) edge. so that would give us new edge lengths of 6, and 0, and 3. and now the problem is, we have changed which path is the shortest one. we added 10 to the top half and only 5 to the bottom half and as a result, they've reversed. so now the bottom path (s,t) is actually the shorter one, so if you run dijkstra's on this graph, it's going to come back with a path (s,t), even though that's not in fact the shortest path in the original network, the one that we actually care about, okay. so that's why you can't just naively reduce shortest paths with negative edge lengths to shortest paths with non-negative edge lengths. moreover, on this very same super simple three nug graph, we can try running dijkstra's shortest path algorithm. it's perfectly well defined, it'll produce some output, but it's actually going to be wrong. it is not going to compute shortest path distances correctly in this graph, so let me show you why. of course, the initialization will work as it always does, so it's going to start by saying the shortest path distance from s to itself is 0 by the empty path. and then, what's it going to do next? it's going to say, okay, well, we need to enlarge the set capital, x, by one vertex, and there are two crossing edges as the (x,v) edge and the (s,t) edge. and what's it going to do? it's going to use the dijkstra's greedy score. so, the score of this upper edge is going to be 1, and the score of this bottom edge is going to be minus 2. because remember, you take the previously computed shortest path value of the tail, that is 0 in both cases, and then you add the edge length. so the edge lengths are 1 and minus 2, so the scores are 1 and minus 2. which of these is smaller? well evidently, the (s,t) arc has the smaller score, minus 2. so what is dijkstra's algorithm going to do? it's going to say, yes, let's go for this edge, (s,t). let's bring t into the set capital x. t is now part of the conquered territory. and of course, as soon as you bring a node into the set x, into conquered territory, you have to commit or dijkstra's algorithm chooses to commit to its shortest path distance and its shortest path. what is the definition of its shortest path distance, as computed by dijkstra? well it's just a greedy score. so it's going to assign the vertex t, the shortest path distance of minus 2, and the path is going to be just the arc, (s,t). but notice that this is in fact wrong. the shortest path distance from s to t is not minus 2 in this graph. there's another path, namely the one that goes through v, that has length minus 4, less than minus 2. so dijkstra computes incorrect shortest path distances on this trivial three note graph. so to summarize the story so far, we've described dijkstra's algorithm. i've showed you that it works in a very simple example that doesn't have negative edge lengths. and i've showed you that it doesn't work in an even simpler example that does have negative edge lengths. so i've both given you some plausibility that it might work generally, at least for non negative edge lengths, but i've also tried to sow some seeds of doubt. that it's not at all clear at this point if dijkstra's algorithm is always correct or not, even if you have non negative edge lengths. and certainly, if it is always correct, there better be a foolproof argument for why. you should be demanding an explanation of a claim that dijkstra is correct in any kind of generality. that's the subject of the next video. 
in this video, i'll prove to you that dijkstra's algorithm does indeed compute to correct shortest paths in any directed graph where all edge links are non negative. so let me remind you about what is dijkstra's algorithm, it's very much in the spirit of our graph search primitives, in particular breath first search. so there's going to be a subset x of vertices, which are the ones that have been processed so far. initially x contains only the source vertex. of course the distance from the source vertex to itself is 0, and the shortest path from s to itself is the empty path. so then we'll have a main while loop, that's going to be n-1 iteration, and each iteration will bring one vertex which is not currently in x into capital x. and a variant that we are going to maintain, is that all the vertices in x we will have computed estimates of the shortest path distance from x to that vertex and also we'll have computed the shortest path itself from x to that vertex. remember our standing assumption stated in the previous video, we're always going to assume there's at least one path from the source vertex s to every other destination v. our job is just to compute the shortest one. and also, we have to assume that the edge links are non negative as we've seen otherwise dijkstra's algorithm might fail. now, the key idea in dijkstra's algorithm is a very careful choice of which vertex to bring from outside of x into capital x. so what we do is we scan the edges crossing the frontier. meaning given the current edges vertices that we've already processed, we look at all of the edges whose tail has been processed and whose head has not been processed. so the tails in capital x, the head is outside of x that is, they cross the cut from left to right in the diagrams that we usually draw. now, there may be many such edges. how do we decide amongst them? well, we compute the dijkstra's greedy score for each. the dijkstra greedy score is defined as the shortest path distance we computed for the tail and that's been previously computed because the tail's in capital x. and then we add to that the length contributed by this edge itself by the edge vw which is crossing the cut from left to right. so amongst all edges crossing the cut from left to right we compute all those dijkstra greedy scores we pick the edge with the smallest greedy score calling that edge just v* w*. for the purposes of notation, w* is the one that gets added to x. so it's the head of the arc for the smallest three score, and then we compute the shortest path distance of that new vertex w* to be the shortest path distance to v* plus the length contributed by this edge v* w* and then was the shortest path. it's just the shortest path previously computed to v star plus this extra edge v* w* tacked onto the end. here's the formal statement we're going to prove. for this video, we're not going to worry at all about running time, that'll be the discussion of the next video. we'll discuss both the running time of the basic algorithm and a super fast implementation that uses the heat data structure. for now, we're going to just focus on correctness. so the claim is that for every directed graph, not just the four node, five arc example we studied. as long as there's no negative edge links, dijkstra's algorithm works perfectly. it computes all the correct shortest path distances. so just to remind you about the notation, what does it mean to correct all shortest path distances correctly? it means that what the algorithm actually computes, which is a(v), is exactly the correct shortest path distance, which we were denoting by l(v) in the previous video. both the algorithm and the proof of correctness where established by esther dijkstra this was back in the late 1950s. dijkstra was a dutch computer scientist, and certainly one of the forefathers of the field as a science, as an intellectual discipline. he was awarded the acm turing award, so that is the nobel prize in computer science effectively. i believe it was 1972, and he worked a long time in the netherlands, but then also spent a lot of his later career at ut austin. so the way this proof is going to go is going to be by induction. and basically, what we're going to do is we're going to say every iteration, when we have to commit to shortest path distance to some new vertex, we do it correctly. and so then the form of the induction will be, well given that we made all of our previous decisions correctly, we computed all our earlier shortest paths in the correct way. that remains true for the current iteration. so formally, it's induction on the number of iterations of dijkstra's algorithm. and as is more often than not the case in proofs by inductions the base case is trivial. so that just says before we start the y loop, what do we do? well we commit to the shortest path distance from s to itself. we set it to 0, we set the shortest path to be the empty path, that is of course true. of course, even here we're using the fact that there are no edges with negative edge length. that makes it obvious that sort of having a non empty path can get you negative edge length better than 0. so the first choice path computation we do s to s is trivially correct. the hard part of course is the inductive step justifying all of the future decisions done by the algorithm. and of course, mindful of that example that not example we had at the end of the previous video in the proof by induction, we'd better make use of the hypothesis that every edge has non negative length. otherwise the theorem would be false. so we better somewhere in the proof use the fact that edges cannot be negative. so let's move on to the inductive step. remember in the inductive step, the first thing to do is state the inductive hypothesis. you're assuming you haven't made any mistakes up to this point. let's be a little bit more formal about that. so that is everything we computed in the past. what did we compute in the past? well for each vertex which is in our set capital x for each vertex that we've already processed, we want to claim that our computed shortest path distance matches up exactly with the true correct shortest path distance. so in our running notation, for every already processed vertex, so for all vertices v, in our set capital x. what we computed as our estimate of the shortest path distance for v is in fact, the real shortest path distance. and also, the computed shortest path is in fact, a true shortest path from s to v. so again remember, this is a proof by induction. we are assuming this is true, and we're going to certainly make use of this assumption when we establish the correctness of the new iteration, the current iteration. what happens in an iteration? well, we pick an edge which we've been calling v* w*. and we add the head of sets w* to the set x. so let's get our bearings and remember what dijkstra's algorithm computes as the shortest path and shortest bath distance for this new vertex w*. so by the definition of the algorithm we assign as a shortest path from s to w*. the previously computed purportedly shortest path from s to v*, and then we tack on the end the direct arc, v* w*. so pictorially, we already had some path that started at s and ended up at v*, and then we tack on the ends this arc going to w* in one hop. and this whole shebang is what we're going to assign as b of w*. so let's use the inductive hypothesis. inductive hypothesis says that all previous iterations are correct. so that is any shortest path we've computed in a previous iteration is in fact a bona fide shortest path from the source x to the vertex. now v*, remember is in x. so that was previously computed. so by the inductive hypothesis, this path bv*, from s to v*, is in fact a true shortest path from s to v* in the graph. so therefore, it has length l of v*. remember, l of v* is just by definition the true shortest path distance in the graph from s to v*. now, given that the path that we've exhibited s to w*, is just the same one as we inherited the v* plus this extra edge tacked on. it's pretty obvious what the length of the left hand side is. it has length, just the length of the old path which we just argued is the shortest path distance from sw* plus the length of this arc that we tacked on. that's going to be lv* w*. so by the definition of the algorithm, what we compute for w* is just the dijkstra's greedy score which is just the computer choice path distance to the tail. the v* plus the length of the direct edge. by the inductive hypothesis, we've correctly computed all previous choice path distances. v* is something we computed in the past by inductive hypothesis is correct. so this is equal to l of v* by the inductive hypothesis. so don't worry if you're feeling a little lost at this point. we've actually really done no content in this proof yet. we haven't done the interesting part of the argument. all we've been doing is setting up our dominoes, getting them ready to be knocked down. so what have we done in the current iteration? well first of all, our estimate of the shortest path distance from the source to w*, to the new vertex that we're including in the set capital x, is the true shortest path distance to v* plus the length of the edge from v* to w*, that's the first thing. secondly, the path that we have in the v array is a bona fide path from s to w* with exactly this distance. and the point is, now it's clear what has to be proven for us to complete the inductive step and therefore, the proof of correctness of dijkstra's algorithm. so what do we need to proof? we need to proof that this isn't just any old path that we've exhibited from s to this vertex w*, but that it's the shortest path of them all. but differently we need to show that every other sw* pattern in this graph has length at least this circled value. so let's proceed let's show that no matter how you get from the source for test to this destination w*. the total length of the path you travel is going to be at least this circled value, at least l(v*) + lv*w*. now on the one hand, we don't have a lot going for us, because this path p could be almost anything. it could be a crazy looking path. so how do we argue that it has to be long? well, here's the one thing we've got going for us for any path p that starts in s and goes to w*. any such path must cross the frontier. remember, it starts on the left side of the frontier, it starts at the source vertex, which is initially and forever in the set capital x. and remember that we only choose edges that cross the frontier whose head is outside of x. and w* is exactly the head of the edge we chose in this iteration, so this is not an x. so any path that starts in x and goes outside of x at some point it crosses from one to the other. so let's think about the graph and it's two pieces, that it's the left of the front tier and not to the right. the stuff is already processed and the stuff which is not been processed. s of course, is on the left hand side, and at the beginning of this iteration of the while loop, w* was on the right hand side. any path, no matter how wacky has to at some point, cross this frontier. maybe it does it a bunch of times, who knows but it's gotta do it once. let's focus on the first time it crosses the frontier, and let's say that it crosses the front here with the vertex y going to the vertex z. that is any path p has the form where there's an initial prefix,where are the vertices are in x. and then there's some first point at which it crosses the frontier and goes to a vertex which is not an x, we're calling the first such vertex outside of x, z. and then it can skip back and forth who knows, but certainly it ends up in this vertex w* which is not an x. so we're going to make use of just this minimal information about an arbitrary path p. and yet this will give us enough of a foothold to lower bound its length. and this lower bound to be strong enough, we conclude that our path that we computed is the best, smaller than any possible competitor. so let's just summarize where we left on the previous slide. we established that every directed path from s to w* p, no matter what it is has to have a prescribed form, where it ambles for a while inside x and then the portal through which it escapes x for the first time we're calling y. and then the first vertex it sees outside of x is z and there has to be one. and then it perhaps ambles further and eventually reaches w*. it could well be that z and w* are exactly the same, that's totally fine for this argument. so here's one of our competitors, this path p and i have to show it's at least as long as our path. so we need a lower bound on the length of this arbitrary path from s to w*. so let's get that lower bound by arguing about each piece separately, and then invoking dijkstra's greedy criterion. so remember, we said we better use the hypothesis that edge links are non negative, otherwise we're toast, otherwise we know the algorithm is not correct. so here's where we use it. this final part of the path from z to w*, if it's not empty then it's gotta have non negative length right. every edge as part of this subpath has non negative edge length, so the total length of this part of the path is non negative. so y to z by construction is direct arc. remember, this is the first arc that path p uses to go from x to get outside of x. so that's how it escapes, the conquer territory x and this just has some length, l of yz. so that leaves the first part of this path, the prefix of this path that lies entirely in capital x. so how do we get a lower bound in the length of this path? well, let's begin with something trivial. this is some path from s to y, so certainly it's as least as long as a shortest path from s to y. and now, we're going to use the inductive hypothesis again. so this vertex y, this is something we treated in a previous iteration. this belongs to the set capital x, we've already processed it, we've already computed our estimate of it's shortest path length, and the inductive hypothesis assures us that we did it correctly. so whatever value we have hanging out in our array capital a, that is indeed the length of the true shortest path. so the length of the shortest sy path is l(y) by definition, and it's a(y) by the inductive hypothesis, and now we're in business. so what does this mean we can say about the total length of this arbitrary path p? well, we've broken it into three pieces and we have a lower bound on the length for each of the three pieces. our lower bounds are, our computed shortest path distance to y, the length of the direct edge from y to z and 0. so adding those up, we get that the length of path p is at least our computed shortest path distance to y plus the length of the arc from y to z. so why is this useful? well, we've got one remaining trick up our sleeve. there's a hypothesis which is presumably very important, which we have not yet invoked. and that is the choice of dijkstra's greedy criterion at no point in the proof yet have we used the facts that we select which vertex to add next according to dykstra's greedy score. so that is going to be the final nail in the coffin, that's what's going to complete the proof. how do we do that? well we have taken an arbitrary path p, we have lower abounded it's length, in terms of the computed shortest path distance up to the last vertex of this prefix y plus the arc length to get from x to l set of x, zyz. so remember, this means y is on the left part of the frontier and z is not. and therefore in this iteration, the edge yz was totally a candidate for us to use to enlarge our frontier. remember, we looked at all of the edges crossing from left to right. yz is one such edge and amongst all of them, we chose the one with the smallest dijkstra's greedy score. that was the dijkstra's greedy criterion. so what have we shown? we've shown that the length of our path is no more than what's a lower bound on the length of this arbitrary other path p. so this completes the proof. so let me just remind you of all the ingredients, in case you got lost along the way. so what we started out with is we realized our algorithm or dijkstra's algorithm it does compute some path from s to w*. it just takes the path it computed previously to v*, and it just depends this final hop at the end. so that gives us some path from s to w* moreover, it was easy to figure out exactly what the length of that path is. and the length of the path that we came up with is exactly the circled quantity at the bottom of the slot. it's the shortest path distance from s to v* plus the length of the direct arc from v* to w*. so that was how well we did. but we had to ask the question, is it possible to do better? we're trying to argue that our algorithm does the best possible, that no competing path could possibly be shorter than ours. so how did we do that? well, we considered an arbitrary competing path p. the only thing we know about it is that it starts at s and it ends up at w*. and we observe that any path can be decomposed into three pieces. a prefix, a direct edge, and a suffix. then we give a lower bound on this path p. the direct edge, the length is just whatever it is. the suffix, we just use the trivial lower bound that's at least 0. and that's where we use the hypothesis that every edge has non negative edge length. and for the prefix, because that's all in the stuff we already computed, we can vote the inductive hypothesis and say, well whatever this path is, it goes from s to come vertex and y. so at least the shortest path distance from s to y which is something we computed in a previous iteration. we lower bounded the length of any other path in terms of the dijkstra's greedy score for that path. since we choose the path with the best greedy score, that's why we wind up with the shortest path of them all, from s to w*. this of course, is embedded in an outer proof by induction on the number of iterations, but this is the inductive step, which justifies a single iteration. since we can justify every iteration giving correctness to the previous ones. that means by induction, all of them are correct. so all of the shortest paths are correct. and that is why dijkstra's algorithm correctly computes shortest paths and any directed graph with non negative edge lengths. 
in this video we'll discuss how we actually implement dijkstra's shortest path algorithm. and in particular, using the heap data structure, we'll give a blazingly fast implementation, almost linear time. let me just briefly remind you of the problem we're solving. it's the single source, shortest path problem. so we're given the directed graph and a source vertex, s. we're assuming that there's a path from s to every other vertex, v. if that's not true, we can detect it with an easy pre-processing step, so our task then is just to find the shortest path amongst all of them from the source vertex, s to each possible destination, v. moreover, every edge of the graph has a non-negative edge length which we're denoting, else of v. so recall that dijkstra's algorithm is driven by a single y loop. so we're going to add one additional vertex to an evolving set capital x as the algorithm proceeds. so x is the vertices that have been processed so far. we maintain the invariant that for every processed vertex we've computed what we think the shortest path distance is to that vertex. so initially x is just the source vertex s. of course, the shortest path distance from s to itself is zero. and then the cleverness of dijkstra's algorithm is in how we figure out which vertex to add to the set capital x each iteration. so the first thing we do is we. focus only on edges that cross the frontier edges that have their tail in capital x and their head outside of capital x. now, of course, there may be many such edges, edges that cross this frontier and we use dijkstra's grady criterion to select one of them. so for each crossing edge, each edge with a tail and x and head outside x, we compute the dijkstra grady score that is defined as the previously computed shortest path distance to the tail of the arc plus the length of the arc. so we compute that for each crossing edge and then the minimum edge we're calling it v star w star. that determines how we proceed. so we add the head of that arc w star to the set capital x and then w e compute the shortest path distance to w star to give the previous. see computed shortest fast distance to v star plus the length of this extra [inaudible] v star, w star. now back when i explained this algorithm i did it using two arrays, array capital a and array capital ba is what computed the shortest path distances, and remember that's what the problem asks us to compute. and for clarity i also filled up this array capital b just to keep track of the shortest paths themselves. now if you look at the code of this algorithm, we don't actually need the array capital b for anything. when we fill in the array capital a, we don't actually refer to the b array. and so now that we're gonna talk about real implementations of dijkstra; i'm actually gonna cross out all of the instructions that correspond to the b array. okay? because you would not, as i told you earlier, use this in a real implementation of dijkstra. you would just fill in the shortest path distances themselves. so in the next quiz, what i want you to think about is the running time of this algorithm if we implemented it more or less as is, according to the pseudo code on this slide without any special data structures. and in the answers to the quiz, we're going to be using the usual notation where m denotes the number of edges in the graph, and n denotes the number of vertices of the graph. so the correct answer to this quiz is the fourth one that the straightforward implementation of dijkstra's algorithm would give you a running time proportional to the product of the number of edges and the number of vertices. and the way to see that is to just look at the main while loop and look at how many times it executes and then how much work we do per iteration of the while loop if we implemented it in a straightforward way. so there's gonna be n minus one iterations of the while loop. and the reason is, is that the algorithm terminates once every single vertex has been added to capital x. their end vertices, initially there's one vertex in x. so after n m inus one iterations, we'll have sucked up all of the vertices. now what's the work done in each wild loop? well basically, we do naively a linear scan through all of the edges. we go through the edges. we check if it's an eligible edge, that is if its tail is in x and its head is outside of x. we can keep track of that just by having an auxiliary bullion variable for each vertex. remembering whether it's an x or not. and then amongst all of the illegible edges, those crossing the frontier, we just, by exhaustive. search remember which edge has the smallest dijkstra store- score, now we can compute the dijkstra score in constant time for each of the edges. so that's a reasonable algorithm. we might be able to get away with graphs that have say hundreds or thousands of vertices using the straight forward of implementation, but of course, we'd like to do better. we'd like the algorithm to scale up to much larger graphs, even graphs with potentially say a million vertices so the answer is yes, we can do better. not by changing the algorithm, but, rather, changing how we organize the data as the algorithm proceeds. so this will be the first time in the course where we use a data structure to get an algorithmic speed-up. so we're gonna see a really lovely interplay between, on the one hand, algorithm design and, on the other hand, data structure design in this implementation of dijkstra's algorithm. so you might well ask what's the clue that indicates that a data structure might be useful in speeding up dijkstra's shortest path algorithm. and the way you'd figure this out is you'd say, well, where is all this work coming from? why are we doing a linear amount of work in the edges for a linear number in the vertices iterations? well, at each iteration of this while loop, what we're doing is, we're just doing an exhaustive search to compute a minimum. we look at every edge, we look at those that cross the frontier, and we compute the one with the minimum dijkstra score. so we could ask ourselves, oh, if we're doing minimum comp utations over and over and over again, is there some data structure which, whose raison d??tre, whose reason for being is in fact to perform fast minimum computations? and in fact there is such a data structure. it's the heap data structure. so in the following description of a fast implementation of dijkstra's algorithm, i'm going to assume you're familiar with this heap data structure. for example, that you watched the review video elsewhere on the course site that explains it. so let me just remind you with a lightning-quick review of what we learned in that video, so heaps are generally logically thought of as a complete binary tree, even though they are usually implemented as a laid-out linear array. and the key property that you get to leverage but that you also have to maintain in a heap is the heap property that at every node the key at that node has to be at least as small as that of both of the children. this property ensures that the smallest key of them all has to be at the root of this tree. to implement extract menu, just pluck off the roots. that's what you return. that's the minimum element. and then you swap up the, bottommost rightmost leaf. the last element, make that the new root, and then you bubble that down as necessary to restore the heap property. when you do insertion, you just make the new element the new last leaf, bottommost rightmost leaf, and then you swap up as needed to restore the heap property. when we use heaps in dijkstra's algorithm we're also going to need the ability to delete an element from the middle of the heap. but again you can do that just by swapping things and doubling up or down as needed. i'll leave it as an exercise for you to think through carefully, how to delete elements from the middle of a heap. because you're maintaining the heap as an essentially perfectly balanced binary tree, the height of the tree is roughly the log base two of n, where n is the number of elements in the heap. and because for every operation, you implement it just by doing a constant amo unt work at each level of the tree, all of these operations run in o of log n time, where n is the number of items that are being stored in the heap. as far as the intuitive connection between the heap data structure and dijkstra's algorithm. in the main wild loop of dijkstra's algorithm, we're responsible for finding a minimum, every single iteration. what are heaps good for? they're good for finding minimums in logarithmic time. that sounds a lot better than the linear time we're spending in the naive implementation of dijkstra's algorithm. so let's now see how to use heaps to speed up dijkstra's shortest path algorithm. now because every iteration of the wild loop is responsible for picking an edge, you might expect that we're going to store edges in the heap. so the first subtle but really good idea is to actually use a heap to store vertices rather than edges. going back to the pseudo-code [inaudible] algorithm, remember that the only reason that we focused on an edge. well so that we can then deduce which vertex, namely the head of that edge, to add to our set capital x. so we're just going to cut to the chase, we're just going to keep vertices not yet in x and then when we extract them in from the heap, it'll tell us which is the next vertex to add into the set capital x. so the picture we're going to wanna have in mind is dijkstra's choice path algorithm at some intermediate iteration. so there'll be a bunch of vertices in the, the set capital x source vertex plus a bunch of other stuff that we've sucked into the set so far. and then there'll be all the vertices we haven't processed yet. a big group v minus x. then there's gonna be edges crossing this cut in both directions from x to v minus x and vice versa. now before i explain the second invariant, let's just recall what the straightforward implementation of dijkstra's algorithm needs to do. what it would do is search through all the edges and it would look for any eligible edges. those with tail and x, and head and v minus x. so in this picture, there w ould be three such edges. i've drawn the example so that two of the edges, the top two edges, both share a common head vertex whereas the third edge has its own head vertex. the straightforward of limitation of dysktra's algorithm we?d compute dystra's greedy score for each of these three edges and remember, by definition, that's the previously computed shortest path distance to the tail of the arc v, plus the length of the arc vw. so the straightforward implementation just computes this. in this case, it would compute it for three edges. and whichever the three edges won had the smallest score. the head of that edge would be the next vertex that gets added to x. so let me specify the second invariant, and then i'll tell you how to think about it. so, because we're storing vertices rather than edges in the heap, we're going to have to be fairly clever with the way we define the key of a vertex that's in this heap. [sound] so we're going to maintain the property that the key of a vertex v is the smallest greedy dijkstra score of any ver, any edge which has that vertex as its head. so let me show you what i mean in terms of our example, where we have three crossing edges. suppose for these three edges in the upper right that happen to have of dijkstra [inaudible] scores of seven, three, and five. let's look at what the key should be for each of these three vertices i've drawn in v minus x. now for the timeline vertex this is pretty interesting. there are two different edges whose tail is in x, and have this vertex as their head. so what should the key of this vertex be? well, it should be the smallest dijkstra greedy score of any of the edges whose tail lies on the left-hand side that terminate at this vertex. so there's two candidate edges. one has dijkstra greedy score three. one has dijkstra greedy score seven. so the key value should be three, the smaller of those two. now, the second vertex, there's only a single edge that has tail in x and that terminates at this vertex. so the key for this vertex should jus t be the score at that weak edge. so in this case that's gonna be five. and then this poor third vertex, there's actually no edges at all, that, started x and terminated at this vertex. there's only one arc going the wrong direction. so for any edge, sorry, for any vertex outside of x that doesn't have any eligible edges terminating at it, we think of the key as being plus infinity. so the way i recommend thinking about these heap keys is that we've taken what used to be one round tournament, winner takes all and we've turned it into a two round knockout tournament. so in our straightforward implementation of dijkstra's algorithm, we did a single linear search through all the edges, and we just computed the [inaudible] dijkstra's score for each and we picked the best. so in this example we would have discovered these three edges in some order. their scores are three, five, and seven. and we would have remembered the edge with score three as being the best. that would have been our winner of this winner take all tournament. now when we use the heap, it, we're factoring it into two rounds. so first, each vertex in v minus x runs a local tournament. to elect a local winner, so each of these vertices in v minus x. says, well let me look at all of the edges. for whom i'm the head and also the tail of that edge is in x. and amongst all of those edges that start in x and terminate at me, i'm going to remember the best of those. so that's the winners of the local tournament of the first round. and now the heap is only going to remember this set of first round winners. right, there's no point in remembering the existence of edges who aren't even the smallest score that terminate at a given vertex, because we only care about the smallest score overall. now when you extract min from the heap, that's in effect. executing the second and final round of this knockout tournament. so each of the vertices of v minus x has proposed their local winner. and then the heat in an extract min just chooses the best of all of those local winners. so that's the final proposed vertex that comes out of the heap. so the point is that if we can successfully maintain these two invariants, then, when we extract min from this heap, we'll get exactly the correct vertex, w star, that we're supposed to add to the set capital x next. that is, the heap will just hand to us on a silver platter exactly the same choice of vertex that our previous exhaustive search through the edges would've computed. the exhaustive search was just computing the minimum in a brute force way, in a single winner take all tournament. the heap implemented in this way chooses exactly the same winner. it just does it in this 2-round process. now, in dijkstra's algorithm, we weren't supposed to merely just find the vertex w star to add to x. we also had to compute its shortest path distance. but remember, we computed the shortest path distance as simply the dijkstra greedy score. and here the dijkstra greedy score is just going to be the key for this heap that's immediate from invariant number two. so we're using the fact here that our keys are, by definition, just. the smallest greedy scores are edges that stick into that vertex w star so again exactly replicating. the computation that we would have done in the straightforward implementation, just in a much slicker way. okay? but we're adding exactly the same vortices, in exactly the same order, and we're computing exactly the same shortest path distances in this heap of notation, provided of course that we do successfully maintain these two invariants throughout the course of the algorithm. so that is now what i owe you. we have to pay the piper. we've shown that if we can have a data structure with these properties. then we can simulate the straight forward implementation now i have to show you how we maintain these invariants without doing too much work. all right. so maintaining invariant number one will really take care of itself. really sort of by definition the vertices which remain in the heap are those that we haven't process ed yet, and those are the ones that are outside of capital x. so really the trick is, how do we maintain invariant number two? now before i explain this let me point out, that this is a tricky problem. there is something subtle going on. so as usual, i want you to think about this shortest path algorithm at some intermediate iteration. okay? so take a, take a snapshot. a bunch of vortices have already been added to x. a bunch of vortices are still hanging out in the heap. they haven't been added to x. there's some frontier, there's a, just crossing, possibly in both directions. and suppose at the end of a current iteration we identify the vortex w, which we're going to extract from the heap and conceptually add to the set x. now the reason things complicated is when we move a vortex from outside x to inside x. the frontier between x and v minus x changes. so in this picture, the old black x becomes this new blue x. and what's really interesting about the frontier changing is that then the edges which cross the frontier change. now, there might be, there are some edges which used to cross the frontier and now don't. those are the ones that are coming into w. those we're not so concerned with. those don't really play any role. what makes things tricky is that there are edges which used to not be crossing the frontier but now they are crossing the frontier. and those are precisely the edges sticking out of w. so in this picture there are three such edges which i will highlight here in pink. to see why it's tricky when new edges all the sudden are crossing a frontier let's remember what invariant number two says. it says that for every vertex which is still in the heap, which is not yet in x, the key for that vertex better be the smallest dijkstra grady score of any edge which comes from capital x and sticks into this vertex of v. now in moving one vertex into x, namely this vertex w, now there can be new edges sticking into vertices which were still on the heap. as a result, the appropriate key value for vertices i n the heap might be smaller. now the w has been moved into x. and the candidates for the vertices in the heap whose keys might have dropped are precisely those vertices on the other end of edges sticking out of w. so summarizing, the fact that we'd added a new vertex to capital x and extracting something from the heap, it's potentially increased the number of crossing edges across the frontier, because the frontier has changed. and therefore, for vertices that remain in the heap, the smallest greedy score of an edge that sticks into them from the set x might have dropped. so we need to update those keys to maintain invariant number two. now, that's the hard part. here's what we have going for us. we've damaged the keys perhaps by changing the frontier, but the damage is local. we can understand exactly whose keys might have dropped, so as suggested by the picture, the vertices whose keys we need to update are precisely those at the head of edges that stick out of w. so for each outgoing edge from w, the vertex we just extracted from the heap, we need to go to the other end of the edge and check if that vertex needs its key to be decreased. so here's the pseudo code to do this. so when we extract the vertex w from the heap, that is when we conceptually add a new vertex w. to the set x, thereby changing the frontier, we say, well, you know, we know the only vertices that might have to have their key changed, they're the ones on the other side of these outgoing arcs from w. so we just have a simple iteration over the outgoing edges, w v, from the vertex v. now i haven't shown you any edges in the picture like this, but there might well be some edges where the head of the arc v is also in the set x, is also already been processes. but anything in x is not in the heap. remember, the heap is only the stuff outside of x. so we could care less about the stuff outside. of the heat, for not maintaining their keys. so we do an extra check. if the head of this edge is in fact still in the heap, that is if it's not in x so i n the picture, for example, this would be true for all three of the vertices that are on the other end of arcs pointing out of w. and for each of these vertices v, we update its key. and the way we're going to update its key is, we're just going to rip this vertex out of the heap. we're going to recompute its key and constant time, and then we're going to reinsert it into the heap. and since all heap operations take logarithmic time, this key update will be logarithmic time. as an additional optimization, i wanna point out that if one of these vertices v's key does change, it can only change in one way. so remember, what is the key? the key is the smallest grady dijkstra score of all of the edges that start next and stick into this vertex. so that's the local tournament or the first round tournament happening at this vertex v. now the only thing which has changed. before and after we added this vertex, w to x, is that now one new edge is sticking into this vertex, v. all of the old edges sticking into it from x are still sticking into it, and now there's one extra candidate in its local tournament, namely this edge, wv. so either wv is the local winner; either it has the smallest dyxtra-greedy score of them all. that terminated this vertex, or it doesn't, in which case the previous winner is still the new winner. so if that is, the new key value can only be one of two things. either it's the old key value--that's the case where this. extra entrance, the edge from w to v is irrelevant. or, if it's changed, it has to have changed to the [inaudible] score of this edge, w-v. and the formula for that is the shortest path distance. that we just computed for w where w has been processed at this point plus the link of the direct arch from w m v. and again conceptually this formula is just a greedy dijkstra score for the arc wv. the new entrance in v's local first round tournament. so now, having updated v's key appropriately, so that invariant #two is restored. and once again, the key of every vertex does reflect the sma llest greedy, dijkstra greedy score of any edge sticking into it from the set x. we can safely reinsert this node back into the heap with its new key value. and these three lines together are just a key update in logarithmic time, for one of these vertices that's at the other end of an arc sticking out of the vertex w. so let's tally up the running time in this new implementation. one thing i want you to check, and this will definitely help you understand this refined implementation of dijkstra's algorithm, is that essentially all the work done is through the heap api. that is, all of the running time that we have to account for is in heap operations. we don't really do nontrivial work outside of heap operations. and again recall that the running time of any heap operation is logarithmic in the number of elements in the heap. our heap is storing vertices. it's never gonna have more than n things in it. so the running time of every heap operation is big o of log n. so what are the heap operations that we do. well, we extract men and we do it once per iteration of the wild loop. so there's n minus one iterations of the wild loop, just like before, but now instead of doing an exhaustive search through the edges, we just do a simple extract men from the heap and it gives us on a silver platter the vortex we should add next. so what do we do beside extract mins? well, we have to do this work paying the piper. we have to maintain invariant #two. and every time we extract a min, that then triggers some subsequent key updates. and remember, each of these key updates is a deletion of an element, from the heap followed by an insertion. so how many deletions and insertions do we do? well, at first this might seem a little bit scary. right? because we do a roughly linear number of extract mins. and a vertex might have as many as n-1 outgoing arcs. so it seems like a vertex could trigger as many as n-1 key updates, which is theta of n [inaudible] operations. and if we sum that up over the n iterations of the wild loop that w ould give us n squared heap operations. so, and indeed, in dense graphs, that can be the case. it is true that a single vertex might trigger a linear in n [inaudible] number of [inaudible] operations. but that's the wrong way to think about it. rather than have this vertex-centric perspective on what, who's responsible for heap operations, let's have an edge-centric view. so for each edge at the graph, let's think about when can this be responsible for some heap operations, in particular a decrease in key in the resulting insertion and deletion. if you have an edge and it points from the vertex v to the vertex w. there's actually only one situation in which this edge is going to be responsible for a, a decrease in key. and that's in the case where the tail of the edge, v. gets sucked into the set x before the head w of this edge gets sucked into the set x. if that happens, if v gets sucked into x and w is still outside of x, then indeed we're gonna have to decrease the key of w, just like we did in the examples. but that's all that's gonna happen: v can only get sucked into x once and never gonna leave it. so it's only responsible for this single decrease in key of its head w. and that's one insertion and one deletion. and in fact, if the endpoints of this edge get sucked into x in the opposite order, if the tail of, excuse me, if the head of this edge w gets sucked into x first. that doesn't even trigger a, a key decrease for v, and v will never have its de key decreased, because of this particular arc, from v to w. so the upshot is that each edge vw of the graph triggers at most one insert delete combo. so what does this mean, this means that the number of heap operations. is big o of n, that's for the extract mins. plus big o of m. that's for the insert the leak combos triggered by edges during the decreased keys. now just to, i'm gonna write this in a, in a simplified way. this is just o of m, the number of edges. and this is because of our assumption that's there's a path to s from every other vertex. if yo u think about it that means that the graph is at least weakly connected if you picked it up it would stay together in one piece. so that means it at least contains a tree, at least an in an undirected sense, which means it contains at least n minus one edges. so we're in the case of weakly connected graphs where n dominates m. m is always as big as n at least up to a plus one. so what that means is the running time of dijkstra's algorithm, with this heap implementation, is just a log factor larger. remember, every heap operation takes time logarithmic. so we do a linear in m number of operations; each takes time logarithmic in n. so the running time is m log n. with, i should say, quite good consistence. so this is a really, really impressively fast algorithm, for computer such a useful problem as shortest paths. so we got a little bit spoiled in our discussion of graph searching connectivity, where it seemed any problem we cared about we could solve in linear time, over m plus n. so here we're picking up this extra logarithmic factor, but i mean, come on, this is still awesome. a running time of m log n is unbelievably faster than a running time of m times n, which is what we had in the straightforward implementation. so this deft use of the heap data structure has given us a truly blazingly fast algorithm for an extremely well motivated problem, computing shortest paths. 
in this video, i'm gonna tell you a little bit about my approach toward teaching data structures in this course. so knowing when and how to use basic data structures is an essential skill for the serious programmer. data structures are used in pretty much every major piece of software. so let me remind you of what's the point, the raison d'etre of the data structure? its job is to organize data in a way that you can access it quickly and usefully. there's many, many examples of data structures and hopefully you've seen a few of them and perhaps even used a few of them in your own programs. and they range from very simple examples like lists, stacks and queues to more intricate but still very useful ones like heaps, search trees, hash tables. relatives thereof like balloon filters. union find structures and so on. so why do we have such a laundry list of data structures? why is there such a bewildering assortment? it's because different data structures support different sets of operations and are therefore well suited for different types of tasks. let me remind you of a concrete example that we saw back when we were discussing graph search. in particular, breadth first search and depth first search. so we discussed how implementing breadth first search the right data structure to use was a queue. this is something that supports fast, meaning constant time insertion to the back, and constant time deletion from the front. depth first search by contrast is a different algorithm with different needs. and because of its recursive nature, a stack is much more appropriate for depth first search. that's because it supports constant time deletion from the front, but constant time insertion to the front. so the last in first out support of a stack is good for depth first search. the first in first out operations of a queue work for breadth first search, now because different data structures are suitable for different types of tasks you should learn the pros and cons of the basic ones. generally speaking the fewer operations th at a data structure supports the faster the operations will be. and the smaller the space overhead require by the data structure. thus, as programmers, it's important that you think carefully about the needs of your application. what are the operations that you need a data structure to export? and then you should choose the right data structure, meaning the one that supports all of the operations you need. but ideally no superfluous ones. let me suggest four levels of data structure knowledge that someone might have, so level zero is the level of ignorance, for someone who has never heard of a data structure, and is unaware of the fact that organizing your data can produce fundamentally better software. for example, fundamentally faster algorithms, level one i think of as being cocktail party level awareness. now obviously, here i'm talking only about the nerdiest of cocktail parties. but nonetheless, this would be someone who could at least hold a conversation about basic data structures. they've heard of things like heaps and binary search trees, they're perhaps aware of some of the basic operations, but this person would be shaky using them in their own program or say in a technical interview context. now, with level two, we're starting to get somewhere. so here i would put someone who has solid literacy about data structures. they're comfortable using them as a client in their own programs, and they have a good sense of which data structures are appropriate for which types of tasks. now level three, the final level, is the hardcore programmers and computer scientists. and these are people who are not content to just be a client of data structures, and use them in their own programs, but they actually have an understanding of the guts of these data structures. how they are coded up, how they're implemented, not merely how they are used. now my guess is that, really a large number of you will wind up using data structures in your own programs, and therefore, learning about what are the operations of different data structures and what are they good for, will be a quite empowering skill for you as a programmer. on the other hand, i'll bet that very few of you will wind up having to implement your own data structures from scratch, as opposed to just using as a client the data structures that already come with the various standard programming libraries. so with this in mind i'm going to focus my teaching on taking up to level two. my discussion gonna focus on the operations reported by various data structures and some of canonical applications. so through this i hope i'll develop your intuition for what kinds of data structures are suitable for what kinds of tasks. time permitting, however, i also want to include some optional material for those of you wanting to take it to the next level and learn some about the guts of these data structure, the canonical limitations of how you code them up. 
so in this video, we'll start talking about the heap data structure. so in this video i want to be very clear on what are the operations supported by heap, what running time guarantees you can expect from [inaudible] limitations and i want you to get a feel for what kinds of problems they're useful for. in a separate video, we'll take a peek under the hood and talk a little bit about how heaps actually get implemented. but for now, let's just focus on how to use them as a client. so the number one thing you should remember about a given data structure is what operations it supports, and what is the running time you can expect from those operations. so basically, a heap supports two operations. there's some bells and whistles you can throw on. but the two things you gotta now is insertion and extract min. and so the first thing i have to say about a heap is that it's a container for a bunch of objects. and each of these objects should have a key, like a number so that for any given objects you can compare their keys and say one key is bigger than the other key. so for example, maybe the objects are employee records and the key is social security numbers, maybe the objects are the edges of a network and the keys are something like the length or the weight of an edge, maybe each object indicates an event and the key is the time at which that event is meant to occur. now the number one thing you should remember about a given data structure is, first of all what are the operations that it supports? and second of all, what is the running time you can expect from those operations? for a heap, essentially there's two basic operations. insert and extract the object that has the minimum key value. so in our discussion of heaps, we're going to allow ties that are pretty much equal to easy with or without ties. so, when you extract men from a heap they may have duplicate key values then there's no specification about which one you get. you just get one of the objects that has a tie for the minimum key value. now, of course, there's no special reason that i chose to extract the minimum rather than the maximum. you either you can have a second notion of a heap, which is a max heap, which always returns the object of the maximum key value. or if all you have at your disposal is one of these extract min-type heaps, you can just, negate the sign of all of the key values before you insert them, and then extract min will actually extract, the max key value. so, just to be clear, i'm not proposing here a data structure that supports simultaneously an extract-min operation and an extract-max operation. if you wanted both of those operations, there'd be data structures that would give it to you; probably a binary search tree is the first thing you'd want to consider. so, i'm just saying, you can have a heap of one off two flavors. either the heap supports extract-min and not extract-max or the heap will support extract-max and not extract-min. so i mentioned that you should remember not just the supported operations of a data structure, but what is the running time of those operations. now, for the heap, the way it's canonically implemented, the running time you should expect is logarithmic in the number of items in the heap. and its log base two, with quite good constants. so when you think about heaps, you should absolutely remember these two operations. optionally, there's a couple other things about heaps that are, might be worth remembering some additional operations that they can support. so the first is an operation called heapify. like a lot of the other stuff about heaps, it has a few other names as well. but i'm going to call it heapify, one standard name. and the point of heapify is to initialize a heap in linear time. now, if you have n things and you want to put them all in a heap, obviously you could just invoke insert once per each object. if you have n objects, it seems like that would take n times log n time, log n for each of the n inserts. but there's a slick way to do them in a batch, which takes only linear time. so tha t's the heapify operation, and another operation which can be implemented, although there are some subtleties. is you can delete not just the minimum, but you can delete an ar-, arbitrary element from the middle of a heap, again, in logarithmic time. i mention this here primarily cuz we're gonna use this operation when we use heaps to speed up dijkstra's algorithm. so that's the gist of a heap. you maintain objects that have keys you can insert in logarithmic time, and you can find the one with the minimum key in logarithmic time. so let's turn to applications, i'll give you several. but before i dive into any one application let me just say; what's the general principle? what should [inaudible] you to think that maybe you want to use a heap data structure in some task? so the most common reason to use a heap is if you notice that your program is doing repeated minimum computations. especially via exhaustive search, most of the applications that we go through will have this flavor. it will be, there will be a naive program which does a bunch of repeated minimums using just brute force search and we'll see that a very simple application of a heap will allow us to speed it up tremendously. so let's start by returning to the mother of all computational problems, sorting and unsorted array. now, a sorting algorithm which is sort of so obvious and suboptimal that i didn't even really bother to talk about it at any other point in the course is selection-sort. what do you do? in selection sort, you do a scan through the unsorted array. you find the minimum element; you put that in the first position. you scan through the other n-1 elements; you find the minimum among them. you put that in the second position. you scan through the remaining n-2 unsorted elements. you find the minimum; you put that in the third position, and so on. so evidently, this [inaudible] sorting algorithm does a linear number of linear scans through this array. so this is definitely a quadratic time algorithm. that's why i didn't bother to tell you about it earlier. so this certainly fits the bill as being a bunch of repeated minimum computations. or for each computation, we're doing exhaustive search. so this, we should just, a light bulb should go off, and say, aha! can we do better using a heap data structure? and we can, and the sorting algorithm that we get is called heap sort. and given a heap data structure, this sorting algorithm is totally trivial. we just insert all of the elements from the array into the heap. then we extract the minimum one by one. from the first extraction, we get the minimum of all n elements. the second extraction gives us the minimum of the remaining n-1 elements, and so on. so when we extract min one by one, we can just populate a sorted array from left to right. boom, we're done. what is the running time of heap sort? well, we insert each element once and we extract each element once so that's 2n heap operations and what i promised you is that you can count on heaps being implemented so that every operation takes logarithmic time. so we have a linear number of logarithmic time operations for running time of n log n. so let's take a step back and appreciate what just happened. we took the least imaginative sorting algorithm possible. selection sort, which is evidently quadratic time. we recognize the pattern of repeated minimum computations. we swapped in the heap data structure and boom we get an nlogn sorting algorithm, which is just two trivial lines. and remember, n log n is a pretty good running time for a sorting algorithm. this is exactly the running time we had for merge sort; this was exactly the average running time we got for randomized quick sort. moreover, heap sort is a comparison based sorting algorithm. we don't use any data about the key elements we just used as a totally [inaudible] set. and, as some of you may have seen in an optional video, there does not exist a comparison-based sorting algorithm with running time better than n log n. so for the question, can we do better? the answer is no, if we use a comparison based sorting algorithm like heap sort. so that's pretty amazing, all we do is swap in a heap and a running time drops from really quite unsatisfactory quadratic to the optimal n log n. moreover, heapsort is a pretty practical sorting algorithm: when you run this it's gonna go really fast. is it as good as quick sort? hm, maybe not quite but its close it's getting into the same [inaudible]. so let's talk of another application which frankly in some sense is almost trivial but this is also a canonical way in which heaps are used. and in this application it will be natural to call a heap by a synonymous name, a priority queue. so what i want you to think about for this example is that you've been tasked with writing software that performs a simulation of the physical world. so you might pretend, for example, that you're helping write a video game which is for basketball. now why would a heap come up in a simulation context? well, the objects in this application are going to be events records. so an event might be for example that the ball will reach the hoop at a particular time and that would be because a player shot it a couple of seconds ago. when if for example the ball hits the rim, that could trigger another event to be scheduled for the near future which is that a couple players are going to vie for the rebound. that event in turn could trigger the scheduling of another event, which is one of these players? commits, an over the back foul on the other one and knocks them to the ground. that in turn could trigger another event which is the player that got knocked on the ground gets up and argues that a foul call, and so on. so when you have event records like this, there's a very natural key, which is just the timestamp, the time at which this event in the future is scheduled to occur. now clearly a problem which has to get solved over and over and over again in this kind of simulation is you have to figure out what's the next event that's going to occur. you have to know what other events to schedule; you have to update the screen and so on. so that's a minimum computation. so a very silly thing you could do is just maintain an unordered list of all of the events that have ever been scheduled and do a linear path through them and compute the minimum. but you're gonna be computing minimums over and over and over again, so again that light bulb should go on. and you could say maybe a heap is just what i need for this problem. and indeed it is. so, if you're storing these event records in a heap. with the key being the time stamps then when you extract them in the hands for you on a silver platter using logarithmic time exactly which algorithm is going to occur next. so let's move on to a less obvious application of heaps, which is a problem i'm going to call median maintenance. the way this is gonna work is that you and i are gonna play a little game. so on my side, what i'm going to do is i'm going to pass you index cards, one at a time, where there's a number written on each index card. your responsibility is to tell me at each time step the median of the number that i've passed you so far. so, after i've given you the first eleven numbers you should tell me as quickly as possible the sixth smallest after i've given you thirteen numbers you should tell me the seventh smallest and so on. moreover, we know how to compute the median in linear time but the last thing i want is for you to be doing a linear time computation every single time step. [inaudible] i only give you one new number? do you really have to do linear time just to re-compute the median? if i just gave you one new number. so to make sure that you don't run a linear time selection algorithm every time i give you one new number, i'm going to put a budget on the amount of time that you can use on each time step to tell me the median. and it's going to be logarithmic in the number of numbers i've passed you so far. so i encourage you to pause the video at this point and spend some time thinking about how you would solve this problem. alright, so hopefully you've thoug ht about this problem a little bit. so let me give you a hint. what if you use two heaps, do you see a good way to solve this problem then. alright, so let me show you a solution to this problem that makes use of two heaps. the first heap we'll call h low. this equal supports extract max. remember we discussed that a heap, you could pick whether it supports extract min or extract max. you don't get both, but you can get either one, it doesn't matter. and then we'll have another heap h high which supports extract min. and the key idea is to maintain the invariant that the smallest half of the numbers that you've seen so far are all in the low heap. and the largest half of the numbers that you've seen so far are all in the high heap. so, for example, after you've seen the first ten elements, the smallest five of them should reside in h low, and the biggest five of them should reside in h high. after you've seen twenty elements then the bottom ten, the smallest ten, should, should reside in h low, and the largest ten should reside in h high. if you've seen an odd number, either one can be bigger, it doesn't matter. so if you have 21 you have the smallest ten in the one and the biggest eleven in the other, or vice-versa. it's not, not important. now given this key idea of splitting the elements in half, according to the two heaps. you need two realizations, which i'll leave for you to check. so first of all, you have to prove you can actually maintain this invariant with only o of log i work in step i. second of all, you have to realize this invariant allows you to solve the desired problem. so let me just quickly talk through both of these points, and then you can think about it in more detail, on your own time. so let's start with the first one. how can we maintain this invariant, using only log i work and time step i, and this is a little tricky. so let's suppose we've already processed the first twenty numbers, and the smallest ten of them we've all worked hard to, to put only in h low. and the biggest ten of th ''em we've worked hard to put only in h high. now, here's a preliminary observation. what's true, so what do we know about the maximum element in h low? well these are the tenth smallest overall and the maximum then is the biggest of the tenth smallest. so that would be a tenth order statistic, so the tenth order overall. now what about in the, the hi key? what s its minimum value? well those are the biggest ten values. so the minimum of, of the ten biggest values would be the eleventh order statistic. okay, so the maximum of h low is the tenth order statistic. the minimum of h high is the [inaudible] statistic, they're right next to each other; these are in fact the two medians right now so when this new element comes in, the twenty-first element comes in, we need to know which heap to insert it into and well it just, if it's smaller than the tenth order statistic then it's still gonna be in the bottom, then it's in the bottom half of the elements and needs to go in the low heap. if it's bigger than the eleventh order statistic, if it's bigger than the minimum value of the high heap then that's where it belongs, in the high heap. if it's wedged in between the tenth and eleventh order of statistics, it doesn't matter. we can put it in either one. this is the new median anyways. now, we're not done yet with this first point, because there's a problem with potential imbalance. so imagine that the twenty-first element comes up and it's less than the maximum of the low heap, so we stick it in the low heap and now that has a population of eleven. and now imagine the twenty-second number comes up and that again is less than the maximum element in the low heap, so again we have to insert it in the low heap. now we have twelve elements in the low heap, but we only have ten in the right heap. so we don't have a 50. 50, 50 split of the numbers but we could easily re-balance we just extract the max from the low heap and we insert it into the high heap. and boom. now they both have eleven, and the low heap has the smallest el even, and the high heap has the biggest eleven. so that's how you maintain, the invariant that you have this 50/50 split in terms of the small and the high, and between the two heaps. you check where it lies with respect to the max of the low heap and the mid of the high heap. you put it in the appropriate place. and whenever you need to do some re-balancing, you do some re-balancing. now, this uses only a constant number of heap operations when a new number shows up. so that's log i work. so now given this discussion, it's easy to see the second point given that this invariant is true at each time step. how do we compute the median? well, it's going to be either the maximum of the low heap and/or the minimum of the high heap depending on whether i is even or odd. if it's even, both of those are medians. if i is odd, then it's just whichever heap has one more element than the other one. so the final application we'll talk about in detail in a different video. a video concerned with the running time of dijkstra's shortest path algorithm. but i do wanna mention it here as well just to reiterate the point of how careful use of data structures can speed up algorithms. especially when you're doing things like minimum computations in an inner loop. so dijkstra's shortest path algorithm, hopefully, many of you have watched that video at this point. but basically, what it does is it has a central wild loop. and so it operates once per vertex of the graph. and at least naively, it seems like what each iteration of the wild loop does is an exhaustive search through the edges of the graph, computing a minimum. so if we think about the work performed in this naive implementation, it's exactly in the wheel-house of a heap, right. so what we do in each of these loop iterations is do an exhaustive search computing a minimum. you see repeated minimum computations, a light bulb should go off and you should think maybe a heap can help. and a heap can help in dijkstra's algorithm. the details are a bit subtle, and they're discussed i n a separate video, but the upshot is, we get a tremendous improvement in the running time. so we're calling that m denotes the number of edges. and n denotes the number of vertices of a graph. with a careful deployment of heaps in dijkstra's algorithm, the run time drops from this really rather large polynomial. the product of the number of vertices and the number of edges. down to something which is almost linear time. anyway, o of m log n. where m is the number of edges and n is the number of vertices. so the linear time here would be o of m. the liner of the number of edges we're picking up an extra log factor but still this is basically as good as sorting. so this is a fantastically fast shortest path algorithm. certainly, way, way better that what you get if you don't use heaps and do just repeated exhaustive searches for the minimum. so that, that's wraps up our discussion of what i think you really want to know about heaps. namely, what are the key operations that it supports? what is the running time you can expect from those operations? what are the types of problems that the data structure will yield speed ups for? and a suite of applications. for those of you that want to take it to the next level and see a little bit about the guts of the implementation, there is a separate optional video that talks a bit about that. 
so, in this video, we're gonna take it at least partway to the next level for the heap data structure. that is, we'll discuss some of the implementation details. i.e., how would you code a, a heat data structure from scratch? so remember what the point of a heap is. it's a container, and it contains objects, and each of these objects, in addition to possibly lots of other data, should have some kind of key. and we should be able to compare the keys of different objects; you know, social security numbers for different employers, edge weights for different edges in a network, or time stamps on different events, and so on. now remember, for any data structure the number one thing you should remember is what are the operations that it supports, i.e., what is the data structure good for. and also, what is the running time you can count on of those operations. so something we promised in a previous video. [inaudible] indicate the implementation of the miss video is these two primary operations that a heap exports. first of all, you can insert stuff into it, and it takes [inaudible] logarithmic in the number of objects that the heap is storing. and second of all, you can extract an object. that has the minimum key value, and again we're going to allow duplicates in our, in our heaps, so if there's multiple objects, then i'll have a minimum, a common minimum key value, the heap will return one of those. it's unspecified, which one. as i mentioned earlier, you can also dress up heaps with additional operations, like you can do batch inserts, and you can do linear time rather than log in time. you can delete from the middle of the heap. i'm not gonna discuss those in the video; i'm just going to focus on how you'd implement inserts and extract [inaudible]. if you wanna know how heaps really work, it's important to keep in mind simultaneously two different views of a heap one, as a tree and one, as a, array. so we're gonna start on this slide with the tree view. conceptually, this will be useful to explain how the heap ope rations are implemented. a conceptual will think of a heap not just as any old tree, but as a tree that's rooted. it'll be binary. meaning that, each node will have zero, one or two children nodes and third, it will be as complete as possible. so let me draw for your amusement an as complete as possible binary tree that has nine nodes. so if the tree had, had only seven nodes it would have been obvious what, is complete as possible means. it would have meant we just would have had three completely filled in levels. if it had, had fifteen nodes it would have had four completely filled in levels. if you're in between, these two numbers that are powers of two minus one, well we're going to call a complete to the tree as just in the bottom level you fill in the leaves from left to right. so here the two extra leaves on the fourth level are both, pushed as far to the left as possible. so, in our minds, this is how we visualize heaps. let me next define the heat property. this imposes an ordering on how the different objects are arranged in this tree structure. the heap property dictates that at every single node of this tree it doesn't matter if it's the root if it's a leaf if it's an internal node whatever. at every node x the key of the object stored in x should be no more than the keys of xs children. now x may have zero children if it's a leaf it may have one child or it may have two children whatever those cases zero one or two children all those children keys should be at least that of key at x. for example, here is a heap with seven nodes. notice that i am allowing duplicates. there are three different objects that have the key value four, in this heap. another thing to realize is that while the heap property imposes useful structure on how the objects can be arranged it in no way uniquely pins down their structure. so this exact same set of seven keys could be arranged differently and it would still be a heap. the important thing is that, in any heap, the root has to have a minimum value key. just like in the se two organizations of these seven keys, the root is always a four, the minimum key value. so that's a good sign, given that one of the main operations we're supposed to. quickly implement, is to extract the minimum value. so at least we know where it's going to be, it' gonna be at the root of a heap. so while in or minds we think of heaps as organized in a tree fashion, we don't literally implement them as trees. so in something like search trees, you actually have pointers at each node and you can traverse pointers to go from a [inaudible] to the children from the children to the parents, yada, yada, yada. turns out it's much more efficient in a heap to just directly implement it as an array. let me show you by example how a tree like we had on the last slide maps naturally onto an array representation. so let's look at a slightly bigger heap, one that has nine elements. so let me draw an array with nine positions. labeled one, two, three all the way up to nine and the way we're going to map this tree which is in our mind to this array implementation is really very natural. we're just going to group the nodes of this tree by their level. so, the root is gonna be the only node at level zero. then the children of the roots are level one, their children constitute level two, and then we have a partial level three, which is just these last two notes here. and now we just stick these notes into the array, one level at a time. so the roots winds up in the privileged first position, so that's going to be the, the first, the object which is the first copy of the four. then we put in the level one object, so that's the second copy of the four and the eight, and then we put in level two which has our third four along with the two nines. and then we have the last two notes from level three rounding out the penultimate and final position of the array. and you might be wondering how it is we seem to be having our cake and eating it, too. on the one hand we have this nice, logical tree structure. on the other hand we have this array implementation and we're not wasting any space on the usual pointers you would have in a tree to traverse between parents and children. so where's the free lunch coming from? well the reason is that because we're able to keep this binary tree as balanced as possible, we don't actually need pointers to figure out who is whose parent and who is whose child. we can just read that off directly from the positions in the array. so, let me be a little bit more specific. if you have a node in the fifth position, i'm assuming i here is not one, right? so the, the root doesn't have any, does not have a parent but any other, any other objects in position i does have a parent and what the position that is, depends on, in a simple way on whether i is even or odd. so if i is even, then the parent is just [inaudible] the position of i/2, and if i is odd, then it's going to be i/2. okay, that's a fraction. so we take the floor that is we round down to the nearest integer. if i is odd, so for example, the objects in positions two and three have as their parent the object in position one, and those in four and five have the one in position two as his parent. six and seven have as their parents the node in, the object in position three and so on and of course we can invert this function we can equally easily determine who the children are of a given node so if we have an object in position i. then you notice that the children of i are gonna be at the position 2i and 2i+1. of course those may be empty so if you have a leaf of course that doesn't have any children and then maybe one node that has only one child. but in the common case of an internal node it's gonna be two children that are gonna be in positions 2i and 2i+1. so rather than traversing pointers it's very easy to just go from a node to its parent to either one of its children just by doing these appropriate trivial calculations with respects to its position. so this slide illustrates, some of the. lower level reasons that heaps are quite a popular data struct ure so the first one, just in terms of storage. we don't need any overhead at all. we are just. we have these objects; we're storing them directly in an array, with no extra space. second of all, not only do we not have to have a space for pointers but we don't even have to do any traversing. all we do are these really simple. divide by two or multiply two operations and using bit shifting tricks. those can also be implemented extremely quickly. so, the next two slides let me indicate, at a high level, how you would implement the two exported operations, namely insertion and extract [inaudible] in time algorithmic in the size of the heap and rather than give you any pseudo code, i'm just going to show you how these work by example. i think it will be obvious how they extended the general case. i think just based on this discussion, you'll be able to code up your own versions, of insert and extract [inaudible], if you so desire. so let's redraw the 9-node heap that we had on the previous slide and again, i'm gonna keep drawing it as a tree and i'm gonna keep talking about it as a tree but always keep in mind the way it's really implemented is in terms of these array and when i talk about the parent of a node, again, what that means is you go to the appropriate position given the position of the node from where you started. so let's suppose we have an existing heap, like this blue heap here and we're called upon to insert a new object. let's say with a key value k. now remember heaps are always suppose to be perfectly balanced binary trees. so if we want to maintain the property that this tree is perfectly balanced is pretty only one place we can try to put the new key k and that's as the next leaf. that is it's going to be the new right most leaf on the bottom level. or in terms of the array implementation we just stick it in the first non empty slot in the array and if we keep track of the array size we're getting constant time of course know where to put the new key. now whether or not we can get away with this depends on what the actual key value k is but, you know, for starters we can say, what if we insert a key that's seven? well, then we get to say, whew, we're done. so, the reason we're done is cuz we have not violated the heap property. it is still the case that every node has key no bigger than that of its children. in particular, this third copy of a four, it picked up a new child, but its key seven was bigger than its key four. so, you can imagine that maybe we get lucky with another insert. maybe the next insertion is a ten and again, we put that in the next available spot in the last level and that becomes the second child of the third copy of the four and again we have no violation of the heap property. no worries still got a heap and in these lucky events insertion is even taking constant time. really all we're doing is putting elements at the end of an array and not doing any rearranging. so where it gets interesting is when we do an insertion that violates the heap property. so let's supposed we do yet another insertion, and the left child of this twelve, it becomes a five. now we got a problem. so now we have a as perfectly as possible balanced binary tree, but the key property is not satisfied. in particular, it's violated at the node twelve. it has one child. the key of that child is less than its own key. that's no good. so is there some way we can restore the heap property? well, a natural idea is just to swap the positions of the five and the twelve, and that's something that of course can be done in constant time, cuz again from a node, we can go to the parent or the child in constant time just with a suitable trivial computation. so we say okay, for starters we put the five at the end, but that's no good. so we're gonna swap the five with the twelve and now we see we're not out of the woods. no longer is there a heap violation at the node twelve. that's been fixed. we've made it a leaf but we've pushed up the heap violations. so now instead it's the eight that has a problem. the eight used to h ave two children, with keys twelve and nine that was fine. now the eight has two children with keys five and nine. the five is less than the eight, that's a violation of the heap property but again, that's the only violation of the heap property. there's no other node you could have screwed up, because eight was the only person whose children we messed around with. alright so now we just it again. let's try [inaudible] again, locally fix the heap violation by swapping the five with the 8s and now we see we've restored order. the only place where there could possibly be a violation of the heap property is at the root. the root, when we did this swap, the only person whose children we really messed with was the root four, and fortunately its new child has key five, which is bigger than it. one subtle point that you might be thinking that in addition to screwing up at the root node, that messing around with his children, maybe we could have screwed up the twill by messing around with its parent. alright, its parent used to be five, and now its parent is eight. so is there some possibility that his parent would all of a sudden have a key bigger than it but if you think about it, this eight and this twelve, they were a parent-child relationship in the original heap right? so back in the blue heap, the twelve was under the 8s. now the twelve is under the eight yet again. since we have the heap property for that pair before, we certainly have it now. so in general, as you push up this five up the tree, there's only going to be one possible edge that could be out of order and that's between where the five currently resides and whatever its parent is. so when the 5's parent was twelve that was a violation when 5's parent was eight that was a violation but now that we've pushed it up two levels and 5's parents is four, that's not a violation because four is less than five. so in general, step two of insertion is, you do this swap, which it's called a lot of different things. i'm gonna call it bubble up because that's how i learned it more years ago than i care to admit but also this is called, sometimes sift up, happily up, and so on. so now told you to just how to implement insertions by repeated bubbling ups in a heap data structure and this is really how it works, there is nothing i haven't told you but you know, i'm not going to really fill in all the details but i'll encourage you to do that on your own time, if it's something that interests you and the two main things that you should check is first of all, is bubbling up process is gonna stop and when it stops, it stops with the heap property restored. the second thing needs to be checked in this, i think is easier to see is that we do have the desire one time log [inaudible] make in the number of elements in the heap. the key observations areas that because this is a perfectly balanced binary tree. we know exactly how many levels there are. so this is basically log based two event levels where n is the number of items in the heap and what is the running time of this insertion procedure while you only do a constant amount of work at each level, just doing the swap and comparison and then in the worst case, you'll have to swap at every single level and there is a lot [inaudible] number of levels. so that's insertion. let's now talk about how do we implement the extract [inaudible] operation and again i'm gonna do this by example and it's gonna be by repeating the [inaudible] of a bubble [inaudible] procedure. so the extract main operation is responsible for removing from the heap an object with minimum key value and handing it back to the client on a silver platter. so it pretty much have to whip out the root. remember the minimum is guaranteed to be at the root. so that's how we have to begin to extract the subroutine is just we pluck out the root and hand it back to the client. so this removal of course leaves a gaping hole in our tree structure and that's no good. one of the [inaudible] responsible for maintaining is that we always have an as perfectly balanced as possib le binary tree and if you are missing a root you certainly don't have an almost perfect. binary tree so, what are we going to do about it? how do we fill this hole? well, there's pretty much only one node that could fill this hole without causing other problems with the tree structure, and that is the very last node. so the rightmost leaf at the bottom level one that simple fix is to swap that up and have that take the place of the original root. so in this case, the thirteen is going to get a massive promotion and get teleported all the way to be the new root of this tree. so now we've resolved our structural challenges. we now again have a, as perfectly balanced as possible, binary tree but of course now we've totally screwed up the heap property, right. so the heap property says that at every node, including the root, the key value at that node has to be less than both of the children, and now it's all messed up. right, so at the root, the key value's actually bigger than both of the children. and matters that are little bit more tricky than they were with insertion, right, when we inserted at the bottom because every note has a unique parent. if you wanna push a note upward in the tree, there's sort of only one place you can go, right, all you can do is swap with your parent, unless you're going to try to do something really crazy but if you want to do something local, pretty much you only have a unique parent you got to swap with. now when you're trying to push notes down to the rightful position in the tree, there is two different swaps you could do, one for the left child, one for the right child and the decision that we make matters to see that concretely, let's think about this example. there's this thirteen at the root, which is totally not where it should be, and there's the two children. the four and the eight, and we could try swapping it with either one. so suppose we swap it in a misguided way with the right child, with the eight. so now the eight becomes the root, and the thirteen gets pushed dow n a level. so on the one hand; we made some progress because now at least we don't have a violation between the thirteen and the eight. on the other hand, we still have violations involving the thirteen. the thirteen is still violated with respect to the twelve and nine and moreover, we've created a new problem between the eight and the four, right? so now that the eight is the root, that's still bigger than its left child, this four. so it's not even clear we made any progress at all when we swapped the thirteen with the eight, okay? so that was a bad idea and if you think about it would made it a bad idea, the stupid thing was to swap it with the larger child. that doesn't make any sense. we really want to swap it with the smaller child. remember, every node should have a key bigger than both of its children. so if we're going to swap up either the four or the eight, one of those is going to become the parent of the other. the parent is supposed to be smaller, so evidently we should take the smaller of the two children and swap the thirteen with that. so we should swap the thirteen with the figure. not with e and now we observe a phenomenon very much analogous to what we saw in insert. when we were bubbling up during insertion, it wasn't necessarily that we fixed violations of the heat property right away but we would fix one and then introduce another one that was higher up in the tree and we had confidence that eventually we could just, push this violation up to the root of the tree and squash it, just like we're trying to win a game of whack a mole. here, it's the opposite. it's just in the opposite direction. so we swap the thirteen with the four. it's true we've created one new violation of the heap property. that's again involving the thirteen with its children nine and four. but we haven't created any new ones. we've pushed the heap violation further down the tree and hopefully again, like in whack a mole. we'll squash it at the bottom. so after swapping the thirteen and the four, now we just gotta do t he same thing. we say, okay, we're not done. we still don't have a heap. this thirteen is bigger than both of its children but now, with our accumulated wisdom, we know we should definitely swap the thirteen with the four. we're not gonna try swapping with the nine, that's for sure. so we move the four up here and we, the thirteen takes the 4's old place. boom! now we're done. so now we have no violations remaining. the thirteen in its new position has no children so there's no way it can have any violations, and the four because it was the smaller child that's gonna be bigger than the 9's so we haven't introduced a huge violation there, and again we have these consecutive 4's but we know that's not gonna be a problem because those were consecutive 4's in the original heap as well. so you won't be surprised to hear that this procedure by which you push something down, by swapping it with its smaller children, is called bubble down, and extract men is nothing more than taking more than, taking this last leaf, promoting it to the top of the tree, and bubbling down until the heap violation has been fixed. so again on a conceptual level that's all of the ingredients necessary for a complete from scratch implementation of extracting the minimum from a heap and as before, i'll leave it for you to check the details. so first of all you should check that in fact this bubble down has to at some point halt and when it halts you do have a bona fide heap. the heap property is definitely restored and second of all the running time is, is logarithmic. here the running time analysis is exactly the same as before so we already have observed that the heights of a heap because it's perfectly balanced is essentially the log base two of the number of elements in the heap and in bubbling down all you do is a constant amount of work per level. all you have to do is a couple comparisons and swap. so, that's a peek at what's under the hood in the heap data structure. a little bit about the guts of its elementation. so after having seen this, i hope you feel like a little bit more hard-core of a programmer, a little bit more hard-core of a computer scientist. 
in this sequence of videos, we'll discuss our last but not least data structure namely the balanced binary search tree. like our discussion of other data structures we'll begin with the what. that is we'll take the client's perspective and we'll ask what operations are supported by this data structure, what can you actually use it for? then we'll move on to the how and the why. we'll peer under the hood of the data structure and look at how it's actually implemented and then understanding the implementation to understand why the operations have the running times that they do. so what is a balanced binary search tree good for? well, i recommend thinking about it as a dynamic version of a sorted array. that is, if you have data store in a balanced binary search tree, you can do pretty much anything on the data that you could if it was just the static sorted array. but in addition, the data structure can accommodate insertions and deletions. you can accommodate a dynamic set of data that you're storing overtime. so to motivate the operations that a balanced binary search tree supports, let's just start with the sorted array and look at some of the things you can easily do with data that happens to be stored in such a way. so let's think about an array that has numerical data although, generally as we've said, in data structures is usually associated other data that's what you actually care about and the numbers are just some unique identifier for each of the records. so these might be an employee id number, social security numbers, packet id numbers and network contacts, etcetera. so what are some things that are easy to do given that your data is stored as a sorted array, most a bunch of things? first of all, you can search and recall that searching in a sorted array is generally done using binary search so this is how we used to look up phone numbers when we have physical phone books. you'd start in the middle of the phone book, if the name you were looking for was less than the midpoint, you recurse on the left hand side, otherwise you'd recurse on the right hand side. as we discussed back in the master method lectures long ago, this is going to run in logarithmic time. roughly speaking, every time you recurse, you've thrown out half of the array so you're guaranteed to terminate within a logarithmic number of iterations so binary search is logarithmic search time. something else we discussed in previous lectures is the selection problem. so previously, we discussed this in much harder context of unsorted arrays. remember, the selection problem in addition to array you're given in order statistic. so, if your order statistic that your target is seventeen, that means you're looking for the seventeenth smallest number that's stored in the array. so in previous lectures, we worked very hard to get a linear time algorithm for this problem in unsorted arrays. now, in a sorted array, you want to know the seventeenth smallest element in the array. pretty easy problem, just return whatever element happens to be in the seventeenth position of the array since the array is sorted, that's where it is so no problem. it's already sorted constant time, you can solve the selection problem. of course, two special cases of the selection problem are finding the minimum element of the array. that's just if the order statistic problem with i = 1and the maximum element, that's just i = n. so this just corresponds to returning the element that's in the first position and the last position of the array respectively. well let's do some more brainstorming. what other operations could we implement on a sorted array? well here's a couple more. so there are operations called the predecessor and successor operations. and so the way these work is, you start with one element. so, say you start with a pointer to the 23, and you want to know where in this array is the next smallest element. that's the predecessor query and the successor operation returns the next largest element in the array. so the predecessor of the 23 is the seventeen, the successor of the 23 would be the 30. and again in a sorted array, these are trivial, right? you just know that predecessors just one position back in the array, the successor is one position forward. so given a pointer to the 23, you can return to 17 or the 30 in constant time. what else? well, how about the rank operation? so we haven't discussed this operation in the past. so what rank is, this has for how many key stored in the data structure are less than or equal to a given key. so for example, the rank of 23 would be equal to 6. because 6 of the 8 elements in the array are less than or equal to 23. and if you think about it, implementing the rank operation is really no harder than implementing search. all you do is search for the given key and wherever it is search terminates in the array. you just look at the position in the array and boom, that's the rank of that element. so for example, if you do a binary search for 23 and then when you terminates, you discover it is, they're in position number six then you know the rank is six. if you do an unsuccessful search, say you search for 21, well then you get stuck in between the 17 and the 23, and at that point you can conclude that the rank of 21 in this array is five. let me just wrap up the list with the final operation which is trivial to implement in the sorted array. namely, you can output or print say the stored keys in sorted order let's say from smallest to largest. and naturally, all you do here is a single scan from left to right through the array, outputting whatever element you see next. the time required is constant per element or linear overall. so that's a quite impressive list of supported operations. could you really be so greedy as to want still more from our data structure? well yeah, certainly. we definitely want more than just what we have on the slide. the reason being, these are operations that operate on a static data set which is not changing overtime. but the world in general is dynamic. for example, if you are running a company and keeping track of the employees, sometimes you get new employees, sometimes employees leave. that is one of the data structure that not only supports these kinds of operations but also, insertions and deletions. now of course it's not that it's impossible to implement insert or delete in a sorted array, it's just that they're going to run way too slow. in general, you have to copy over a linear amount of stuff on an insertion or deletion if you want to maintain the sorted array property. so this linear time performance when insertion and deletion is unacceptable unless you barely ever do those operations. so, the raison d'etre of the balanced binary search tree is to implement this exact same set of operations just as rich as that's supported by a sorted array but in addition, insertions and deletions. now, a few of these operations won't be quite as fast or we have to give up a little bit instead of constant time, the one in logarithmic time and we still got logarithmic time for all of these operations, linear time for outputting the elements in sort of order plus, we'll be able to insert and delete in logarithmic time so let me just spell that out in a little more detail. so, a balanced binary search tree will act like a sorted array plus, it will have fast,  meaning logarithmic time inserts and deletes. so let's go ahead and spell out all of those operations. so search is going to run in o(log n) time, just like before. select runs in constant time in a sorted array and here it's going to take logarithmic, so we'll give up a little bit on the selection problem but we'll still be able to do it quite quickly. even on the special cases of finding the minimum or finding the maximum in our, in our data structure, we're going to need logarithmic time in general. same thing for finding predecessors and successors they're not, they're no longer constant time, they go with logarithmic. rank took as logarithmic time and the, even the sorted array version and that will remain logarithmic here. as we'll see, we lose essentially nothing over the sorted array, if we want to output the key values in sorted order say from smallest to largest. and crucially, we have two more fast operations compared to the sorted array of data structure. we can insert stuff so if you hire a new employee, you can insert them into your data structure. if an employee decides to leave, you can remove them from the data structure. you do not have to spend linear time like you did for sort of array, you only have to spend the logarithmic time whereas always n is the number of keys being stored in the data structure. so the key takeaway here is that, if you have data and it has keys which come from a totally ordered set like, say numeric keys, then a balanced binary search tree supports a very rich collection of operations. so if you anticipate doing a lot of different processing using the ordering information of all of these keys, then you really might want to consider a balanced binary search tree to maintain them. well then, keep in mind though is that we have seen a couple of other data structures which don't do quite as much as balanced binary search trees but what they do, they do better. we already, we just discussed in the last slide of the sorted array. so, if you have a static data set, you don't need inserts and deletes. well then by all means, don't bother with balanced binary search tree that use a sorted array because it will do everything super fast. but, we also sought through dynamic data structures which don't do as much but do it, but what they do, they do very well. so, we saw a heap, so what the heap is good for is it's just as dynamic as a search tree. it allows insertions and deletions both in logarithmic time. and in addition, it keeps track of the minimum element or the maximum element. remember in a heap, you can choose whether you want to keep track of the minimum or keep track of the maximum but unlike in a search tree, a heap does not simultaneously keep track of the minimum and the maximum. so if you just need those three operations, insertions, deletions and remembering the smallest, and this would be the case for example in a priority queue or scheduling application as discussed in the heap videos. then, a binary search tree is over kill. you might want to consider a heap instead. in fact, the benefits of a heap don't show up in the big o notation here both have logarithmic operation time but the constant factors both in space and time are going to be faster with a heap then with a balanced binary search tree. the other dynamic data structure that we discussed is a hash table. and what hash tables are really, really good at is handling insertions and searches, that is look ups. some, sometimes, depending on the implementation also handle deletions really well also. so, if you don't actually need to remember things like minima, maxima or remember ordering information on the keys, you just have to remember what's there and what's not. then the data structure of choice is definitely the hash table, not the balance binary search tree. again, the balance binary search tree would be fine and we'd give you logarithmic look up time but it's kind of over kill for the problem. all you need is fast look ups. a hash table recall will give you constant time look ups. so that will be a noticeable win over the balanced binary search tree. but if you want a very rich set of operations for processing your data. then, the balanced binary search tree could be the optimal data structure for your needs. 
so, in this video, we'll go over the basics behind implementing binary search trees. we are not going to focus on the balance aspect in this video that will be discussed in later videos and we are going to talk about things which are true for binary search trees on general, balanced or otherwise. but let's just recall, you know, why are we doing this, you know, what is the raison d'tre of this data structure, the balance version of the binary search tree and basically, its a dynamic version of a sorted array. so, that's pretty much everything you can do on a sorted array, maybe in slightly more expensive time. they are still really fast but in addition to this dynamic, it accommodates insertions and deletions. so, remember, if you want to keep a sorted array data structure, every time you insert, every time you delete, you're probably going to wind up paying a linear factor which is way too expensive in most applications. by contrast with the search tree, a balanced version, you can insert and delete a logarithmic time in the number of keys in the tree. and moreover, you can do stuff like search in logarithmic time, no more expensive than binary search on a sorted array and also you can sort of say the selection problem in the special cases, the minimum or maximum. okay, it's not constant time like in a sorted array but still logarithmic pretty good and in addition, you can print out all of the keys from smallest to largest and in linear time, constant time per element just like you could with the linear scan through a sorted array. so, that's what they're good for. everything a sorted array can do more or less plus insertions and deletions everything in logarithmic time. so, how are search trees organized? and again, what i'm going to say in the rest of this video is true both for balanced and unbalanced search trees. we're going to worry about the balancing aspect in the later videos. alright, so, let me tell you the key ingredients in a binary search tree. let me also just draw a simple cartoon example in the upper right part of the slide. so, this one to one correspondence between nodes of the tree and keys that are being stored. and as usual in our data structure discussions we're going to act as if the only thing that we care about, the only thing that exists at each node is this key when generally, this associated data that you really care about. so, each node in the tree will generally contain both the key plus a pointer to some data structure that has more information. maybe the key is the employee id number, and then there's a pointer to lots of other information about that employee. now, in addition to the nodes, you have to have links amongst the nodes and there's a lot of different ways to do the exact implementation of the pointers that connect the node of the tree together but the video i'm just going to keep is straightforward as possible and we're just going to assume that in each node, there's three pointers. one to a left child, another one to the right child and then the third pointer which points to the parent. now, of course, some of these pointers can be null and in fact in the five node binary search tree i've drawn on the right for each of the five nodes, at least one of these three pointers is null. so, for example, for the node with key one it has a null left child pointer, there was no left child. it's the right child pointer going to point to the node with key two and the parent pointer was going to a node that has key three. similarly three is going to have a null parent pointer and the root node in this case, three is a unique node but has a null parent pointer. here the node with key value three, of course, has a left child pointer points to one and has a right child pointer that points to five. now, here is the most fundamental property of search trees. let's just go ahead and call it the search tree property. so, the search tree property asserts the following condition at every single node of the search tree. if the node has some key value then all of the keys stored in the left subtree should be less than that key. and similarly, all of the keys stored in the right subtree should be bigger than that key. so, if we have some node who's stored key value is x and this is somewhere, you know, say deep in the middle of the tree so upward we think of as being toward the root. and then if we think about all the nodes that are reachable, after following the left child pointer from x, that's the left subtree. and similarly, the right subtree being everything reachable via the right child pointer from x, it should be the case that all keys in the left subtree are less than x and all keys in the right subtree are bigger than x. and again, i want to emphasize this property holds not just to the root but at every single node in the tree. i've defined the search to a property assuming that all of the keys are distinct, that's why i wrote strictly less than in the left sub tree and strictly bigger than in the right subtree. but search trees can easily accommodate duplicate keys as well. we just have to have some convention about how you handle ties. so, for example, you could say that everything in the left subtree is less than or equal to the key at that node and then everything in the right subtree should be strictly bigger than that node. that works fine as well. so, if this is the first time you've ever heard of the search tree property, maybe at first blush it seems a little arbitrary. it seems like i pulled it out of thin air but actually, you would have reversed engineer this property if you sat down and thought about what property would make search really easy in a data structure. the point is, the search tree property tells you exactly where to look for some given key. so, looking ahead a little bit, stealing my fire from a slide to come, suppose you were looking for say, a key 23, and you started the root and the root is seventeen. the point of the search tree property is you know where 23 has to be. if the root is seventeen, you're looking to 23, if it's in the tree, no way is it in the left subtree, it's got to be in the right subtree. so, you can just follow the right child pointer and forget about the left subtree for the rest of the search. this is very much in the spirit of binary search where you start in the middle of the array and again, you compare what you're looking for to what's in the middle and either way, you can recurse on one of the two sides forgetting forevermore about the other half of the array and that's exactly the point of the search tree property. we're going to have to search from root on down, the search tree property guarantees we have a unique direction to go next and we never have to worry about any of the stuff that we don't see. we can also draw a very loose analogy with our discussion of heaps and may recall heaps were also logically, we thought of them as a tree even though they are implemented as an array. and heaps have some heap property and if you go back to review the heap property, you'll find that this is not the same thing as the search three property. these are two different properties and that's going to trying to make different things easy. back when we talk about heaps, the property was that this is for the extract min version. parents always have to be smaller than their children. that's different than the search tree property which says stuff to the left, that's smaller than you, stuff to the right is bigger than you. and heaps, we have the heap property so that identifying the minimum value was trivial. it was guaranteed to be at the root. heaps are designed so that you can find the minimum easily. search trees are, are defined so that you can search easily that's why, you have this different search tree property. if you want to get smaller, you go left. if you want to get bigger you go right. one point that's important to understand early, and this will be particularly relevant once we did, once we try to enforce balancing in our subsequent videos is that, for a given set of keys, you can have a lot of different search trees. on the previous slide , i drew one search tree containing the key values one, two, three, four, five. let me redraw that exact same search tree here. if you stare to this tree a little while you'll agree that in fact that every single node of this tree, all of the things in the left subtree are smaller, all of the things in the right subtree are bigger. however, let me show you another valid binary search tree with the exact same sets of keys. so, in the second search three, the root is five, the maximum value. and everybody has no right children, only the left children are populated and that goes five, four, three, two, one in descending order. if you check here again, it has the property that at every node, everything in the left subtree is smaller. everything in the right subtree, in this case, empty, is bigger. so, extrapolating from these two cartoon examples, we surmised that for a given set of n keys, search trees that contain these keys could vary in height anywhere from the best case scenario of a perfectly balance binary tree which just going to have logarithmic height to the worst case of one of these link list like chain which is going to be linear in the number of keys n. and so just to remind you the height of a search tree which is also sometimes called the depth is just the longest number of hops it ever take to get to from a root to a leaf. so, in the first search tree, here the height is two and then the second search tree, the height is four. if the search tree is perfectly arranged with the number of nodes essentially doubling at every level, then the depth is you're going to run out of nodes around the depth of log2n. and in general, if you have a chain of n keys that that's going to be n - 1 but we'll just call it n amongst friends. so, now that we understand the basic structure of binary search trees, we can actually talk about how to implement all of the operations that they support. so, as we go through most of the supported operations one at a time, i'm just going to give you a really high level description. it should be enough for you to code up on implementation if you want or as usual, if you want more details or actual working code, you can check on the web or in one of the number of good programming or algorithms textbooks. so, let's start with really the primary operation which is search. searching, we've really already discussed how it's done when we discuss the search tree property. again, the search tree property makes it obvious how to look for something in a search tree. pretty much you just follow your nose you have no other choice. so, you started the root, it's the obvious place to start. if you're lucky, the root is what you are looking for and then you stop and then you return to root. more likely, the root is either bigger than or less than the key that you're looking for. now, if the key is smaller, the key you are looking for is smaller than the key of the root, where you're going to look? well, the search tree property says, if it's in the tree, it's got to be in the left subtree so you follow the left sub child pointer. if the key you're looking for is bigger than the key at the root, where is it got to be? got to be in the right subtree. so, you're just going to recurse on the right subtree. so, in this example, if you're searching for, say the key two, obviously you're going to go left from the root. if you're searching for the key four, obviously you're going to go right from the root. so, how can the search terminate? well, it can terminate in one of two ways. first of all, you might find what you're looking for so in this example, if you search for four, you're going to traverse to right child pointer then a left child pointer and then boom, you're at the four and you return successfully. the other way the search can terminate is with a null pointer. so, in this example, suppose you were looking for a node with key six, what would happen? well, you start at the root, three is too small so you go to the right. you get to five, five is still too small cuz you're looking for six so you try to go right but the right child pointer is null. and that means six is not in the tree. if there was anywhere in the tree, it had to be to the right of the three, it had to be to the right of the five but you tried it and you ran on the pointers so the six isn't there. and you return correctly with an unsuccessful search. next, let's discuss the insert operation which really is just a simple piggy backing on the search that we just described. so, for simplicity the first think about the case where there are no duplicate keys. the first thing to do on this insertion is search for the key k. now, because there are no duplicates, this search will not succeed. this key k is not yet in the tree. so, for example, in the picture on the right, we might think about trying to insert the key six. what's going to happen when we search for six, we follow a right child pointer. we go from three to five and then we try to spot another one and make it stuck. there's a null pointer going to the right of five. then when this unsuccessful search terminates at a null pointer, we just rewire that pointer to point to a node with this new key k. so, if you want to permit duplicates from the data structure, you got to tweak the code and insert a little bit but really barely at all. you just need some convention for handling the case when you do in counter the key that we are about to insert. so, for example, if the current note has the key equal to the one you're inserting, you could have the convention that you always continue on the left subtree and then you continue the search as usual again, eventually terminating at a null pointer and you stick the new inserted node you rewire to null pointer to point to it. one good exercise for you to think through which i'm not going to say more about here is that when you insert a new node, you retain the search tree property. that is if you start with the search tree, you start within tree where at every node stuff to the left is smaller, the stuff to the right is bigger. you insert something and you follow this procedure. you will still have the search tree property after this new node has been inserted. that's something for you to think through. 
so what i want to do next is test your understanding about the search and insertion procedures by asking you about their running time. so of the following four parameters of a search tree that contains n different keys, which one governs the worst case time of a search or insertion. so the correct answer is the third one. so, the heights of a search tree governs the worst case time of the search or of an insertion. notice that means merely knowing the number of keys n is not enough to deduce what the worst case search time is. you also have to know something about the structure of the tree. so, to see that, just let's think about the two examples that we've been running so far. one of which is nice and balanced. and the other of which, which contains exactly the same five keys is super unbalanced, it's this crazy linked list in effect. so, in any search tree, the worst case time to do is search or insertion is proportional to the largest number of pointers left to right child pointer that you might have to follow to get from the root all the way to a null pointer. of course in a successful search, you're going to terminate before you encounter a null pointer but in the worst case, you want insertion you go all the way to a null pointer. now on the tree on the left you're going to follow at most 3 such pointers. so for example, if you're searching for 2.5. you're going to follow a left pointer followed by a right pointer. by another pointer and that one is going to be null. so we're going to follow three pointers. on the other hand, in the right tree, you might follow as many as five pointers before that fifth pointer is null. for example, if you search for the key zero, you're going to traverse five left pointers in a row and then you're finally going to encounter the null at the end. so, it is not constant time certainly, you have to get to the bottom of the tree. it's going to be from proportional to logarithmic, logarithm in the number of keys if you have a nicely balanced binary search tree like this one on the left. it's going to be proportional to the number of keys n as in the fourth answer if you have a really lousy search tree like this one on the right and in general. search time or the insertion time is going to be proportional to the height. the largest number of hops we need to take to get from the root to the leaf of the tree. let's move on to some more operations that search tree support but that, for example, the dynamics data structures of heaps and hash tables do not. so let's start with the minimum and the maximum. so, by contrast and a heap remember, you can choose one or the two. you can either find the minimum, usually you find the maximum easily but not both. and the search tree is really easy to find, either the min or the max. so, let's start with the minimum. one way to think of it is that you do a search for negative infinity in the search tree. so, you started the root. and you just keep following left child pointers until you run out, until you hit a null. and whatever the last key that you visit has to be the smallest key of the tree, right? because, think about it, suppose you started the root. supposed that the root was not the minimum, then where is the minimum got to be, it's got to be in the left sub-tree so you follow the left child pointer and then you just repeat the argument. if you haven't already found the minimum, where it's got to be with respect to current place, it's got to be in the left sub tree and you just iterate until you can't go to the left any further. so for example, in our running search tree. you'll notice that if we just keep following left child pointers, we'll start at the three, we'll go to the one, we'll try to go left from the one. we'll hit a null pointer and we'll return one and one is indeed the minimum key in this tree. now, given that we've gone over how to compute the minimum, no prizes to guess how we compute the maximum. of course, if we want to compute the maximum instead of following left child pointers we follow right child pointers by symmetric reasoning as guaranteed upon the largest key in the tree. it's like searching for the key plus infinity. alright. so what about computing the predecessor? so remember this means you're given key in the tree, in the element of the tree and you want to find the next smallest element so for example the predecessor of the three is two. the predecessor of the two in this tree is the one. the predecessor of the five is the four. the predecessor of the four is the three. so, here i'll be a little hand wavy just in the interest of getting through all of the operations in reasonable amount of time but let me just point out that there is one really easy case and then there is one slightly trickier case. so the easy case. is when the node with the key k has a non-empty left sub tree. if that's the case, then what you want is simply the biggest element in this node left sub tree. so, i'll leave it for you to prove formally that this is indeed the correct way to compute predecessors for keys that do have a non-empty left sub tree, let's just verify in our example by going through the trees that have a left sub tree and checking this is in fact what we want. now, if you look at it, there's actually only two nodes that have a non-empty left sub tree. the three has a non-empty left sub tree and indeed the largest key in the left sub tree three is the two and that is the predecessor of the three so that worked out fine. and then the other node with a non-empty left subtree is the five and it's left subtree is simply the element four of course the maximum of that tree is also four. and then you'll notice that is indeed the predecessor of five in this entire search tree. so, the trickier case is what happens if you know the key with no left subtree at all. okay. so, what are you going to do if you not in the easy case, well, given at this node with key k, you only have three pointers and by assumption, the left one is null so that's not going to get you anywhere, now, the right childpointer if you think about it is totally pointless for computing the predecessor. remember, the predecessor is going to be a key less than the given key k. the right subtree by definition of a search tree only has keys that are bigger than k. so, it stands for reason to find the predecessor we got to follow the parent pointer. maybe in fact more than one parent pointer so to motivate exactly how we're going to follow parent pointers, let's look at a couple of examples in our favorite search tree here on the right. so, let's start with a node two. so, we know we got to follow a parent pointer. when we follow to this parent pointer, we get to one and boom, one in fact is two's predecessor in this tree so that was really easy to computer two's predecessor. it seemed that all we have to do is follow the parent pointer. so, for another example though which think about the node four. now, four when we follow which parent pointer, we get to five and. five is not 4's predecessor, it's 4's successor. what we wanted a key that is less than where we started, we follow the parent pointer and it was bigger. but, if we follow one more parent pointer, then we get to the three. so, from the two we needed to follow one parent pointer, from the four we needed to follow two parent pointers. but the point is, you just need to follow parent pointers until you get to a node with key smaller than your own. and at that point you can stop and that's guaranteed to be the predecessor. so, hopefully, you would find this intuitive. i should say, i have definitely not formally proved that this works and that is a good exercise for those of you that want to have a deeper understanding of search trees and this magical search tree property and all of the structure that it grants you. the other thing i should mention is another way to interpret the, the terminating criteria. so what i've said is you stop your search of parent pointers as soon as you get to through smaller than yours if you think it about a little bit, you'll realize you'll get to a key smaller than yours, the very first time you take a left turn. so the very first time that you go from a right child to it's parent. look at the example, when we started from two, we took a left turn, right? we went from upper link going leftward to it's a right child of one, and that's when we got to the predecessor in just one step. by contrast when we started from the four, our first step was to the right. so, we got to a node that was bigger than where we started for five is four's left child which is going to be smaller than five. but the first time we took a left turn on the next step, we got to a node that is not only smaller than five but actually smaller from four, smaller from the starting point. so, in fact, you're going to see a key smaller than your starting point at very first time, you take a left turn, the very first time you go from a node to a parent and in fact, that node is that parent's right child. so this is another statement which i think is intuitive but which formally is not totally obvious. and again i encourage you to think carefully about why these two descriptions of the terminating criteria are exactly the same so it doesn't matter if you stop when you first find a key smaller than your starting point. it doesn't matter if you first stop when you follow a parent pointer that goes from a node that's the right child of a node. either way you're going to stop at exactly the same time so i encourage you to think about why those are the exact same stopping condition. a couple of other details if you start from the unique node that has no predecessor at all, you'll never going to trigger this terminating condition so for example if you start from the node one in the search tree, not only is the left subtree empty which says you're suppose to start traversing parent pointers but then when you traverse a parent pointer, you only go to the right. you never turn left and that's because there is no predecessor so that's how you detect that you're at the minimum of a search tree. and then of course if you wanted to be the successor of the key instead of the predecessor, obviously you just flip left and right through out this entire description. so that's the high level explanation of all of these different ordering operations, minimum and maximum predecessor and successor work in a search tree. let me ask you the same question i asked you when we talked about search in insertion. how long that these operations take in the worst case? well, the answer is the same as it was before. it's proportional to the height of the tree and the explanation is exactly the same as it was before. so to understand the dependence on the height was just focused on the maximum operation that has the state within the question. the other three operations, the running time is proportional to the height in the worst case for exactly the same reasons. so, what is the max operation do when you started the root and you just follow the right child pointers until you run out them so you hit null. so, you know, that the running time is going to be no worse in the longest such paths. it's particular path from the root to essentially a leaf. so instead we're going to have a running time more than the height of the tree, on the other hand for all you know. the path from the root to the maximum key might well be the longest one in the tree. it might be the path that actually determines the height of the search tree. so, for example in our running unbalanced example, that would be a bad tree for the minimum operation if you look for the minimum in this tree, then you have to traverse every single pointer from five all the way down to one. of course there's an analogious bad search tree for the maximum operation where the one is the root and the five is all the way down to the left. another thing you can do is search trees which mimics what you can do with sorted arrays is you can print out all of the keys in the sorted order in linear time with constant time per element. obviously, in the sorted array this is trivial. use your for loop start ing at the beginning at the array pointing up the keys one at a time and there's a very elegant recursive implementation for doing the exact same thing in a search tree. and this is known as an in order traversal of binary search tree. so as always you begin at the beginning namely at the root of the search tree. and a little bit of notation of which call, all of the search tree that starts at r's left child t sub l and the search tree routed at r's right child t sub r. in our running example of course the root is three t sub l with correspondent in the search tree comprising only the elements one and two, t sub r would correspond to the sub-tree comprising only the elements five and four. now, remember we want to print out the keys in increasing order. so in particular, the first key we want to print out is the smallest of them all. so it's something we definitely don't want to do is we don't want to first print out the key at the root. for example in our search tree example, the root's key is three, we don't want to print that out first. we want to print out the one first. so where is the minimum lie? well, by the search tree property, it's got to lie in the left subtree t sub l, so we're just going to recurse on t sub l. so by the magic of recursion or if you prefer induction, what re-cursing on t sub l is going to accomplish is we're going to print out all of the keys in t sub l in increasing order from smallest to largest. now that's pretty cool because t sub l contains exactly the keys that are smaller than the key of the root. remember that's the search tree property. everything bigger than the root's key has to be in the left sub tree. everything bigger than the root's key have to be in its right sub tree. so in our concrete example of this first recursive call is we're going to print the keys one and then two. and now, if you think about it it's the perfect time to print out the key at the root, right? we want to print out all the keys in increasing order we've already done everything less than the root's key where re-cursing and on the right hand side will take you everything bigger in it so in between the two recursive calls, this is why it's called an in order traversal, that's when we want to print out. r's key. and clearly this works in our concrete example, the first recursive call print out one and two, it's the perfect time to print out three and then a recursive call of print out four and five. and more generally, the recursive call on there right subtree will print out all of the keys bigger than the roots key and increasing order again by the magic of recursion or induction so, the fact that the pseudo-code is correct. the fact that the so-called in-order traversal indeed print out the keys in increasing order. this is a fairly straightforward proof by induction. it's very much in the spirit or the proofs by induction, correctness of divide and conquer algorithms that we've discussed earlier in the course. so what about the running time of an in order traversal? the claim is that the running time of this procedure is linear. it's o of n where n is the number of keys in the search tree. and the reason is, there's exactly one recursive call for each node of the tree and constant work is done in each of those recursive calls. and a little more detail, so what is the in order] traversal do, it will print out the keys in increasing. in particular it prints out each key exactly once. each recursive call prints out exactly one key's value. so there's exactly n recursive calls and all of the recursive call does is print one thing. so n recursive calls constant time for each that gives us a running time of o(n) overall. in most data structures, deletion is the most difficult operation and in search trees. there are no exception. so let's get into it and talk about how deletion works, there are three different cases. so the first order of business is to locate the node that has the key k, locate the node that we want to get rid off. right so for starters, maybe we're trying to delete the key two from our running example search tree. so the first thing we need to do is figure out where it is. so, there are three possibilities for the number of children that a node in a search tree might have and might have zero children that might have one child it might have two children, corresponding to those three cases that the deletion pseudo-code will also have three cases. so, let's start with the happy case where there's only zero children like in this case where deleting the key 2 from the search tree. then of course, we can, without any reservations just delete the node directly from the search tree, nothing can go wrong, there's no children depending on that node. then there is the medium difficult case. this is where. the node containing k has one child. an example here would be, if we wanted to delete five from the search tree so the medium case is also not too bad. all you got to do is splice out the node that you want to delete. that creates a hole in the tree but then that node, deleted node's unique child assumes the previous position of the deleted node. i can make a nerdy joke about shakespeare right here but i'll refrain. for example, in our five node search tree if we wanted to, let's say we haven't actually deleted two out of this one, if we wanted to delete the five. the five when we take it out of the tree that would leave a hole but then we just replace the position previously held by five by it's unique child four. and if you think about it that works just fine in the sense of that preserves the search tree property. remember the search tree property says that everything in say, a right subtree has to be bigger than everything in the nodes key, so we've made four the new right child of three but four and any children that it might have were always part of 3's right subtree so all that stuff has got to be bigger than three so there's no problem putting four and possibly all of its descendants. as the right child of three. the search tree property is in fact retained. so, the final difficult case then is when the node being deleted has both of its children, has two children. so, in our running example with five nodes, this would only transpire if you wanted to delete the root, you want to delete the key three from the tree. the problem, of course, is that, you know, you can try ripping out this node from the tree but then, there's this hole and it's not clear that it's going to work to promote either child. into that spot. you might stare at our example search tree and try to understand what would happen if you try to bring one up to be the root or if you try to bring five up to be the root. problems would happen, that's what would happen. this is an interesting contrast to when we faced the same issue with heaps. because the heap property in some sense is perhaps less stringent, there we didn't have an issue. when we wanted to delete something with two children, we just promoted the smaller of the two children assuming we wanted to export and extract them in operation. here, we're going to have to work a little harder. in fact this is going to be really neat trick. we're going to do something that reduces the case of two children to the previously solved cases of zero or one children. so here's a very sneaky way we identify a node to which we can apply either the case zero or the case one operation. what we're going to do is we're going to. start from k and we're going to compute k's predecessor. remember, this is the next smallest key in the tree. so, for example, the predecessor of the key three is two. that's the next smallest key in the tree. in general, let's call case predecessor l. now, this might seem complicated. we're trying to implement one tree operation and with deletion and all of a sudden we're invoking a different tree operation predecessor which we covered a couple of slides ago. and to some extent you're right you know, delete, this is a nontrivial operation. but, it's not quite as bad as you think for the following reason. when we compute this predecessor, we're actually in the easy case of the predecessor operation conceptually . remember how do you get a predecessor, well it depends. what does it depend on? it depends on whether you got a non-empty left sub tree or not. if you don't have a non-empty left sub tree, that's how you got to those things and follow a parent pointers upward until you find a key which is smaller than what you've started. but. if you've got a left sub tree, then it's easy. you just find the maximum of the left sub tree and that's got to be the predecessor and remember, finding maximum are easy. all you have to do is follow right child pointers until you can't anymore. now, what's cool is because we only bother with this predecessor computation in the case where case k's node has both children. we only have to do it in the case where it has a non-empty left subtree. so really when we say compute k's predecessor l. all you got to do is follow k's left child. that's not null because it has both children. and then, follow right child pointers until you can't anymore and that's the predecessor. now, here's the fairly brilliant parts of the way you do implement deletion in the search tree which is you swap these two keys, k and l. so for example in our running search tree, instead of this three at the root we would put a two there and instead of this two at the leaf, it would put a three there. and the first time you see this, it just strikes you as a little crazy, maybe even cheating or just simply disregarding the roles of, rules of search trees. and actually, it is like check out what happen to our example search tree. we swap the three and the two and this is not a search tree anymore, right? so, we have this three which is in two left sub tree and a three is bigger than the two and that is not allowed. that is violation of the search tree property. oops. so, how can we get away with this and we get away with this is we're going to delete three anyway. so, we're going to wind up with the search tree at the end of the day. so we may have messed up the search tree property a little bit but we've swapped k in the position where its really easy to get rid of. well how did we compute case predecessor l? ultimately that was the result of a maximum computation which involves following right child pointers until you get stuck and l was the place we got stuck. what's the meaning to get stuck? it means l's right child pointer is null. it does not have two children. in particular it does not have a right child. once we swap k in the l's old position, k now does not have a right child. it may or may not have a left child and the example on the right it does not have a left child either in this new position but in general it might have a left child. but, it definitely doesn't have a right child. because that was a position at which a maximum computation got stuck. and if we want to delete a node that has only zero or one child, well that we know how to do. that we covered in the last slide. either you just delete it, that's what we do in the running example here. or in the case where k's new node does have a left child, you would do the splice out operation. so you would rip out the node that contains k and that the unique child of that node would assume the previous position of that node. now an exercise which i'm not going to do here but i strongly encourage you think through in the privacy of your own home, is that , in fact, this deletion operation retains the search tree property. so roughly speaking, when you do the swap, you can violate the search tree property as we see in this example but all of the violations involved the node you're about to delete so once you delete that node, there's no other violations of the search property so bingo, you're left with the search tree. the running time this time no get, no prizes for guessing what it is because it's basically just one of these predecessor computations plus some pointer rewiring just like the predecessor and search is going to be governed by the height of the tree. so let me just say a little bit about the final two operations mentioned earlier, select and rank. remember select is just a selection problem. i'll give you an order statistic like seventeen and i want you to return the seventeenth smallest key in the tree. rank is i give you a key value and i want to know how many keys in the tree are less than or equal to that value. so, to implement these operations efficiently, we actually need one small new idea which is to augment binary search trees with additional information at each node. so, now the search tree will contain not just a key but also information about the tree itself. so, this idea is often called augmenting your data structure and perhaps the most canonical augmentation of the search tree like these is to keep track in each node, not just to the key value but also over the population of tree nodes in the sub tree that is rooted there. so let's call this size of x. which is the number of tree nodes in the subtree rooted at x. so to make sure you know what i mean, let me just tell you what the size field should be for each of the five nodes in our running search tree example. so again example, we're thinking about how many nodes are in the subtree rooted given node. or equivalently, following child pointers from that node how many different tree nodes can you reach? so from the root of course, you can reach everybody. everybody's in the tree rooted at the root so the size there is five. by contrast, you start at the node one, well, you can get to the one or you can follow the right child pointer to get to the two. so at the one. the size would be two and the node with the key value five for the same reason, the size would be two. at the two leaves, the subtree where the leaf is just the leaf itself so there, the size would be one. there's an easy way to compute the size of a given node once you know the size of its two sub trees. so, if the given node in the search tree has children y and z, then, how many nodes are there in the sub tree rooted x, well, there's those that are rooted at y. there are those in the left sub tree, there are those that are reachable from z that is there are the children that are also children of z and then there's x itself. now in general, whenever you augment a data structure, and this is something we'll talk about again when we discuss red black trees, you've got to pay the piper. so, the extra data that you maintain it might be useful for speeding up certain operations. but whenever you have operations that modify the tree, specifically insertion and deletion, you have to take care to keep that extra data valid, keep it maintained. now, in the case of the subtree sizes, there are quite straightforward to maintain under insertion and deletion without affecting the running time of insertion and deletion very much but that's something you should always think about offline. for example, when you perform an insertion remember how that works. you do as, essentially a search. you follow left and right child pointers down to the bottom of the tree until you get a null pointer then that's where you stick a new node. now what you have to do is you have to trace back up that path, all of the ancestors of the new node you just inserted and increment their subtree sizes by one. so let's wrap up this video by showing you how to implement the selection procedure given an nth order statistic in a search tree that's been augmented so that at every node you know the size of a subtree rooted at that node. well of course as always you start at the beginning which in the search tree is the root. and let's say the root has a sub-children y and z. y or z could be null, that's no problem. we just think of the size of a null node as being zero. now, what's the search tree property? it says, every, these keys that are less than the keys sorted x are precisely the one that are in the left sub tree of x. the keys in the tree, they are bigger than the key to x or precisely the ones that you're going to find in x's right sub tree. so, supposed we're asked to find the seventeenth order statistic in the search three. seventeenth smallest key that's stored in the tree, where is it going to be? where should we look? well, it's going to depend on the structure of the tree and in fact it's going to depend on the subtree sizes. this is exactly. we're keeping track of them so we can quickly make decisions about how to navigate through the tree. so for a simple example, suppose that x's left subtree contains say 25 keys. so remember y know locally exactly what the population of the subtree is so in constant time from x, we can figure out how many keys are in y subtree let's say its 25. now, by the defining property of search trees, these are the 25 smallest keys anywhere in the tree. right, x is bigger than all of them. everything in x's right subtree is bigger than all of them. so, the 25 smallest order statistics are all in the subtree rooted to y, clearly that where we should recurse. clearly that's where the answer lies so in recursing the subtree root of y and then we are again looking for the seventeenth order statistic in this new smaller search tree. on the other supposed when we started x and we look, we ask why. how, how many nodes are there in your subtree. maybe y locally have stored the number twelve. so there's only twelve things in x's left subtree. well, okay, x itself is bigger than all of them so that's going to, x is going to be the thirteenth biggest order statistic. it's going to be the thirteenth biggest element in the tree. everything else is in the right sub tree. so, in particular, the seventeenth order statistic is going to be in the right sub tree so we're going to recurse in the rght sub tree. now, what are we looking for, we're not looking for the seventeenth order statistic anymore. the twelve smallest things all in x's sub tree, x itself is the thirteenth smallest so we are looking for the fourth smallest of what remains. so, the recursion is very much along the lines of what we did in the divide and conquer selection algorithms earlier in the course. so to fill in some more details, let's let a denote the subtree size at y. and if it happens that x has no left child, we'll, the point would be a to be zero. so the super lucky case is when there's exactly i - 1 nodes in the left subtree. that means the root here, x is itself the ith order statistic remember it's bigger than everything in it's left subtree it's smaller than everything in its right subtree.  but, in the general case we're going to be recursing either on the left subtree or in the right subtree. we recurse on the left subtree when its population is large enough that we guarantee it and compasses the ith order statistic. and that happens exactly when it sides is at least i. that's because the left subtree has the smallest keys that are anywhere in the search tree. and in the final case when the left subtree is so small that the only does it not contain the ith order statistic but also x is too small to be an ith order statistic then we recurse in the right subtree knowing that we have thrown away a + 1, the a + 1 smallest key values anywhere in the original tree. so, correctness of this procedure is pretty much exactly the same as the inducted correctness for the selection algorithms we've discussed earlier in effect to the root of the search tree is acting as a pivot element with everything in the left sub tree being less than the root everything in the right sub tree being greater than the element in the root so that's why the recursion is correct. as far as the running time, i hope it's evident from the pseudo code that we do constant time each time they recurse. how many times can we recurse when we keep moving down the tree that maximum number of times we can move down the tree is proportional to the height of the tree. so, it was again is proportional to the height. so, that's the select operation, there is an analogous way to write the rank operation. remember, this is where you're given the key value and you want to count up the number of stored keys that are less than or equal to that target value, again, you use this augmented search trees and again, you can get running time porportional to the height and i encourage you to think through 
so, in this video, we'll graduate beyond the domain of just vanilla binary search trees, like we've been talking about before, and we'll start talking about balanced binary search trees. these are the search trees you'd really want to use when you want to have real time guarantees on your operation time. cuz they're search trees which are guaranteed to stay balanced, which means the height is guaranteed to stay logarithmic, which means all of the operations search trees support that we know and love, will also be a logarithmic in the number of keys that they're storing. so, let's just quickly recap. what is the basic structure tree property? it should be the case that at every single node of your search tree, if you go to the left, you'll only see keys that are smaller than where you started and if you go to the right you only see keys that are bigger than where you started. and a really important observation, which is that, given a set of keys, there's going to be lot and lots of legitimate, valid, binary search trees with those keys. so, we've been having these running examples where the keys one, two, three, four, five. on the one hand, you can have a nice and balanced search tree that has height only two, with the keys one through five. on the other hand, you can also have these crazy chains, basically devolved to link lists where the heights for, and elements could be as high as n - 1. so, in general, you could have an exponential difference in the height. it can be as small, in the best case, as logarithmic and as big, in the worst case, as linear. so, this obviously motivates search trees that have the additional property that you never have to worry about their height. you know they're going to be well balanced. you know they're going to have height logarithmic. you're never worried about them having this really lousy linear height. remember, why it's so important to have a small height? it's because the running time of all of the operations of search trees depends on the height. you want to do search, you want to insertions, you want to find predecessors or whatever, the height is going to be what governs the running time of all those properties. so, the high level idea behind balanced search trees is really exactly what you think, which is that, you know, because the height can't be any better than logarithmic in the number of things you're storing, that's because the trees are binary so the number of nodes can only double each level so you need a logarithmic number of levels to accommodate everything that you are storing. but it's got to be logarithmic, lets make sure it stays logarithmic all the time, even as we do insertions and deletions. if we can do that, then we get a very rich collection of supported operations all running in logarithmic time. as usual, n denotes, the number of keys being stored in the tree. there are many, many, many different balanced search trees. they're not super, most of them are not super different from each other. i'm going to talk about one of the more popular ones which are called red black trees. so, these were invented back in the '70s. these were not the first balanced binary search tree data structures, that honor belongs to avl trees, which again are not very different from red black trees, though the invariants are slightly different. another thing you might want to look up and read about is a very cool data structure called splay trees, due to sleator and tarjan, these, unlike red black trees and avl trees, which only are modified on insertions and deletions, which, if you think about it, is sort of what you'd expect. splay trees modify themselves, even when you're doing look ups, even when you're doing searches. so, they're sometimes called self-adjusting trees for that reason. and it's super simple, but they still have kind of amazing guarantees. and then finally, going beyond the, just the binary tree paradigm many of you might want to look up examples of b trees or also b+ trees. these are very relevant for implementing databases. here what the idea is, in a given node you're going to have not just one key but many keys and from a node, you have multiple branches that you can take depending where you're searching for falls with respect to the multiple keys that are at that node. the motivation in a database context for going beyond the binary paradigm, is to have a better match up with the memory hierarchy. so, that's also very important, although a little bit out of the scope here. that said, what we discuss about red-black trees, much of the intuition will translate to all of these other balance tree data structures, if you ever find yourself in a position where you need to learn more about them. so, red black trees are just the same as binary search trees, except they also always maintain a number of additional invariants. and so, what i'm going to focus on in this video is, first of all, what the invariants are, and then how the invariants guarantee that the height will be logarithmic. time permitting, at some point, there will be optional videos more about the guts, more about the implementations of red black trees namely how do you maintain these invariants under insertions and deletions. that's quite a bit more complicated, so that's appropriate for, for optional material. but understanding what the invariants are and what role they play in controlling the height is very accessible, and it's something i think every programmer should know. so, there, i'm going to write down four invariants and really, the bite comes from the second two, okay, from the third and the fourth invariant. the first two invariants you know, are just really cosmetic. so, the first one we're going to store one bit of information additionally at each node, beyond just the key and we're going call this bit as indicating whether it's a red or a black node. you might be wondering, you know, why red black? well, i asked my colleague, leo guibas about that a few years ago. and he told me that when he and professor sedgewick were writing up this article the journals were, just had access to a certain kind of new printing technology that allowed very limited color in the printed copies of the journals. and so, they were eager to use it, and so they named the data structure red black, so they could have these nice red and black pictures in the journal article. unfortunately, there was then some snafu, and at the end of the day, that technology wasn't actually available, so it wasn't actually printed the way they were envisioning it but the name has stuck. so, that's the rather idiosyncratic reason why these data structures got the name that they did, red black trees. so, secondly we're going to maintain the invariant that the roots of the search tree is always black, it can never be red. okay. so, with the superficial pair of invariants out of the way, let's go to the two main ones. so, first of all, we're never going to allow two reds in a row. by which, i mean, if you have a red node in the search tree, then its children must be black. if you think about for a second, you realize this also implies that if a notice red, and it has a parent, then that parent has to be a black node. so, in that sense, there are no two red nodes in a row anywhere in the tree. and the final invariant which is also rather severe is that every path you might take from a root to a null pointer, passes through exactly the same number of black nodes. so, to be clear on what i mean by a root null path, what you should think about is an unsuccessful search, right? so, what happens in an unsuccessful search, you start at the root depending on whether you need to go smaller or bigger, you go left or right respectably. you keep going left right as appropriate until eventually you hit a null pointer. so, i want you to think about the process that which you start at the root and then, eventually, fall off the end of the tree. in doing so, you traverse some number of nodes. some of those nodes will be black some of those nodes will be red. and i want you to keep track of the number of black nodes and the constraints that a red black tree, by definition, must satisfy, is that no matter what path you take through the tree starting from the root terminating at a null pointer, the number of black nodes traversed, has to be exactly the same. it cannot depend on the path, it has to be exactly the same on every single root null path. let's move on to some examples. so, here's a claim. and this is meant to, kind of, whet your appetite for the idea that red black trees must be pretty balanced. they have to have height, basically logarithmic. so, remember, what's the most unbalanced search tree? well, that's these chains. so, the claim is, even a chain with three nodes can not be a red black tree. so, what's the proof? well, consider such a search tree. so, maybe, with the key values one, two and three. so, the question that we're asking is, is there a way to color the node, these three nodes, red and black so that all four of the invariants are satisfied. so, we need to color each red or black. remember, variant two says, the root, the one has to be black. so, we have four possibilities for how to use the color two and three. but really, because of the third invariant, we only have three possibilities. we can't color two and three both red, cuz then we'd have two reds in a row. so, we can either make two red, three black, two black, three red, or both two and three black. and all of the cases are the same. just to give one example, suppose that we colored the node two, red, and one and three are black. the claim is invariant four has been broken and invariant four is going to be broken no matter how we try to color two and three red and black. what is invariant four says? it says, really on any unsuccessful search, you pass through the same number of black nodes. and so, one unsuccessful search would be, you search for zero. and if you search for a zero, you go to the root, you immediately go left to hit a null pointer. so, you see exactly one black node. namely one. on the other hand, suppose you searched for four, then you'd start at the root, and you'd go right, and you go to two, you'd go right, and you go to three, you'd go right again, and only then will you get a null pointer. and on that, unsuccessful search, you'd encounter two black nodes, both the one and the three. so, it's a violation of the fourth invariant, therefore, this would not be a red black tree. i'll leave that for you to check, that no matter how you try to code two and three red or black, you're going to break one of the invariants. if they're both red, you'd break the third invariant. if at most one is red, you'd break the fourth invariant. so, that's a non-example of a red-black tree. so, let's look at an example of a red-black tree. one, a search tree where you can actually color the nodes red or black so that all four invariants are maintained. so, one search tree which is very easy to make red black is a perfectly balanced one. so, for example, let's consider this three nodes search tree has the keys three, five, and seven and let's suppose the five is the root. so, it has one child on each side, the three and the seven. so, can this be made a red black tree? so, remember what that question really means. it's asking can we color theses three nodes some combination of red and black so that all four of the invariants are satisfied? if you think about it a little bit, you realize, yeah, you can definitely color these nodes red or black to make and satisfy for the invariants. in particular, suppose we color all three of the nodes, black. we've satisfied variant number one, we've colored all the nodes. we've satisfied variant number two, and particularly, the root is black. we've satisfied invariant number three. there's no reds at all, so there's certainly no two reds in a row. and, if you think about it, we've satisfied invariant four because this tree is perfectly balanced. no matter what you unsuccessfully search for, you're going to encounter two black nodes. if you search for, say, one, you're going to encounter three and five. if you search for, say, six, you're going to encounter five and seven. so, all root null paths have exactly two black nodes and variant number four is also satisfied. so, that's great. but, of course, the whole point of having a binary search tree data structure is you want to be dynamic. you want to accommodate insertions and deletions. every time you have an insertion or a deletion into a red black tree, you get a new node. let's say, an insertion, you get a new node, you have to color it something. and now, all of a sudden, you got to worry about breaking one of these four invariants. so, let me just show you some easy cases where you can accommodate insertions without too much work. time permitting we will include some optional videos with the notion of rotations which do more fundamental restructuring of search trees so that they can maintain the four invariants, and stay nearly perfectly balanced. so, if we have this red black tree where everything's black, and we insert, say, six, that's going to get inserted down here. now, if we try to color it black, it's no longer going to be a red black tree. and that's because, if we do an unsuccessful search now for, say, 5.5, we're going to encounter three black nodes, where if we do an unsuccessful search for one, we only encounter two black nodes. so, that's not going to work. but the way we can fix it is instead of coloring the six black, we color it red. and now, this six is basically invisible to invariant number four. it doesn't show up in any root null paths. so, because you have two black nodes in all roots in all paths before, before the six was there, that's still true now that you have this red six. so, all four invariants are satisfied once you insert the six and color it red. if we then insert, say, an eight, we can pull exactly the same trick, we can call it an eight red. again, it doesn't participate in invariant four at all so we haven't broken it. moreover, we still don't have two reds in a row, so we haven't broken invariant number three either. so, this is yet another red black tree. in fact, this is not the unique way to color the nodes of this search tree, so that it satisfies all four of the invariants. if we, instead, recolor six and eight black, but at the same time, recolor the node seven, red, we're also golden. clearly, the first three invariants are all satisfied. but also, in pushing the red upward, consolidating the red at six and eight, and putting it at seven instead, we haven't changed the number of black nodes on any given path. any black, any path that previously went through six, went through seven, anything that went through eight, went through seven so there's exactly the same number of red and black nodes on each such path as there was before. so, all paths still have equal number of black nodes and invariant four remains satisfied. as i said, i've shown you here only simple examples, where you don't have to do much work on an insertion to retain the red black properties. in general, if you keep inserting more and more stuff and certainly if you do the deletions, you have to work much harder to maintain those four invariants. time permitting, we'll cover just a taste of it in some optional videos. so, what's the point of these seemingly arbitrary four invariants of a red black tree? well, the whole point is that if you satisfy these four invariants in your search tree, then your height is going to be small. and because your height's going to be small, all your operations are going to be fast. so, let me give you a proof that if a search tree satisfies the four invariants, then it has super small height. in fact, no more than double the absolute minimum that we conceivably have, almost two times log base two of n. so, the formal claim, is that every red-black tree with n nodes, has height o of log n, were precisely in those two times log base two of n + 1. so, here's the proof. and what's clear about this proof is it's very obvious the role played by this invariants three and four. essentially, what the invariants guarantee is that, a red black tree has to look like a perfectly balanced tree with at most a sort of factor two inflation. so, let's see exactly what i mean. so, let's begin with an observation. and this, this has nothing to do with red black trees. forget about the colors for a moment, and just think about the structure of binary trees. and let's suppose we have a lower bound on how long root null paths are in the tree. so, for some parameter k, and go ahead and think of k as, like, ten if you want. suppose we have a tree where if you start from the root, and no matter how it is you navigate left and right, child pointers until you terminate in a null pointer. no matter how you do it, you have no choice but to see at least k nodes along the way. if that hypothesis is satisfied, then if you think about it, the top of this tree has to be totally filled in. so, the top of this tree has to include a perfectly balanced search tree, binary tree of depth k - 1. so, let me draw a picture here of the case of k = three. so, if no matter how you go from the root to a null pointer, you have to see at least three nodes along the way. that means the top three levels of this tree have to be full. so, you have to have the root. it has to have both of its children. it has to have all four of its grandchildren. the proof of this observation is by contradiction. if, in fact, you were missing some nodes in any of these top k levels. we'll that would give you a way of hitting a null pointer seeing less then k nodes. so, what's the point is, the point is this gives us a lower bound on the population of a search tree as a function of the lengths of its root null paths. so, the size n of the tree must include at least the number of nodes in a perfectly balanced tree of depth k - 1 which is 2^k - 1, so, for example, when k = 3, it's 2^3 (two cubed) - 1, or 7  that's just a basic fact about trees, nothing about red black trees. so, let's now combine that with a red black tree invariant to see why red black trees have to have small height. so again, to recap where we got to on the previous slide. the size n, the number of nodes in a tree, is at least 2^k - 1, where k is the fewest number of nodes you will ever see on a root null path. so, let's rewrite this a little bit and let's actually say, instead of having a lower bound on n in terms of k, let's have an upper bound on k in terms of n. so, the length of every root null path, the minimum length of every root null path is bounded above by log base two of quantity n + 1. this is just adding one to both sides and taking the logarithm base two. so, what does this buy us? well, now, let's start thinking about red black trees. so now, red black tree with n nodes. what does this say? this says that the number of nodes, forget about red or black, just the number of nodes on some root null path has to be the most log base two of n + 1. in the best case, all of those are black. maybe some of them are red, but in the, in, the maximum case, all of them are black. so, we can write in a red black tree with n nodes, there is a root null path with at most log base two of n + 1, black nodes. this is an even weaker statement than what we just proved. we proved that it have some, somehow must have at most log based two, n + 1 total nodes. so, certainly, that path has the most log base two of n + 1 black nodes. now, let's, now let's apply the two knockout punches of our two invariants. alright, so fundamentally, what is the fourth invariant telling us? it's telling us that if we look at a path in our red black tree, we go from the root, we think about, let's say, that's an unsuccessful search, we go down to a null pointer. it says, if we think of the red nodes as invisible, if we don't count them in our tally, then we're only going to see log, basically a logarithmic number of nodes. but when we care about the height of the red black tree, of course, we care about all of the nodes, the red nodes and the black nodes. so, so far we know, that if we only count black nodes then we're good, we only have log base two of n + 1 nodes that we need to count. so, here's where the third invariant comes in. it says, well actually, black nodes are a majority of nodes in the tree. in a strong sense, there are no two reds in a row, on any path. so, if we know the number of black nodes is small, then because you can't have two reds in a row, the number of total nodes on the path is at most twice as large. in the worst case, you have a black route, then red, then black, then red, then black, then red, then black, et cetera. at the worst case, the number of red nodes is equal to the number of black nodes, which doubles the length of the path once you start counting the red nodes as well. and this is exactly what it means for a tree to have a logarithmic depth. so, this, in fact, proves the claim, if the search trees satisfies the invariants one through four, in particular if there's no two reds in a row and all root null paths have an equal number of black nodes, then, knowing nothing else about this search tree, it's got to be almost balanced. it's perfectly balanced up to a factor of two. and again, the point then is that operations in a search tree and the search trees are going to run in logarithmic time, because the height is what governs the running time of those operations. now, in some sense, i've only told you the easy part which is if it just so happens that your search tree satisfies these four invariants, then you're good. the height is guaranteed to be small so the operations are guaranteed to be fast. clearly that's exactly what you want from this data structure. but for the poor soul who has to actually implement this data structure, the hard work is maintaining these invariants even as the data structure changes. remember, the point here is to be dynamic, to accommodate insertions and deletions. and searches and deletions can disrupt these four invariants and then one has to actually change the code to make sure they're satisfied again, so that the tree stays balanced, has low height, even under arbitrary sequences of insertions and deletions. so, we're not going to cover that in this video. it can be done, without significantly slowing down any of the operations. it's pretty tricky, takes some nice ideas. there's a couple well-known algorithms textbooks that cover those details. or if you look at open source and limitations of balanced search trees, you can look at code that does that implementations. but, because it can be done in a practical way and because red black tree supports such an original array of operations, that's why you will find them used in a number practical applications. that's why balanced search trees should be part of your programmer tool box. 
in this video and the next we are going to take you to the next level and here under the hood into implementations of balanced pioneer research trees. now frankly when any great details of all balance binary research tree implementations get pretty complicated and if you really want to understand them at a fine grain level there is no substitute for reading an advanced logarithms textbook that includes coverage of the topic and/or studying open source implementations of these data structures. i don't see the point of regurgitating all of those details here. what i do see the point in doing, is giving you the gist of some of the key ideas in these implementations. in this first video i want to focus on a key primitive, that of rotations which is common to all balanced by the [inaudible] of limitations. whether is red, black trees, evl trees, b or b+ trees, whatever all of them use rotations. in the next video we'll talk a little bit more about the details of red, black trees in particular. so, what is the points behind these magically rotation operations? well the goal is to do just constant work, just re-wire a few pointers, and yet locally re-balance a search tree, without violating the search tree properties/g property. so there are two flavors of rotations, left rotations and right rotations. in either case, when you invoke a rotation it's on a parent child pair, in a search tree. if it's a right child of the parent, then you use a left rotation. we'll discuss that on this slide. and a right rotation is, in some sense, an inverse operation which you use when you have a left child of the parent. so what's the generic picture look like when you have a node x in a search tree and it has some right child y? well x, in general, might have some parents. x might also have some left subtree. let's call that left subtree of x, a. it could, of course, be and then y has it's 2 subtrees, lets call the left subtree of y b and the right subtree c. it's going to be very important that rotations preserve the search tree property so to see why that's true lets just be clear on exactly which elements are bigger then which other elements in this picture. so first of all y being a right child of x, y's going to be bigger than x. now all of the keys which line the subtree a because they're to the left of x these keys are going to be even smaller than x. by the same token anything in the subtree capital c, that's to the right of y so all that stuff's going to be even bigger than y. what about sub tree capital b? what about the nodes in there? well, on the one hand, all of these are in x's right sub tree, right? to get to any node in b, you go through x and then you follow the right child to y. so that means everything is b is bigger than x, yet, at the same time, it's all in the left sub tree of y so these are all things smaller than y. summarizing all of the nodes in b have keys strictly in between x and y. so now that we're clear on how all of these search keys in the different parts of this picture relate to each other, i can describe the point of a left rotation. fundamentally the goal is to invert the relationship. between the nodes x and y. currently x is the parent and y is the child. we want to rewire a few pointers so that y is now the parent and x is the child. now what's cool is given that is our goal is pretty much a unique way to put all these pieces back together to accomplish it. so lets just follow our nose. so remember the goal y should be the parent and x should be the child. well, x is less then and why and there's nothing we can do about that. so if x is going to be a child of y, its got to the left child. so your first question might be well what happened that x is parent. so x use to have some parent lets call it p and x's new parent is y. similarly y used to have parent x and there's a question what should y new parent be? well y is just going to inherit x's old parent p. so this change has no bearing for the search tree property. either of this collection of nodes was p's left sub tree, in that case all these nodes were less than p, or this sub tree was p's right sub tree which in that case all of these are bigger than p, but p can really care less , which of x or y is it's direct descendant. now lets move on to thinking about how...what we should do with the sub-trees a, b and c. so, we have 3 sub-trees we need to re-assemble into this picture, and fortunately we have 3 slots available. x has both of its child pointers available and y has its right child available. so what can we put where? well, a, is the sub tree of stuff which is less than both x and y. so, that should sensibly be x's left child. that's exactly as it was before. by the same token, capital c is the stuff bigger than both x and y, so that should be, y, the bigger nodes child, right child just as as before. and what's more interesting is what happens to subtree capital b. so b used to be y's left child but that can't happen anymore, 'cause now x is y's left child. so, the only hope is to slot capital b into the only space we have for it, x's right child. fortunately for us this actually works, sliding b into the only open space we have for it. x's right child does indeed preserve the switch tree property. recall we notice that every key in capital b is strictly between x and y, therefore it better be and x's right sub tree and it better be in y's right sub tree, but it is, that's exactly where we put it. so that's a left rotation, but if you understand a left rotation then you understand a right rotation as well. because a right rotation is just the inverse operation. so that's when you take a parent child pair, where the child is the left child of the parent, and now you again want to invert the relationship. you want to make the old child the new parent and the old parent the new child. and once again given this goal there's really a unique way to reassemble the components of this picture so that the goal's accomplished, so that y is now the parent of x. so what are the laudable properties of rotations? well first of all i hope it's clear that they can be implemented. in a constant time or you were doing a rewiring a constant number of pointers. further more as have discussed they preserve the search tree property. so these nice properties are what make rotations the ubiquitous primitive common to all balanced search tree implementations. so this of course, is not the whole story. in a complete specification of a balanced search tree implementation, you have to say exactly when and how you deploy these rotations. you'll get a small taste of that in the next video but if you really want to understand it in more depth, i again encourage you to check out either a comprehensive data structures textbook. check out a number of balance search tree demonstrations, which are readily available on the web. or have a peek at an open source implementation of one of these data structures. 
for this final video on binary search trees i want to talk a little bit about implementation, implementation details for the red black tree data structure in particular the insertion operation. as i've said in the past it really doesn't make sense for me to spell off all of the gory details about how this is implemented. if you want to understand them in full detail. detail you should check out various demonstrations readily available on the web, or a comprehensive textbook, or an open source implementation. red black trees, you'll recall satisfy four invariants and the final two invariants in particular ensure that the red black tree always has logarithmic height and therefore all of the supported operations run in logarithmic time. the problem is we've got to pay the piper. whenever we have a operation that modifies the data structure, it potentially destroys one or more of the invariants, and we have to then restore that invariant. without doing too much work. now amongst all of the supported operations there are only two that modify the data structure insertion and deletions. so from thirty thousand feet the approach to implementing insert and delete is to just implement them as if it's a normal binary search tree as if we didn't have to worry about these invariants and then if an invariant is broken we try to fix it with minimal work and two tools that we have our disposal to try to restore an invariant are first of all. recoloring, flipping the color of nodes from to black and second of all left and right rotations as covered in the previous video. my plan is to discuss the insertion operation not in full detail but i'll tell you about all of the key ideas. now deletion you got to remember that even in a regular binary search tree deletion is not that trivial and in a red black tree its down right painful. so, that i'm not going to discuss onto for you to text books or online resources to learn more about deletion. so here's how insert is going to work. so suppose we have some new node with the key x. and we're inserting it into a red black tree. so we first just forget about the invariance, and we insert it as usual. and remember, that's easy. all we do is follow left and right shot pointers, until we fall off the end of the tree until we get to a null pointer, and we install this new node with key x, where we fell off the tree. that makes x a leaf in this binary search tree. let's let y denote x's parent, after it gets inserted. now in a red-black tree every node has a color. it's either red or black. so we have a decision to make. we just added this new node with key x and we gotta make get either red or black. and we're sort of between a rock and a hard place, whichever color we make it we have the potential of destroying one of the invariants. specifically, suppose we color it red. well remember what the third invariant says, it says you cannot have two reds in a row. so if y, x's new parent is already red, then when we color x red, we have 2 reds in a row. and we've broken invariant number 3. on the other hand, if we color this new node, x, black, we've introduced a new black node to certain root null paths in this tree. and remember, the 4th invariant insists, that all the root null paths have exactly the same number of black. notes, so by adding a black note to some but not all of the paths, we're in general, going to destroy that invariant, if we color x black. so what we're going to do is, we're going to choose the lesser of two evils, and in this context the lesser of the two evils is to color x red. again, we might destroy the third invariant, we'll just deal with the consequences later. so why you ask, is coloring x red and destroying the third invariant, the lesser of two evils? well, intuitively, it's because this invariant violation is local. the flaw in our not quite red black tree is small and manageable, it's just a single double red and we know exactly the word is it's x and y. so.this sort of more hope in squashing it with minimal work. i can't trust if we coated x black then we violated this much more global type of property involving all of the route in all paths and that's a much more intimidating violation to try to fix. then just as local one of having a double red between x and it's parent. indeed some of the time we'll just get lucky and it will just so happen that x is parent y is colored black and then we're golden. this new node x that's colored red, it doesn't create a double red, there's no other violations of the other invariants and so boom, we've got a new red black tree and we can stop. so, the tricky case then is when x's parent y is also red in this case we do not have a red, black tree we have a double red and we have to do some more work to restore the third invariant. so suppose y is red. what do we then know? well remember, before we inserted x, we had a red black tree, all 4 of the invariants were satisfied. so therefore y, by virtue of being red, it could not have been the root. it has to have a parent. let's call that parent w. moreover by the third invariant there was no double red in this tree before we inserted x so by virtue of y being red, it's parent w must have been black. so, now the insertion operation branches into 2 different cases and it depends on the color, on the status of w's other child. so in the first case we're going to assume that w's other child that is not y but the other child of w exists in its colored red. in the second case, we're going to treat when w either doesn't have a second child. y is its only child or when its other child is colored black. so let's recap where things stand. so we just inserted this new node, and it has the key x. and our algorithm colored this node red. so x is definitely red. now, if it's parent y was black, we already halted. so we've already dealt with that case. so now, we're assuming that y. x's parent is also red, that's what's interesting. now by virtue of y being red, we know that y's parent, that is x's grandparent w, has to be colored black. and, for case two of insertion, we are assuming that w has a second child, call it z, and that z is colored red. so, how are we going to quash this double red problem? we again, we have 2 tools at our disposal. one is to re-color nodes. the second is to do rotations. so for case 1, we're only going to actually have to do re-coloring. we're not even going to have to bust out per rotations. in particular what we're going to do is, we're going to recolor z and y black and we're going to recolor w red. so, in some sense we take the reds that are at z and y and we consolidate them at w. the important property of this recovering is that it does not break the fourth invariant, remember the forth invariant says that no matter which path you take from the root to a no pointer you see exactly the same number of black nodes. so why is invariance still true after this recoloring, well for any path from a route to a no pointer which doesn't go through the vertex w its relevant. none of these nodes are on that path, so the number of black dots is exactly the same. so think about a path which does go through w. well if it goes through w to get to a no pointer has to go through exactly one of z or y. so before we did the recoloring this path picked up a black node via w and it did not pick up a black node via z or y both of those were red. now any such path does not pick up a black node w that's now red but it does pick up exactly one black node either z or y. so, for every single path in the tree, the number of black nodes it contains is exactly the same before or after this recoloring, therefore since the fourth invariant held previously, it also holds after this recoloring. the other great thing is that it seems like we've made progress on restoring the third invariant. the property that we don't want any double-reds at all in the entire tree. remember, before we did this recoloring, we only had a single double-red. it involved x and y. we just recoded y from red to black. so certainly we no longer have a double reded walling x and y and that was the only one in the tree. so are we done, do we now have a bonafied red black tree? well the answer depends, and it depends on the core of w's parent. so remember w just got recolored from black to red. so there's now a possibility that w being this new red node participates in some new double red violation . now w's children, z and y, are black. so those certainly can't be double reds. but w also has some parent, and if w's parent is red, then we get a double red involving w and its parent. of course, if w's parent was black, then we're good to go. we don't get a double red by recoloring double. w red, so we have no w reds in the tree, and we can just stop. summarizing, this recoloring preserves the fourth invariant, and either it restores the third invariant, or if it fails to restore the third invariant, at least it propagates the double red violation upward into the tree, closer to to the root.. we're perfectly happy with the progress represented by propagating the double red upward. why? well, before we inserted this new object x, we had a red black tree. and we know red black trees have logarithmic height. so the number of times that you can propagate this double red upward is bounded above by the height of the tree, which is only logarithmic. so we can only visit case 1 a logarithmic number of times before this w is propagated all the way to the top of the tree, all the way of the root. so we are not quite done, the one final detail is what happens when this recoloring procedure actually recolors the root. so, you could for example look at this green picture on the right side and ask, well what if w is actually the root of this red black tree and we just recolored it red? now notice in that situation where the, we are dealing with the root of the tree we're not going to have a double red problem. so invariant three is indeed restored when we get to the top of the tree, but we have a violation of invariant number two which states that the root must always be black. well if we find ourselves in this situation, there's actually a super simple fix which is this red root, we just recolor it black. now clearly that's not going to introduce any new double reds. the worry instead is that it breaks invariant four. but, the special property of the root for text is that it a lies exactly once on every route on all path. so if we flip the color of the roof from red to black it increases the number of black nodes on every single routinal path by exactly 1. so if they all have the same number of black nodes before, they'll have the same number of black nodes now, after the recoloring. that completes case 1 of how insertion works. let's move on to case 2. so case 2 gets triggered when we have a double red and the deeper node of this double red pair, call it x, its uncle, that is if it has grandparent w, parent y and w's other child, other than y either. doesn't exist or if it exists it's labeled it's colored black. that is case 2. i want to emphasize you might find yourself in case 2 right away when you insert this new object x it might be there immediately it has some uncle which is covered x or it might be that if already visited case 1 a bunch of times propagating this double red up the tree and now at some point. the deeper red node x has a black uncle. either way, as soon as that happens, you trigger case 2. well it turns out, case 2 is great in the sense that, with nearly constant work, you can restore in variant number 3 and get rid of the double red without breaking any of the other invariants. you do have to put to use both of the tools we have available in general. both recolorings and rotations, left and right rotations, as we discussed in the previous video. but, if you do just a constant number of each, recolorings and rotations, you can get all four of the invariants simultaneously. there are unfortunately a couple of sub cases depending on exactly the relationships between x, y, z, and w. for that reason i'm not going to spell out all the details here, check out a textbook if you're interested, or, even better, work it out for yourself. now that i've told you that two to three rotations plus some recolorings is always sufficient in case two to restore all of the in variance, follow your nose and figure out how it can be done. so let's summarize everything that we've said about how insertion works in a red black tree. so, you have your new node with key x, you insert it as usual. so you make it a leaf, you tentatively color it red. if it's parent is black, your done. you have a red black tree, and you can stop. in general, the interesting case is this new. and you know that x's parent is red. that gives you a double-red of violation of invariant three. now, what happens is you visit this case 1, propagating this double red upward imagery. this upward propagation process can terminate in one of three ways. first of all, you might get lucky and at some point the double-red doesn't propagate, you do the recoloring in case 1. and it just so happens you don't get a new double red. at that point you have a red black tree and you can stop. the second thing that can happen is the double-red propagation can make it all the way to the root of the tree, then you can just recolor the root black and you can stop with all of the invariants satisfied. alternatively at some point when you're doing this upward propagation you might find yourself in case 2 as was discussed on this slide. where the lower red node on the double red pair x has a black or non-existent uncle, z. in that case, with constant time, you can restore all of the fourier theories. so the work done overall is dominated by the number of double red propagations you might have to do, that's bounded by the height of this tree and that's bounded by o of log n. so in all of the cases you restore all 4 invariants, you do only a logarithmic amount of work, so that gives you a logarithmic insertion operation for red black trees, as promised. 
in this video we'll begin our discussion of hash tables; we'll focus first on the support operations, and on some of the canonical applications. so hash tables are insanely useful. if you want to be a serious programmer or a computer scientist you really have no choice but to learn about hash tables. i'm sure many of you have used them in your own programs in the past in fact. now on the one hand what's funny is they don't actually do that many things in terms of the number of supported operations, but what they do, do they do really, really well. so what is a hash table? well conceptually, ignoring all of the aspects of the implementation, you may wanna think of a hash table as an array. so one thing that arrays do super well is support immediate random access. so if you're wondering what's the position number seventeen of some array, boom, with a couple of machine instructions you can find out, wanna change the contents of position number 23 in some array? done, in constant time. so let's think about an application in which you want to remember your friends phone numbers. so if you're lucky your friends parents were all u nu, unusually unimaginative people and all of your friends names are integers let's say between one and 10,000. so if this is the case then you can just maintain an array of link 10,000. and to store the phone number of say, your best friend, 173, you can just use position 173 of this modest sized array. so this array based solution would work great, even if your friends change over time, you gain some here you lose some there, as long as all your friends names happen to be integers between 1-10,000. now, of course, your friends have more interesting names: alice, bob, carol, whatever. and last names as well. so in principal you could have an array with one position in the array for every conceivable name you might encounter, with at least 30 letters set. but of course this array would be way too big. it would be something like 26 raised to the thirtieth power and you could never implement it. so what you'd really want is you'd want an array of reasonable size, say, you know ballpark the number of friends that you'd ever have, so say in the thousands or something, where it's positions are indexed not by the numbers, not integers. [inaudible] between one and 10,000, but rather by your friends names and what you'd like to do is you'd like to have random access to this array based on your friend's name. so you just look up the quote unquote alice position of this array and. boom, there would be alice's phone number in constant time. and this, on a conceptual level is basically what a hash table, can do for you. so there's a lot of magic happening under the hood of a hash table and that's something we'll discuss to some extent in other videos. so you have to have this mapping between the keys that you care about, like your friends' names, and, numerical positions of some array. that's done by what's called a hash function, but properly implemented, this is the kind of functionality that hash tables gives you, so like an array with its positions indexed by the keys that you're storing. so you can think of the purpose of the hash table as to maintain a possibly evolving set of stuff. where of course the set of things that you're maintaining, you know, will vary with the application. it can be any number of things. so if you're running an e-commerce website, maybe you're keeping track of transactions. you know, again, maybe you're keeping track of people, like for example, your friends and various data about them. so maybe you're keeping track of i-p addresses, for example if you wanna know, who was, were there unique visitors to your websites. and so on. so a little bit more formally, you know, the basic operations, you need to be able to insert stuff into a hash table. in many, but not all applications, you need to be able to delete stuff as well. and typically the most important operation is look-up. and for all these three operation you do it in a key based way. where as usual a key should just be a unique identifier for the record that you're concerned with. so, for example, for employees you might be using social security numbers. for transactions you might have a transaction id number. and then ip addresses could act as their own key. and so sometimes all you're doing is keeping track of the keys themselves. so, for example, in ip addresses, maybe you just want to remember a list of ip addresses. you don't actually have any associated data but in many applications, you know, along with the key, is a bunch of other stuff. so along with the employee's social security number, you gotta remember a bunch of other data about that employee. but when you do the insert, when you do the delete, when you do the look up, you do it based. on this key, and then for example, on look up you feed the key into the hash table and the hash table will spit back out all of the data associated with that key. we sometimes hear people refer to data structures that support these operations as a dictionary. so the main thing the hash table is meant to support is look up in the spirit of a dictionary. i find that terminology a little misleading actually. you know, most dictionaries that you'll find are in alphabetical order. so they'll support something like binary search. and i want to emphasis something a hash table does not do is maintain an ordering on the elements that it supports. so if you're storing stuff and you do want to have order based operations, you wanna find the minimum or the maximum, or something like that, a hash table's probably not the right data structure. you want something more. you wanna look at a heap or you wanna look at a, a search tree. but for applications in which all you have to do is basically look stuff up you gotta, you gotta know what's there and what's not, then there should be a light bulb that goes off in your head. and you can say, let me consider a hash table, that's probably the perfect data structure for this application. now, looking at this menu-supported operations, you may be left kinda unimpressed. alright, so a hash table, in some sense, doesn't do that many things; but again, what it does, it does really, really well. so, to first order. what hash tables give you is the following amazing guarantee. all of these operations run in constant time. and again this is in the spirit of thinking of a hash table as just like an array. where its positions are conveniently indexed by your keys, so just like an array supports random access in constant time, you can see if, you know, there's anything in the array position, and what it is. as similarly a hash table will let you look up based on the key in constant time. so what is the fine print? well, there's basically two caveats. so the first thing is that hash tables are easy to implement badly. and if you implement them badly you will not get this guarantee. so this guarantee is for properly implemented hash tables. now, of course if you're just using a hash table from a well known library, it's probably a pretty good assumption that it's properly implemented. you'd hope. but in the event that you're forced to come up with your own hash table and your own hash function and unlike many of the other data structures we'll talk about, some of you probably will have to do that at some point in your career. then you'll get this guarantee only if you implement it well. and we'll talk about exactly what that means in other videos. so the second caveat is that, unlike most of the problems that we've solved in this course, hash tables don't enjoy worst case guarantees. you cannot say for a given hash table that for every possible data set you're gonna get cost and time. what's true is that for non-pathological data, you will get cost and time operations in a properly implemented hash table. so we'll talk about both of these issues a bit more in other videos, but for now just high order bits are, you know, hash tables, constant time performance, subject to a couple of caveats. so now that i've covered the operations that hash tables support and the recommend way to think about them, let's turn our attention to some applications. all of these applications are gonna be in some sense, you know, kinda trivial uses of hash tables, but they're also all really practical. these come up all the time. so the first application we'll discuss, which again is a conical one, is removing duplicates from a bunch of stuff, also known as the deduplication problem. so in the de-duplication problem, the input is essentially a stream of objects. where, when i say a stream i have kinda, you know two different things in mind as canonical examples. so first of all you can imagine you have a huge file. so you have, you know, a log of everything that happened on some website you're running. or all of the transactions that were made in a store on some day, and you do a pass through this huge file. so you're just in the middle of some outer for loop going line by line through this massive file. the other example of a stream that i had in mind, is, where you're getting new data over time. so here, you might imagine that you're running software to be deployed on an internet router. and data packets are coming through this router at a constant extremely fast rate. and so you might be looking at, say, the ip addresses and the sender, and use your data packet which is going through your router. so it would be another example of a stream of objects. and now, what do you gotta do? what you gotta do is you gotta ignore the duplicates. so remember just the distinct objects that you see in this stream. and i hope you find it easy to imagine why you might want to do this task in various applications. so, for example, if you're running a website you might want to keep track of the distinct visitors that you ever saw in a given day or a given week. if you're doing something like a web crawl, you might want to identify duplicate documents and only remember them once. so, for example, it would be annoying if in search results both the top link and the second link both led to identical pages at different urls, okay, so search engines obviously want to avoid that, so you want to detect duplicate web pages and only report unique ones. and the solution using a hash table is laughably simple. so every time a new object arrives in the stream, you look it up. if it?s there, then it?s a duplicate and you ignore it. if it?s not there, then this is a new object and you remember it. qed, that's it. and so then after the string completes, so for example after you finish reading some huge file, if you just want to report all of the unique objects, hash tables generally support a linear scan through them and you can just report all of the distinct objects when this stream finishes. so let's move on to a second application slightly less trivial maybe but still quite easy, and this is the subject of programming projects number five. so this is a problem called the two sum problem. you're given as input an array of n number. these images are in no particular order. you're also given a target sum, which i'll call t. and what you want to know is are there two integers from amongst these n you are given that sum to t. now the most obvious and naive way to solve this problem is just to go over all n, choose two pairs of integers in the input, and check each one separately. so that's clearly a quadratic time algorithm. but now, of course, we need to ask, can we do better? and, yes, we can. and first of all let's see what you'd do if you couldn't use any data structures. so if you were clever, but you didn't use any data structures like a hash table, here would be a reasonable improvement over the naive one. so the first step of a better solution is to sort a upfront, for example, using word sort or heap sort, something that runs in end log and time. so you may be asking about the motivation for sorting. well, again, you know, one thing is just, you know whenever you're trying to do better than n squared; you might think that sorting your data somehow helps. right and you can sort of do it almost for free in n log n time. now, why would sorting the array up front help us? well, then the clever insight is that for each entry of the array a, say the first entry, now we know what we're looking for to achieve this given target, right. if the target that we're trying to get to is summed to 100 and the first entry in the sorted array is 43, then we know we're looking for a 57 somewhere else in. this now sorted array. and we know that searching a sorted array is pretty easy, right. that just binary search. that just takes logarithmic time. so for each of the n array entries, we can look for a complementary. entry, namely of reach x we can look for t - x using binary search. and to use binary search takes log n time. so the sorting upfront speeds up this entire batch of n searches. so that's why it's a win. so, in the second step, because we do a linear number of binary searches, again, this is just n, the number of searches, times log-n, the time per search. so, this is just another theta of n log n factor. alright, so that's pretty cool. you, i don't think you could come up with this n log n solution without having some basic, facility with algorithms. this is already a really nice improvement over the naive n squared. but we can do even better. it is no reason we're stuck with an n log n lower bound for the [inaudible] problem. obviously, because the array is unsorted, we have to look at all the integers. so we're not gonna do better than linear time. but we can do linear time via a hash table. so a good question you might ask at this point is what's the clue about this problem, about this task that suggests we want to use a hash table. well, so hash tables are going to dramatically speed up any application where the bulk of the word is just repeated look-ups. and if we examine this n log n solution, once we have this idea of doing a search for t minus x for each value of x, we realize actually, you know, the only thing we needed the sorted array for was to support look-ups. that's all binary search here is doing, is just looking stuff up. so we say, ah-ha. all of the work here in step two is from repeated look-ups. we're paying an exorbitant relatively, logarithm per amount of time per look-up, whereas hash tables can do them in cost and time. so, repeated look-ups, ding, ding, ding, let's use a hash table; and indeed that's what gives us linear time in this problem. so from the amazing guarantee of hash tables, we get the following amazing solution for the true [inaudible] problem, although again this is subject to the same fine print about you better use it properly implemented hash table and you better not have pathological data. so rather than sorting, you just insert everything in the array into a hash table. so insertions cost time. so this is gonna be linear time rather than the end log [inaudible] we were paying before. once all the stuff is in the hash table, we just do the same thing as in the n log-n solution. for each x in the array, we look for its matching elements, t-x in the hash table using the cost and time look-up operation exported by the hash table. and of course if for some x, you do find the matching element t minus x. then you can just report x and t minus x. that proves that there is indeed a pair of integers of target sum t. if for every single element of the input array a, you fail to find this matching element t minus x in the hash table. then, for sure there is no pair of integers in the input that sums to t. so this solves the problem correctly. moreover, constant time insertion, so that means this first step is going to be o of end time. and constant time look-up. so that means that the second step is also gonna be linear time. that leaves subjects to the caveats that we discussed on the previous slide. so it's kind of amazing how many different applications of computer science boil down in their essence to repeated look up operations. therefore, having a super fast look up operation, like that supported by a hash table, permits these applications to scale to fantastic sizes. it's really amazing, and it drives a lot of modern technology. so let me just mention a couple examples. again, if you look around or do some research on the web, you'll quickly find many more. so originally what prompted researchers to think hard about data structures that support super fast look ups, was back when people were first building compilers. so this is a long time ago. this is in the fifties or so. and these repeated look ups to figure out, you know, what has and has not been defined before was, was emerging as a bottleneck in compilers. back in the early days of programming languages. and that was one of the early applications of hash tables. was to support super fast look ups to speed up compile time. to keep track of the function of variable names and things like that. hash table technology is also super useful for software on routers in the internet. so, for example, you might want to block network traffic from certain sources. so, for example, maybe you suspect that a certain ip address has been taken over by spammers and so any traffic coming from that ip address you just want to ignore. and you don't wanna even let it get to the end host, to the computer on someone's desktop, or to someone's mobile device but rather inside the internet. you wanna just drop packets that are coming certain, certain centers. so what is that problem boil down to? well, you might have a blacklist of ip addresses that you're refusing traffic from and then the tasks faced by the router is really the look up problem. so if data packet comes in at some insanely fast data rate, and when you wanna. you immediately, just look up, is this in the blacklist or not, and if it is in the blacklist then you drop the packet, if it?s not, then you let it go through. so a very different application is for speeding up search algorithms. and when i say a search algorithm, what i'm thinking about here is something like a chess playing program. so something that does game tree exploration. so we've already talked a fair amount about graph search in this class, but in our discussion of breadth first and depth first search, we were thinking about graphs that you could basically write down. you could store them in the main memory of your machine or, in the worst case, on some big cluster. so maybe graphs, you know, about the size of the web graph or possibly smaller. but in a context of something like a chess playing program the graph that you're interested in is way, way, way bigger than the web graph. so what's the graph we care about for a chess playing program? well, the nodes of the graph are going to correspond to all possible configurations of chess pieces on a chess board. so every chess board that you might ever encounter in a game of chess. so that's a. massive, massive number of configurations. and you're never gonna be able to write down these vertices. the edges in this graph are going to take you from one configuration to another. and there gonna correspond to legal moves. so if you can move a bishop from. one place to another place, and you get from one configuration to another configuration, there's an edge in the graph corresponding to that move. now you can't write down this graph. so you can't implement breadth versus depth versus search exactly as we discussed it before. but, you'd still like to do graph exploration, right? so you'd like to have your computer program, reason about the at least short term ramifications of your possible next move. so that will correspond to searching through this graph. now, how are you gonna, it's remembering graphs search a really important property was you don't want to do redundant work, you don't want to re-explore things you've already explored. that would be silly and might lead into loops and so on. and you can't write down the graph just remembering where you've been, is suddenly a non-trivial problem; but what is remembering where you've been, fundamentally? fundamentally that's a look up operation. so that is exactly what hash tables are for. so to be a little more concrete, you know, one where you use the hash table in, say, a chess playing program, is you'd stake, take the initial configuration. you would sort of imagine trying all possible moves from this configuration. and then you'd try, you'd sort of have all moves from your opponent and then you'd have all your moves in response. and you would always remember, as you were doing this reasoning, every chessboard configuration you'd ever looked at before and you'd stick in the hash table. and before you go exploring some configuration, you'd look it up in your hash table to see if you've already explored it in the past. and if you have, you don't bother. you've already seen it. all right. so chess playing programs operate by exploring systematically as many configurations as they'd have time for. you know, obviously, in a budget of three minutes or whatever you don't wanna waste any work exploring any given configuration more than once. how do you remember where you've been? well everything you've explored you stick in a hash table before you explore a configuration you look it up in a hash table and see if you've already done it. so these of course are just scratching the surface. i just wanted to highlight a couple, you know, fairly different looking applications, you know to convince you that hash tables come up all the time. and the reason they come up all the time is because you know the need for fast look-ups comes up all the time. it's kind of amazing how much technology is being driven just by you know repeated fast look-ups. so as homework i encourage you to just sort of think about you know your own life, or think about technology out there in the world, and come up with some. you know, guesses about where probably hash tables are making something out there running blazingly fast. i think it won't take you more than a few minutes to come up with some good examples. 
so in this video we'll take a peek under the hood of hash functions. and i'll discuss some of the high level principles by which their implemented. so let's briefly review the raison d'etre of a hash table. so the purpose in life for a hash table is to support super-fast lookups. so maybe you're keeping track of the transactions that happened on your website yesterday. maybe you're keeping track of your employees; maybe you're keeping track of ip addresses in an internet router. maybe you're keeping track of chess configurations in a, in a chess-playing program, whatever, the point is, you want to be able to insert stuff into a hash table, and later remember whether something's there or whether something's not there. so the implementations we'll discuss will generally also support deletions. but that's pretty much it. it's a very restricted set of operations. but the hash, it was going to execute them at very, very well. so, basically in constant time, again subject to some fine print, which we'll discuss a little bit in this video but, then more deeply in a separate optional video. so, the two caveats are first of all the hash table had better be properly implemented. it's actually pretty easy to screw up a hash table to screw up hash functions. we'll talk a bit about that in a few minutes and, then, also, the data should, in some sense, be non-pathological, and that, we will discuss more deeply in a separate video. alright, so let me give you an initial glimpse of some of the magic that's happening under the hood in hash functions. so, at first let me say exactly what the setup is. the first step is to identify all the things that you might want to be storing. so, in other words, the universe of your application, so this would be something like, all possible i-p addresses, of which there's 2^32 . all possible names you might encounter, perhaps with a maximum of, say, 30 characters. all possible configurations of a chessboard, and so on, and one thing i hope you can appreciate from these examples is that, in many cases, this universe is really big. sothe number of ] ip address is, quote unquote, only two 2^32. the number of all names, you're probably talking more like 26 raised to the 30. all chessboard configurations i don't even wanna think about. and what you wanna accomplish is, you wanna maintain an evolving subset of this universe u. so maybe you wanna keep track of all the ip addresses you've seen on your website in the last 24 hours. you wanna keep track of the phone numbers of all of your friends. you wanna keep track of the chessboard configurations that you've explored in the past three seconds, whatever. and again i hope what is clear from the applications we've been discussing, is that the set s is usually of a reasonable size. it's, it's something you could store in main memory. you know, it maybe it's tens of thousands of ip addresses. maybe it's, you know, a few hundred names of your various friends. you know, maybe it's in the, you know, millions of chessboard configurations, but still way, way, way smaller than the size of the universe. so without data structures, you'd have to resort to other unsatisfactory solutions to maintaining this set. so the first thing you could try, as we discussed in the previous video, would be just have an array with one position for every imaginable thing you might want to store in your set. so this is the solution that's going to work well if all of your friends happen to have names that are integers between 1 and 10,000, but doesn't scale when the universe size becomes really big, as in most of these applications. so, the good news is, is of course, is an array of and it's of course fast random access so you can access any position in constant time. so, if you have an array base solution index by all the elements of the universe, you can do constant time, insert, delete and look up. the bad news is, is the space requirement is proportional to the universe. and again, forget about being unsatisfactory. that's just literary impossible. infeasible in many applications in which you'd use hash tables.  now of course to get the memory proportional to the size of the set stuff that you're storing, an easy solution would just be to use a list. you know, say a doubly-linked list. something like that. now with a list-based solution the good news is, is your memory is certainly proportional to the size of the set that you're storing, and independent of the size of the universe from which these elements are drawn. the bad news is that to figure out whether something is, or is not, in a list you generally have to traverse through most of that list. and that's gonna take up time proportional to the length of the list. so, really the question we're faced in implementing cache table is, can we get the best of both worlds, of these two naive solutions. and the one hand, we want to have the constant time operations enjoyed by the array based solution. but on the other hand, we wanna have the, linear space in the size of the set that we're storing; that we get in the list based solution. so to get the best of both worlds, we are going to use an array based solution. but the array will not be big. it'll not be with size proportional to the universe. the array will only have size, you know, roughly the same as the set that we're storing, so somewhere in the ball park of the cardinality of s. so the first thing we do is we decide on how big we want our array to be. so that, that length is gonna be called n. we're gonna have an array of length n. and n is gonna be in the ballpark of the size of s. it's gonna depend on a few things. exactly how n compares to s, but for now think of n as like double the size of s. we're gonna be calling each entry of the array a bucket, so there's n buckets, and then, the size of s is about 50 percent of the number of buckets, let's say. so one objection you might legitimately raise at this point is, you know i thought, i said the set was dynamic. the set s. right? stuff can be added, stuff can be deleted. so the size isn't always the same. it can fluctuate over time. so what does it mean to define an array which is the, roughly the same length as this changing set. so for simplicity, for the purposes of this video to focus on the key points i am going to assume that the set size s. while s itself can be changing, i'm going to assume that the size of s doesn't fluctuate too much. so there are additional bells and whistles you can add to a hash table implementation, and they're all quite natural. i think most of you could probably figure them out on your own, to deal with the fact that s might be changing sizes. so for example, you can just keep track of how many elements are in your hash table. and when it exceeds a big, a certain threshold, so when it's too big relative to the size of your array, you just double the array. and then you reinsert all of the elements into this new doubled array. similarly, if you want to, if the set shrinks, you can have tricks for shrinking the array dynamically as well. so i'm not gonna discuss these bells and whistles for resizing your hash table dynamically. they are, of course important for a real implementation, and they are part of the implementations in the standard programming libraries. but i view those as sort of a, a second order point in the implementation of a hash table. and i wanna focus on the first order points, in this video. so, summarizing, think of the set s. there are insertions and deletions we have to accommodate. but, you know, s is gonna be roughly the same size. and the number of buckets will be, you know, within a constant factor of the size of the set. all right so there we have our array with totally reasonable space, space proportional to the size of the set that we are storing. and now what we want is we want is some way of translating between the things that we care about, say our friends names or whatever the elements in the universe are to the positions in this array. so the object responsible for that translation from keys drawn from this universe to positions in this array is called a hash function. so formally, a hash function takes as input a key. so this is gonna be an ip address or the name of somebody or a chessboard configuration or whatever. and it's going to spit out an position in this array. so i'm gonna label the array entries from 0 to n-1 for this lecture. obviously at the moment this is super underspecified. there's a zillion functions you could choose. which one you use, we'll talk about that, but for now there's just gonna be some hash function mapping from elements of the universe to buckets, to positions in this array. now, as far as the semantics of this hash function, what the hash function is doing, it's telling us in which position we should store a given key from the universe. so, if we have some new friend named alice.  and we run alice, we key alice through the hash function and it gives us a 17. it says we should store alice's phone number in position 17 of the array. if we have some crazy chessboard configuration, we feed it into a hash function and it spits out 172, it says we should remember this chessboard configuration in the 172nd bucket of this array. so again, given x, which is some key from this universe, we invoke a hash function to get a position in this array, to get a bucket. and then that is where we try to store this x and any associated data with it. so that's the high leveled idea of how you implement a hash table, but we're quite far from done, and in particular there is a serious issue, that we're going to have to deal with, that's fundamental to implementing hash tables, and that's the notion of a collision. so probably many of you may have already noticed that this problem might occur. which is well what happens if we're storing our friend's phone numbers, and you know alice shows up and we ask our hash function where to store alice's phone number, and it says oh bucket number 17, and then our friend bob shows up, and we ask our hash function where to store bob's phone number, and what if the hash function also says bucket number 17 for bob? what do we put in bucket at 17? do we put alice there, do we put bob there, do we put them both there? how do we deal with these so-called collisions? so, the next quiz is meant to give, to get you thinking about collisions, and in some sense, how truly unavoidable they really are. [sound], [sound] all right. so the correct answer to this question is the first answer, believe it or not. all you need is 23 people in a room before you're equally likely to have two people with the same birthday as not. so if you're looking to, to skim a little money off of your non-mathematical friends, this is one way you can do it. go to cocktail parties with about 40 people and place bets with people that there are two people in the room with the same birthday. so if you have 367 people, well there's only 366 distinct birthdays, i'm counting february 29th here as one of them. so by the pigeonhole principle, certainly the probability is 100%. by the time you get to 367. now, by the time you're at 57. you're already at 99%. so you already have overwhelming probability to have a duplicate birthday with 57 people. so of course, with 184 you're gonna be almost at 100%, 99.99. who knows? some large number of 9's, and at 23, you're at 50%. so many people find this quite counter-intuitive that you only need 23 people to get a duplicate birthday on average. and so this is a, this is a quite famous example and it sometimes goes by the birthday paradox. calling it a paradox is sort of a misnomer. a paradox, you know, often suggests some kind of logical inconsistency. there's no logical inconsistency here. it's just that people's brains are not really wired to have this intuition, for whatever reason. so, but it's really just math. you can work out the math, and, and, and you can just solve it. so, more generally, the principle behind the birthday paradox is the following. so suppose you have a calendar, perhaps on some different planet, which has k days. where each, everybody's equally likely to have each of the k days as their birthday. then it's about the square root of k people that you need in a room before you're equally likely to have a duplicate, or not have a duplicate. okay, and the reason that you get the square root effect is because if you think about it. there's a quadratic number of pairs of people in the room, so that's a quadratic, and the number of people opportunities to have a duplicate. right? so, each pair of people could be a duplicate, there's a quadratic number of pairs. and so, that's why, once the number of pairs starts reaching about the number of different days, you're, you're about, you're likely to see a duplicate around that point. so you might be wondering why i'm telling you about the birthday paradox in the middle of a lecture about hashing, but really it's quite relevant. so imagine for example you defined a hash function in the following way. now to be clear, this is not a practical hash function, but just for the purposes of discussion, imagine you have a hash function which randomly assigned every single key to a uniform bucket. 'kay, so for each, each of the 1/n buckets equally likely. then what the birthday paradox says is, even for a very small dataset, you are already gonna have a pair of things colliding. all right, so if you have an n buckets, so maybe your n is like, 10,000, all you need is roughly 100 elements in your data set, and despite the fact that the table is only going to be one percent full, you're already going to see a collision, okay? so 99 percent of them are empty, but you're going to have one bucket that has two, so that's sort of annoying. so the birthday paradox says, you start getting collisions with the hash function, even with the really tiny data sets. so in this sense, if you're going to have hash tables, you've got to deal with collisions. there's going to be a fair number of them, and you need some method for resolving them so, collisions are a fact of life when you're talking about hashing. where again, by collision, what i mean is two different keys. so two different elements x and y from the universe that hash to the same bucket, who have the same hash value, so in general we can think of a hash function as doing a compression of sorts. so we have a huge universe u and we have this very modest size array a with the only n buckets. where n, we're thinking of as being much, much, much smaller than u. so, of course, this hash function has to map various elements of u to the same bucket. so what are we gonna do about it? how are we going to resolve these collisions? well, there's two different solutions which are both quite prevalent in practice. so solution number one is called chaining, or sometimes you'll also see it called separate chaining. and this is a very natural solution; it's also the one that's relatively easy to analyze mathematically. what you do is just for elements that hash to the same bucket, you just revert to the list-based solution that we talked about in a previous slide. so, each of the n buckets will not necessarily contain just merely 0 or 1 element , it will contain a list within a principle unbounded number of elements. okay, so when we use chaining, it's done quite straight-forward to figure out how to implement all of the hash table operations, namely, insert, delete and look-up, you just hash something to the appropriate bucket and then you just do insert, delete or look-up, as appropriate, in the list that's in that bucket. so just to make clear that everything is type checking, so here h(x), this is the bucket for x. that's what's specified by the hash function. and then, in the h(x) position of this array a, in the h (x), the bucket is where we find the linked list that is going to contain x. so just to give a cartoon example, if you had, say, four buckets, maybe, you know, the first bucket has exactly one record. corresponding to alice, maybe the second bucket just has a null pointer. no one's been inserted in the second bucket. and then the third bucket we have, let's say, both bob as well as daniel. and then maybe in the fourth bucket we have carol. okay, so because we have a collision between bob and daniel, both map to the third bucket, and we resolve that just by having a linked list, with bob and daniel in some order. so the second solution which is trickier to talk about mathematically but still quite important practically is called open addressing. and the principal in open addressing is you're not going to use any space for pointers. you're not gonna have lists. so you're only gonna have one object per bucket of the array. so another question is what happens if, you know, you try and insert daniel and you go, you invoke the hash function on daniel and it takes you to a bucket that already contains bob? that means there's no room for daniel. so what you're going to do is you're gonna probe the hash table in some other position. so a hash function is, is now gonna be replaced by a hash sequence, where you try, the hash function tells you the first bucket to try to insert daniel; failing that, a second bucket in which to try to insert daniel; failing that, a third bucket to try to insert daniel; and so on. and you just keep trying till you find an open position somewhere in the array. so there's various strategies for trying to figure out the probe sequence. one strategy is if you fail and save bucket 17, which is where the hash function tells you to go first. you just try bucket 18, then 19, then, 20, then 21 and so on, until you find your first open slot. so that's called linear probing. and another approach is double hashing. so this is a solution where you actually have two hash functions, hash function 1 and hash function 2. and the idea is, suppose you're trying to insert, say, daniel, into a hash table with open addressing, and you evaluate both of the hash functions. and the first one comes up 17, and the fir-, the second one comes up 23. so, as usual, the has-, first hash function will specify where you look first. so if it evaluates on daniel to 17, you look in the seventeenth position of the array, and if, if it's empty, that's where you insert daniel. now, if it's full, what you do is you use the second hash value to be an additive shift, so. unlike linear probing where after seventeen, you look at eighteen. with double hashing, if the second hash function gives you 23, that's gonna be your offset. so after seventeen, you look at bucket 40. if 40 is already full, you look at bucket 63. if bucket 63 is already full then you look at bucket 86. so you keep adding increments of 23 until you finally find a bucket where, that's empty and that's where you insert daniel. now of course, if you try to insert some other name, if you try to insert elizabeth, you're gonna get two totally different numbers in general. so maybe you'll get 42 and 27, and so here the probed sequence will be 42, failing that 69, failing that 96 failing that 123 and so on, so a question you should have at this point, is, you know. i've told you two solutions to resolving collisions in a hash table. and you're probably asking, well, which ones should you use if you have to implement your own hash table? and, you know, as usual, if i present you with two different solutions for the same problem. you can probably rest assured that neither one dominates the other, right? otherwise i wouldn't waste your time by presenting both of them to you. so, sometimes chaining's gonna perform better, and sometimes, open addressing's gonna perform better. and of course, it also depends on what kind of metric that you care about. so there are a couple of rules of thumb that i can tell you. so first of all if space is at a real premium you might want to consider open addressing instead of chaining, and that's cause with chaining you do have this excess, not huge but you do have this little bit of space overhead and dealing with all these pointers in this link list. so if you want to avoid that, you might want to think about open addressing. the second rule of thumb is deletion is trickier with open addressing than with chaining, but deletion is clearly not difficult at all, either to code or understand when you use chaining cause it just reduces chaining to a linked list which of course you all know how to do. open addressing is, it's not impossible to implement deletion but it's much trickier. so if deletion's a, a crucial operation for you, that might steer you towards thinking about chaining. but ultimately, if it's really kinda mission critical code, probably the best thing to do is implement both kinds of solutions and just see which one works better. it's a little hard to predict how they're gonna interact with memory hierarchies and that kind of thing. they're both useful in their own contexts. alright so we've covered the two most prevalent ways of handling collisions. and we argued that collisions are inevitable no matter you design you hash function. you're stuck with collisions and you can do chaining or linked lists per bucket, or you can do addressing, where you actually have a probe sequence in order of which you look at buckets until you find an empty one. and the elephant in the room at this point is, you know, what is this hash function? i have told you nothing about hash functions. all i told you is there is some mapping from the set of the universe, so ip addresses, or names, or whatever to a bucket number. well what kind of function should you use? excellent question, tons of research on that question, and to this day as much art as science. but let's start talking about it. 
let's begin by building up some intuition about what we would want from a hash function, now that we know how hash functions are usually implemented. so let's start with a hash function which is implemented by chaining. so what's going to be the running time of our lookup, insert, and delete operations in a hash table with chaining? well, the happy operation in a hash table with chaining is insertion. insertion, we can just say without any qualifications, is constant time. this requires the sort of obvious optimization that when you do insert, you insert something at the front of the list in its bucket. like, there's no reason to insert at the end. that would be silly. so the plot thickens when we think about the other two operations, deletion and the lookup. so let's just think about lookup. deletion's basically the same. so how do we implement lookup? well, remember when we get some key x, we invoke the hash function. we call h(x). that tells us what bucket to look in. so if it tells us 17, we know that, you know, x may or may not be in the hash table. but at this point we know that if it's in the hash table, it's got to be in the linked list that's in the seventeenth bucket. so now we descend into this bucket. we find ourselves a linked list. and now, we have to resort to just an exhaustive search through this list in the seventeenth bucket, to see whether or not x is there. so we know how long it takes to search a regular list for some element. it's just linear and the list length. and now we're starting to see why the hash function might matter. right, so suppose we insert 100 objects into a hash table with 100 buckets. if we have a super lucky hash function, then perhaps each bucket will get its own object. there'll be one object in each of the lists, in each of the buckets, so. theta of the list length is just theta of one. we're doing great. okay? so, a constant, constant link to lists, means constant time insert delete. a really stupid hash function would map every single object to bucket number zero. okay, so then if you insert 100 objects, they're all in bucket number zero. the other 99 buckets are empty. and so every time you do insert or delete, it's just resorting to the naive linked list solution. and the running time is going to be linear and the number of objects in the hash table. so the largest list length could vary anywhere from m/n, where m is the number of objects, this is when you have equal linked lists, to if you use this ridiculous constant hash function, m, all the objects in the same list. and so the main point i'm trying to make here, is that you know, first of all, at least with chaining, where the running time is governed by the list length, the running time depends crucially on the hash function. how well the hash function distributes the data across the different buckets. and something analogous is true for hash tables that use open addressing. alright so here there aren't any lists. so you don't, there's no linked lists to keep track of. so the running time is going to be governed by the length of the probe sequence. so the question is how many times do you have to look at different buckets in the hash table before you either find the thing you're looking for, or if you're doing insertion, before you find an empty bucket in which to insert this new object. so the performance is governed by the length of the probe sequence. and again, the probe sequence is going to depend on the hash function. for really good hash functions in some sense, stuff that spreads data out evenly, you expect probe sequences to be not too bad. at least intuitive, and for say the constant function you are going to expect these probe sequences to grow linearly with the numbers of object you insert into the table. so again this point remains true, the performance of a hash table in either implementation really depends on what hash function you use. so, having built up this intuition, we can now say what it is what we want from a hash function. so first we want it to lead to good performance. i'm using the chaining implementations as a guiding example. we see that if we have a size of a hash table, n, that's comparable to the number of objects, m, it would be really cool if all of the lists had a length that was basically constant; therefore we had our constant time operations. so equal length lists is way better than unequal length lists in a hash table with chaining. so we, we want the hash function to do is to spread the data out as equally as possible amongst the different buckets. and something similar is true with open addressing; in some sense you want hash functions to spread the data uniformly across the possible places you might probe, as much as possible. and in hashing, usually the gold standard for spreading data out is the performance of a completely random function. so you can imagine for each object that shows up you flip some coins. with each of the n buckets equally likely, you put this object in one of the n buckets. and you flip independent coins for every different object. so this, you would expect, you know, because you're just throwing darts at the buckets independently, you'd expect this to spread the data out quite well. but, of course, it's not good enough just to spread data out. it's also important that we don't have to work too hard to remember what our hash function is and to evaluate it. remember, every time we do any of these operations, an insert or a delete or a lookup, we're going to be applying our hash function to some key x. so every operation includes a hash function evaluation. so if we want our operations to run a constant time, evaluating the hash function also better run in constant time. and this second property is why we can't actually implement completely random hashing. so there's no way we can actually adjust when say we wanted to insert alice's phone number, flip a new batch of random coins. suppose we did. suppose we flipped some random coins and it tells us to put alice's phone number into the 39th bucket, while. later on, we might do a lookup for alice's phone number, and we better remember the fact that we're supposed to look in the 39th bucket for alice's phone number. but what does that mean? that means we have to explicitly remember what choice we made. we have to write down. you get a list of effects that alice is in bucket number 39. in every single insertion, if they're all from in the point of coin flips, you have to remember all of the different random choices independently. and this really just devolves back to the naive list base solution that we discussed before. so, evaluating the hash function is gonna take us linear time and that defeats the purpose of a hash table. so we again we want the best of both worlds. we want a hash function which we can store in ideally constant space, evaluate in constant time, but it should spread the data out just as well as if we had this gold standard of completely random hashing. so i want to touch briefly on the topic of how you might design hash functions. and in particular good hash functions that have the two properties we identified on the previous slide. but i have to warn you, if you ask ten different, you know, serious hardcore programmers, you know, about their approach to designing hash functions, you're likely to get ten somewhat different answers. so the design of hash functions is a tricky topic, and, it's as much art as science at this point. despite the fact that there's a ton of science, there's actually a very beautiful theory, about what makes good hash functions. we'll touch on a little bit of that in a, in a different optional video. and if you only remember one thing of, you know, from this video or from these next couple slides, the thing to remember is the following. remember that it's really easy to inadvertently design bad hash functions and bad hash functions lead to poor hash table performance. much poorer than you would expect given the other discussion we've had in this video. so if you have to design your own hash function, do your homework, get some examples, learn what other experts are doing and use your best judgment. if you do just something without thinking about it, it's quite possible to lead to quite poor performance, much poorer than you were expecting. so to drive home this point, suppose that you're thinking about keys being phone numbers. so let's say, you know, i'm gonna just be very kinda united states centric here. i'm just gonna focus on the, the ten digit phone numbers inside the us. so the universe size here is ten to the ten, which is quite big. that's probably not something you really want to implement explicitly and let's consider an application where, you know, you're only say, keeping track of at most, you know, 100 or 1,000 phone numbers or something like that. so we need to choose a number of buckets, let's say we choose 1,000 buckets. let's say we're expecting no more than 500 phone numbers or so. so we double that, we get a number of buckets to be equal 1,000. and now i got to come up with a hash function. and remember, you know, a hash functions by definition. all it does is map anything in the universe to a bucket number. so that means it has to take as input a ten digit phone number and spit as output some number between zero and 999. and, beyond that we have flexibility of how to define this mapping. now, when you are dealing with things that have all these digits it's very tempting to just project on to a subset of the digits. and, if you want a really terrible hash function, just use the most significant digits of a phone number to define a mapping from phone numbers to buckets. alright, so i hope it's clear why this is a terrible choice of a hash function. alright, so maybe you're a company based in the san francisco bay area. the area code for san francisco is 415. so if you're storing phone numbers from customers in your area. you know maybe twenty, 30, 40 percent of them are gonna have area codes 415. all of those are going to hash to exactly the same bucket, bucket number 415 in this hash table. so you're gonna get an overwhelming percentage of the data mapping just to this one bucket. meanwhile you know not all 1000 possibilities of, of these three digits are even legitimate area codes. not all three digit numbers are area codes in the united states. so there'll be buckets of your hash table which are totally guaranteed to be empty. so you waste a ton of space in your hash table, you have a huge list in the bucket corresponding to 415, you have a huge list in the bucket corresponding to 650, the area code at stanford. you're gonna have a very slow look up time for everything that hashes to those two buckets and there's gonna be a lot of stuff that hashes to those two buckets, so terrible idea. so a better but still mediocre hash function would be to do the same trick but using the last three digits instead of the first three digits. this is better than our terrible hash function because there aren't ridiculous clumps of phone numbers that have exactly the same last three digits. but still, this is sort of assuming you're using this hash function as tantamount to thinking that the last three digits of phone numbers are uniformly distributed among all of the 1,000 possibilities. and really there's no evidence if that's true. okay? and so there's going to be patterns and phone numbers that are maybe a little subtle to see with the naked eye, but which will be exposed if you try to use a mediocre hash function like this. so let's look at another example. perhaps you are keeping track of objects just based by where they are laid out in memory. so in other words the key for an object is just gonna be its memory location and if these things are in bytes, then you are guaranteed that every memory location will be a multiple of four. so for a second example let's think about a universe where the possible keys are the possible memory locations, so here you're just associating objects with where they're laid in memory, and a hash function is responsible for taking in as input some memory location of some object and spitting out some bucket number. now generally, because of, you know the structure of bytes and so on, our memory locations are going to be multiples of some power of two. in particular, memory locations are going to be even, and so a bad choice of a hash function. would be to take, remember, the hash function takes the input of the memory location, which is, you know, some possibly really big number, and we wanna compress it, we want to output a bucket number. now let's think of a hash table where we choose n equals 10^3, or 1000 buckets. so then the question is, you know, how is this hash function going to take this big number, which is the memory location, and squeeze it down to a small number. which is one of the buckets and so let's just use the same idea as in the mediocre hash function, which is we're gonna look at the least significant bits so we can express that using the mod operator. so let's just think about we pick the hash function h(x) where h is the memory location to be x mod 1000 there again, you know, the meaning of 1,000 is that's the number of buckets we've chosen to put in our hash table because, you know, we're gonna remember roughly at most 500 different objects. so don't forget what the mod operation means, this means you just, essentially subtract multiples of 1,000 until you get down to a number less than 1,000. so in this case, it means if you write out x base ten, then you just take the last three digits. so in that sense, this is the same hash function as our mediocre hash function when we were talking about phone numbers. so we discussed how the keys here are all going to be memory locations; in particular they'll be even numbers. and here we're taking their modulus with respect to an even number. and what does that mean? that means every single output of this hash function will itself be an even number. right, you take an even number, you subtract a bunch of multiples of a 1000, you're still going to have an even number. so this hash function is incapable of outputting an odd number. so what does that mean? that means at least half of the locations in the hash table will be completely empty, guaranteed, no matter what the keys you're hashing is. and that's ridiculous. it's ridiculous to have this hash table 50 percent of which is guaranteed to be empty. and again, what i want you to remember, hopefully long after this class is completed is not so much these specific examples, but more the general point that i'm making. which is, it's really easy to design bad hash functions. and bad hash functions lead to hash table performance much poorer than what you're probably counting on. now that we're equipped with examples of bad hash functions. it's natural to ask about, you know, what are some good hash functions? well it's actually quite tricky to answer that question. what are the good hash functions, and i'm not really going to answer that on this slide. i don't promise about hash functions that i'm going to tell you about right now, are good in a very strong sense of the word. i will say these are not obliviously bad hash functions, they're let's say, somewhat better hash functions. and in particular if you just need a hash function, and you need a quick and dirty one, you don't want to spend too much time on it. the method that i'm going to talk about on this slide is a common way of doing it. on the other hand, if you're designing a hash function for some really mission-critical code, you should learn more than what i'm gonna tell you about on this slide. so you, you should do more research about what are the best hash functions, what's the state of the art, if you have a super important hash function. but if you just need a quick one what's, what we say on this slide will do in many, in most situations. so the design of a hash function can be thought of as two separate parts. so remember by definition a hash function takes as input something from the universe. an ip address, a name, whatever and spits out a bucket number. but, it can be useful to factor that into two separate parts. so first you take an object which is not intrinsically numeric. so, something like s string or something more abstract. and you somehow turn an object into a number, possibly a very big number. and then you take a possibly big number and you map it to a much smaller number, namely the index of a bucket. so in some cases i've seen these two steps given the names like the first step is formulating the hash code for an object, and then the second step is applying a compression function. in some cases, you can skip the first step. so, for example, if your keys are social security numbers, they're already integers. if they're phone number, they're already integers. of course, there are applications where the objects are not numeric. you know, for example, maybe they're strings, maybe you're remembering names. and so then, the production of this hash code basically boils down to writing a subroutine that takes, as input, a string, and outputs some possibly very big number. there are standard methods for doing that, it's easy to find resources to, to give you example code for converting strings to integers you know, i'll just say one or two sentences about it. so you know each character in a string it is easy to regard as a number in various ways. either you know just say it is ascii, well ascii code then you just have to aggregate all of the different numbers, one number per character into some overall number and so one thing you can do is you can iterate over the characters one at a time. you can keep a running sum. and with each character, you can multiply the running sum by some constant, and then add the new letter to it, and then, if you need to, take a module list to prevent overflow. and the point of me giving you this one to two sentence of the subroutine is just to give you a flavor of what they're like, and to make sure th at you're just not scared of it at all. okay? so it's very simple programs you can write for doing things like converting from strings to integers. but again, you know, i do encourage you to look it up on the web or in a programming textbook, to actually look at those examples. okay? but there are standard methods for doing it. so that leaves the quest, question of how to design this compression function. so you take as input this huge integer. maybe your keys are already numeric, like social security numbers or ip addresses. or maybe you've already some subroutine to convert a string, like your friend's name, into. some big number, but the point is you have a number in the millions or the billions, and you need to somehow take that and output one of these buckets. and again think of there being maybe a thousand or so buckets. so the easiest way to do that is something we already saw in a previous slide, which is just to take the modulus, with respect to the number of buckets. now certainly one positive thing you can say about this compression function is its super simple, both in the sense that it's simple to code and in the sense that it's simple to evaluate. now remember that was our second goal for a hash function. it should be simple to store, it is actually nothing to store. and it should be quick to evaluate. and this certainly fits the bill. now the problem is, remember the first. property of a hash function that we wanted is that it should spread the data out equally. and what we saw in the previous slide is that at least if you choose the number of buckets n poorly, then you can fail to have the first property. and in that respect you can fail to be a good hash function. so if for example if n is even and all of your objects are even, then it's a disaster, all of the odd buckets go completely empty. and honestly, you know, this is a pretty simplistic method. like i said, this is a quick and dirty hash function. so, no matter how you choose the number of buckets n, it's not gonna be a perfect hash function in any sense of the word. that said, there are some rules of thumb for how to pick the number of buckets, how to pick this modulus, so that you don't get the problems that we saw on the previous slide. so, i'll conclude this video just with some standard rules of thumb. you know, if you just need a quick and dirty hash function, you're gonna use the, the modulus compression function, how do you choose n? well, the first thing is we definitely don't want to have the problem we had in the last slide, where we're guaranteed to have these empty buckets no matter what the data is. so what went wrong in the previous slide? well. the problem is that all of the data elements were divisible by two. and the hash function modulus, the number of buckets, was also divisible by two. so because they shared a common factor, namely two, that guaranteed that all of the odd buckets remained empty. and this is a problem, more generally, if the data shares any common factors with n, the number of buckets. so, in other words, if all of your data elements are multiples of three, and the number of buckets is also a multiple of three, you got a big problem. then everything's gonna hash into bucket numbers which are multiples of three, too, that's if your hash table will go unfilled. so the upshot is, you really want the number of buckets to not share any factors with the data that you're hashing. so, how do you reduce the number of common factors? well, you just make sure the number of buckets has very few factors, which means you should choose n to be a prime number, 'kay? a number that has no nontrivial factors, and let me remind you, the number of buckets should also be comparable to the size of the set that you're planning on storing. again, at no point did we need "n" to be, you know, very closely connected to the number of elements that you're storing just within, say some small constant factor. and you can always find a prime within a small constant factor of a target number of elements to store. if the number of buckets in your hash table isn't too big, if it's just in say the thousands or maybe the tens of thousands, then, you know, you can just look up a list of all known primes up to some point, and you can just sort of pick out a prime which is about the magnitude that you're looking for. if you're gonna use a really huge number of buckets in the millions or more, then there are algorithms you can use for primarily testing which will help you find a prime in about the range that you're looking for. >> so that's the first order rule of thumb you should remember if you're using the modulus compression function, which is set the number of buckets equal to a prime. so you're guaranteed to not have non-trivial common factors of the modulus shared by all of your data. so there's also a couple of second order optimizations, which people often mention. and you also don't want the prime; you want the prime to be not too close to patterns in your data. so what does that mean, patterns in your data? well, in the phone number example we saw that patterns emerged in the data when we expressed it base ten. so for example, there is crazy amounts of lumping in the first three digits when we expressed a phone number-based ten, because that corresponded with the area code. and then, with. memory locations when we express, express it base two, there are crazy correlations in the low orbits. and these are the two most common examples. either there's some digit, to the base ten representation or digits in the base two representation where you have, you know, patterns that is non-uniformity. so that. suggests that the prime, that, n that you choose, you know, all else being equal, shouldn't be too close to a power of two, and shouldn't be too close to a power of ten. the thinking being that, that will spread more evenly data sets that do have these patterns in terms of base two representation, or base ten representations. so in closing, this is a recipe i recommend for coding of hash functions if what you're looking to do is sort of minimize program ming, programmer time, subject to not coming up with a hash function, which is completely broken. but i want to reiterate, this is not the state of the art in hash function design. there are hash functions which are in some sense better than the ones that expand on this slide. if you're responsible for some really mission critical code that involves a hash function, you should really study more deeply than we've been able to do here. we'll touch on some issues in, of the different optional video, but really you should do additional homework. you should find out about the state-of-the-art about hash function design. you should also look into implementations of open addressing in those probing strategies. and above all you really should consider cold, coding up multiple prototypes and seeing which one works the best. there's no silver bullet, there's no panacea in the design of hash tables. i've given you some high-level guidance about the different approaches. but ultimately it's gonna be up to you to find the optimal implementation for your own application. 
this sequence of videos are going to take it to the next level with hash tables and understand more deeply the conditions under which they perform well, amazingly well in fact as you know, we've constant time performance for all of their operations. the main point of this first video is to explain in sense in which every hash function has its own kyptonite. a pathological data set for it which then motivates the need to tread carefully with mathematics in the subsequent videos. so, a quick review so remember that the whole purpose of a hash table is to enable extremely fast look ups, ideally constant time lookups. now, of course, to have anything to look up, you have to also allow insertions. so all hash tables are going to export those two operations and then, sometimes, a hash table also allows you to delete elements from it. that depends a little bit on the underlying implementation. so certainly when you have it implemented using chaining, that's when you have one linked list per bucket, it's very easy to implement deletion. sometimes, with open addressing, it's tricky enough, you're just gonna wind up punting on deletion. so when we first started talking about hash tables, i encouraged you to think about them logically. much the way you do as an array, except instead of being indexed just by the positions of an array, it's indexed by the keys that you're storing. so just like an array via random access supports constant time look up, so does a hash table. there was some fine print however with hash tables. remember there were these two caveats. so the first one is that the hash table better be properly implemented and this means a couple of things. so, one thing that it means is that the number of buckets better be commensurate with the number of things that you're storing in the hash table, we'll talk more about that in a second. the second thing it means is you better be using a descent hash function. so we discussed in a previous video the perils of bad hash functions, and it will be even more stringent with our demands on hash functions in the videos to come. the second caveat which i'll try to demystify in a few minutes is that you better have non-pathological data. so, in some sense, for ever hash table there's kyptonite , a pathological data set that will render its performance to be quite poor. so in the video on implementation detail we also discussed how hash tables inevitably have to deal with collisions. so you start seeing collisions way before your hash table's start filling up so you need to have some sort of method for addressing two different keys that map to exactly the same bucket. which one do you put there? do you put both there or what? so there's two popular approaches let me remind you what they are first called chaining. so this is a very natural idea, where you just keep all of the elements that hash to a common bucket in that bucket. how do you keep track of all of them? well, you just use a linked list. so, in the seventeenth bucket, you will find all of the elements which ever hashed to bucket number 17. the second approach which also has plenty of applications in practice is open addressing. here the constraint is that you are only going to store one item one key in each bucket. so if two things mapped the bucket number seventeen, you gotta find a separate place for one of them and so the way that you handle that is you demand from your hash function not merely one bucket but rather a whole probe sequence. so the sequence of buckets so that if you try to hash something into bucket number seventeen, 17's already occupied then you go on to the next. bucket in the probe sequence. you try to insert it there and if it, you fail again you go to third bucket in the sequence. you try to insert it there, and so on. so we mentioned briefly the sorta two ways you can specify probe sequences. one is called linear probing. so this is where if you fail in bucket seventeen you move on to eighteen and then nineteen and then twenty and then 21. and you stop once you find an empty one and that's where you insert the new element and another one is double hashing and this is where you use a combination of two hash functions where the first hash function specifies the initial bucket that you probe. the second hash function specifies the offset for each subsequent probe. so for example if you have a given elements say the name alice and the two hash functions give you the number 17 and 23 then the corresponding finding probe sequence is going to be initially 17, failing that we'll try 40, still failing that we'll try 63, failing that we'll try 86 and so on. so in a course on the design and analysis of algorithms like this one you typically talk a little bit more about chaining than you do open addressing. that's not to imply that chaining is somehow the more important one both of these are important but chaining is a little easier to talk about mathematically. so we will talk about it a little bit more cuz i'll be able to give you complete proofs for chaining whereas complete proofs for open addressing would be outside the scope of this course. so i'll mention it in passing but the details will be more about chaining just for mathematical ease. so, there's one very important parameter which plays a big role in governing the performance of a hash table, and that's known as the load factor, or simply the load, of a hash table. and it's a very simple definition. it just talks about how populated, a typical bucket of the hash table is. so it's often denoted by alpha and in the numerator is the number of things that have been inserted, and not subsequently deleted in the hash table. divided by the number of buckets in the hash table. so, as you would expect, as you insert more and more things into the hash table, the load grows, keeping the number of items in the hash table fixed as you scale up the number of buckets, the load drops. so just to make sure that the notion of the load is clear, and that also you're clear on the different strategies for resolving collisions, the next quiz will ask you about the range of relevant alphas for the chaining and open addressing implementations of the hash table. alright, so the correct answer to this quiz question is the third answer. load factors bigger than one do make sense they may not be optimal but they at least make sense for hash tables that implement with chaining but they don't make sense for hash tables with open addressing. and the reason is simple remember in open addressing you are required to store only one object per bucket so as soon as the number of objects exceeds the number of buckets there is no where to put the remaining objects. so the hash the hash table will simply crash if, if load factor is bigger than one. on the other hand a hash table with chaining there is no obvious problems with load factor bigger than one so you can imagine a load factor equal to two say, say you insert 2,000 objects into a hash table with 1,000 buckets. you know that means, hopefully at least in the best case. each buckets just gonna have a length list with two objects in it. so there's no big deal. with having load factors bigger than one, and hash tables with chaining. alright, so let's then make a, a quite easy but also very important observation about a necessary condition for hash tables to have good performance and this goes into the first caveat that you better properly implement the hash table if you expect to have good performance. so the first point is that you're only gonna have constant time look-ups if you keep the load to be constant. so for a hash table with open addressing, this is really obvious, because you need alpha not just o(1) but less than one, less than 100 percent full, otherwise the hash table is just gonna crash, cuz you don't have enough room for all of the items, but even for hash tables that you implement using chaining, where they at least make sense for load factors which are bigger than one, you'd better keep the load not too much bigger than one if you want to have constant-time operations. right, so if you have, say, a hash table with. n buckets and you hash in nlogn objects, then the average number of objects in a given bucket is gonna be logarithmic and remember, when you do a lookup, after you hash to the bucket, you have to do an exhaustive search through the linked list in that bucket. so if you have nlogn objects and you hashed it with n buckets, you're expecting more like logarithmic lookup time - not constant lookup time and then, as we discussed with open addressing, of course, you need not just alpha = o(1), but alpha less than one. and in fact, alpha better be well below one. you don't want to let an open addressing table get to a 90 percent load or something like that. so i'm going to write need alpha less than less than one. so that just means you don't want to let the load grow too close to 100%, you will see performance degrade. so again, i hope the point of this slide is clear. if you want good hash table performance, one of the things you're responsible for is keeping the load factor under control. keep it at most a small constant with open address and keep it well below 100%. so you might wonder what i mean by controlling the load. after all, you know you writing this hash table when you have no idea what some client's gonna do with it. they can insert or delete whatever they want. so how do you, how do you control alpha? well, what you can control under the hood of your hash table implementation is the number of buckets. you can control the denominator of this alpha so if the numerator starts growing at some point the denominator is going to grow as well. so what actual implementations of hash tables do is they keep track of the population of the hash table. how many objects are being stored, and as this numerator grows, as more and more stuff gets inserted, the implementation ensures that the denominator grows at the same rate so that the number of buckets also increases. so if alpha exceeds some target, you know, that could be say 75%, .75, or maybe it's.5. then what you can do is you can double the number of buckets, say in your hash table. so you define a new hash table, you have a new hash function with double the range, and now having doubled the denominator. the load has dropped by a factor two. so that's how you can keep it under control. optionally, if space is at a real premium, you can also shrink the hash table if there's a bunch of deletions, say, in a chaining implementation. so that's the first take away point about what has to be happening correctly under the hood in order to get the desired guarantees for hash table performance you gotta control the load. so you have to have a hash table whose size is roughly the same as the as the number of objects that you are storing. so the second thing you've gotta get right and this is something we've touched on in the implementation videos is you better use a good enough hash function and so what's a good hash function? it's something that spreads the data out evenly amongst the buckets and what would really be awesome would be a hash function which works well independent of the data and that's really been the theme of this whole course so far, algorithmic solutions which work independent of any domain assumptions. no matter what the input is, the algorithm is guaranteed to, for example, run blazingly fast and i can appreciate that this is exactly the sort of thing you would want to learn from a course like this, right? take a class in the design analysis of algorithms and you learn the secret hash function which always works well. unfortunately i'm not going to tell you such a hash function and the reason is not cuz, i didn't prepare this lecture. the reason is not because people just haven't been clever enough to discover such a function. the problem is much more fundamental. the problem is that such a function cannot exist. that is for every hash function it has its own kryptonite. there is a pathological dataset under which the performance of this hash function will be as bad as the most miserable constant hash function you'd ever seen. and the reason is quite simple; it's really an inevitable consequence of the compressing that hash functions are effectively implementing from some massive universe to some relatively modest number of buckets. let me elaborate. fix any hash function as clever as you could possibly imagine. so this hash function maps some universe through the buckets indexed from 0 to n -1. remember in all of the interesting situations of hash functions, the universe size is huge, so the cardinality of u should be much, much bigger than n. that's what i'm going to assume here. so for example, maybe you're remembering people's names, and then the universe is strings, which have, say at most, 30 characters, and n, i assure you in any application is going to be much, much, much smaller than say, 26 raised to the thirtieth power. so now let's use a variant on the pigeon hole principle, and acclaim that at least one of these n buckets has to have at least a 1/n fraction of the number of keys in this universe. that is, there exists a bucket i, somewhere between 0 and n -1 such that, at least the cardinality of the universe over n keys hash to i get mapped to i under this hash function h. so the way to see this is just to remember the picture of a hash function mapping in principal any key from the universe, all keys from the universe to one of these buckets. so the hash function has to put each key somewhere in one of the n buckets so one of the buckets has to have at least a 1/n fraction above all of the possible keys. one more concrete way of thinking, thinking about it is that you might want to think about a hash table implemented with chaining. you might want to imagine, just in your mind, that you hash every single key into the hash table. so this hash table is going to be insanely over-populated. you'll never be able to store it on the computer because it will have the full cardinality of u objects in it, but it has u objects, it only has n buckets. one of those buckets has to have at least u /n fraction of the population. so the point here is that no matter what the hash function is no matter how clever you build it there's gonna be some buckets say bucket number 31 which gets its fair share of the universe maps to it. so having identified this bucket, bucket number 31 where it gets at least its fair share of the universe maps to it now to construct our pathological dataset all we do is picked from amongst these elements that get mapped to bucket number 31. so for such as a data set and we can make this data set basically as large as we want because the cardinality of u/n is unimaginably big, because u itself is unimaginably big then in this data set, everything collides. the hash function maps every single one to bucket number 31 and that's gonna lead to terrible hash table performance. hast table performance which is really no better than the naive linked list solutions. so for example an hash table with collisions, in bucket 31, you'll just find a link listed every single thing that's ever been inserted into the hash table and for open addressing maybe it's a little harder to see, what's going on but again if everything collides, you're gonna basically wind up with linear time performance a far cry from constant time performance. now, for those of you to whom this seems like just sort of pointless, abstract mathematics, i want to make two points. the first is that at the very least these pathological data sets. tells us, indicates, that we will have to discuss hash functions in a way differently than how we've been discussing algorithms so far. so when we talked about merge sort, we just said it runs in n log n time. no matter what the input is. whether we discuss the dijkstra's algorithm it runs in n log n time, no matter what the input is. that first search, that first search many a time no matter what the input is. we're gonna have to say something different about hash functions. we'll not be able to say that a hash table has good performance, no matter what the input is. this slide shows that's false. the second point i want to make is that while this pathological data sets of course are not likely to arise just by random chance. sometimes you're concerned about somebody constructing a pathological data set for your hash function, for example in a denial service attack. so there's a very clever illustration of exactly this point in a research paper, from 2003 by crosby and wallach. so the main point of crosby and wallach was that there's a number of real world systems and maybe there, most interesting application was a network intrusion detection system, for which you could bring them to their knees by exploiting badly designed hash functions. so these were all applications that made crucial use of hash tables and, the feasibility of these systems really, completed depended on getting cost and time performance from the hash tables. so if you could exhibit a pathological data set for these hash tables and make the performance devolve to linear, devolve to that of a simple link list solution, the systems would be broken, they would just crash of they'd fail to function. now we saw in the last slide that every hash table does have its own kryptonite. has a pathological data set but the question is. how can you come up with such a pathological data set if you're trying to do a denial of service attack on one of these systems? and so the systems that crosby and wallach looked at generally shared two properties. so first of all, they were open source. you could inspect the code. you could see what hash function it was that they were using and second of all, the hash, function was often very simplistic. so it was engineered for speed more than anything else and as a result, it was easy to, just be inspecting the code, reverse engineer a data set. that really did break the hash table. that led it that devolved the performance to linear. so for example, in the network intrusion detection application, there was some hash table that was just remembering the ip addresses of packets that we re going through, because it was looking for patterns of pack, data packets that seemed to indicate some sort of intrusion. and crosby and wallach just showed how sending a bunch of data packets to this system with cleverly chosen sender ip's really did just crash the system because the hash table performance blew up to an unacceptable degree. so how should we address this fact of life that every hash function has a pathological data set? and that question is meaningful both from a practical perspective, so what hash functions should we use if we are concerned about someone constructing pathological data sets, for example, the implemented denial-of-service attack, and secondly, mathematically, if we can't give the kinds of guarantees we've given so far, data-independent guarantees, how can we mathematically say that hash functions have good performance. so let me mention two solutions, the first solution is, is meant more just on the practical point. you know what hash function should you implement if you are concerned with someone creating pathological data sets? so there are these things called cryptographic hash functions. there for example, one cryptographic hash function or really family of hash functions, for different numbers of buckets, is sha-2 and these are really outside the scope of this course. i mean, you'll learn more about this in a in a course on cryptography and obviously using these keywords you can look it up on the web and read more about it. the one point i want to make is, you know these cryptographic hash functions like sha2, they themselves, they do have pathological datasets. they have their own version of kryptonite. the reason that they work well in practice is because it's infeasible to figure out what this pathological dataset is so unlike the very simplistic hash functions which crosby and wallach found in the source code of their applications, where it was easy to reverse-engineer bad datasets, for something like sha2 nobody knows how to reverse-engineer a bad dataset for it and when i say infeasible, i mean in the usual cryptographic sense, in a way similar to how one would say it's infeasible to break the rsa cryptosystem if you implement it properly, or it's infeasible to factor large numbers except in very special case, and so on. so that's all i'm going to say about cryptographic hash functions. i also want to mention a second solution, which would be reasonable both in practice, and also for which we can say a number of things mathematically about it which is to use randomization. so more specifically we're not going to design, a single. clever hash function because again, a single hash function we know must have a pathological data set but we're gonna design a very clever, family of hash functions and then, at run time. we're going to pick, one of these hash functions at random. another kind of guarantee you are going to want, we are going to be able to prove on a family affairs function would be very much in the spirit of quick sort. so you would call that in the quick sort algorithm, pretty much. for any fixed pivot sequence there is a pathological input for which quick sort will devolve to quadratic running time. so our solution was randomize quick sort which is rather than committing up front to any particular method of choosing pivots at run time we are gonna pick pivots randomly. what did we prove about quick sort? we proved that for any possible input for any possible array the average running time with quick sort was o(nlogn) with the average was over the run time random choices of quick sort. here we are gonna do the same thing we'll now be able to say for any dataset. on average, with respect to our run time choice of a hash function. the hash function will perform well, in the sense that it will spread out the data of the data set evenly. so we flip to the quantifiers from the previous slide. there, we said if we pre-commit to a single hash function. if we fix one hash function, then there's a data set that breaks the hash function. here we're flipping it, we're saying for each fixed data set a random choice of a hash function is going to do well on average on that data set. just like in quick sort. this doesn't mean notice that we can't make our program open source. we can still publish code which says here is our family of hash functions and in the code we would be making a random choice from this set of hash functions but the point is by inspecting the code you'll have no idea what was the real time random choices made by the algorithm. so you'll know nothing about what the actual hash function is so you won't be able to reverse engineer pathological dataset for the real time choice of the hash function. so the next couple of videos are gonna elaborate on the second solution of using a real time random choice of a hash function as a way of saying. you do well on every data set at least on average. so let me just give you a road map of where we're going to go from here. so i'm going to break the discussion of the details of a randomized solution into three parts, spread over two videos. so in the next video we're going to begin with the definition of what do i mean by a family of hash functions, so that if you pick one at random, you're likely to do pretty well. so that's a definition that's called a universal family of hash functions. now a mathematical definition by itself is worth approximately nothing. for it to be valuable, it has to satisfy two properties. so first of all there have to be interesting and useful examples that meet the definition. so that is, there better be useful looking hash functions that meet this definition of a universal family. so the second thing will be to show you that they do indeed exist and then the other thing a mathematical definition needs is applications. so that if you can meet, the definition. then, good things happen. so that'll be part 
so, in this video, we're going to start reasoning about the performance of hash tables. in particular, we'll make precise this idea that properly implemented they guarantee constant time lookup. so, let me just briefly remind you of the road map that we're in the middle of. so, we observed that every fixed hash function is subject to a pathological data set and so exploring the solution of making a real time decision of what hash function to use. and we've already gone over this really quite interesting definition of universal hash functions and that's the proposed definition of a good random hash function. more over, in the previous video i showed you that there are simple such families of hash functions. they don't take much space to store, they don't take much time to evaluate. and the plan for this video is to prove formally, that if you draw a hash function uniformly at random from a universal family of hash functions, like the one we saw in the previous video, then you're guaranteed expected constant time for all of the supported operations. so, here's the definition once more of a universal family of hash functions. we'll be using this definition, of course, when we prove that these hash functions give good performance. so, remember, we're talking now about a set of hash functions. these are all of the conceivable real time decisions you might make about which hash function to use. so, the universe is fixed, this is something like ip addresses, the number of buckets is fixed. you know that's going to be something like 10,000, say, and what it means for a family to be universal is that the probability that any given pair of items collides is as good, as small as with the gold standard of completely perfect uniform random hashing. that is for each pair xy of distinct elements of the universe, so for example, for each distinct pair of ip addresses, the probability over the choice of the random hash function little h from the family script h is no more than one over n, where n is the number of buckets. so, if you have 10,000 buckets, the probability that any given pair of ip addresses winds up getting mapped to the same bucket is almost one in 10,000. let me now spell out the precise guarantee we're going to prove if you use a randomly chosen hash function from a universal family. so, for this video, we're going to only talk about hash tables implemented with chaining, with one length list per bucket. we'll be able to give a completely precise mathematical analysis with this collision resolution scheme. we're going to analyze the performance of this hash table in expectation over the choice of a hash function little h drawn uniformly from a universal family script h. so, for example, for the family that we constructed in the previous video, this just amounts to choosing each of the four coefficients uniformly at random. that's how you select a universal, that's how you select a hash function uniformly at random. so, this theorem and also the definition of universal hash functions dates back to a 1979 research paper by carter and wegman. the idea of hashing dates back quite a bit before that, certainly to the 50s. so, this just kind of shows us sometimes ideas have to be percolating for awhile before you find the right way to explain what's going on. so, carter and wegman provided this very clean and modular way of thinking about performance of hashing through this universal hashing definition. and the guarantee is exactly the one that i promised way back when we talked about what operations are supported by hash tables and what kind of performance should you expect, you should expect constant time performance. as always, with hashing, there is some fine print so let me just be precise about what the caveats of this guarantee are. so, first of all, necessarily this guarantee is an expectation. so, it's on average over the choice of the hash function, little h. but i want to reiterate that this guarantee does hold for an arbitrary data set. so, this guarantee is quite reminiscent of the one we had for the rand omized quick sort algorithm. in quicksort, we made no assumptions about the data. it was a completely arbitrary input array and the guarantee said, on average over the real time randomized decisions of the algorithm, no matter what the input is, the expected running time was in log in. here we're saying again, no assumptions about the data. it doesn't matter what you're storing in the hash table and expectation over the real time random decision of what hash function you use, you should expect constant time performance, no matter what that data set is. so, the second caveat is something we've talked about before. remember, the key to having good hash table performance, not only do you need a good hash function which is what this universality key is providing us but you also need to control the load of the hash table. so, of course, to get constant time performance, as we've discussed, a necessary condition is that you have enough buckets to hold more or less the stuff that you're storing. that is the load, alpha, the number of objects in the table divided by the number of buckets should be constant for this theorem to hold. and finally, whenever you do a hash table operation, you have to in particular invoke the hash function on whatever key you're given. so, in order to have constant time performance, it better be the case that it only takes constant time to evaluate your hash function and that's, of course, something we also discussed in the previous video when we emphasized the importance of having simple universal hash functions like those random linear combinations we discussed in the previous video. in general, the mathematical analysis of hash table performance is a quite deep field, and there is some, quite mathematically interesting results that are well outside the scope of this course. but what's cool, in this theorem i will be able to provide you a full and rigorous proof. so, for hash tables with chaining and using randomly chosen universal hash functions, i'm going to now prove that you do get cons tant time performance. right, so hash tables support various operations, insert, delete and lookup. but really if we can just bound a running time of an unsuccessful lookup, that's going to be enough to bound the running time of all of these operations. so, remember in hash table with chaining, you first hash the appropriate bucket and then you do the appropriate insert, delete or lookup in the link list in that bucket. so, the worst case as far as traversing though this length list is if you're looking for something but it's not there cuz you have to look at every single element in the list and followup into the list before you can conclude that the lookup has failed. of course, insertion, as we discussed, is always constant time, deletion and successful searches, well, you might get lucky, and stop early before you hit the end of the list. so, all we're going to do is bother to analyze unsuccessful lookups that will carry over to all of the other operations. so, a little more precisely, let's let s be the data set. this is the objects that we are storing in our hash table. and remember that to get constant time lookup, it really needs to be the case that the load is constant. so, we are assuming that the size of s is bigger over the number of buckets n. and let's suppose we are searching for some other object which is not an s, call it x. and again, i want to emphasize we are making no assumptions about what this data set s is other than that the size is comparable to the number of buckets. so, conceptually doing a lookup in a hash table with chaining is a very simple thing. you just hash to the appropriate bucket and then you scan through the length list in that bucket. so, conceptually, it's very easy to write down the what the running time of this unsuccessful lookup is. it's got two parts. so, the first thing you do is you evaluate the hash function to figure out the right bucket. and again, remember we're assuming that we have a simple of a hash function and it takes constant time. now, of course, the magic of hash functions is that given that this hash value, we can zip right to where the lenght list is to search for x using random access into our array of buckets. so, we go straight to the appropriate place in our array of buckets and we just search through the list ultimately failing to find what we're looking for s. traversing a link list, as you all know, it takes time proportional to the length of the list. so, we find something that we discussed informally in the past which is that's the running time of hash table operations implemented with chaining is governed by the list lengths. so, that's really the key quantity we have to understand. so, lets call this, lets give this a name, capital l, it's important enough to give a name. so, what i want you to appreciate at this point is that this key quantity, capital l, the list of the length in x's bucket is a random variable. it is a function of which hash function little h, we wind up selecting in a real time. so, for some choices of our hash function, little h, this list length might be as small as zero but for other choices of this hash function h, this list, list length could be bigger. so, this is exactly analogous too in quicksort where depending on the real time decision of which random pivot element you use, your going to get a different number of comparisons, a different running time. so, the list length and hence the time for the unsuccessful storage, depends on the hash function little h, which we're choosing at random. so, let's recall what it is we're trying to prove. we're trying to prove an upper bound on the running time of an unsuccessful lookup on average, where the on average is over the choice of the hash function little h. we've expressed that this lookup time in terms of the average list length in x's bucket where again the average is over the random choice of h. summarizing, we've reduced what we care about, expected time for lookup to understanding the expected value of this random variable capital l, the average list length in x's bucket. so, that's what we've got to do, we've got to compute the expected value of this random variable, capital l. now to do that, i want to jog your memory of a general technique for analyzing expectations which you haven't seen in awhile. the last time we saw it, i believe, was when we were doing the analysis of randomized quicksort and counting its comparisons. so, here's a general decomposition principle which we're now going to use in exactly the same way as we did in quicksort here to analyze the performance of hashing with chaining. so, this is where you want to understand the expect, expectation of random variable which is complicated but what you can express as the sum of much simpler random variables. ideally, 0,1 or indicator random variables. so, the first step is to figure out the random variable, capital y, it's what i'm calling it here that you really care about. now, we finished the last slide, completing step one. what we really care about is capital l, the list length in x's bucket. so, that governs the running time a bit unsuccessful look up, clearly that's what we really care about. step two of the decomposition principle is well, you know, the random variable you care about is complicated, hard to analyze directly but decompose it as a sum of 0,1 indicator random variable. so, that's what we're going to do in the beginning of the next slide. why is it useful to decompose a complicated random variable into the sum of 0,1random variables? well, then you're in the wheelhouse of linear of expectations. you get that the expected value of the random variable that you care about is just the sum of the expected values of the different indicator random variables and those expectations are generally much easier to understand. and that will again be the case here in this theorem about the performance of hash tables with chaning. so, let's apply this three-step-decomposition principle to complete the proof of the carter-wegman theorem. so, for the record, let me just remind you about the random variable that we actually really care about, that's capital l. the reason that's a random variable is that because it depends on the choice of the hash function, little h. this number could vary between zero and something much, much bigger than zero, depending on which each you choose. so, this is complicated, hard to analyze directly, so let's try to express it as the sum of 0,1 random variables. so, here are the0,1 random variables that are going to be the constituents of capital l. we're going to have one such variable for each object y in the data set. now, remember this is an unsuccessful search, x is not in the data set capital s. so, if y is in the data set, x and y are necessarily different. and we will define each random variable z sub y, as follows. we'll define it as one if y collides with x under h and zero otherwise. so, for a given zy, we have fixed objects x and y so, x is some ip address, say, y is some distinct ip address, x is not in our hash table, y is in our hash table. and now, depending on which hash function we wind up using, these two distinct ip addresses may or may not get mapped to the same bucket of our hash table. so, this indicates a random variable just indicating whether or not they collide, whether or not we unluckily choose a hash function little h that sends these distinct ip addresses x and y to exactly the same bucket. okay, so, that's zy, clearly by definition, it's a 0,1 random variable. now, here's what's cool about these random variables is that capital l, the list length that we care about decomposes precisely into the sum of the zy's. that is, we can write capital l as being equal to the sum over the objects in the hash table of zy. so, if you think about it, this equation is always true, no matter what the hash function h is. that is if we choose some hash functions that maps these ip address x to, say, bucket number seventeen, capital l is just counting how many other objects in the hash table wind up getting mapped to bucket number seventeen. so, maybe ten different ob jects got mapped to bucket number seventeen. those are exactly the ten different values of y that will have their zy equal to1, right? so, so l is just counting the number of objects in the data set s that's collide with x. a given zy is just counting whether or not a given object y in hash table is colliding with x. so, summing over all the possible things that could collide with x, summing over the zy's, we of course get the total number of things that collide with x which is exactly equal to the number, the population of x's bucket in the hash table. alright, so we've got all of our ducks lined up in a row. now, if we just remember all of the things we have going for us, we can just follow our nose and nail the proof of this theorem. so, what is it that we have going for us? well, in addition to this decomposition of the list length in to indicate random variables, we've got linear expectation going for us, we've got the fact that our hash function is drawn from a universal family going for us. and we've got the fact that we've chosen the number of buckets and to be comparable to the data set size. so, we want to use all of those assumptions and finish the proof that the expected performance is constant. so, we're going to have a few inequalities and we're going to begin with the thing that we really care about. we care about the average list length in x's bucket. remember, we saw in the previous slide, this is what governs the expected performance of the lookup. if we can prove that the expected value of capital l is constant, we're done, we've finished the theorem. so, the whole point of this decomposition principle is to apply linear of expectation which says that the expected value of a sum of random variables equals the sum of the expected values. so, because l can be expressed as the sum of these zy's, we can reverse the summation and the expectation and we can first sum over the y's, over the objects in the hash table and then take the expected value of zy. now, something which came up in our quicksort an alysis but which you might have forgotten is that 0,1 random variables have particularly simple expectations. so, the next quiz is just going to jog your memory about why 0,1 random variables are so nice in this context. okay, so the answer to this quiz is the third one, the expected value of zy is simply the probability that x and y collide, that just follows from the definition of the random variable zy and the definition of expectation, namely recall how do we define zy. this is just this one, if this object y in the hash table happens to collide with the object x that we are looking for under the hash function x and zero otherwise, again, this will be, this will be one for some hash functions and zero for other hash functions. and then we just have to compute expectations. so, one way to compute the expected value of a 0,1 random variable is, you just say, well, you know, there are the cases where the random variable evaluates to zero and then there's the cases where the random variable evaluates to one, and of course we can cancel the zero. so, this just equals the probability that zy = one. and since zy being one is exactly the same thing as x and y colliding, that's what gives us the answer. okay, so it's the probability that x collides with y. so, plenty of that into our derivation. now, we again have the sum of all the objects y in our hash table and the set of the expected value of zy what's right that in the more interpretable form, the probability that this particular object in the hash table y collides with the thing we are looking for x. now, we know something pretty cool about the probability that a given pair of distinct elements like x and y collide with each other. what is it that we know? okay, so i hope you answered the second response to this quiz. this is really in some sense the key point of the analysis. this is the role, that being a universal family of hash functions plays in this performance guarantee. what does it mean to be universal? it means for any pair of objects distinct like x and y in that proof, if you make a random choice of a hash function, the probability of collision is as good as with perfectly random hashing, hashing. namely at most, 1/ n where n is the number of buckets. so, now i can return to the derivation. what that quiz reminds you is that the definition of a universal family of hash functions guarantees that this probability for each y is at most 1/n, where n is the number of buckets in the hash table. so, let's just rewrite that. so, we've upper bounded the expected list length by a sum over the objects in the data set of 1/n. and this, of course, is just equal to the number of objects in the data set, the [inaudible] of s divided by n. and what is this? this is simply the load, this is the definition of the load alpha which we are assuming is constant. remember, that was the third caveat in the theorem. so, that's why as long as you have a hash function which you can compute quickly in constant time. and as long as you keep the load under control so the number of buckets is commensurate with the size of the data set that you're storing. that's why, universal hash functions in a hash table with chaining guarantee expected constant time performance. 
in the last video, we discussed the performance of hash tables that are implemented using chaining, using one link list per bucket. in fact, we proved mathematically that if you use a hash function chosen uniformly at random from a universal family, and if you keep the buckets, number of buckets, comparable to the size of your data set, then in fact, you're guaranteed constant time expected performance but, recall that chaining is not the only implementation of hash tables. there's a second paradigm which is also very important called open addressing. this is where you're only allowed to store one object in each slot, and you keep searching for an empty slot when you need to insert a new object into your hash table. now it turns out it's much harder to mathematically analyze hash tables implemented using open addressing but, i do want to say a few words about it to give you the gist of what kind of performance you might hope to expect from those sorts of hash tables. so recall how open addressing works. we're only permitted to store one object in each slot so this is unlike the case with chaining where we can have an arbitrarily long list in a given bucket of the hash table. with at most one object per slot, obviously open addressing only makes sense when the load factor alpha is less than one. when the number of objects you're storing in your table is less than the number of slots available because of this requirement we have at most one object per slot we need to demand more of our hash function. our hash function might ask us to put a given object, say with some ip address into say bucket number seventeen but bucket number seventeen might already be full, might already be populated. in that case, we go back to our hash function and ask it where to look for an empty slot next. so maybe it tells us to next look in bucket 41. if 41 is full it tells us to look in bucket number seven and so on two specific strategies for producing a probe sequence that we mentioned earlier were double hashing and linear probing. d ouble hashing is where you use two different hash functions, h1 and h2. h1 tells you which slot in which to search first and then every time you find a full slot you add an increment which is specified by the second hash functions h2. linear probing is even simpler you just have one hash function that tells you where to search first and then you just add one to the slot until you find an empty slot as i mentioned at the beginning, it is quite nontrivial to mathematically analyze the performance of hash tables, using these various open addressing strategies. it's not impossible. there is some quite beautiful and quite informative theoretical work. that does tell us how hash tables perform but that's well outside the scope, of this course. so instead what i wanna do is i want to give you a quick and dirty calculation. that suggests, at least in an idealized world. what kind of performance we should expect from a hash table with open addressing if it's well implemented as a function of the load factor, alpha. precisely, i'm going to introduce a heuristic assumption. it's certainly not true but we'll do it just for a quick and dirty calculation, that we're using a hash function in which each of the n-factorial possible probe sequences is equally likely. now, no hash function you're ever going to use is actually going to satisfy this assumption, and if you think about it for a little bit, you realize that if you use double hashing or linear programming, you're certainly not going to be satisfying that assumption. so this will still give us a kind of best case scenario against to which you can compare the performance of your own hash table implementations. so if you [inaudible] hash table, and you're seeing performance as good, as what's suggested by this idealized [inaudible] analysis, then you're home free. you know your hash table is performing great. so what is the line in the sand that gets drawn, under this heuristic assumption? what is this idealized, idealized hash function performance? as a function of the lo ad alpha well here it is. what i'm gonna argue next is that under this heuristic assumption, the expected amount of time to insert a new object into the hash table, is going to be essentially one over one minus alpha, where alpha is the load. remember the load is the number of objects in the hash table divided by the number of available slots. so if the hash table is half full, then alpha's going to be.5. if it's 75 percent full then alpha's going to be three-fourths. so what this means is that, in this idealized scenario, if you keep the load pretty under control. so, say if the load is 50%, then the insertion time is gonna be great, right? if alpha's.5 and 1/ (1-alpha) =two, so you expect just two probes before the successful insert of the new object and of course, if you're thinking about lookup, that's going to be at least as good as insert, so if you're lucky a lookup might terminate early if you find what you are looking for. in the worst case you go all the way until an empty slot in an unsuccessful search, and that's gonna be the same as insertion. so if alpha is small bounded away from one, you're getting constant time performance. on the other hand, as the hash table gets full, as alpha gets close to one, this operation time is blowing up; it's such a going to infinity as alpha gets close to one. so if you need to have a nice. 90 percent full hash table with open addressing. you're gonna start seeing, ten probes. so, you really wanna keep hash tables with open addressing. you wanna keep the load under control certainly no more than probably.7. maybe even less than that to refresh your memory, with chaining, hash tables are perfectly well-defined even with loads factors bigger than one. what we derived is that under universal hashing, under a weaker assumption, we had an operation time of one plus alpha, for a load of alpha. so with chaining, you just gotta keep alpha, you know, at most, some reasonably small constant with open address, and you really got to keep it well bounded a way below one. so next let's understand why this observation is true. why under the assumption that every probe sequence is equally likely do we expect a one over one minus alpha running time for hash tables with open addressing? so, the reason is pretty simple. and we can derive it by analogy with a simple coin flipping experiment. so, to motivate the experiment, think just about the very first probe that we do. okay, so we get some new objects, some new ip address that we want to insert into our hash table. let's say our hash table's currently 70 percent full. say there's 100 slots, 70 are already taken by objects. well, when we look at this first probe, by assumption it's equally likely to be any one of the 100 slots. 70 of which are full, 30 which are empty so, with probability of one minus alpha, or in the case, 30%, our first probe will, luckily, find an empty slot and we'll be done. we'll just insert the new object into that slot if we get unlucky with a probability, 70%. we find a slot that's already occupied and then we have to try again. so we try a new slot, drawn at random and we again check is it full, or is it not full? and again, with 30 percent probability, essentially it's going to be empty and we can stop and if it's already full. then we try, yet again. so doing random probes, looking for an empty slot, is tantamount to flipping a coin with the probability of heads 1-alpha, or, in this example, 30 percent and the number of probes you need until you successfully insert is just the number of times you flip this last coin until you see a heads. in fact this biased coin flipping experiment slightly overestimates the expected time for insertions and the heuristics assumptions and that's because in the insertion time whenever we're never going to try the same slot twice. we're going to try all end buckets in some order with each of the impact [inaudible] ordering equally likely so back to our example, where we have a hash table with 100 slots, 70 of which are full. the first probe indeed, we have a 30 in 100 chance of ge tting an empty slot. if that one fails then we're not going to try the same slot again. so there is only 99 residual possibilities. again, 30 of which are empty. the one we checked last time was full. so we actually have a 30 over 99 percent chance of getting an empty slot on the second try. like 30 over 98 on the third try, if the second one fails, and so on but, a valid upper bond is just to assume a 30 percent success probability with every single probe, and that's precisely, what this coin flipping experiment will get us. so the next quiz will ask you to actually compute the expected value of capital n, the number of coin flips, needed to get heads when you have a probability of heads of one minus alpha. as a hint, we actually analyzed this exact same coin flipping experiment when alpha equals a half, back when we discussed the expected running time of randomized linear time selection. alright, so the correct answer is the first one. one over 1-alpha so to see why, let's return to our derivation, where we reduced analyzing the expected insertion time to this random variable. the expected number of coin flips until we see a heads. so, i'm gonna solve this exactly the same way that we did it back when we analyzed a randomized, selection algorithm. and it's quite a sneaky way, but very effective. what we're going to do is we're going to express the expected value of capital n, in terms of itself, and then solve. so how do we do that? well on the left hand side let's write the expected number of coin flips, the expected value of capital n, and then let's just notice that there's two different cases, either the first coin flip is a heads or it's not. so in any case you're certainly going to have one coin flip so let's separate that out and count it separately. with probability alpha, the first coin flip is gonna be tails and then you start all over again and because it's a memory less process, the expected number of further coin flips one requires, given that the first coin flip was tails, is just the same as the expected number of coin flips in the first place. so now it's a simple matter to solve this one linear equation for the expected value of n, and we find that it is indeed one over one minus alpha, as claimed. summarizing, under our idealized heuristic assumption, that every single probe sequence is equally likely, the expected insertion time is upper bounded by the expected number of coin flips, which by this argument is, at most, one over one minus alpha. so, as long as your load, alpha, is well bounded below one, you're good. at least in this idealized analysis, you're hash table will, will work extremely quickly. now i hope you're regarding this idealized analysis with a bit of skepticism. right, from a false hypothesis you can literally derive anything you want. and we started with this assumption which is not satisfied, by hash functions you're actually going to use in practice. this heuristic assumption, that all probe sequences are equally likely. so, should you expect this one over one minus alpha bound to hold in practice or not? well, that depends to some extent. it depends on what open addressing strategy you're using. it depends on, how good a hash function you're using. it depends on whether the data is pathological or not. so, just to give course rules of thumb if you're using double hashing and you have non-pathological data, i would go ahead and look for this 1/1-alpha bound in practice. so implement your hash table, check its performance as a function of the load factor alpha and shoot for the 1/1-alpha curve. that's really what you'd like to see. with linear probing, on the other hand, you should not expect to see this performance guarantee of 1/1-alpha even in a totally idealized scenario. remember, linear probing is the strategy where your initial probe, the hash function, tells you where to look first, and then you just skim linearly through the hash table until you find what you're looking for, an empty slot, the. that you're looking up or whatever so a linear probing, even in a best case scenario, it's going to be subject to clumping. you're going to have contiguous groups of slots which are all full, and that's because of the linear probing strategy. now i encourage you to do some experiments with implementations to see this for yourself. so because of clumping with linear probing, even in the idealized scenario, you're not going to see one over one minus alpha. however, you're going to see something worse, but still in idealized situations. quite reasonable so that's the last thing i want to tell you about in this video. now needless to say, with linear probing the heuristic assumption is badly false. the heuristic assumption is pretty much always false to no matter what hashing strategy you're using, but with linear programming it's quote on quote really false. so to see that, the heuristic assumption, say that all in factorial probe sequences are equally likely. so your next probe is going to be uniform or random amongst everything you haven't probed so far but when you're probing, it's totally the opposite. right once you know the first slot that you're looking into say bucket seventeen, slot a7 is gonna be the first slot, you know the rest of the sequence because it's a linear [inaudible] cancel the table. so it's kind of [inaudible] the opposite from each successive probe being independent from the previous ones except not exploring things twice. so to state a conjectured or idealized performance guarantee for hash tables with linear probing, we're going to place, replace the blatant false heuristic assumption by a still false, but more heuristic reasonable assumption. so what do we know? we know that the initial probe with linear probing determines the rest of the sequence. so let's assume that these initial probes are uniform at random, and independent for different keys. of course, once you have the initial probe, you know everything else, but let's assume independence and uniformity amongst the initial probes. now, this is a strong assumption. this is way stronger than assuming you ha ve a universal family of hash functions. this assumption is not satisfied practice, but performance guarantees we can derive under this assumption are typically satisfied in practice by well implemented hash tables that use linear probing. so, the assumption is still useful for deriving the correct, idealized performance of this type of hash table. so what is that performance? well this is an utterly classic result from exactly 50 years ago from 1962 and this is a result by my colleague, the living legend, don canuth, author of art of computer programming. at what can proved is, was that is that under this weaker [inaudible] assumptions, suitable for linear probing. the expected time to insert an object into a hash table with a load factor alpha, when you're using linear probing is worse than one over one minus alpha, but. it is still a function of the load alpha only and not a function of the number of objects in the hash table. that is with linear programming you will not get as good a performance guarantee, but it is still the case that if you keep the load factor bounded away from one. if you make sure the hash table doesn't get too full you will enjoy constant time operations on average so for example if with linear probing your hash table is 50 percent full then you are going to get an expected insertion time of roughly four probes. note however this quantity does approach does blow up pretty rapidly as the hash table grows full. if it is 90 percent full this is already going to be something like a hundred probes on average. so you really don't wanna let hash tables get too full when you are using linear probing. you might well wonder if it's ever worth, implementing linear probing, given that it has the worst performance curve, one over one minus alpha squared. then the performance curve you'd hope from something like double hashing, one over one, minus alpha. and it's a tricky cost benefit analysis between linear probing and more complicated but better performing strategies. that really depends on the ap plication. there are reasons that you do want to use linear probing sometimes, it is actually quite common in practice for example, it's often interacts very well with memory hierarchies so again, as with all of this hash and discussion. you know the costs and benefits are, are very subtle trade-offs between the different approaches. if you have mission critical code that's using a hash table and you really want to optimize it. try a bunch of prototypes, and just test. figure out which one is the best, for your particular type of application. let me conclude the video with a quote from canuck himself where he talks about the rapture of proving this one of our one man is half a square theorem and how it was life changing. he says i first formulated the following derivation, meaning, the proof of that last theorem in 1962. ever since that day, the analysis of algorithms has, in fact, been one of the major themes in my life. 
so, in this video, we're going to discuss bloom filters which is a data structure developed appropriately enough by burton bloom back in 1970. bloom filters are variant on hash tables, you'll recognize a lot of the ideas from our hash table discussion. the win that you get in bloom filters is that they are more space efficient than run of the mill hash tables and they're going to handle, they do allow for errors, there is a non zero false positive probability when you do look ups but that's still a win for some applications. so, it's a very cool idea, very cool data structure. you do see it used quite a bit in practice so let's start talking about it. so, we'll go through the usual topics that we do whenever we discuss a new data structure. so first, i want to tell you what operations they support and what kind of performance you're going to expect from those operations, in other words, what is the api corresponding to the data structure. secondly, i'm going to talk a little bit about what it's good for. so, what are some potential application and then we'll take a peek under the hood. i'll tell you some of the implementation details with an emphasis on explaining why you get the kinds of performance trade offs that you do with bloom filters. so, to first order, the raison d'tre of bloom filters is exactly the same as a hash table. it supports super fast inserts, super fast look ups. you can put stuff in there and you can remember what you put in earlier. now, of course, what you should be wondering is what we already know what data structure that supports super fast in certain look ups, a hash table. why am i bothering to tell you about yet another data structure with exactly those same operations? so, let me tell you about the pros and cons of bloom filters relative to run off the mill hash tables as we've already discussed. the big win is that bloom filters are more space efficient than hash tables. no matter whether they are implemented with chaining or with open addressing, you can store much less space per objects. in fact, as we'll see, less space than that of an object itself using a bloom filter. as far as the cons, well, first of all, this is really for applications where you just want to remember what kind of values you see. you are not trying to store pointers to the objects themselves and just trying to remember values. so, the first drawback of the bloom filter is that because we want to be so space efficient, we don't even want to remember the object itself just whether or not we've seen it before. we're not going to be able to store the objects or even pointers to the objects in a bloom filter. we're just going to remember what we've seen and what we haven't. so, some of you might know the terminology hash set for this kind of variant of a hash table as opposed to a full blown hash table or hash map. the second con is at least in the vanilla implementation of bloom filters that i'm going to describe here, deletions are not allowed. you can only insert, you can't delete. the situation with deletions is very much similar to hash tables implemented with open addressing. it's not that you can't have a bloom filter that accommodates deletion, you can, there are very instances of it but that requires significantly more work and we're not going to discuss it here. so, the first order at least for vanilla bloom filters, you want to think of them as suitable for applications or deletions or not a first order of operation. now, the third con and this is a drawback that we have not see previously using any data structures is bloom filters can actually make mistakes. now, what kind of mistake could this kind of data structure possibly make when all you're really doing is looking something up. well, one of mistake would be a false negative and that means you have inserted something previously then you look it up and the hash table or the bloom filter says, it's not there. so, bloom filters will not have false negatives of this form. you've insert something, you look it up later, it's definitely going to confirm that you inserted it in the past. but bloom filters will have false positives, that means that despite the fact you have never inserted say, a given ip address into the, into the bloom filter, if you look it up later, it will say that you have. so, there will sometimes be in some sense phantom objects in bloom filters, objects which it thinks have been inserted even though they haven't been. so, given that, i am now showing you two data structures with essentially the same functionality, hash tables and bloom filters, at least, if we ignore the deletion issue. you might want to wonder which one is more appropriate, which one is more useful. and because there is these trade offs between the two, the answer as you expect is, it depends on the application, right? so, if it's an application where space is really at a premium, you might want to turn to bloom filters especially if a small chance of a false positive is not deal breaker. if you have some kind of application where false positives are absolutely out of the question, of course, you should not use a bloom filter and you want to think about a hash table. so, what are some situations where people actually do use bloom filters where you either really care about space and/or you don't really care about this false positive probability. for one of the earliest applications of bloom filters, this is not long time ago, this is something like 40 years ago, was the spell checkers. so, how would you implement a spell checker using a bloom filter? well, first you have this insert phase where you basically just go through the entire dictionary word-by-word and you insert every valid word into the bloom filter. then, afterwards, when you're presented with a new document that somebody has written, you're going to go through the document word-by-word for each word, you say, is this in the bloom filter? that is, is this one of the legitimate word from the dictionary which is previously inserted? if the bloom filters says yes, this word is in the dictionary as in we've stored and seen that before, then you treat is as a correctly spelled word and if it's not in the bloom filters, then you treat it as incorrectly spelled word. now, the false positive probability means this isn't a perfect spell checker. i mean sometimes, you're going to look up a misspelled word and the bloom filter won't catch it and it willl actually say yes, with small probability, we'll say, this is a legitimate word. so, you know, it's not ideal but, you know, the, the english language is pretty big and space was definitely at a premium, 40 plus years ago. so, it was a win for that application at that time, to use bloom filters to implement a spell checker. another application which, you know, remains relevant today is to keep track of a list of forbidden passwords. now, why would you have forbidden passwords? well, maybe, you want to keep track of password which are too weak or too easy to guess or too common. you may, yourself, have used the piece of software or website at some point where it asked you for a password and if you typed in something which is too simple or too easy, rejected it and asked you to type in another one. so, one way to implement a list of forbidden passwords is just with the bloom filter and the idea is similar to the spell checker. you first, insert into the bloom filter all of the passwords that you don't want anybody to use for whatever reason. then, when a client comes and tries to type in a new password, you look it up in the bloom filter and if you get a positive look up, then you tell the user, no, that's no good, you can't use that password, choose another one. and this is an application where you really don't care about the errors, you really don't care about the fact that there's a false positive rate. let's assume that the error rate is something like one percent or 0.1%. so, what would that means in context, that would just mean once in a while, one in a hundred clients or one in a thousand clients actually types in a perfectly strong password that gets rejected by the bloom filter and they have to type in a second one. okay, but big deal and if space is at the, the premium, this is definitely a win to use this super lightweight data structure to keep track of these blocked passwords. these days certainly one the killer applications of bloom filters is in software deployed on network routers. so, the machinery out in the internet which is responsible for transmitting packets from one place to another. so, what are the reasons why bloom filters have found fertile application in network routers? well, first of all, you do have a budget on space, typically on network routers. there's a lot of things that you got to do and you don't want to waste that much of it on some random data structure to do one's specific task. so, you do have a budget on space and also, you need super, super fast data structures, right? since these packets are coming in at this torrential rate which you can't even imagine and you want to process these packets in real time, sending them off to the next top. bloom filters are the work force behind a lot of different tasks that is done in the network router. you can imagine wanting to keep track of blocked ip addresses, you can imagine keeping track of the contents of some cache so you don't do spurious look ups. you can imagine maintaining statistics to check for denial of service attacks and so on and so forth. so, summarizing as a expert programmer, what is it that you should remember about bloom filters, what purpose does this tool serve in your tool box? well, as far as the operation supported which is the same as a hash table, the point is to have super fast inserts, super fast look ups. but bloom filters are more lightweight version of a hash table. so, they are more space efficient but they do have this drawback of having a small error probability. so, those are the key features you should remember when deciding whether or not you are working on an application that could make good use of this data structure. so, having discussed one of th e operations and what these data structures are good for, let's take it to the next level, let's peer under the hood and see how they are actually implemented. cuz this is really a quite simple, quite cool idea. so, like hash tables, bloom filters have essentially two ingredients. first of all, there's an array and second of all, there's a hash function or in fact, several hash functions. so, we're going to have a random access array except, instead of having n buckets or n slots as we've been calling them, each entry in this array is just going to be a single bit. each entry in this array can only take on two values, zero or one. and the way they think about the space occupied by bloom filters is in terms of the number of bits per object that has been inserted into the bloom filter. so, if you have inserted the data set capital s, then the total number of bits is n, the number of objects that have been inserted is cardinality of s. so, n / |s| is the number of bits in this data structure that you are using per entry in the data set. now, you can tune a bloom filter so this ratio is any number of different quantities but for now, i encourage you to think of this ratio as being eight, that is for each object stored in the bloom filter, you are using only eight bits of memory. that will help you appreciate just how amazing this data structures are, right, cuz maybe our data set is something like ip addresses which is 32 bits so what i'm saying here, if this is eight, i'm saying we are not, definitely not actually storing the ip address. so, we have this 32-bit object we are inserting and we are only using eight bits of memory. this is how we are going to remember whether its there or whether its not. and again, certainly, eight bits per object is way less than keeping a pointer to some associated memory somewhere. so, this is a really impressive minimal use of space to keep track of what we've seen and what we haven't. and secondly, we need mappings of given an object to say, given the ip address, what are the relevant bits for seeing if we've seen this ip address before or not? so, in a bloom filter, its important to have not one hash function, but several hash functions. so, k is going to denote the number of hash functions in the bloom filter which you think of k is some small constant somewhere, you know, three, four, five, or something like that. so, obviously it's a little bit more complicated to use multiple hash functions as supposed to just one hash function. but it's really not that big of deal. so, we'll call from our discussion of say, universal hashing, we have identified the entire families of hash functions which will work well on average. so, instead of choosing just using one hash function at random from universal family, you gave me k independent random choices from universal family. in fact, in practice, it seems to typically be enough to just use two different hash functions and then generate k different linear combinations of those two hash functions. but for the purposes of this video, let's just assume that we've done enough work to come up with k, different good hash functions and that's what we're going to be using in our bloom filter. so, the code for both insert and delete is very elegant. so, let's start by insertion. so, suppose we have some new ip address and we want to stick into these bloom filter, what we do? well, we'll just evaluate each of our k hash functions on this new object. each of those tells us an index into our array of bits and we'll just set those k bits equal to one. and when we do this insert, we don't even bother to look at what the previous values of these bits were.. so, zero or one, we don't care. we'll just blithely go in and set this k bits equal to one, whatever they were before. so, what about looking up? how are we going to implement that? well, all you have to do is check for the footprint that was inevitably left by a prior insertion. so, if we're looking up an ip address and we know was inserted sometime in the past, what happened when we evaluated the k hash functions, we went t o appropriate positions in the array and we set all of those bits to one. so now, i'll just check that, that indeed happened, that is when we get a new ip address, we're looking it up. we evaluate the hash functions, all k of them. we look at the corresponding k positions and we verified that indeed those k bits have been set to one. so, what i hope is clear fairly quickly from inspecting this very elegant code is that we will not ever have false negatives, yet, we might have false positives. so, let's discuss those one other time. so, remember, a false negative would mean that the bloom filter says, something isn't there when in fact, it is, that is we insert something and we'll look it up later and the bloom filter rejects us. well, that's not going to happen. cuz when we insert something, we set the relevant k bits to one. notice when a bit is one, it remains one forevermore. that bits are never reset back to zero. so, if anything was ever inserted in the subs when we look it up, definitely we well confirm that all those bits are one. so, we're never going to be rejected by something we inserted before. on the other hand, it is totally possible that we will have a false positive. it's totally possible that there will be a phantom object and we'll do a look up and the bloom filter will turn yes when we never inserted that object. suppose for example, the k = three. so, we're using three different hash functions. consider some ip address, fixed ip address, maybe the three hash functions tell us the relevant bits are seventeen, 23, and 36. maybe we never inserted this ip address, but we have inserted ip address number two and in its insertion, the seventeenth bit got set to one. we inserted some other ip address, ip address number three and the twenty-third bit got set to one. and then we inserted ip address number four and the 36th bit got set to one. so, three different ip addresses were responsible for setting these three different bits but whatever, its not like we are remembering that. and that once, once we look up this ip address we really care about, what do we do, we just inspect bit seventeen, its one. inspect the 23, its one. we inspect the 36, its also one. for all we know, this thing really was inserted and the bloom filter is going to say, yes, it's in the table. so, that's how we have false positives. all of the bits that are indicating whether or not a given object are in, are in the bloom filter were previously set by insertions from other objects. so, there are two points that i hope are clear at this stage of the discussion. first of all, that this bloom filter, the idea does suggests a possibility of a super space efficient variant of a hash table, right. so, we've been talking about setting the number of bits to be say roughly eight times the number of objects that you're storing so you're only using eight bits per object and for most objects, that's going to be radically smaller than just the simple array, storing the objects themselves. again, if their ip addresses we're only have 25 percent much space as we actually stored those ip address in just an array with no extra bells and whistles. the second point is that we're inevitably going to have errors in a bloom filter, we will have false positives or we look something up and it says, its there when in fact, its not. so, those two points i hope are clear. what's actually not clear is the bottom line. is this actually a useful idea? for this to be useful, it'd better be the case that the error probability can be quite small even while the space per object is quite small. if we can't get those two things small simultaneously, this is a bad idea and we should always just use a hash table instead. so, to evaluate the quality of this idea, we're going to have to do little bit of mathematical analysis. that's what i'm going to show you in the next couple of slides. 
so, before we embark in the analysis, what are we hoping to understand? well, it seems intuitively clear is that there is going to be some trade off between the two resources of the bloom filter. one resource is space consumption, the other resource is essentially correctness so the more space we use, the larger number of bits, we'd hope that we'd make fewer and fewer errors. and then as we compress the table more and more, we use bits more and more for different objects then presumably the error rate is going to increase. so, the goal of the analysis that we're about to do is to understand this trade off precisely at qualitative level. once we understand the trade off occur between these two resources, then we can ask is there is a sweet spot which gives us a useful data structure? quite small space and quite manageable error probability. so the way we're going to proceed with the analysis, we'll be familiar to those of you who watched the open addressing video about hash tables so to make the mathematical analysis tractable, i'm going to make a heuristic assumption the strong assumption which is not really satisfied by hash functions you would use in the practice. we're going to use that assumption to derive a performance guarantee for bloom filters but as all as any implementation you should check that your implementation actually is getting performance comparable to what the idealizing analysis suggest. that said, if you use a good hash function and if you have a non-pathological data, the hopes and this is going out many empirical studies is that you will see performance comparable to what this heuristic analysis will suggest. so, what is the heuristic assumption? well, it's going to be again familiar from my hashing discussions. we're just going to assume that all the hashing is totally random. so, for each choice of a hash function hi and for each possible object ax, the slots, the position of the array which the hash functions gives for that object is uniformly random and first of all and it's independen t from all other outputs of all hash functions on all objects. so the set up then is we have n bits. we have a data set, s which we have inserted into our bloom filter. now our eventual goal is to understand the error rate or the false positive probability. that is the chance that an object which we haven't inserted into the bloom filter looks as if it has been inserted into the bloom filter but as a preliminary step, i want to ask about the population of 1s after we've inserted this data set s into the bloom filter. so, specifically let's focus on a particular position of the array and by symmetry it doesn't matter which one. and let's ask what is the probability that a given bit, a given position on this array has been set to one after we've inserted this entire data set s? alright, so this, this is a somewhat difficult quiz question actually. the correct answer is the second answer. it's one - quantity one - 1/n raised to the number of hash functions k the number of objects cardinality of s, that's the probability let's say the first bit of the bloom filter has been set to one after the data set s has been inserted. so the, maybe the easiest way to see this is to first focus on the first answer. so, the first answer is going to be the probability i claim that the first bit is zero after the entire data set has been inserted. then of course it's probably it's a one, is just the one - its quantity which is equal to the second answer. so we just seem to understand why the first choice is probably the first bit = zero. well, it's initially zero, remember stuff is only set from zero to one. so we really need to analyze the probability that this first bit survives all of these darts that are getting thrown to the bloom filter over the course of this entire data set being inserted. so there, the cardinality of these objects each get inserted on an insertion k darts uniformly at random and independent from each other or effectively thrown at the array at the bloom filter. any position of the dart hits, gets set to one. maybe it was one already but if it was zero, it gets set to one. if it's one then it stays one. so, how is this first pick going to stay zero? we'll have to be missed by all of the darts. a given dart, a given bit flick is uniformly likely to be any of the n bits so the probability of the ones that being this bit is only 1/n but, if it even it's fortunately somebody else? well, that's one - 1/n so you have a chance of surviving a single dart with probably one - 1/n there is the number of hash functions k the number of objects cardinality that's a dart being thrown. right k per object that gets inserted so the overall probability of eluding all of the darts is one - one or n raised to the number of hash functions k the number of insertions cardinality of s. again, the probability that is one which is the one - that quantity which is the second option in the quiz. so, let's go ahead and resume our analysis using the answer to that quiz. so, what do we discover, discover the probability that a given bit is one, is one - quantity one - 1/n or n is the number of position raised to the number of hash functions k the number of insertions cardinality of s. so, that's the kind of messy quantity so let's recall a simple estimation facts that we used once earlier. you saw this when we analyzed cardinals construction algorithm and the benefit of multiple repetitions or cardinals contraction algorithm. and the trick here is to estimate a quantity that's on the form of one + x or one - x by either the x or the - x as the case maybe. so you take the function one + x which goes through the points -ten and 01. and of course it's a straight line and then you also look at the function e to the x. well, those two functions are going to kiss at the point 0,1 and everywhere else e to the x is going to be above one + x. so for any real value of x we can always upper bound the quantity one + x by either the - x. so let's apply this fact to this quantity here, one - 1/n raise to the k cardinality of s. we're going to take x to the - 1/n so that gives us an upper bound on this probability of one - e to th e - k the number of insertions over n, okay? so that's taking x to the - 1/n. let's simplify and finalize a little bit further by introducing some notation. so, i'm going to let b denote the number of bits that were using per object. so this is the quantity i was telling you to think about as eighth previously. this is the ratio n, the total number of bits divided by the cardinality of s. so, this green expression becomes one - e^k where b is the number of bits per object. and now we're already seeing this type of trade off that we're expecting. remember we're expecting that as we use more and more space, then the error rate we think should go down so if you can press the table a lot or use bits for lots of different objects that's when you start going to see a lot of false positives so in this light blue expression if you take the number of bits per objects with the number space, the amount of space, little b if you take that going very large expanding to infinity, this exponent to zero. so either the -zero is one. so overall, this probability of a given bit being one is turning to zero. so, that is, the more bits you have, the bigger space you have. the, well, the smaller of the fraction of 1s. the bigger the faction of 0s. that should translate to a smaller false positive probability unless we will make precise on the next and final slot. so let's, let's rewrite the upshot form the last slide but probability that a given bit is equal to one is that at above by one - e to the - k over b where k is the number of hash functions and b is the number of bits we're using per object. now this is not the quantity that you care about. the quantity we care about is a false positive probability where something looks like it's in the bloom filter even though it's never been inserted so it's focused on some object like some ip address which is never ever been inserted into this bloom filter. so for a given object x which is not in the data set, that this has not been inserted into the bloom filter or what has to happen for us to have a success ful look up for false positive for this object? well each one of its k bits has to be set to one. so, we already computed the probability that a given bit is set to one. so, what has to happen for all k of the bits that indicates x's membership in the bloom filter all k of them has to be set to one. so we just take the quantity we computed on the previous slide and we raise that to the kth power. indicating that it has to happen k different times. so believe it or not we now have exactly what we wanted. what we set out to do which is derive a qualitative understanding of the intuitive trade off between the one hand space used and on the other hand on the error probability. the false positive of probability. so, we're going to call this green circle quantity and name it. we'll call it epsilon for the error rate and again all errors are false positives. and again as b goes to infinity, as we use more and more space, this exponent goes to zero so one - e to that quantity is going to zero as well. and of course, once we power it through the kth power, it gets even closer to zero. so if the bigger b gets the small of this error rate epsilon gets. so now let's get to the punch line. so remember the question is, is this data structure actually useful? can we actually set all of the parameters in a way that we could both really usefully small space but a tolerable error epsilon? and, of course we wouldn't be giving this video if the answer wasn't yes. now one thing i've been alluding all along is how do we set k? how do we choose the number of hash functions? i told you at the very beginning we think of k as a small constant like 2345. and now that we have this really nice qualitative version of how the error rate in the space trade off with each other. we can answer how to set k. namely set k optimally so what do i mean? well, fix the number of bits that you're using per object. eight, sixteen, 24, whatever. for fixed b, you can just choose the k that minimize the screen quantity. that minimizes the error rate epsilon. so, how do you minimize t his quantity? well, you do it just like you learn in calculus and i'll leave this as an exercise for you to do in the privacy of your own home. but for fixed b, the way to get this green quantity epsilon as small as possible is to set the number of hash functions k to be roughly the natural log of two. that's a number of < one notice that's like .693 b. so, in other words the number of hash functions for the optimal implementation of the bloom filter is scaling linearly than the number of bits that you're using per object. it's about .693 the bits per object. of course this is generally not going to be an integer so you just pick k either this number rounded up or this number rounded down. but, continuing the heuristic analysis, now that we know how to set k optimally to minimize the error for a given amount of space we can plug that value of k back in and see well, how does the space and the error rate trade off against each other and we get a very nice answer. specifically, we get that the error rate epsilon is just under an optimal trades to the number of hash functions k decreases exponentially in the number of bits that you use per object. so, it's roughly one half raised to the natural log of two or .693 roughly the number of bits per object b. but, again the key qualitative point here is notice that epsilon is going down really quickly as you scale b. if you double the number of bits that you're allocating per object, you're squaring the error rate and for small error rates, squaring it makes it much, much, much smaller. and of course this is just one equation in two variables. if you prefer, you can solve this equation to express b, the space requirement as a function of an error requirement. so if you know that the tolerance for false positives in your application is one percent you can just solve this for b and figure out how many bits per object you need to allocate. and so rewriting what you get is that the number of bits per object that you need is roughly 1.44 the log base two of one over epsilon. so, as expected as epsilon gets smaller and smaller, you want fewer and fewer errors, the space requirements will increase. so, the final question is, is it a useful data structure? can you set all the parameters so that you get you know, really interesting space error trade off and the answer is totally. so, let me give you an example. let's go back to having eight bits of storage per object so that corresponds to b = eight. then, what this pick formula indicates is we should use five or six hash functions and already you have an error probability of something like two percent which for a lot of the motivating applications we talked about is already good enough. and again, if you double the number of bits to say sixteen per object, then this error probability would be really small. pushing you know one in 5,000 or something like that. so, to conclude at least in this idealized analysis which again, you should check against at any real world implementation although empirically, it is definitely achievable with well implemented bloom filter in nonpathological data to get this kind of performance even with really a ridiculously minuscule amount of space per object much less generally than storing the object itself, you can get fast inserts, fast look ups, you do have to have false positives but with a very controllable amount of error rates and that what's make bloom filters a win in a number of applications. 
the designer analysis of algorithms is the interplay between on the one hand general principles and on the other hand its stantiations of those principles to solve specific problems. while there's no silver bullet in algorithm design, no one technique which solves every computational problem that's ever going to come up. there are general design principles which have proven useful over and over again over the decades for solving problems that arise in different application domains. those, of course, are the principles that we focus on in this class. for example, in part one we studied the divide and conquer algorithm design paradigm, principles of graph search amongst others. on the other hand, we study specific substantiations of these techniques. so in part one, we studied divide and conquer and how it applies to say, strassen matrix multiplication, merge short and quicksort. in graph search, we culminated with the rightfully famous dijkstra's algorithm for computing shortest paths. this, of course, is useful not just, because as any card-carrying computer scientist or programmer, you want to know about what these algorithms are and what they do, but it also gives us a toolbox, a suite of four free primitives, which we can apply to our own computational problems as a building block in some larger program. part two of the course will continue this narrative. we'll learn very general algorithm paradigms. like greedy algorithms, dynamic programming algorithms and many applications, including a number of algorithms for the greatest hits compilation. and in this video and the next, i want to whet your appetite for what's to come, by plucking out two of the applications that we'll study in detail later in the course. specifically, in the dynamic programming section of the course. first of all, for both of these problems, i think their importance is self evident. i don't think i'll have to really discuss why these are interesting problems. why, in some sense, we really need to solve these two problems. secondly, these are quite tricky computational problems. and i would expect that most of you do not currently know good algorithms for these problems and it would be challenging to design one. third, by the end of this class you will know efficient algorithms for both of these problems. in fact, you'll know something much better. you'll know general algorithm design techniques which solve as a special case these two problems and have the potential to solve problems coming up in your own projects as well. and one comment before we get started on these two videos. they're both at a higher level than most of the class, by which i mean there won't be any equations or math. there won't be any concrete pseudo-code, and i'll be glossing over lots of details. the point is just to convey the spirit of what we're going to be studying, and to illustrate the range of applications of the techniques that we're going to learn. so what i want to talk about first is distributed shortest path routing and why it is fundamental to how the internet works. so let me begin with a kind of non very mathematical claim. i claim that we can usefully think of the internet as a graph, as a collection of verticies and a collection of edges. so this is clearly an, clearly an ambiguous statement. there's many things i might mean as we'll discuss. but here's the primary interpretation i want you to have for this particular video. so to specify this, the vertices i intend to be the end-hosts and the routers of the internet. so machines that generate traffic, machines that consume traffic, and machines that help traffic get from one place to another. so the edges are going to be directed and they are meant to represent physical or wireless connections indicating that one machine can talk directly to another one by either a physical link between the two or a direct wireless connection. so it's common that you'll have edges in both directions, so that if machine a can talk to machine b directly, then also machine b can talk directly to machine a, but you definitely want to allow the possibility of asymmetric communication. so, for example, imagine i send an email from my stanford account to one of my old mentors at cornell, where i did my graduate studies. so this piece of data, this email, has to somehow migrate from my machine local at stanford to my mentor's machine over at cornell. so how does that actually happen? well, initially there's a phase of, sort of local transportation, so, this piece of data has to get from my local machine to a place within the stanford network that can talk to the rest of the world. just like if i was trying to travel to cornell, i would have to first use local transportation to get to san francisco airport and only from there could i take an airplane. so this machine from which data can escape from the stanford network to the outside world is called the gateway router. the stanford gateway router passes it on to a networks, whose job is to cross the country. so last i checked, the commercial internet service provider of stanford was cogent so they, of course, have their own gateway router which can talk to the stanford one and vice versa. and of course, these two nodes and the edges between them are just this tiny, tiny, tiny piece embedded in this massive graph, comprising all the end hosts and routers of the internet. so that's the main version of a graph that we're going to talk about in this video, but let me just pause to mention a couple of other graphs that are related to the internet, and quite interesting in their own right. so one graph that is generated an enormous amount of an interest in study is the graph induces by the web. so here, the vertices are going to represent web pages and the edges which is certainly directed represent hyperlinks. not one web page points to another one. so for example, my homepage is one node in this massive, massive graph. and as you might expect, there is a link from my home page to the course page for this class. it is of course essential to use directed edges to faithfully model the web. there is for example, no directed edge from this courses homepage to my own homepage at stanford. so the web really exploded around, you know, mid 90's, late 90's, so for the past 15 plus years, there's been lots of research, about the web graph. i'm sure you won't be surprised to hear that, you know, around the middle of the last decade, people got extremely excited about properties of social networks. those, of course, can also be fruitfully thought of as graphs. here, the vertices are going to be people, and the lengths are going to denote relationships. so, for example, friend relationships and facebook or the following relationship on twitter. so notice the different social networks may correspond to undirected or directed graphs. facebook for example corresponding to an undirected graph, twitter corresponding to a directed graph. so let's now return to the first interpretation i wanted to focus on, that where the vertices are in-hosts and routers and it does represent direct physical or wireless connections indicating that two machines can talk directly to each other. so going back to that graph, let's go back to the story where i'm sending an email to somebody at cornell. and this data has to travel from my local machine to some local machine at cornell. so, in particular, this piece of data has to get from the stanford gateway router, in effect to the airport for stanford's network to the cornell gateway router. so there will be landing airport over on cornell's side. now it's not easy to figure out exactly out what the structure of the routes between stanford and cornell look like. but one thing i can promise you is that there is not a direct physical link between the stanford gateway router and the cornell gateway router. any route between the two is going to comprise multiple hops. it will have intermediate stops. and there's not going to be a unique such route. so if you have the choice between taking one route which stops in houston and then atlanta and then in washington d.c., how would you compare that to one which stops in salt lake city and chicago. well hopefully your first instinct and a perfectly good idea is all else being equal, prefer the path that is in some sense the shortest. now in this context, shortest could mean many things, and it's interesting to think about different definitions. but for simplicity let's just focus on the fewest number of hops, equivalently the fewest number of intermediate stops. well, if we want to actually execute this idea, we clearly need an algorithm that given a source and destination computes the shortest path between the two. so hopefully you feel well equipped to discuss this problem, because one of the highlights of part one of this class, was the discussion of dijkstra's shortest pathalum rhythm in a blazing fast of limitation using heaps that run's in almost linear time. we did mention one caveat when we discussed dijkstra's algorithm mainly that it requires all edge lengths to be non negative but in the context of internet routing almost any edge metric you'd imagine using will satisfy this non negativity assumption. there is, however, a serious issue with trying to apply dijkstra's shortest path algorithm off the shelf to solve this distributed internet routing problem, and the issue was caused by the just massive, distributed scale of modern day internet. probably back in the 1960's, when you had the 12-note arpanet, you could get away with running dijkstra's shortest path algorithm, but not in the twenty-first century. it's not feasible for the stanford gateway router to maintain locally a reasonably accurate model of the entire internet graph. so how can we elude this issue? is it fundamental that because the internet is so massive, it's impossible to run any shortest path algorithm? well, the ray of hope would be if we could have a shortest path algorithm that admitted a distributed implementation. whereby, a node could just interact, perhaps iteratively, with its neighbors with the machines to which its directly connected. and yet, somehow converge to having accurate shortest paths to all of the destinations. so perhaps, the first thing you'd try would be to seek out an implementation of dijkstra's algorithm, where each vertex uses only local computation. that seems hard to do. if you look at the pseudo-code of dijkstra, it just doesn't seem like a localizable algorithm. so instead, what we're going to do is we're going to learn a different shortest path algorithm. it's also a classic. definitely on the greatest hits compilation. it's called the bellman-ford algorithm. so the bellman-ford algorithm, as you'll see, can be thought of as a dynamic programming algorithm. and indeed, it correctly computes shortest path using only local computation. each vertex only communicates in rounds with the other vertices to which it's directly connected. as a bonus, we'll see this algorithm also handles negative edge lengths. which of course, dijkstra's algorithm was not. but don't think dijkstra's algorithm is obsolete. it still has faster running time in situations where you can get away with centralized computation. now, it was really kind of amazing here is that the bellman-ford algorithm, it dates back to the 1950's. so, that's not just pre-internet, that pre-arpanet. so that's before the internet was even a glimmer in anybody's eye. and yet, it really is the foundation for modern internet routing protocol's. needless to say, there's a lot of really hard engineering work and further ideas required to translate the concept from bellman-ford to actually doing routing in the very complex modern day internet. but yet, those protocol's at their foundation, goes all the way back to the bellman-ford algorithm. 
in this video we'll cover a second problem to whet your appetite for things to come, namely the problem of sequence alignment. so this is a fundamental problem in computational genomics. if you take a class on the subject it's very likely to occupy the very first couple of lectures. so in this problem you're given two strings over an alphabet and no prizes for guessing which is the alphabet we're most likely to care about. typically, these strings represent portions of one or more genomes. and just as a toy running example you can just imagine that the two strings were given are a, g, g, g, c, t and a, g, g, c, a. know that the two input strings do not necessarily need to be of the same length. and informally speaking, the goal of this sequence alignment problem is to figure out how similar the two input strings are. obviously, i haven't told you what i mean by two strings being similar. that's something we'll develop over the next couple of slides. why might you want to solve this problem? well, there's actually a lot of reasons. let me just give you two of many examples. what will be the conjecture or the function of regions of a genome that you don't understand, lets say the human genome, from similar regions that exist in genomes that you do understand or at least understand better, say the mouse genome. if you see a string that has a known function in the well understood genome and you see something similar in the poorly understood genome, you might conjecture it has the same or similar function. a totally different reason you might want to compare the genomes of two different species, is to figure out whether one evolved directly from the other and when. a second totally different reason you might want to compare the genomes of two different species is to understand their evolutionary relationship. so for example, maybe you have three species a, b, and c, and you're wondering whether b evolved from a and then c evolved from b, or whether b and c evolved independently from a common ancestor, a. and you might then take genome similarity as a measure of proximity in the evolutionary tree. so having motivated the informal version of the problem, let's work toward making it more formal. in particular, i owe you a discussion of what i mean by two strings being similar. so to develop intuition for this, let's revisit the two strings that we introduced on the previous slide a, g, g, g, c, t, and a, g, g, c, a. now, if we just sort of eyeball these two strings, i mean clearly they're not the same string. but, we somehow feel like they're more similar than they are different. so, where does that intuition come from? well, one way to make it more precise is to notice that these two strings can be nicely aligned in the following sense. lets write down the longer string, a, g, g, g, c, t. and, i'm going to write the shorter string under it, and i'll insert a gap, a space to make the two strings have the same length. i'm going to put the space where there seems to be quote unquote a missing g. and then, what sense is this a nice alignment, well, it's clearly not perfect. we don't' get a character, by character match of the two strings, but there's only two minor flaws. so on the one hand, we did have to insert a gap and we do have to suffer one mismatch in the final column. so this institution motivates defining similarity between two strings with respect to their highest quality alignment, their nicest alignment. so we're getting closer to a formal problem statement, but it's still somewhat underdetermined. specifically, we need to make precise why we might compare, why we might prefer one alignment over another. for example, is it better to have three gaps and no mismatches or is it better to have one gap and one mismatch? so if in this video, we're effectively going to punt on this question. we're going to assume this problem's already been solved experimentally, that it's known and provided this part of the input which is more costly, gaps and various types of mismatches. so here, then, is the formal problem statement. so, in addition to the two strings over a, c, g, t, we are provided as part of the input, a non-negative number indicating the cost we incurred in alignment for each gap that we insert. similarly, for each possible mismatch of two characters, like, for example, mismatching an a and t. we're given as part of the input a corresponding penalty. given this input, the responsibility of a sequence alignment algorithm is to output the alignment that minimizes the sum of the penalties. another way to think of this output, the minimum penalty allignment is, we're trying to find in affect the minimum cost explanation for how one of these strings would've turned into the other. so we can think of a gap as sort of undoing a deletion that occurred some time in the past and we can think of a mismatch as representing a mutation. so this minimum possible total penalty, that is these values of this optimal alignment is famous and fundamental enough to have its own name namely the needleman-wunsch score. so this quantity is named after the two authors that proposed efficient algorithm for computing of the optimal alignment. that appeared way back in 1970, in the journal of molecular biology. and now, at last, we have a formal definition of what it means for two strings to be similar. it means they have a small nw score, a score close to 0. so for example, if you have, if you have a database with a whole bunch of genome fragments, according to this, you're going to define the most similar fragments to be those with the smallest nw score. so, to bring the discussion back squarely into the land of algorithms, let me point out that this definition of genome sum, similarity is intrinsically algorithmic. this definition would be totally useless, unless there existed in efficient algorithm that given two strings and its penalties computes the best alignment between those two strings. if you couldn't compute the score, you would never use it as a measure of similarity. so this observation puts us under a lot of pressure to devise an efficient algorithm for finding the best alignment. so how are we going to do that? well, we can always fall back to brute-force search, where we iterate over all of the conceivable alignments of the two strings, compute the total penalty of each of those alignments, and remember the best one. clearly, correctness is not going to be an issue for brute-force search. it's correct essentially by definition. the issue is how long does it take? so let's ask a simpler question. let's just think about, how many different alignments there are? how many possibilities do we have to try? so if [inaudible] let's imagine, i gave you two strings of length 500, which is a knot of a reasonable length. which of the following english phrases best describes the number of possibilities, the number of alignments given to strings with 500 characters each? so i realize this is sort of a cheeky question, but i hope you can gather that what i was looking for was part d. so you know? so, how big are each of these quantities, anyways? well, in a, in a typical version of this class, you might have about 50,000 students enrolled or so. so that's somewhere between 10^44 and 10^5.5. the number of people on earth is roughly 7,000.000.000. so that's somewhere between 10^9 and 10^10/10. the most common estimate i see for the number of atoms in the known universe is 10^80. and believe it or not, the number of possible alignments of two strings of length 500 is even bigger than that. so i'll leave it for you to convince yourself that the number of possibilities is at least two raised to the 500. the real number is actually noticeably bigger than that. and because 10 is at most 2^4, we can lower bound this number by 10^125 quite a bit bigger than the number of atoms in the universe. and the point of course, is just that it's utterly absurd to envision implementing brute-force search even at a scale of a few hundred characters. and you know, forgetting about these sort of astronomical, if you will, comparisons even if you had string lengths much smaller, say in the you know, a dozen or two, you'd never ever run brute-force or this is not going to work. and of course, notice this is not the kind of problem that's [inaudible] this just doesn't go away if you wait a little while for moore's law to help you. this is a fundamental limitation. it says, you are never going to compute alignments of the strings that you care about, unless you have a fast, clever algorithm. . i'm happy to report that you will indeed learn such a fast and clever algorithm later on in this course. even better, it's just going to be a straightforward instantiation of a much more general algorithm design paradigm. that of dynamic programming. 
today, we're going to embark on the discussion of a new algorithm design paradigm. namely, that of designing and analyzing greedy algorithms. so to put this study of greedy algorithms in a little bit of context, let's just zoom out. let's both review some of the algorithm design paradigms that we've already seen, as well as look forward to some that we're going to learn later on, in this course. so it's sort of a sad fact of life that in algorithm design, there's no one silver bullet. there's no magic potion that's the cure for all your computational problems. so instead, the best we can do, and the focus of these courses, is to discuss general techniques that apply to lots of different problems that arise and lots of different domains. so that's what i mean by algorithm design paradigms. high level problem solving strategies that cut across multiple applications. so let's look at some examples. back in part one, we started with the divide and conquer algorithm design paradigm. a canonical example of that paradigm being the merge sort algorithm. so remember in divide and conquer what you do is you take your problem you break it into smaller sub problems, you solve the sub problems recursively and then you combine the results into a solution for the original problem. like how in merge sort you recursively sort two sub arrays and then merge the results to get a sorted version of the original input array. another paradigm that we touched on in part one, although we didn't discuss it anywhere near as thoroughly, is that of randomized algorithms. so the idea that you could have code flip coins, that is, make random choices, inside the code itself. often, this leads to simpler, more practical, or more elegant algorithms. a canonical application here is the quick sort algorithm using a random pivot element. but we also saw applications for example, to the design of hash functions. so the next measure paradigm we're going to discuss is that of greedy algorithms. so these are algorithms that iteratively make myopic decisions. in fact, we've already seen an example of a greedy algorithm in part one namely dijkstra's shortest path algorithm. and then, the final paradigm we're going to discuss in this class is that of dynamic programming, a very powerful paradigm which solves, in particular, two of the motivating questions we saw earlier namely, sequence alignment, and distributed shortest paths. so what is a greedy algorithm anyways? well, to be honest, i'm not going to offer you a formal definition. in fact, much blood and ink has been spilled over which algorithm is precisely greedy algorithms. but, i'll give you a sort of informal description. a sort of rule of thumb for what greedy algorithms usually look like. generally speaking, what a greedy algorithm does, is make a sequence of decisions with each decision being made myopically. that is, it seems like a good idea at the time and then you hope that everything works out at the end. the best way to get a feel for greedy algorithms is to see examples and the upcoming lectures will give you a number of them. but i want to point out we've actually already seen an example of a greedy algorithm in part one of this course, namely dijkstra's shortest path algorithm. so in what sense is dijkstra's algorithm a greedy algorithm? well if you recall the psuedo code for dijkstra's algorithm, you'll recall there's one main wild loop and the algorithm process's exactly one new destination vertex in each iteration of this wild loop, so there's exactly n - 1 iterations overall, where n is the number of vertices. so the algorithm only gets one shot to compute the shortest path to a given destination. it never goes back and revisits the decision, in that sense the decisions are myoptic, irrevocable and that's the sense in which dijkstra's algorithm is greedy. so let me pause for a moment to discuss the greedy algorithm design paradigm generally. probably this discussion will seem a little abstract so i recommend you revisit this discussion on the slide after we've seen a few examples so at that point i think it will really hit home. so let me proceed by comparing it and contrasting it to the paradigm we've already studied in depth. that of divide and conquer algorithms. so you'll recall that in a divide and conquer algorithm what you do is, you break the problem into sub-problems. so, maybe you take an input array and you split it into two sub-arrays. then you solve the smaller sub-problems recursively, and then you combine the results of the sub-problems into a solution to the original input. so the greedy paradigm is quite different in several respects. first, both a strength and a weakness of the greedy algorithm design paradigm is just how easy it is to apply. so it's often quite easy to come up with plausible greedy algorithms for a problem, even multiple difference plausible greedy algorithms. i think that a point of contrast with divide and conquer algorithms. often it's tricky to come up with a plausible divide and conquer algorithm, and usually you have this eureka moment where you finally figure out how to decompose the problem in the right way. and once you have the eureka moment, you're good to go. so secondly, i'm happy to report that analyzing running time of greedy algorithms will generally be much easier than it was with divide and conquer algorithms. for divide and conquer algorithms it was really unclear whether they were fast or slow, because we had to understand the running time over multiple levels of recursion. on the one hand problems were size was getting smaller, but on the other hand, the number of some problems was proliferating. so we had to work hard, we developed these powerful tools like the master method, and some other techniques, for figuring out just how fast an algorithm like merge sort runs, or just how fast an algorithm like strassen's fast matrix multiplication algorithm runs. in contrast with greedy algorithms, it will often be a one liner. often it will be clear that the work is dominated by say, a sorting sub routine and of course we all know that sorting takes n log and time if you use a sensible algorithm for it. now the catch, and this is the third point of comparison, is we're generally going to have to work much harder to understand correctness issues of greedy algorithms. for divide-and-conquer algorithms we didn't talk much about correctness. it was generally a pretty straightforward induction proof. you can review the lectures on quicksort if you want an example of one of those canonical inductive correctness proofs. but the game totally changes with greedy algorithms. in fact, given a greedy algorithm we often won't even have very good intuition for whether or not they are correct. let alone how to prove they're correct. so even with a correct algorithm, it's often hard to figure out, why it's correct. and in fact, if you remember only one thing from all of this greedy algorithm discussion many years from now, i hope one key thing you remember is they're often not correct. often, especially if it's one you proposed yourself which you're very biased, in favor of. you will think the algorithm, the greedy algorithm must be correct because it's so natural. but many of them are not, so keep that in mind. so to give you some immediate practice with the ubiquitous incorrectness of natural algorithm. let's review a point that we already covered in part one of this class concerning dijkstra's algorithm. now, in part one we made a big deal of what a justly famous algorithm dijkstra's shortest path algorithm is, it runs brazenly fast and it computes all the shortest paths. what else do you want? well remember there was an assumption when we proved that the dijkstra's algorithm is correct. we assumed that every edge of the given network has a non negative length. we did not allow negative edge lengths. and as we discussed in part one, you know, for many applications, you only care about non negative edge lengths. but there are applications where you do want negative edge lengths. so let's review on this quiz why dijkstra's is actually incorrect, despite being so natural. it's incorrect when edges can have negative lengths. so i've drawn in green, a very simple shortest path network with three edges and i've annotated the edges with their links. you'll notice one of those edges does have a negative length, the edge from v to w with length minus two. so the question is consider the source vertex s and the destination vertex w. and the question is, what is the shortest path distance computed by dijkstra's algorithm and you may have to go and review just a pseudo code in part one or on the web. to answer that part of the question and then what is in fact the actual shortest path distance from s to w where as usual the length of a path is just the sum of the lengths of the edges in the path. all right, so the correct answer is d. so let's start with the second part of the question, what is the actual length of a shortest path from s to w when there's only two paths at all in the graph? the one straight from s to w that has length 2, and the one that goes by the intermediate point v that has length 3 + -21, = 1 which is shorter. so, svw is the shortest path that has length 1. why is dijkstra incorrect? well if you go back to the pseudo code of dijkstra, you'll see that in the very first iteration it will greedily find the closest vertex to s in that case this is w, w is closer then v. it will greedily compute the shortest path distance to w knowing the information it has right now and all it knows is there's this one hot path from s to w, so it will irrevocably commute to the shortest path distance from s to w as 2. never reconsidering that decision later. so dijkstra will terminate with the incorrect output that the shortest path link from s to w is 2. this doesn't contradict anything we proved in part one, because we established correctness of dijkstra only under the assumption that all edge links are non-negative, an assumption which is violated in this particular example. but again, the takeaway point here is that, you know, it's easy to write down a greedy algorithm, especially if you came up with it yourself. you probably believe deep in your heart that it's got to be correct all the time, but more often than not, probably your greedy heuristic is nothing more than a heuristic. and there will be instances in which it does the wrong thing. so keep that in mind in greedy algorithm design. so now that my conscience is clear, having warned you about the perils of greedy algorithm design, let's turn to proofs of correctness. that is if you have a greedy algorithm that is correct. and we will see some notable examples in the coming lectures. how would you actually establish that effect? or if you have a greedy algorithm, and you don't know whether or not it is correct, how would you approach trying to understand which one it is, whether it's correct or not? so let me level with you. proving greedy algorithm is correct. frankly, is sort of, more art than science. so, unlike the divide and conquer paradigm, where everything was somewhat formulaic. we had these black box ways of evaluating recurrences. we had this sort of, template for proving algorithms correct. really, proving correctness of greedy algorithms takes a lot of creativity. and it has a bit of an ad hoc flavor. that said, as usual, to the extent that they are recurring themes. that is what i will spend our time together emphasizing. so let me tell you just again about very high level. how you might go about this. you, again, might want to revisit this context aft-, content after you've seen some examples where i think it'll make a lot more sense. so method one is our old friend or perhaps nemesis depending on your disposition, namely proofs by induction. now for a greedy algorithms remember what they do, they sequentially make a bunch of irrevocable decisions, so here the induction is going to be on decisions made by the algorithm. and if you go back to our proof of correctness of dijkstra's algorithm, that in fact is exactly how we proved dijkstra's algorithm correct. it was by induction of the number of iterations, in each iteration of the main wild loop. computed the shortest path to one new destination. and we always proof that assuming all of our previous computations were correct, that's the inductive hypothesis. then so is the computation in the current iteration. and so then by induction, everything the algorithm ever does is correct. so that's a greedy proof by induction that a greedy algorithm can be correct. and we might see some more examples of those in, for other algorithms in the lectures to come. some of the text books call this method of proof greedy stays ahead, meaning you always proof greedy's doing the right thing iteration by iteration. so a second approach to approving the correctness of greedy algorithms which works in a lot of cases is what's called an exchange argument. so you haven't yet seen any examples of exchange arguments in this class so i can't really tell you what they are but that's what we're going to proceed next. i'm going to argue by an exchange argument that a couple of difference famous greedy algorithms are in fact corrected. it has a couple of different flavors one flavor is to approach it by contradiction. you assume for contradiction that a greedy algorithm is incorrect and then you show that you can take an optimal solution and exchange two elements of that optimal solution and get something even better which of course contradicts the assumption that you started with an optimal solution. a different flavor would be to gradually exchange an optimal solution into the one output by a greedy algorithm without making the solution any worse. that would show that the output of the greedy algorithm is in fact optimal. and formally that's done by an induction on the number of exchanges required to transfer an optimum solution into yours. and finally, i've already said it once, but let me say it again, there's not a whole lot of formula behind proving greedy algorithms correct, you often have to be quite creative, you might have to stitch together aspects of method one and method two, you might have to do something completely different. really, any rigorous proof is fair game. 
so what are greedy algorithms good for? well, it turns out they're well suited for a number of fundamental problems across different domains of computer science. and to wet your appetite, for the many examples that we're going to see, i want to begin by discussing the problem of optimal caching. the punchline of the lecture is going to be that a natural greedy algorithm in fact minimizes the number of cache misses over all possible ways of managing a small fast cache. so what is the caching problem? well on the one hand, there's going to be a big, but slow memory which you can think of as holding everything you might be interested in. and then there's also going to be what we call a cache and so this is a much smaller memory to which access is much faster. so this situation comes up all the time across different doings, computer science, architecture, operating systems, networking. just to mention a couple of really obvious examples. you could imagine, the small fast memory being something like an l2 cache. and the big slow memory being main memory. or perhaps actually main memory is the fast memory, and the big slow memory would be disc. now, your task, in the caching problem is to process what we're going to call a sequence of page requests. so a page request just means that the client wants to access something in memory and it's guaranteed to be in the big slow memory. but if its not already in the small fast memory then you got to bring it in, you got to put it in there for the subsequent access. the algorithmic aspect of the problem answers the picture when there is a cache miss or also known as a page fault. that is when there is a request for some data which is not already in the cache. when that is the case, you have to bring it into the cache. the design question then is what do you evict from the cache in order to make room for this new piece of data which you have to bring in. so to illustrate the issues, let's look at extremely simple example. a cache that just has four slots for pieces of data. let's assume that initially the cache is seated with the four pieces of data i'll call a, b, c, and d. now remember the input is a sequence of page requests, so these are requests for different pieces of data. now when a new request comes in, you're basically sitting there crossing your fingers that what's been requested is already in the cache. so for example, if the first request comes in for the piece of data marked c, you said good we're good to go it's already in the cache go ahead and access it. similarly if the next request is for d, you don't have to do anything. good times roll, and d just gets accessed directly. the problem arises when something is requested that's not in the cache. so let's say the next request is for the data item e. now remember, you have to bring e into the cache and of course you have to evict one of these four pieces of data to make room for it, and your algorithm has to decide which. can you get rid of a or b or c or d. for this example, let's assume that we evict a to make room for e. assume further that the next request that comes in is for a new piece of data f. again it's not in the cache so we have to evict something to make room for it. let's assume we get rid of b in order to bring in f. and now an unhappy situation but something that could certainly occur is we get a request for something that used to be in the cache but which we have since evicted. so for example if the next request is for a, then we're stuck. it's going to be another page fault, we have to evict something to bring in a. and similarly, if there's a b, again we're paying the price for evicting b to make room for f in the past. so in this example with these particular choices for page evictions, we incur four page faults. now the first two the e and the f there's nothing we could have done about it. we were given a cache that initially did not have e and f and then e and f showed up, well what are you going to do, you're going to miss no matter what. however, two were caused by our unfortunate eviction decisions early on, to evict a and b only to find them requested after eviction. and with 20/20 hindsight, we can conclude we really should have evicted c and d, not a and b, to make room for e and f. so the point of this example is to illustrate first of all the caching problem, how it works. you have this small fast memory, it can't contain everything at once and so you have to sort of manage the cache and evict things, to make room for stuff as it gets accessed. that's the first point of the example. the second point is to illustrate there's really two types of cache misses or page faults. there's the ones which you know, really you can't do anything about. no matter what algorithm you use, you're going to suffer those faults. but then, depending on the eviction algorithm, you maybe able to avoid some of the cache misses that you would incur with an inferior algorithm. algorithm. so, now the obvious question is, how well can we do? what's the best algorithm? how do we minimize the number of cache misses, suffering only the ones that are inevitable? so this question was given a very elegant answer by belady back in the 1960's. and i'm going to state the answer as a theorem. it's a theorem we're not going to prove, for reasons i'll discuss in a second. but what the theorem says is that a natural greedy algorithm is an optimal algorithm for the caching problem. that is, it minimizes the number of cache misses over any way you might think about managing the cache. and the natural greedy algorithm that is optimal, is called the furthest in the future algorithm. so what is the furthest in the future algorithm? well, it's exactly what you think it would be. it's basically what seems like a good idea at the moment you have to perform a eviction from the cache. basically you want to put off judgment day. you want to put off the regret of evicting this particular piece of data as long as possible. when are you going to regret evicting a piece of data? well, it's when it gets requested next. so if we have four things in the cache, you know, one is one is requested next, one is requested in seven time steps and one is requested in you know, 70 time steps, that's the one you want to evict now because it will take the longest until you actually regret that eviction. so for example, in the example on the previous slide, you can check that the furthest in the future algorithm would in fact evict the ones you want to evict, a and b, not the ones that we evicted in the example, c and d. now at this point, many of you are probably justifiably scratching your heads. you're wondering, you know, why is this useful. it doesn't seem like this is what we wanted. the objection, to this result being that the furthest in the future algorithm is clairvoyant. its very definition assumes that you know the future, it assumes that at the moment that you have to make an eviction you're aware of when each of the pieces data in the cache will be requested next. but if you think for a minute about the motivating applications for sudding the ultimate caching problem, this assumption simply doesn't hold, you simply do not know the future, you simply do not know when each of the pieces of data in your cache will be requested next. so this algorithm is not defined, it is unimplementable. despite that, this is still an extremely useful result to know. why. well, two reasons. first of all, this unimplementable algorithm can never the less, serve as a guide line for practical. implementable algorithms. for example it naturally motivates the lru or least recently used caching algorithm. so what you do in the lru algorithm is that instead of looking forward in the future, which you can't do generally, you look in the past. and you say, well, let me guess that whatever's been requested recently, will be requested again soon. whatever hasn't been request been requested for a long time, will continue to not be requested for a long time. so, that sets as a proxy for the piece of data that's going to be referenced the furthest down the future, you look for the one that was most recently referenced the furthest back in the past. so that's the lru algorithm. and as long as data exhibits what's called locality of reference, meaning whatever's being requested a lot in the recent past is also going to be what's requested in the near future. then lru is going to approximate furthest in the future. and indeed, lru is in many applications, the gold standard amongst practical implementable caching algorithms. the second reason this theorem is useful in practice is because it served as an idealized benchmark. a hypothetical perfect scenario against which you can compare your latest and greatest cashing hereistic. so for example, maybe you have a caching application and you start by implementing the lru least recently used caching algorithm and then as a sanity check you probably want to go back later once you have hindsight you look at the last few days of traces of logs of page requests and you say, how well did we do. let's look at how well our caching algorithm lru did. and let's look at how well we would have done had we known the future. and hopefully, you're just a few percent away. and then you can conclude that, yes indeed, the data seem to have locality reference. yes indeed, lru is doing almost as well as if we know the future, and we can proceed. on the other hand, if you go back through the last few days of logs, and you find that your caching algorithm is doing much worse than furthest in the future, then it's back to the drawing board with respect to your caching algorithm. you should work harder, understand the data better, and come up with a smarter heuristic. so for almost all of the greedy algorithms that we cover in this course, i'm going to explain to you why they are correct. i'm going to prove it rigorously. this algorithm is an exception. i'm actually not going to prove this theorem for you. the way it works is by what's called an exchange argument. so again, you may not have seen any examples, but you will soon. but the exchange argument to prove the latter result, as far as i know it's pretty tricky, believe it or not. even though the algorithm is natural, you might think this result feels a little self-evident. try to prove it rigorously. not easy. not easy at all. indeed if you look at a textbook and say operating systems or a field like that, generally you'll see a description of this algorithm. you'll see the claim that it's optimal but you won't find the proof. some algorithms textbooks, for example algorithm design by kleinberg and tardos, do include the proof of this theorem. those of you that are interested, i challenge you to prove it yourself without looking it up on the web or in a textbook. i think if you try to prove it you'll appreciate the subtleties that come up in understanding whether greedy algorithms are correct or not. in the best case scenario, i would love. love to see, a simple proof of this theorem, something that i could explain in a class like this, in say five minutes or less, that would be amazing. 
for our next case study of how to use greedy algorithms, we're going to turn to the application domain of scheduling. that is how do you schedule jobs on shared resources in order to accomplish some objective. so, the domain of scheduling there's lots of different applications of greedy algorithms. we'll see two in this course. we'll start for today just with the following simple scenario. so, we'll assume, for today, that there's just one shared resource. this resource could represent any number of things. for concreteness, you can think of it as a computer processor. and then, there's a lot of different things that got to get done. so, for example, there's a lot of processes that have to be handled by this processor. in the algprithmic question, we are going to study, is, in what order should we sequence these jobs? which one should we do first, which one should we do second, and so on, all the way up to which one should we do last. so, obviously to answer this question, we need to pin down the mathematical model a little bit more precisely. and lets start with just, you know, what is the characteristics of jobs, what information do we have that might lead us to prefer one job over another. but for this problem, we're going to assume that each job comes with two known parameters. so, first of all, job j has what we're going to call a weight w sub j. that's a non-negative real number. and you should think of the weight of a job as quantifying its importance. that is, jobs with a higher weight, in some sense, deserve to be processed earlier than those with a lower weight. and secondly, each job j is going to come with a non negative length l sub j. depending on the application, you may or may not have a good estimate of how long jobs are going to take, but for today to keep things simple, let's assume we know what the length of every job is, and that's l sub j, it's part of the input to are problem. so. we have now defined the input to this computational problem. we get n jobs each specified by a weight and a length. and we know that the output is going to be a sequence of these n jobs in some order. so, what we have to understand now is what criterion do we want to optimize? what are we trying to accomplish with this sequence? to explain that, i need to tell you about completion times of jobs. so, the completion time of a job is defined hopefully in exactly the way you'd think. so, for the job which is scheduled first, it's just the length of the job because that's how long it takes to process that job. for whatever job gets scheduled second, its completion time is the length of the first job and then, the length of that job itself. so, in other words, it's just the total time which elapses before that job gets completed, okay? so, in general, the completion time of a job is just the sum of the lengths of the jobs scheduled to before that job plus the length of that job itself. to make sure this is clear, let's go through a quick example. so, suppose there are three jobs with lengths one, two, and three. i'm not going to tell you the job weights because they're irrelevant for the purposes of computing the completion time. and let's suppose we do the schedule where we just schedule job one first, the job two, then job three. so, pictorially, i'm going to represent that schedule just by stacking the jobs on top of each other with the interpretation that time starts at the bottom. so, time zero is where we schedule job one. and then, time increases as we go from the bottom to the top of the diagram. and the question then is, what are the completion times of these three jobs? okay. so, the correct answer is answer c. so, for the first job, it gets scheduled first so it's very happy and it just takes one unit of time to complete, so its completion time is one. the second job, well, it has to wait for the first job to complete so one unit of time elapses and then, it itself has to complete so that's two more units so it gets to the completion time of three. for the third job, it has to wait for the first two to complete, so that adds three to the clock, and then plus it takes three units of time for a total of six. so, that's the definition of job completion times. in some sense, we obviously want completion times to be as small as possible. but it's not so simple. in any given schedule, the jobs that are give early on are going to have small completion times and the jobs towards the end are going to have big completion times. so inevitably, we're going be have to trading off the completion times between different jobs. so, what is the optimal way to so that? well, that depends on our objective function, and in scheduling, there's many different objective functions you might want to use. today, i'm just going to tell you about one. it's not the only natural objective function, but it's one of several most natural objective functions. it's called minimizing the weighted sum of completion times. you translate this english phrase into mathematics in the obvious way. what you want to do is you want to minimize the sum over all n jobs of their completion time, but then multiplied by their weight of [unknown] j. okay. so, the sum over j of w j times c j. the w j is the weight and c j is the completion time as defined on the previous slot. if you think about it for a second, you'll realize this is equivalent to minimizing the weighted average of the completion times with the weights given as in the input. so, just to make sure this makes sense, let's go back to the example that we saw. in that example, we had jobs with lengths one, two, and three, and we thought about to schedule or we scheduled them in that order. to evaluate the subjective function, i'd have to tell you their weights, so let's suppose their weights are three, two, and one, respectively. in this case, the weighted sum of completion times in the schedule, in the previous slide, well first, we begin with a, the first job, which has weight three. its completion time, remember, was one. then, we have the second job with weight two, its completion time is three. then, we have the third job with weight one, its completion time was six. so, we sum up the weighted completion times and we get a total of fifteen. and i'll let you verify that, in fact, all of the three factorial or six schedules in that example, this is, in fact, the schedule that minimizes the weighted sum of completion times. and the algorithmic question we're going to study next, is how do we do this in general? given arbitrary input in jobs, weights, and lengths, what is the sequence that minimizes this sum over all n factorial sequences you might consider? 
so the plan for this video is to develop a greedy algorithm that always minimizes the waited sum of completion times of a given set of in jobs. but more than the specific problem, more than the specific algorithm i want you to focus on the process by which we arrive. at this greedy algorithm. because i think this process is really something which you can use yourself, in your own applications. the process we're going to use is we're going to first look at just a special case in the problem, where it's reasonably intuitive what should be the optimal thing to do. looking at these special cases will then motivate a couple of natural greedy algorithms. then, we'll figure out how to narrow a couple of greedy algorithms down to just a single candidate, and that in fact, as we will prove in the following lectures, will always be correct. so let's just briefly recall what is it we're trying to do. the computational problem and instant to specify by end jobs which come along with waits and links, and among all end factorial ways that we can sequence the jobs, we want to somehow home in on the one that minimizes the sum of waited completion times. recall from the previous video that the completion time of a job is just the amount of time that elapses before it's done. so that's going to be the length of all the previous jobs plus the length of job j itself. what we're hoping is going to work out is that we can devise a greedy algorithm that always solves this problem. so maybe i should take a step back and ask, you know why do greedy algorithms seem like a sensible way to approach this scheduling problem? well, you know. in general, greedy algorithms are not guaranteed to work. you may have to do something more complicated. but scheduling still seems like a good place to try them out. remember what a greedy algorithm does, it iteratively makes myopic decisions. an then you hope you have a, a reasonably good result at the end. now, what are we doing? we're studying a sequencing problem. the definition of the problem is to schedule a job then another job then another job all the way up to the last job and so this iterative nature of the solution suggests that at least if you're lucky if the problem is simple enough maybe there's a greedy algorithm which simply schedules the jobs in the correct order one at a time. so, we're going to see if that works for minimizing the sum of waited completion times. so let's start by thinking positive, being optimistic. so let's pause it that the greedy algorithm does exist for this problem. given that we're in the greedy algortihm section of the course you're, probably you'll going to find this hard to believe. but suppose one existed, how would we discover what it is? well a useful technique not just for this problem but, you know more generally in real life, first focus on some special cases of the problem, where it's relatively clear how you should proceed. and the two special cases i want you to think about for this problem are first of all, suppose i told you all of the jobs had exactly the same length but they had different weights, then, what order do you think it makes sense to schedule the jobs in? secondly, suppose that i told you, that all of the jobs had exactly the same weight, but they had different lengths. then, what order do you think you should schedule the jobs in? so first if all the jobs have the same length, you should prefer jobs with larger weights. certainely, eh, this intuitively jives with our semantics of weights, that says more importance, which suggest that higher weight jobs should go first, if you look at the actual formula of minimizing the sum of weight and completion times, if the jobs all have the same length. then the completion times are going to be the same your going to see the same set of them. if all the jobs have length one, then the completion times of the jobs are going to be one, two, three, four all the way up to n. no matter what sequence you use. so to make this as small as possible, you want the highest waits to be associated with the smallest completion times that is, you want them upfront as early as possible. the second special case where jobs have equal waits but varying lengths, i think is a little more subtle. here what you want to do is you always want to favor small jobs, jobs with the smallest lengths, everything else being equal. the reason for that is that scheduling a job at a given position forces all the other jobs to wait for that job to complete. so all the, so whatever job you schedule first has a negative impact on all of the rest of the n minus one jobs. so i'll. things being equal, you want the smallest job there that minimizes the consequences for the jobs that are to follow. if you find this a little unintuitive, i suggest just looking at a very simple example. two jobs. both have weight one, one has length one, one has length two. if you schedule the small job first you'll have completion times of one and three for a total of four but if you schedule the bigger job first you get completion times of two and three for the bigger sum of completion times of five. so the next step is to move beyond special cases, which we understand well. to the general case, which perhaps we don't understand. so suppose all of the weights are different, and all of the lengths are different. well, if we have two jobs, and both of these rules of thumb give us the same advice, we're good. if there's one job which is both higher weight, and smaller than another job, then clearly that job should go first. but what if our two rules of thumb to prefer high weight jobs and to prefer small jobs, give us conflicting advice. what if we have a pair of jobs, where one of them is on the one hand higher weight, higher priority but on the other hand, bigger than the other one. which one should go first? well let's again stay positive, and let's try to think about the simplest kind of algorithm that could conceivably work. it won't be a guarantee that it works, but it might work. so we have these two different parameters, length and weights. maybe we can aggregate these two parameters into a single one, into a single sort of score for each of the jobs so that if we schedule the jobs from high score to low score, we'll always be optimal. that would be great. if we could compile these two numbers into one for each job, and then just sort and be done. there is of course the question of exactly how do we choose this aggregation function. how do we compile length and weight into a single number. well as guidelines we should recall our special case and make sure we respect our two rules of thumb. so all else being equal we should prefer jobs with higher weight. so that says higher weight should meet the higher scores if we're going to schedule the job from high score to low score. and then also if a length is bigger that should decrease the score. we should prefer jobs that have a small length. so this idea leaves open the question of exactly how do we aggregate the length and the weight of a job into a single number. so what i want you to do now is i want you to think for a minute about what kind of simplest possible functions you could use. so again, these are mathematical functions. they take as input two numbers, a length and a weight of a job, and they output a single number, a score. and the function should have the properties that it's increasing in the job's weight, and it's decreasing in the job's length. so there's more than one answer to this question but just sort of dream some sort of ideas of what this function might look like. alright so there is certainly any number of functions that have these properties, but i'm just going to write down for concreteness two of what i think are of the simplest functions that have these properties. so one is going to be based on taking the difference of the two numbers and one is going to be based on taking the ratio of the two numbers. so if you're going to use a function based on the difference, then you're want to be increasing in the way of decreasing the length. then of course, the obvious difference to use is weight minus length, this can be negative sometimes but that doesn't bother us the algorithm is still well defined. and if you're going to use a ratio and you want it to be increasing in weight, decreasing in length, then the sensible ratio to use is, the weight of a job, divided by the length of a job. it is of course possible that you have ties for either one of these scoring functions, so let's just allow ties to be broken arbitrarely. now, what we're seeing here is a concrete instantiation of something i promised you in our high level discussion of greedy algorithms, namely, it's both a strength and a weakness of them that they're really easy to come up with and propose. so here we have just, you know, this one simple problem, and we now have two different, competing greedy algorithms for the problem. now, because these two algorithms don't do the same thing. only one of them, at most, can be always correct. at least one of them has to be wrong sometimes. so, as the algorithm designer, what the process now is. maybe we can rule out at least one of these two proposed greedy algorithms, by showing an example where it doesn't do the right thing. so i want to emphasize this is the type of scenario that's very likely to arise in your own algorithm designed adventures. you might have some problem. you're not sure how to solve it yet. you've brainstormed up a couple of proposed algorithms. and a good thing to do, a good time saver is too quickly rule out some of those algorithms as not the right way to go as a poor approach to the problem. so in this context we have these two greedy algorithms. let's quickly break one of them. show that's it's not always correct. how do we do that? well, a smart way to go would be to come up with an input where the two algorithms do different things. if they do different things, at most one of them is going to be correct. at least one of them is going to be incorrect so that's the plan. now to execute this goal, as usual we want to keep things as simple as possible but no simpler. so, what's the simplest possible instance that could lead to different behavior by two algorithms? well, obviously one job is not enough because there's only one possible feasible solution. but already with two jobs we might be able to have one algorithm flip them one way and the other algorithm schedule them in the opposite order. in fact it is not difficult to come up with an instance with two jobs where they do different things. let me just go ahead and write such an instance down for you now. so suppose i give you two jobs, the first one is both longer and more important than the other one, specifically, its length is five, its weight is three. the second job, its length is merely two but its weight is merely one. so what i want you to do is i want you to take our two proposed greedy algorithms, the first one which orders by difference, the second one which orders by ratio. i want you to execute them on this 2-job input and compute the sum of weighted completion times. and then answer what is the sum of weighted completion times of the corresponding two schedules. alright, so the correct answer, is answer b. let's just briefly go through why. so first let's just make sure we understand which algorithm produces which schedule. so the first job has the better ratio. it's ratio is five 3rds, where as the ratio of the second job is one-half which is smaller. where as the second job has the larger difference, it has a difference of -1 where as the first job has the more negative difference of -2/ so, the first algorithm which orders by difference will schedule the second job first then the first job. the second algorithm will schedule the first job first and then the second one. so it just remains to compute the objective function value of those two schedules. so for the first schedule, with the second job first, while we're, the second job has waits of one, has a completion time of two. the second job has a weight of three and a completion time of seven. so that gives us a total of 23. whereas the schedule produced by the second algorithm we have the weight three job first. it's completion time, now that it's first, is only five and then the second job with weight one get the completion time of seven for a total of 22. so ordering by difference gives us a value of 23. ordering by ratio gives a value of 22. so in this case the ratio does better than the difference. so certainly the difference is not optimal for this specific example. so what have we accomplished? well, what we've done is we very quickly ruled out one of our natural proposed greedy algorithms. we know that ordering by difference is not always correct. again, it's going to be correct in special cases like when all the lengths are equal, where all weights are equal but it is not correct in general. that said, please remember the warning i gave you in the high level discussion of greedy algorithms which is greedy algorithms are very often wrong. just because we know algorithm number one is incorrect sometimes does not at all imply that algorithm number two is guaranteed to be correct. it's really easy to come up with multiple incorrect greedy algorithms for the same problem. it does, however, turn out. in this case for this greedy algorithm, algorithm number two driven by ratio it is happily always correct. but you certainly shouldn't believe this claim until i provide you with a proof. a rigorous argument explaining the correctness. always maintain healthy skepticism about the performance of a greedy algorithm until you learn otherwise. so, this fulfills another promise i gave you in the high-level discussion of greedy algorithms. namely, when they're correct it's often quite difficult to prove it. so this would be the topic of the next couple videos the correctness proof for this greedy heuristic. the third and final thing we discussed about greedy algorithms typically is that their running time is not difficult to analyze. so that's a break that we catch relative to divide and conquer algorithms, and again that's certainly true here, right? so, what does this algorithm do? all it does is compute these ratios and then sort the jobs by ratio, so essentially the algorithm reduces to a single sorting computation and of course from part one, we know very well, how to sort in n log n time. 
so now let's turn our attention to proving the correctness of this greedy algorithm that we devised that proportatedly minimizes the some of the waited completion times. let me remind you of the formal correctness claim. the claim is that our second greedy algorithm. the one which looks at the ratio of each job. the ratio of the weight to the length and sorts them in decreasing order, is always correct. for every possible input, it ouputs the, the sequence of jobs, which minimizes the weighted sum of completion times. and as promised, it wasn't hard to devise the greedy algorithm. it's certainly not hard to analyze its running time, which is n log n. the same time as sorting, but it is quite tricky to prove it correct. the way we're going to do that, is going to be our first example of what's called an exchange argument, which is one of the few recurring principles in the correctness proofs of greedy algorithms. so i'm actually going to give you proofs of two different versions of this claim. both will make use of an exchange argument in slightly different ways. for starters that's going to be in the next video and the next one. i'm going to make a simplifying assumption that there are no ties amongst the ratios, that each job has its own distinct ratio of its weight versus its length. in this case, we will be able to get away with proof by contradiction. so, on this slide lemme give you the high level proof plan and of how this is going to go, we will start delving into the details on the next slide. so on the high level we're going to begin by fixing an arbitrary instance. by which i mean, you know just the description of the weights and lengths that. so remember, we have to prove that our algorithm is always correct. so we just fix an arbitrary instance, and prove correctness on this arbitrary instance. so, as i said, for the case with no ties, we're going to proceed by contradictions. remember, this means we assume what we're trying to prove is false. and from that, we derive something which is obviously false inconsistent. so, what would it mean to assume that this claim is false? it means there exists an instances for which this greedy algorithm does not produce an optimal solution. for which there's some other solution not output by the greedy algorithm, which is better than that of the greedy algorithm. so let me just give you some notation to set this up. we're going to let sigma denote the greedy schedule. and if our claim is false, that means this is not an optimal schedule there's some other one which is better so call this better optimal sigma star. so, to complete a proof by contradiction, we need to derive something which is obviously false and the way we're going to do that here might strike you as initially as a little weird but it turns out to work really well in this context. from this assumptionm that the greedy algorithm is not optimal, and there is a better scheduled sigma star, we're actually going to exhibit yet another schedule which is even better than sigma star. strictly smaller picto.function value than sigma star has. why is that a contradiction? well, by assumption, sigma star is optimal so if you show that there is something even better than sigma star, sigma star is not optimal and that completes the proof by contradiction. so now lets start filling the details of this proof plan and making it rigorous. so as i said in this video and the next we're going to be assuming that all of the ratios are distinct. in general, of course that need not be true and i'll give you a separate argument to handle the case of ties. i'm going to make a second assumption. but, unlike the first assumption, the second assumption has no content. it's just an assumption about notation. i'm going to assume by renaming jobs, that job one, number one, is the one with the highest ratio. job number two is the one with the second highest ration and so on. job ending the one with smallest ratio. as a consequence of this switch in notation the greeter schedule is very simple to describe. it just schedules job one first, then job two second, then job three third, and so on, all the way up to job n. okay, so we have one assumption which is not without loss of generality, and we'll have a separate argument for handling ties. we have a second assumption which is without loss of generality. it's just an agreement amongst friends who want to minimize notation. and now, lets actually derive something with content. so given that the greedy schedule is just the jobs in order, one, two, three, all the way up to n. and given our assumption that the greedy solution is not optimal, and instead there's some other distinct optimal schedule sigma star. i claim that sigma star must contain consecutive jobs, that it is somewhere in the schedule, sigma star, i can isolate a pair of jobs, one executed after the other, such that the earlier of those two consecutive jobs has a larger index. i'm going to call these jobs i and j with i being earlier. so again by virtueof the optimal solution sigma star being something other than the schedule one, two, three up to n there must be two jobs somewhere in the schedule executed in a row one after the other so that the earlier job i has a higher index then the subsequent job j. why is this true? well the reasoning is that, the only schedule. it has the property that indices only go up as you go from the earliest job to the latest job. the only way that indices will always go up is if you schedule the jobs one, two, three, all the way up to n. there is no other schedule with a property that indices always go up other than one, two, three, all the way up to n. so this is an observation that's going to be important in the rest of the proof, so make sure you pause, give yourself enough time to stare at it and commit yourself it is in fact true. any, any schedule other than one, two, three, all the way through n, have to have consecutive jobs, the earlier one having a higher index than the later one. so i'm now in a position to explain the exchange in the exchange argument. so let me just distill the two key points from the discussion so far. so first of all we have changed notations so that ratios are decreasing with index and this is exactly the same as the schedule that the greedy algorithm will output. and then assuming that the optimal schedule sigma star is something else we know it has consecutive jobs with a earlier one having a higher index. keep in mind, our high level proof plan from the first slide of this video. where, we're doing a proof by contradiction. we need to derive a contradiction and what we're going to do is we're going to exhibit a schedule even better than sigma star thereby contradicting it's purported ophthalmology. so how do we do that? we do that with an exchange. so this exchange is going to take the place of methodic experiment. we're going to take this purportedly optimal schedule sigma star and we're going to switch the order just of the two jobs i and j leaving all of the other jobs unchanged. so sigma star consists of various jobs. it's called stuff collectively. then next is job i and after that immediately is job j. and then there's possibly some more jobs that get executed after j. and remember, we observe that, we can chose i and j so that i has a higher index than j despite being scheduled earlier. then we execute this exchange. the stuff before inj is the same as before. the more stuff after jni, is the same as before. but we're going to have them occur in opposite order. and the key thing we have to understand next is, what are the ramifications of this exchange? what are the costs? what are the benefits? that's how we'll begin, the next video . 
okay let's continue the perfect correctness of our greedy algorithm for minimizing the sum of the weight of completion times and let's move onto understanding the ramifications of the exchange of jobs suggested at the conclusion of the previous video. so recall the basic ideas to use this observation that the optimal schedule sigma star by virtue of our assumption of being different from the greedy one has to have this pair of consecutive jobs where the earlier one has the higher index. so my question for you involves thinking about how the completion times of all of the jobs change after we make this exchange of the two jobs i and j. which ones go up? which ones go down? which ones are unaffected? which ones can we not actually predict whether they go up or down? all right, so the answer to this, quite key question is the third answer. jobs other than i and j are unaffected, the completion time of job i goes up, and the completion time of job j goes down. so let's review why. consider a job k, other than i or j, it's probably easiest to see if in sigma-star, k completes before i and j, scheduled earlier than i and j. don't, remember what the completion time of a job is, it's just the time that needs to elapse before it gets done, so it's the sum of the job lengths up to, and including that job itself. so if k was scheduled before i and j before, it's exactly the same after i and j are swapped. you don't know the difference. exactly the same set of jobs precedes job k is the force whose completion time is the same. but if you think about it, this exact same argument is true for jobs k that succeed inj. so before we do the swap, it has to wait for a bunches ops to complete, including inj and after we swap inj, it still has to wait for inj to complete. yeah, they complete in opposite order but who cares? the amount of time of the lapse is exactly the same. so importantly, jobs other than inj are completely agnostic to the swap. their completion time is exactly the same as before. so that's the first. part. so job i, it's completion time goes up. it's easier to see why it used to be before j, now it has to wait for j. in fact we can say exactly how much completion time goes up by. it goes up by exactly the length of j. that is the extra time that now needs to elapse before i gets completed. by exactly the same reasoning the completion time of job j actually drops it has to wait for the same jobs to complete before it being accepted no longer has to wait for job i. so, not only can we say its completion time goes down we can say that it goes down so by precisely the length of job i. so now we are in a great position to finish off this proof. let's summarize what we got so far. so, for a cost benefit analysis of exchanging i and j, we discovered the cost is limited to an increase in the completion time of job i and the benefit is limited to the decrease in the completion time of job j. specifically the cost, the new cost incurred by the exchange is the weight of job i times the amount by which it's completion time went up, namely the length of job j. similarly, the benefit of the exchange is the drop of li and j's completion time and that gets multiplied by it's weight's of wj. so now finally we are at the point where we can use the fact that this purportedly optimal schedule sigma star schedules i and j in some sense incorrectly, with a higher index job first despite it having a lower ratio in contrast to the principles followed by our greedy algorithm. so why is it relevant that the earlier job i has a higher index? well, a higher index corresponds to a lower ratio. remember we did this notation switch so that the first job has the highest ratio, the second job has the second highest ratio and so on. so the bigger your index, the further you are down in the ordering, the lower your ratio. so because i is bugger than j, that means i's ratio is worse than j. the usefulness of this becomes even more apparent when we clear denominators. we multiply both sides by the, by li times lj. and then we find that wi times lj is strictly less than wj times li. but what are these, these are just the cost and benefit terms respectively of our thought experiment exchange, exchanging i and j. so what does it mean that the benefit of the swap outweighs the cost? it mean if we take sigma star, this reportedly optimal solution, and we invert the jobs i and j, we get a schedule with an even better weighted sum of completion times, but that is nuts. that's absurd, sigma star was supposed to be optimal. so here's our contradiction. that completes the proof. 
so in this video, we're going to revisit our greedy algorithm for minimizing the wait and somewhat completion times. and we're going to give a more robust more general correctness proof that also accommodates ties amongst the ratios of the different jobs. the main reason i'm doing this is not because you know, i think the result is, is so earth-shattering in its own right, but rather to give you further examples of exchange arguments in action, in particular outside of a proof by contradiction. so let's be formal about our new more general correctness proof. so we're again talking about the greedy algorithm which orders jobs by the ratio of weight to the length. we're no longer assuming these are distinct. so we can't really say decreasing order. we'll say non-increasing order. and ties can be broken any way you want. we'll prove the algorithms commit, correct, no matter how the ties are resolved. so fortunately,, we'll be able to reuse much of the work that we did for the previous correctness proof. but actually, our overall proof plan is going to change a little bit. we're no longer going to proceed by contradiction. so here's the high level, here's the high level plan of that. so as before, we're going to argue correctness on every separate instance, so fix an arbitrary one. so, the notation will be similar to last time, so on this input, we'll let sigma denote the output of our greedy algorithm and then we'll let sigma star denote an arbitrary other competitor, any other schedule of the jobs. so now, we're going to do is were going to show that sigma, the output of our greedy algorithm, is at least as good as sigma star, since sigma star was arbitrary, that means the greedy algorithms output is at least as good as every other schedule, and therefore, sigma has to be optimal. so, let's now fill in the details. we're going to make a similar notational assumption as last time and that we're going to assume that the greedy schedule is just 1, 2, 3, all the way up to n. and again, that's a content free assumption, we can get away with it just by changing the names of the jobs, changing notation. so, recall the proof plan, we have to take any other competing schedule sigma star and show that it's no better than sigma, show that sigma is at least as good as it. so fix any such schedule sigma star, obviously, if sigma star is sigma, then they're they same or just as good as each other so there's nothing to do. now, if sigma star is not the same thing as sigma, if it's not just the sequence 1, 2, 3, all the way up to n, we're going to reuse a key observation from our previous proof, namely any schedule other than just 1 through n has to contain in it a consecutive pair of jobs, i and j, where j is executed immediately after i where i has the higher index. so now we argue similarly to last time. what does it matter that's, one job has a higher index than another. well, that means it's further along in the ordering, which means its ratio can only be smaller. remember, the ratios are non-increasing, they can only go down in the order. so higher index, means lower ratio. but there may be ties, so we can't claim a strict inequality, just a weak inequality. and so again, by clearing denominators, this boils down to the weight of job i times the length of job j is at most the weight of job j times the length of job i. now, the next thing i want you to recall from our previous proof is that there are nice semantics with wilj and wjli namely as the cost in the benefit of exchanging jobs i and j in the schedule sigma star. so arguing as in the previous videos, we can argue, we can claim that exchanging i and j [sound] from a schedule sigma star has a net benefit, that is a benefit minus cost of wjli, that's because job j's completion time drops by li minus wilj. that's because job i's completion time increases by lj with this exchange and so this is non-negative. [sound] so in comparison with our previous proof, in our previous proof, with the assumption of bought us, it bought us the stronger fact than we exchange i and j, in fact sigma star gets strictly better. it's we get a better schedule than what we started with. here with ties, that need not be true. if i and j have exactly the same ratio and we exchange them, then the cost equals the benefit so the net change in the objective function is 0. so we can only claim that by inverting i and j, we don't make sigma star worse. it can only get better and might stay the same. so let's see why that's good enough to nonetheless complete the proof. so what the previous slide gives us, it gives us a method of changing a schedule, massaging a schedule, so that, it doesn't get any worse, it can only get better. specifically, if we take any schedule sigma star, we take any adjacent inversion, by which i just mean two consecutive jobs with the higher one having a higher index. we exchange the jobs in any adjacent inversion, we get a new schedule which can only be better. some of weighted completion times might be the same, but if it's different, it has to be smaller. so in our previous proof, we knew it was strictly better because we had no ties and then our proof by contradiction said we were done. so what are we going to do now? what we're going to do is take this operation, which massages the schedule without making it worse, and we're just going to repeat it over and over and over again, because actually, this operation has a second property. not only can it not make a schedule worse, but, it also decreases the number of inverted job pairs. and here by an inversion, i mean the same thing as when we counted inversions with the divide and conquer algorithm back in part 1. i just need a job pair somewhere in the schedule, where the higher next to one occurs earlier in the ordering. when we exchange adjacent inversion, we uninvert that aversion and because they are adjacent, we don't create any new inversions. so the number of inverted job pairs drops by exactly one, each time we do one of these exchanges. so what does that mean? so here is the proof in a nutshell. we take an arbitrary competitor, some schedule sigma star and we find, either it's the same as the greedy schedule. if it's not, there is an adjacent inversion, in which case we exchange it. we get a schedule that's at least as good and fewer inversions. either this new schedule is sigma, in which case we're done, or it's not, and then we find an adjacent inversion, and we exchange it, it only gets better and we keep going. why can this not continue forever? well, the number of inversions can only be n choose 2 initially, that's if you start with the schedule n, n - 1, n - 2, all the way 1 if the jobs are initially backward. so, we can only do this exchange n choose 2 times before we necessarily terminates witht the greedy schedule, 1 through n. at that point what have we done? we've taken an arbitrary schedule sigma star, we've massaged it, making it only better, and better, and better, and better, terminating with our greedy schedule sigma. what does that say? that says our greedy schedule sigma is at least as good as when we started with sigma star. so the greedy schedule is at least as good as this arbitrary schedule sigma star, so it's optimal. it's better than everything. so one final note before i write down the qed. for those of you familiar with the bubble sort algorithm and it's totally fine if you're not familar with bubble sort, but if you are familiar with bubble sort, you will recognize that essentially what we're doing here inside the proof, not in tour algorithm but inside our proof, we're applying bubble sort in effect to this arbitrary competing schedule sigma star. and by uninverting the inversion we transform it in to the, the greedy schedule, making it only better, thereby justifying as optimal our greedy algorithm schedule sigma. 
so in this sequence of videos, we're going to apply the greedy algorithm design paradigm to a fundamental graph problem, the problem of computing minimum spanning trees. the mst problem is a really fun playground for greedy algorithm design, because it's the singular problem in which pretty much any greedy algorithm you come up with seems to work. so we'll talk about a couple of the famous ones, show why they're correct, and show how they can be implemented using suitable data structures to be blazingly fast. so, i'll give you the formal problem definition on the next slide but first let me just say informally what it is we're trying to accomplish. essentially, what we want do is connect a bunch of points together as cheaply as possible. and, as usual with an abstract problem the objects can mean something very literal. so maybe the points we're trying to connect are servers in some computer network, or it could represent something more abstract. like maybe we have a model of documents like web pages where we represent them as points in space. and we want to somehow connect those together. now the main reason i'm going to spend time on the minimum expenditure problem is pedagogical. it's just a great problem for sharpening your skills with greedy algoritum design and proof of correctness. it'll also give us another opportunity to see the beautiful interplay between data structures and fast limitation of graph algorithms. that said that minimum expenditure problem does have applications. one very cool one is in clustering, and that i'll talk about in detail in a later video, it also comes up in networking. so if you do a web search on spanning tree protocol you'll also find some information about that. so as i said at the beginning the minimum spanning tree problem is remarkable in that it doesn't just admit one greedy algorithm that's correct, but in fact it admits multiple greedy algorithms that are correct. we're going to talk about two of them, the two most well known ones. but there are even some others believe it or not. so the first one we're going to discuss beginning in the next video is prim's mst algorithm. this dates back over 50 years to 1957. in fact as you'll see prim's algorithm shows a remarkable number of similarities with dijkstra's shortest path algorithm. so you might not be surprised to know that dijkstra also independently had discovered this algorithm a couple of years later. but in fact it was only noticed much later that this exact same algorithm had been first discovered over 25 years earlier by a mathematician named jarnick. for that reason you'll sometimes hear this called jarnick's algorithm or the prim-jarnick algorithm. for gravity and to be consistent with some of the main text books in the area i'm just going to call this prim's algorithm throughout the lectures. the other algorithm we're going to cover which is also rightfully famous is kruskal's mst algorithm. as far as i know this was indeed first discovered by kruskal roughly the same time as prim was doing his algorithm in the mid 50s. and in what sense do i say these algorithms are blazingly fast? well, they run in almost linear time, linear in the number of edges of the graph. specifically we'll see how using appropriate data structures will get each of them to run in time big o of m log n, where m is the number of edges in the graph, and n is the number of vertices in the graph. we'll employ data structures to speed up prim's algorithm in exactly the same way we did for dijkstra's algorithm, that is we'll be using the heap data structure, one thing that's cool about crystal's algorithm is it'll give us an opportunity to study a new data structure, mainly the union fine data structure and that's a lot of fun to think about, in its own right, as you'll see. so to put this amazing running time on perspective i want to emphasize that only is it awesome in the sense it's you know, barely, it's almost linear. it takes almost barely more time to compute the spanning tree than it does to read the input graph. reading the input graph alone, remember would take linear time. o of m time. but more over, graphs can have an enormous number of spanning trees. an exponential number. so some of these algorithms are honing in really quickly on a needle in a haystack. there's no way they have time to look at all these spanning tees, and yet they find the one which is the best which is optimal amongst all of them. how do these seemingly magical algorithms do it? well, to discuss the details let's start by formalizing the minimum spanning tree, or mst problem on the next slot. so in the msd problem this is a graph problem so the main part of the input is a graph comprising verticies and edges. i do want to emphasize for the mst problem we are be considering only undirected graphs. this is different notice, than when we discussed shortest-path problems in part one of the course. there we worked with directed graphs. there is an analogous problem to the [inaudible] signature problem for directed graphs. it's often called the optimal branching problem. and there are fast algorithms for it, but those algorithms are just slightly beyond the scope of this course. so we're not going to cover it. we're going to discuss only undirected graphs, and then minimum spanning trees for them. now, whenever you talk about graph problems, you need to talk about, how is the graph actually represented. so that's something we discussed at length in part one. if you don't remember, i suggest going back and reviewing the video on graph representations. for the mst problem, we're going to assume that the graph is given as an adjacency list. that means, we're given an array of vertices, an array of edges. and we have pointers, wiring vertices to their incident edges and wiring edges back to their two endpoints. in addition to the graph of self the input includes a cost, for each of the edges, we're going to use the notation c sebies of note the cost of a edge, e. and in another contrast, to are discussion of shortest path problems, we're actually not going to care if the edge cost are positive or negative, they can be any number whatsoever. so no prizes for guessing what the outputs supposed to be, it's right there in the problem definition, the output is supposed to be a minimum cost spanning tree of the graph, but let's drill down and explain exactly what we mean by that. so first of all what do we mean by the cost of a tree or generally the cost of a sub graph, as a subset of the edges. well we're just going to be looking at summing up the edges in the tree that we output. now the other question is what do i mean by a tree that spans all vertices? so let me tell you exactly what this means, the sub graph t should have two properties, first of all there can not be any cycles, there can not be any loops in this tree. and by spanning all vertices, what i mean is that this sub graph is what's called connected. that is, there's a path, using the edges and t, from any vertex of the graph to any other vertex. that's what it means to span all of the vertices. so for example, consider the following graph with four vertices and five edges. i've labeled each of the five edges with a cost, which in this case, is just an integer between one and five. so, let's look at some example subgraphs, let's start with the three edges, a, b, b, d and cd. this sub-graph satisfies properties one and two. that is, it has no cycles, there's no loops and it spans all of the vertices. if you start at any one of these four vertices, you can get to any of the other four vertices by using only red edges. so in that sense, this red sub-graph is a spanning tree. however, it is not the minimum cost spanning tree. there is another spanning tree which is even cheaper, has a smaller sum of edge costs, namely the edges ac, ab, and bd. this also has no cycles and it's also connected but the sum of the edge cost is only seven, smaller than the eight of the previous spanning tree. in fact, this pixograph is the unique minimum spanning tree of this graph. there is a sub graph that has three edges which has an even smaller sum, of edge costs, namely the triangle ab, bd and ad. but this light blue sub graph, this triangle, is not a spanning tree. in fact, it fails on both counts. it does obviously have a cycle. it has a loop. that's, what it is by definition. it's also not connected, so there's no way to get from c, the vertex, to any of the other three vertices by following only light blue edges. it's disconnected, and so it fails property one as well. so the mst problem in general is you're given it under a graph, like, for example, this four note, five edge graph, or presumably. something much larger and an interesting problem and your suppose to quickly identify the minimum spanding tree like in this example the pink subgraph. so what i want to do next is something you're probably quite accustomed to me doing by this point, is i want to make a couple of mild simplifying assumptions just among friends. so these assumptions are not important in the sense that all of the conclusions of these lectures will remain true, will remain valid even if these assumptions are violated but it'll make the lectures a little bit easier. it'll allow us to focus on the main points and not get distracted by less relevant details so here are the two assumptions that we're going to make throughout all of the lectures on minimum spanning trees. the first assumption we're going to make is that the input graph g is itself connected. that is g contains a path from any vertex to any other vertex. so why am i making this assumption? well if this assumptions violated then the problem isn't even well defined. if the graph isn't connected then certainly none of it's subgraphs are connected so it has no spanning trees and it's not clear what we're trying to do. so, those of you who still remember the stuff we covered in part one in particular, graph search. should recognize that this condition's easy to check in a pre-processing step. just run something like breadth first search or depth first search. remember, we know how to implement those in linear time. and those will, in particular, tell you whether or not the input graph is connected. now, another thing you might be wondering is, suppose it was disconnected. then what? should be really just sort of throw up our hands and give up? you can define a version of the minimum spanning tree problem. a more general one called minimum spanning forest. where, basically you want the minimum cost sub graph that spans as much stuff as possible. essentially, it's responsible for computing a spanning tree within each of the connected components of the original graph. and using the algorithms i'll show you here, prim's algorithm, kruskal's algorithm, they're easily modified to solve the more general problem with disconnected input graphs as well. but again, for simplicity among friends, let's just focus on the connected graph case that contains all of the main ideas. our second standing assumption throughout all of the minimum of spanning tree lectures will be that in the input graph the edge costs are distinct. so you're already use to this sort of no ties kind of assumption from our foray into scheduling algorithms, and we're going to do something similar here. now again this assumption is not important in the sense that the algorithms that we cover prims algorithm crustgrals algorithm. they remain correct even if the input has equal cost edges, irrespective of how ties are broken. so the algorithms are correct as widely as you would want. that's it. i'm not going to actually prove for you that they are correct with ties. remember we had our scheduling, application it was a little bit easier to get a proof of correctness without ties, i gave you that, and then optionally there was a slightly more complicated argument that handled ties. you can do the same thing here, but i'm just not going to give it to you. i'll leave that for the keen viewer to work out for themselves. 
okay, so it's time to discuss our first minimum spanning tree algorithm namely prim's algorithm. definitely a candidate for the greatest hits compilation. and again remember even though it's called prim's algorithm, it was actually discovered earlier by jarnik. so how's it work? well before showing you any pseudo code, let's first illustrate it on an example. as we go through the example, i hope that the similarities to dijkstra's shortest path algorithm will be evident. i'm going to work with the same example graph from the previous video with four vertices and five edges. the plan is to grow a tree one edge at a time. and we're going to keep growing this tree like a mold. we're going to start from just a seed vertex. and then we're going to suck up one new vertex with each iteration of the algorithm. so, this is similar to dijkstra's algorithm. in dijkstra's algorithm, it was clear where we should grow the initial mold from, because we were given a source vertex, that they're trying to compute the shortest paths out of. we have no source vertex in the minimum spanning tree problem, but it turns out that we can just pick an arbitrary vertex to start. doesn't matter which one, which is cool. so the plan is in e generation we're going to add one edge and span one new vertex adjacent to the ones we're already spanning. now as a greedy algorithm prim is simply going to select the cheapest edge that allows it to span one additional new vertex. now the start of the algorithm here we're not really spamming anything. we are sort of thinking of ourselves as growing from and currently spanning the vertex in the upper right. so what are the edges in which we can span an adjacent vertex? well, there is two inches. there is the top inch that costs one then we'll span an addition in the upper left vertex or the is the edge with cost two on the right. if we include that, we'll be able to span the vertex in the bottom right. so we're not going to be greedy, we're just going to choose the cheaper edge, the edge of cost one. now, the vertices that our tree thus far spans are the top two vertices. so, in the next iteration, we want to add one more edge [cough] to span one additional new vertex. and now we see that there are three edges sticking out of what we're spanning thus far that will allow us to span a new edge. there's the edges that have cost two, three, and four. the two and the three will allow us to span the vertex in the bottom right. if we pick the four, that will allow us to span the vertex in the bottom left. yeah, and we're going to be greedy, so of these three candid edges, we're going to pick the cheapest one which is the edge of cost two. so now the mold that we've been growing is in effect, covers all of the verticies except for the one in the bottom left. so now in the final iteration we want to include one more edge so that we span that final remaining vertex. the one in the bottom left. note that there's there was this edge of cause three that we never added. but it got sucked up into the tree that we grew anyways. so we're going to go ahead and ignore that. adding the three wouldn't allow us to span any more vertices. in fact, it would create a loop which we don't want. so we're going to say, okay. we'll have the two edges that would allow us to span an extra vertex. there's the four and there's the five. we're going to be greedy, we're going to pick the four. and once we have the edges of the cost one and two and three and four we have a spanning tree there's no loops there's a path from any vertex to any other vertex along the pink edges, the cost is seven you might recall from the previous video this is indeed the minimum cost spanning tree of this graph. of course, the fact that we have this simple procedure that works correctly in this toy example, which is four vertices and five edges, really means nothing. i mean you shouldn't draw any immediate conclusions that this is a good algorithm in general even though that is going to be the case. so let's next go and actually define the algorithm generally. so if you have a general graph, what does it mean to start somewhere and grow a mold, span one more vertex each iteration, always proceeding greedily until you are done. so lets spell out the pseudo code on the next slide. so here is prim's minimum spanning tree algorithm. we're going to start with just two lines of initialization. we're going to maintain a set of vertices, capital x. this is meant to the be the vertices that we span so far. again, we need some seed vertex from which to start the process. it doesn't matter where, which one we pick. we're going to get the same tree no matter what, so just call it little s. that's an arbitrary vertex from which we start growth. the other thing we're maintaining is, of course, the tree. so that's initially going to be empty. we're going to add one edge to it in each iteration. an invarient that we are going to maintain throughout the algorithm is that the edges that currently reside in the set capital t span the verticies that currently reside in the set capital x. then we're going to have our main while loop. this is the workhorse of the algorithm. and it's very similar to the one in dijkstra's algorithm. namely, each iteration is responsible for picking one edge crossing the current frontier. advancing to include one new vertex. and again, it's going to be greed. the criterion's going to be different, in fact, simpler, than with dijkstra's algorithm instead of looking at links. we're just going to say, what's the cheapest edge that allows us to span a new vertex? so the loop's going to keep going, as long as there are vertices that we don't yet span. and then what we do is we search to the edges that allow us to span a new vertex. so which edges are those? well we want there to be one endpoint in the set x of vertices we already have our tree spanning and we want the other end point to be non-redundant, so we want it to be outside of x. so if we have an edge that crosses the frontier in this sense, one endpoint in x, in endpoint outside that's how we increase the number of spanned vertices by one in an iteration. so if e is the cheapest edge amongst all of those that cross the front here with one end point on either side, that's the one we're going to add to our tree so far capital t in this iteration, it's end point that's not already in capital x, that's going to be the very text that we add to x in this iteration. and again the semantics of an iteration is that we're trying to increase the number of spanned vertices while paying as little as possible, that's the sense in which a prim's algorithm is a greedy algorithm. so as usual with a greedy algorithm, this seems natural enough, but it's not at all clear that it's correct, that it always computes in minimum spanning tree. in fact, if you think about it's not even obvious, it actually computes a spannin tree at all, minimum or otherwise, but it is correct. let's make that statement precise on the next slide. so the key claim is that prim's algorithm is correct. given any connected input graph, it is guaranteed to output a spanning tree with minimum possible cost. so before we delve into any details, let me just finish this video by telling you about the proof plan. we're going to prove this theorem in two parts. first, we're going to establish that it outputs some spanning tree. maybe, maybe not minimum. even that's non trivial. then we'll worry about arguing that the spanning tree output actually is one of minimum cost. both parts of the proof are interesting. for part one to argue that we output some spanning tree, we're going to review some preliminaries about graphs and about cuts and about spanning trees and graphs. for part two to argue optimality, we're going to rely on a very neat property of spanning trees, minimum spanning trees called the cut property. i'm happy to report so that the work that we do here and in both parts will bear further fruit later we're going to reuse these ingredients when we prove the correctness of another mst algorithm named mccrustgrals algorithm. for those of you who would much rather talk about running time than correctness don't worry your time will come after we wrap up this correctness proof i'll address how do you implement prim's algorithm quickly in particular using heaps we'll get the running time down to the near linear bound of o of m log n. 
okay. so in this video we're going to begin our discussion about why prim's algorithm is correct. why always, for every connected graph outputs a minimum spanning tree of that graph. for this video, we're going to content ourselves with a much more modest school. we're only going to prove for now the prim's algorithm outputs a spanning tree. we're not going to make any claims yet about optimality. even just this fact is not trivial and proving it will give us a good opportunity to get our hands dirty with some basic properties of graphs and specifically graph cuts. graduates of part 1 of this online class of course are already familiar with graph cuts. we studied them at length via karger's randomized algorithm for computing the minimum cut of a graph. so, the concept is the same here, let me state it again to jog your memory. so a cut of a graph is simply a partition of its vertex set, two groups, and each of those two groups should be non-empty. so pictorially, we envision some of the vertices of g, this blob a being in one group, and the rest of the vertices, this graph b being in a different group. now, what's up with the edges? how can they be distributed in this picture? well, the two endpoints of an edge, there's three cases, either both of the endpoints can be in the set a. so there's various edges internal to a. similarly, an edge might have both of its endpoints inside of b. but we're going to be most interested in the third case, edges that have one point exactly in each of a and b. so these are edges that we say cross the cut, a, b. so hopefully the definition of a cut seems simple enough, but cuts in particular their relationship to edges can be quite interesting, quite useful. so as shown here in the picture, of course for a given cut, there can be many edges crossing it. by the same token for a given edge of a graph, in general, there will be many cuts of the grap, that's, that edge crosses. so, to understand this a little bit better, let's just review a simple property that cuts through the graph. let me just ask you just how many there are. specifically, for a graph that has n vertices, roughly how many cuts does it have? roughly n, roughly n squared, roughly 2^n, or roughly n^n? now, none of these four answers is exactly right, but one of the four is a lot closer to the exact expression than the other three and i'm asking you, which of them is it? alright. so the correct answer is the third one, 2^n. a graph of n vertices has essentially 2^n cut, so there's an exponential number of cuts there's a lot of them. so why is this true? well, in effect you can imagine making a binary decision for each of the n vertices. they either go into a. what were they going to be? so n binary decisions results in 2^n different outcomes. now why is this slightly incorrect? well, in fact, a cut has to have two non-empty sets. a is not allowed to be empty, b is not allowed to be empty, so that rules out two of the possibilities. so actually, strictly speaking, it's 2^n - 1 different cuts of a graph. so what we're going to do next is we're going to state and prove three easy facts about cuts in graphs. once we have these three easy facts, we will be able to prove the claim at the beginning of this video, namely the prim's algorithm always outputs a spanning tree. the first of these three properties about cuts, i'm going to call the empty cuts lemma. the point of the empty cut lemma is to give us a characterization that is a new way of saying when a graph is connected. so in particular, i'm going to phrase in terms of a graph not connected. and the claim is that a graph is not connected if and only if we can find a cut of the graph that has no edges crossing it. so remember how we defined a graph being connected, that means for any two vertices in the graph we can find a path in the graph from one vertex to the other. so what we're saying is that being not connected, that is, there existing a pair of vertices with no path between them is equivalent to there being a cut with no crossing edges. so let's go ahead and prove this real quick. so as an if and only if statement, really this proof, we have to do in two parts. first, we have to prove that assuming the first statement, we can derive the second. then we have to show that assuming the second statement, we can derive the first. i think the easier direction is to assume the right-hand side and then derive the left-hand side. so let's start with that one. that is, consider a graph g so that there's a cut, a, b with no edges of g crossing this cut. the plan is to exhibit a pair of vertices that do not have a path between them, there, thereby certifying that the graph is not connected. so, it's pretty easy to figure out which pair of vertices we should look at, just take one vertex from each side of the cut which has no crossing edges. so why is it that there's no path from u to v in the graph g? well the path from u to v would surely have to cross the cuts, a, b, but there's no edges available for crossing the cut. so therefore, this path from u to v cannot exist. so that completes the first part of the proof. we assume the right-hand side, we derive the left-hand side, now we start all over again, but we assume the left-hand side and we have to prove the right-hand side. so by virtue of, by the assumption that the graph is not connected, there has to exist a pair of verticies u and v that have no path between them. we are now responsible for exhibiting some cut a, b such that no edges of the graph g crossing. so where are we going to get these sets capital a and capital b from? well, here is the trick, which is going to make the proof go really nicely. we define the set of verticies of capital a to be those reachable from u in the graph g. another way to think about this is that capital a is simply used connected components in the sense that we discussed in part 1 of the course. now because we want to cut and a cut is our partition, we better well put in the group, capital b, all of the verticies that are not in a. if you like, this is all of the connected components other than the one that contains u. note that by definition, u is in capital a, certainly u is reachable from itself. and by assumption, v and u are not reachable from each other, so v is going to be in capital b. so neither of these sets is non-empty. this is indeed a bonafide cut of the graph g. all that remains is to notice that there are no crossing edges across this cut. and why is that true? well, if there was an edge crossing the cut a, b with one endpoint in a, one endpoint in b. well, by definition, there are paths from u to everything else in a, so if there is any edge sticking out of a, that would give us a path to some vertex in b. but, b definition of vertices not reachable from capital a, so that's a contradiction. so again, the point is that if there were edges crossing this cut, then we can expand a and make it even bigger. so therefore, there aren't any edges crossing the cut. the cut is empty, that's what we needed to prove. assuming the graph was disconnected, we have exhibited a cut, a, b with no crossing edges. so that wraps up of the first of our three facts, and in fact, the most difficult of our three facts about cuts in graphs. and again,, what did the empty cut lemma say? it gives us a new way of talking about whether or not a graph is connected. so it's disconnected if and only if there's an empty cut. it's connected if and only if there are no empty cuts. so that's the keypoint from this slide. let's now knock off the other two facts we're going to need. the first one i'm going to call the double crossing lemma. in essence, what the double crossing lemma says, is that, if a cycle in a graph crosses a cut, then it has to cross it twice, it cannot cross it only once. so pictorially, we look at a cut of a graph, so there's the two vertex groups a and b. by hypothesis, there's some edge e with one endpoint in each side, and by assumption, this e, this edge e, participates in some cycle that we're calling capital c. and if you look at the picture, you realize that the claim in this lemma is obvious, that, because the cycle has to loop back on itself, if it has an edge with one endpoint on either side, there has to be a path connecting the two dots, connecting those two endpoints back to each other and that path has to cross back for, over this cut a, b. indeed, the double crossing lemma is a special case of a stronger statement which is equally easier to see, which is that if you take any cut of a graph and you take any cycle you know, it starts and ends at the same point, then it has to cross this cut an even number of times. it might cross it 0 times, but it's not going to cross it once. it could cross it twice. it could cross it four times, if it crisscrosses back and forth. it could cross it six times, and so on. but if it crosses it strictly more than 0 times, then it has to cross it at least twice. that's the point of the double crossing lemma. so, we'll use this in its own rights later on. but i'm also, for the moment, interested in easy corollary of the double crossing lemma. i will call this the lonely cut corollary. let me tell you the point of the lonely cut corollary. in general, in these spanning tree algorithms, to ensure that we output a spanning tree, then we have to, in particular, make sure we don't create any cycles. the point of this corollary is it's a tool to argue that we don't create cycles. so how can we be sure that an edge doesn't create cycles? well, here is a way. suppose there's a cut, so we're looking at an edge e, suppose we can identify a cut a, b so that edge e is the only cut crossing it, it's the lonely edge crossing this cut. well then, by the double crossing lemma, there is no way this thing is in any cycle. if it were in a cycle and a cross to cut, that cycle would have to cross it again and it's edge wouldn't be lonely, it would have company. so if you're lonely on a cut, it mean's you cannot be in a cycle. so now we've got all of our ducks lined up in a row and we're ready to prove the first part of the correctness of prim. that is, we're ready to argue that prim's algorithm, given a connected graph, outputs a spanning tree. again, for the moment, we're making no claims about optimality, that will be in the next video. so we're going to make this argument in three steps. and for the first step, you might want to go look again at the pseudocode of prim's algorithm just to remember what the notation was. the first step, we're just going to notice that the semantics of the algorithm are respected. so the algorithm maintains two different sets throughout its evolution. on the one hand it maintains a set capital x, intended to be the vertices spanned so far. the other hand, it maintains a set of edges, capital t, the edges that have been picked so far. and the intent was that the current edges capital t always spans the current vertex at capital x. so the first thing is just to verify that that is in fact true. this i'm not going to prove formally. in my experience, students find this kind of obvious and the intuition is correct. if you want a rigorous proof, go go ahead and fill in the details yourself. it's a straightforward induction with no nasty surprises. [sound] now, we're trying to argue the output of this algorithm is a spanning tree. so let's recall what that means. what is it that we have to check? so there's two properties. first of all, there can't be any cycles, there can't be any loops. second of all, it has to span all of the vertices. it has to be a path inside the tree edges from any vertex to any other vertex. so let's go ahead and prove both those things in reverse order. so, the second step of the proof is going to be to argue that the algorithm outputs something which does span all of the vertices. so at the end of the day, we'll have a path from any vertex to any other vertex using only the edges in our chosen set, capital t. now, by part one of this proof, all we need to prove is that the algorithm halts with capital x equal to capital v, then we know that capital t spans everything in v. so how could that not happen? how could prim's algorithm somehow halts with this spanned vertices capital x, not being all of capital b,? we'll go back and check out the pseudocode and look at the main wild loop. so every wild loop, every iteration, we add one new vertex to capital x. what could go wrong? the only thing that could go wrong would be is if some iteration, before we're spanning everything, when we scan the frontier around capital x, there aren't any edges. that's the only way we can fail to increase the vertices in capital x in a given duration. but what would that mean? what would it mean if in some iteration we couldn't find edges with one endpoint in capital x and the other endpoint in v - x? well then we would have exhibited an empty cut. the cut x, v - x would have no crossing edges. and now we can use the empty cut lemma, which says if there's an empty cut, then the graph is disconnected. but by assumption, we're working with a connected input graph, so that can't happen. okay? so the algorithm never gets stuck, we always increase capital x by one vertex because the original graph was connected, that means that halt was something spanning all of the verticies. for the final step, we need to argue that prim's algorithm never creates any cycles in the edges that it, it's choosing capital t. so, why are there no cycles? well, what we're going to do is we're going to talk about each edge in turn, the prim's algorithm adds, and argue that whenever a new edge gets added, there's no way that edge creates any cycles in the set capital t. and, to see why, take a snapshot of the algorithm of some given iteration, to the sum current set capital t, and there's some set verticies capital x that the edges in t span. v - x to the verticies not yet spanned by t and of course we can think of x, v - x as a cut of the graph. and at this moment in time, at this snapshot, the edges of capital t, they're all of one type. they all have both of their endpoints inside capital x, none of them have any endpoints inside v - x. so in particular, none of the edges chosen thus far cross the cut x, v - x. that's by construction, they only span the verticies of x. now what type of edge is going to get added in this iteration. well, prim's algorithm searches only over edges that have one endpoint inside x and one endpoint outside. that is, it searches only over edges that cross the cut x, v - x. so the edge that gets added in this iteration is going to be a trailblazer for this cut. none of the edges yet shows and cross the cut, but the edge showed in this iteration will definitely, cross the cut. so the moment edge e gets added to the tree capital t, it is going to be lonely across the cut v sorry, x, v - x. so by the lonely cut corollary as the sole member crossing this cut in capital t, it cannot possibly participate in any cycles. remember, if it participated in a cycle in capital t, that cycle would have to cross this cut somewhere else. but there aren't any other edges crossing this cut, this is the only one. so that's why when we add a new edge, there's no way it can create any cycles. it's the sole member crossing this particular cut. 
alright. so now that we've completed our warm up by showing that at the very least, prim's algorithm outputs a spanning tree. let's move on and actually show it outputs a minimum cost spanning tree. and to prove this theorem, we're going to have to tackle head-on the kind of crisis which you always face when designing a greedy algorithm. so in a greedy algorithm, you're making an irrevocable decision, like in prim's algorithm, we're including an edge in our tree and never revisiting it later. and, how can you be sure that you're not making a mistake? how can you have a guarantee that the decision you're making seemingly myopically right now is actually a good decision won't come back to bite you later? so it turns out for minimum spanning trees, there is a beautiful condition which tells you when you're guaranteed to not regret including an edge into a spanning tree, but guarantees when an edge has to belong to the minimum spanning tree. so that's called the cut property, it's the subject of the next slide. so this is a cool enough property that we're going to bestow it not just with all caps but even with a box. now, that's a pretty good property. so what does it states? well, consider an edge of a graph, an edge that we are wondering if it's safe to include it in the tree so far. so here is this sufficient condition guaranteeing that you won't regret including this edge in the tree so far. the condition is stated in terms of the cut. so suppose you can find a cut, a, b with the property that amongst all edges in the graph g, that happened across this cut, the edge e is the cheapest edge crossing this cut. okay? so not only should edge e cross this cut, a, b, but it should cheapest such edge. if this condition is met, then we definitely want them include the edge e in our solution. indeed, the edge e has to be a member of any minimum spanning tree of the graph. so in this video, we're going to assume that the cut property is true. it is by no means obvious. it definitely requires a proof. i'll give you the proof in a separate video. it's not, it's a little bit tricky. it's based on a subtle exchange argument. for this video, we're going to assume that it's true, however, and we just want to be quiet, so that we want to figure out what it's good for. now, i will soon show you that it actually implies correctness of prim's algorithm. but just to get a feel for it, let's look at a much simpler graph. let's just look at a four cycle. four nodes, four edges with edge costs 1, 2, 3, and 4. so let's look at, let's look at a few cuts. so, let's look at the cut. we're on one side of the cut, i put the upper right vertex, and on the other side of the cut, i put the other three vertices. so there are two edges crossing this cut, the edge that has cost 1, the edge that has cost 2. so the edge with cost 1 is the cheapest edge crossing this cut, so by the cut property, the edge of cost 1 has to be in the mst. okay. so we looked at one cut and both the cut property [inaudible} to stick in the mst. that was pretty cool. let's look another cut. let's look at a cut where on one side, we just put the bottom right vertex, and on the other side, we put all now this cut has two edges crossing it the edges that have cost 2 and cost 3. the edge of cost 2 is the cheapest edge crossing this cut. so by the cut property, it has to be in the mst. so that's cool. so we know the two has got to be there. now let me point out something interesting that's happened, which is that, it is not the case that this edge of cost 2 is the cheapest crossing, every single cut that it crosses. remember when we looked at cut number 1, this edge of cost 2 was actually the most expensive edge crossing that cut. but, we found a different cut that is the cheapest crossing and that's enough to justify the cut property. so in other words, all that's important for the cut property, i just got to find you one cut for which an edge is the cheapest, that's enough to conclude its presence in the mst. so similarly, we can look at a third cut just consisting of the bottom left vertext and the other three vertices. and it's the same story, there are two edges crossing this cut, the edge of cost 3, the edge of cost 4. the edge of cost 3 is the cheapest edge crossing this cut, so we know it's got to be in the mst. and again, when we look at cut number 2, it didn't tell us whether or not the edge of cost 3 is in the mst, but when we looked at cut number 3, that was enough to conclude that the edge of cost 3 has to be in the mst. so there we go. so we could use the cut property that construct an entire mst. on the other hand, there's no way to use the cut property to try to justify the edge of cost 4. any cut that you pick for which the edge of cost 4 crosses, there will be some other cheaper edge crossing it. so you can never use the cut property as one would hope to justify the inclusion of the edge of cost 4 and you'd better not be able to, because 4 is not in the mst. now a quick side note, some of you might be wondering when i wrote in the conclusion of the cut property, i said the mst of g, so that would seem to indicate that the minimum spanning tree is unique. so that deserves a quick comment. so first of all, if the edge costs are not distinct, if you have ties between edges, then you can certainly have multiple different minimum spanning trees and you have to state the cut property a little bit differently. but again, in the lectures we are just going to assume distinct edge costs, so that's not a problem. and in fact, something that will be a consequence of the next slide, we'll notice that the minimum spanning tree is unique with distinct edge cost. it's not obvious, but we'll prove it shortly. all right. so what i want to do to finish up this video is i want to assume that the cut property is true. and then, from that, i want to derive, i want to argue that prim's algorithm is correct, always outputs an mst. the proof of the cut property is non-trivial and deserves its own video, which you can see separately. all right. so given the tools that we've developed, this argument is actually going to be quite short. so let's assume that the cut property is a true statement and let's begin by building on the previous video. the previous video argued that prim's algorithm outputs a spanning tree, didn't argue it was a minimum one, but it argued it's a spanning tree, it spans all the vertices and has no cycles. let's call the output of prim's algorithm at the end of algorithm t star. now, stare at the cut property, stare at the pseudocode of prim's algorithm. what happens in each iteration of prim's algorithm? well, we have our set capital x, that's what we spanned so far. there's the rest of this stuff, v - x, so that's a cut x, v - x. what does prim choose to include next? well, in brute-force searches through the edges, the cross is cut and it adds the cheapest one of them. well, that is right in the wheelhouse of the cut property. what does the cut property says? it says cheapest edges crossing cuts have to be in the mst. so they just fit together beautifully. prim's algorithm explicitly picks an edge at each iteration which satisfies the hypothesis of the cut property and therefore has to be in the mst. so remember, the conclusion of the cut property says edges so justified must belong to the mst. so if everything in t star is justified by the cut property, then everything in t star is in the mst so t star is a subset of the mst. but t star, of course, as we have argued, is already a spanning tree in it of itself. and, if you add more edges to t star, it's no longer going to be a spanning tree, because you are going to pick up cycles, right? if you ever have something that is connected, there is a path from each pair of vertices, and you add a new edge, you are going to close a path, you're going to get a loop. okay? so t star is already a spanning tree, and you can't have anything bigger and still be a spanning tree. so therefore, this has to be the minimum spanning tree, there cannot be anything else. so for this reason, t star must in fact be the minimum cost spanning tree of the graph. since the input graph was arbitrary, assuming only it was connected, this completes, assuming the cut property, the proof of correctness of prim's minimum spanning tree algortihm. 
okay. so to this point we've proven the correctness of prim's minimum spanning sheet algorithm under an assumption, under an assumption that the cut property is true. so, the purpose of this video was to supply the missing proof to convince you of this cut property. let me remind you where we stand. so through all the minimum spanning tree videos, we're assuming distinct edge costs. all of this can be extended to edge costs with ties. in particular, there's a version of the cut property when the edges have ties, but we're just going to focus on the main ideas which were exposed already with distinct edge costs. so what does the cut property say? well, it's meant to be a guarantee that an edge is safe to include on, in your tree so far. so it justifies an iteration of a greedy algorithm like prim's algorithm. specifically, consider an edge of a graph, and suppose you can find some cut a, b. so that, amongst all the edges that are crossing this cut, e is the cheapest. so e, the edge e has to not just cross this cut, but it has to be cheaper than any edge that crosses this cut. if you can find just one cut of this form, so that e is the cheapest crossing edge, then it's definitely not a mistake to include e in your tree. you definitely want it. it is in the mst. so this is a non-trivial claim and, let's turn to the proof. at a high level, the plan will be not that different than the correctness of our greedy scheduling algorithm for minimizing the weighted sum of the completion times. that is, we're going to use an exchange argument embedded in a proof by contradiction. [cough] the type of contradiction will be of the same form. we'll start with an optimal solution. suppose it doesn't have the property that we want it to have, and then we do an exchange to make it even better, contradicting the assumption that this solution is optimal. so specifically, if we argue by contradiction we assume, that the cup property is false. so let's just make sure we understand what that means. if the cup property is false, then there's a graph and there's an edge, which actually is the cheapest crossing some cut, and yet, that edge does not belong to the minimum cost spanning tree t star. the plan then is to exchange this missing edge e with some edge that isn't a tree t star, which is more expensive, thereby getting a better spanning tree providing the contradiction. so, this idea currently is a little bit hand-wavy. to really execute it, we have to specify which edge we're going to exchange the edge e with. that's a subtle point and we'll develop it over the next couple of slides. so let's begin with a sort of first cut attempt at an exchange argument. so what's the world look like? well, we have some cut of a graph. so at one blob, the vertices is a and then the rest of the vertices are in this blob b. this is the cut for which edge e is the cheapest. and by our assumption in this proof by contradiction, this cheapest edge e does not belong to the minimum spanning tree t star. that said, i claim while t star may not have this edge e to cross in this cut, it better possess some other edge crossing this cut a, d. so why is that true? well, suppose the opposite, suppose in fact t star did not have any edge crossing this cut a, b, well then, t star wouldn't be connected. it wouldn't span all the vertices, right? remember our proof of empty cut lemma, so if you had this empty cut and there's no way to have a path from one side to the other side. okay? but that's spanning trees have to have paths between each pair of vertices. so t star as a spanning tree have to contain something from this cut, by assumption it doesn't contain edge e. so it contains some other edge, let's call it f, that crosses this cut. now of course, since e is the cheapest edge crossing this cut and f is some other edge crossing this cut, f is strictly more expensive than e. and at this point, we seem beautifully set up to execute the desired exchange arguement. we have the edge that the optimal solutions missing. we have a canvid replacement edge f, which is more expensive. so if we swap e and f, hopefully we get a new spanning tree that has strictly smaller cost providing the desired contradiction. but, things are more settled than they were with these scheduling applications. the reason being is that schedules are simply sequences of jobs. so whenever you do an exchange of two jobs, it's clear you get another schedule. but spanning trees and graphs are subtle objects and there's a question, if we take a spanning tree and we add one new edge and delete an edge, do we get another spanning tree of the graph or not? so the following quiz is going to ask you to think about that question carefully. okay. so what we wish that the answer to this quiz was, was either answer a or answer c. so a would be the cleanest solution. if it were always true that when you take a spanning tree, you take an edge outside of the spanning tree and then you swap those two edges, you get a new spanning tree, then in fact, our proof of the cut property would be done, right? we would just go on that previous slide. we would rip out the edge f from the spanning tree. we'd plug in the edge e, because e costs less than f, we'd get a spanning tree which was cheaper. and we'd be done, that would be our contradiction. now if a wasn't true, we'd still be okay if c was true. if maybe not every swap yields a new spanning tree, but at least if you're swapping in the edge that's the cheapest crossing some cut, you get a spanning tree. then we'd also be golden, because in fact, we're only are trying to execute the swap, the exchange, using an edge, which is the cheapest crossing some cut. if you go back to the previous slide, you'll see that was the only case we needed this fact to be true. unfortunately, the correct answer to this quiz is d. you need not get a new spanning tree when you execute an exchange, even if you're plugging an edge which is the cheapest edge crossing some cut. so to understand this better, let me the picture that we had on the previous slide. we had our cut a, b, we had our cheapest edge e which by assumption does not belong to the spanning three t star, but we observed that t star has to contain at least one other edge crossing this cut because it's connected and we called that f. and we're wondering if swapping e and f yields a new spanning tree or not. so, to reason about this, let me just draw you what the rest of the spanning tree might look like. so in this picture, this spanning tree t star is given by the pink edges. and it's just to this path of five edges on the six vertices. so, what happens if we exchange e and f? well unfortunately, something bad happens. so we certainly get a new subgraph of five edges, after all, we just subtracted one and added one. but this new spanning, this new subgraph fails to be a spanning tree. it fails on both counts. first of all, it obviously has a cycle and it's a four cycle and secondly, it's not connected. the upper right vertex is just totally disconnected from the rest of the rest of the vertices. so that's no good. that's an exchange which just does not produce a feasible solution and it is therefore not useful in our proof by contradiction. now, if you just want to salvage some hope from this seemingly promising proof plan, we could take solace from the fact that there is not just one pink edge crossing the cut a, b. f isn't the only one, there's actually this other one on the bottom. so let me call that e prime. now, e being the cheapest edge crossing this cut overall. not only is e cheaper than f, it's cheaper than e prime also. so in some sense with our motivation, we could care less which edge we exchange e from crossing this cut, because it's cheaper than all of them. and we see that at least in this example, swapping with e prime yields a good solution, yields a feasible spanning tree. so what have we learned? what we've learned is that if we want to execute this exchange argument, we cannot blithely exchange with any edge of t star that crosses this cut. so the best case scenario, so what we're hoping is true that we can always find some suitable edge, like e prime on the previous slide. so that when we execute this swap, we do in fact, get a spanning tree. and i'm happy to report that we can indeed always do this. so what i need to explain is the procedure by which we exhibit edge, this edge e prime, which doesn't get us into trouble after we swap, which still gives us a spanning tree after the swap. so let me explain the procedure by which we identify this magical edge e prime that we can swap with and still be a spanning tree. so here's the way to think about it, so we've got this spanning tree t star, we've got this edge which is not yet in t star. now, if we just plug e into t star, we're going to get a cycle. why? well, a spanning tree, remember, it has a path between each pair of vertices. so if this new edge, maybe its endpoints are u and v. t star already has a path between u and v, so when you plug in this new direct edge between u and v, it closes the loop, it gives you a cycle. so let's go ahead and call that cycle capital c. let me also redraw the picture from the example on the previous slide so you can see what that cycle was in that special case. now, here's the pattern to notice about this cycle capital c, at least in this example, which is that the lousy edge, the edge f, for which when we swapped, we didn't get a spanning tree, that's off of this capital c. whereas, the good edge, the edge where we could do a swap and get a spanning tree, e prime that's on this same cycle capital c. and that turns out to be true in general. so, when you add the edge to the spanning tree and you get a new cycle, that cycle is what furnishes the candidates for swaps that will give you a new spanning tree. so the one lingering concern then, we have this cycle. we would, all edges of the cycle are going to be good candidates for the swapping. wee just need to make sure that there is some edge that actually crosses this cut a, b like the edge e prime does in the picture. but here, we're going to rely on a fact from a previous video, the double crossing lemma. remember the double crossing lemma says, that if you have a cycle that crosses a cut at least once, then it has to cross it twice. all right. so if it'd cross once, then it has to loop back around, then in looping back around, it's going to cross it a second time. so, in this cycle capital c, we know it crosses the cut a, b once, that's because it includes the original cheapest edge across the cut e. so, it's got to cross it a second time. there's got to be an e prime in the cycle crossing the cut and that's going to allow us to do the swap and get a new, cheaper spanning tree completing the contradiction. so, just to spell things out in a little more detail. so what we do is we first say we use the double crossing lemma. so, we have this reported minimum spanning tree t star. we have this cheap edge e knot in it. we plug e into the spanning tree, we get cycle, we call the cycle capital c. the cycle crosses the cut a, b once, through the edge e. it crosses it a second time by the double crossing lemma. we're going to call that edge e prime. since e prime crosses the same cut as e, we know that e prime is strictly more expensive than e. remember we use the cheapest one crossing this cut, a, b. so now what we do is we execute the swap with this new edge e prime. so e prime in t star. the cheapest edge, e, is not in t star so we can swap them. we can take, we can rip e prime out, we can stick e in. now something i want you to think through carefully at home, convince yourself this is true, is that because we plucked e prime from the cycle, this new tree which i'm going to call capital t, this is a spanning tree necessarily. you know, intuitively, the reason being, you plug in e and you get this one cycle involving e, and then when you rip out e prime, you destroy the cycle. and because it's on a cycle, you don't destroy connectivity between any pair of veriticies, there is still one path between each pair. but make sure you believe that, convince yourself at home. and once you're so convinced, you will also realize that we've finished a proof. we've executed our proof by contradiction. since e was the cheapest edge crossing the cut, and e prime is another edge crossing the cut, e is got to be cheaper. since t differs from t star only in the swap of these two edges, it's aggregated cost has gone down and that contradicts the purported optimality of t star comp completing the proof of the cut property. 
so at this point we understand, prim's algorithm and we also know why it's correct. that is why it always computes the minimum cost spanning tree of a graph. so in this video, we're going to turn to implementation issues and running time analysis. we'll begin by just analyzing the straightforward implementation of prim's algorithm. that's already reasonable. it's polynomial running time, but not especially close to linear. then we'll see how a suitable deployment of heaps very much in the way that we did for dijkstra's algorithm leads to a blazingly fast, near linear running time. so let's briefly review the pseudocode for prim's algorithm. recall that prim grows a tree one edge at a time spanning one new vertex at each iteration. so it maintains two sets, capital x, a set of vertices that have spanned so far, and capital t, these are the edges we've committed to thus far. they start out by just being some arbitrary vertex, little s, and the empty set, and in each iteration of this main while-loop, we add one new edge to the tree. and whatever new vertex that edge spans, we add that to capital x. the algorithm terminates when we're spanning all of the vertices and as we've seen, it halts not just with a spanning tree but with a minimum cost spanning tree. so suppose we just literally implemented this algorithm as is, what would be the running time? well, the initialization stick, step takes only constant time, so let's ignore that. so let me just have this one loop. so let's just ask how many iterations does the loop take and how much time is needed to execute each iteration? well, the number of loop iterations is going to be exactly n - 1, so, where n is the number of vertices. x starts out with one vertex and terminates when it has all n vertices. how much work do we need to implement each iteration? well, essentially, what each iteration is doing is a brute-force search through the edges. it's looking for the edges that have one endpoint inside x and one endpoint outside, and amongst those, it just remembers the cheapest. and it's easy to see that you could implement each iteration in o of m time, where m is the number of edges. for example, you can just, with each vertex associate a boolean variable that keeps track of whether or not it's in this capital x, that way when you see an edge, you know whether it's crossing the frontier or not in constant time. so putting it together, o of m iterations with o of m works for each gives us a running time of o of m times n. so this running time is already nothing to sneeze at. as we discussed, graphs can have an exponential number of spanning trees. so, this algorithm is doing far less work than examining each of these spanning trees. it's homing in in polynomial time, to the minimum cost point. so that's pretty cool. but remember the mantra of any algorithm designer worth their salts, confronted with a solution, you should always ask but can we do better? and can we do better than running time o of m times n? we can as we'll see in the rest of this video. the big idea for speeding up prim's algorithm is exactly the big idea we used in part 1 to speed up dijkstra's algorithm, namely we're going to deploy a suitable data structure. so, what data structure seems like it might be a good idea for making prim run faster? well, what's happening in the main workhorse while-loop of prim's algorithm? over and over again, we keep meaning to do a minimum computation amongst all edges crossing the frontier, we need to find the cheapest one. so, the question we should ask ourselves is what kind of data structure would facilitate, would speed-up repeated minimum computations. and if you recall from part 1, we have a data structure where that's exactly what it's raison d'etre is, the heap, the meaning of life for a heap is to speed-up repeated minimum computations, just like in prim's algorithm. so let me just remind you briefly, what are the operations exported by heap data structure and what is the running time? so first recall that a heap contains a bunch of objects, and each of those objects should have some key value from some totally ordered set, like a number, like for example, an edge cost. so what can you do with a heap? well, the salient operations for our purposes today are, first of all, you can insert new stuff into the heap with their, whatever their key value is. you can extract the object with the minimum key value. and you can also delete stuff from the middle of the heap. and all of these can be done in logarithmic time, logarithmic in a number of objects stored by the heap. so it's not going to be important for us today to know how heaps are implemented and what they look like under the hood. we're just going to be clients of them. we're just going to make use of these operations and the fact that they run in logarithmic time. but you know, just for those of you who are curious, and/or want to have your memory jogged. under the hood, heaps are implemented logically as complete binary tree. they're actually laid out in an array, but you sort of think of them conceptually as being in a complete binary tree. and they, they, they satisfy what's called the heap property. and the heap property is to make sure that you know where the object with the minimum key value is. so the actual definition is, every parent should have a key value which is less than that of its children. so as you go up the tree, the key value can only drop and that means you know where the minimum is got to be. it's got to be at the root of this tree orr the front of the array. so that's great. that's how you locate the minimum so quickly in a heap. now, what do you do when you want to extract the minimum? so you rip off the root of this tree, and now, you have to rearrange the tree to restore the heap property. so you swap the last leaf up to where the root was, you bubble-down as needed to restore the heap property. how do you insert? you put the new object as the new last leaf and you bubble it up as needed to restore the heap property. to delete from the middle of a heap, you just sort of rip it out and then bubble things up or down as necessary to restore the heap property. again, that's not supposed to, if you're hearing this for the first time, i know this is too fast, this is just to jog your memory for those of you who already learned this in part 1 of the course. for more details, you can go review the appropriate videos there. so now that i've reminded you about the salient properties of heaps. let's return to the question of how do we deploy them cleverly to speed-up prim's algorithm. so our intuition is that because we're doing repeated minimum computations in prim's algorithm, each time that it's while-looped, compute the cheapest edge cross in your frontier, that's sort of in the wheelhouse of heaps. so how should we use heaps? well, the first idea, which is a pretty good idea, is to use the heap to store edges, right? because our minimum computation should result in us choosing an edge, so when we extract-min from a heap, we want it to hand us an edge on a silver platter. so it would seem this would be your first thought, that the heap should store edges and that the key value that you use should just be the edge cost, because you want to find the cheapest edge. so this already a quite good idea using heaps in this manner. we'll already definitely speed-up prim's algorithm relative to the naive implementation. and in fact. and i'll leave this as an exercise for you to work out. using heaps in this way results in an implementation that has, that runs in time big o of m log n. what i'm going to show you instead is not that implementation, but an even cleverer implementation of prim using heaps. we're not going to see a benefit in the asymptotic running time. this more sophisticated version will also give us m log n running time, but it would give you better constants and it is the version you would want to implement in practice. [sound] so, the one slightly tricky point in this exercise is remembering prim's algorithm, you don't just want the cheapest edge overall [inaudible] you want the cheapest edge which crosses the current cut that has one endpoint in each of x and v - x. and, when you use heaps in this way, it might hand you in a silver platter and edge which is cheap, but isn't necessarily crossing the frontier. so, you need some extra checks to ensure that you're always finding the minimum edge and that that edge crosses the frontier between x and v - x. so i'll leave it to you to work out the details of this implementation in the privacy of your own home. what i want to spend our time together on instead is this somewhat more sophisticated, more practical way to use heaps. and for those of you who remember our fast implementation of dijkstra, this will be very familiar to you. it will be the same kinds of ideas that we used for dijkstra, and the keypoint is, instead of using the heap to store edges, were going to use it to store vertices. so, in a bit more detail, our plan is going to be to maintain two invariants. the first invariant is going to describe what the heap contains. the second invariant is going to be what the key values of those heap object are. so as i said, we're now going to be starting at vertices in the heap, not edges. which vertices? exactly the vertices that we don't yet span. the vertices of v - x. the motivation here is that rather than getting on a silver platter, the edge in which to add next to the tree, we're going to get from a heap on a silver platter, the vertex, that we're next going to add to capital x. so the second invariant tells us what the key values of these vertices in v - x are supposed to be. and we're going to define them to be the cheapest cost of an edge incident of this vertex that crosses the current frontier. so, i think a picture will make this definition clear. so, consider some snapshot of prim's algorithm at some iteration. we have our vertices x that were already spanning. we have our vertices v - x that were not spanning. and remember, the elements of the heap by invariant 1 are exactly the vertices on the right-hand side, the vertices of v - x. so were trying to find the key value for some vertex in the heap. so some vertex v, which is on the right side, which is not in x. and so, what we do is we look at the edges incident on this vertex v that go back to the left-hand side, so, edges incident to v that are crossing the frontier and there may be of course be many such edges. and the invariant we want to maintain is that the key value for this vertex v is the cheapest of all the incident edges crossing their frontier or in this picture the key should be equal to two. there is the niggling issue of how do you define the key if there are no incident edges at all that are crossing the frontier. so maybe you have a vertex w, which is buried deep inside of v - x, and actually, none of the incident edges go back to the left blob at all. so in that case we just define the key to be plus infinity. so given this high level approach to implementing prim's algorithm using heaps, we now have a few things to think through. so first of all we have to think about how to initialize the heap so that these invariants are satisfied at the beginning of prim's algorithm. second of all, we have to check that if these invariants are satisfied, then we can faithfully simulate each iteration of the while-loop in prim's algorithm, hopefully very quickly. and then third, we have to think about how do we make sure these invariants are maintained throughout the course of prim's algorithm, so let's do those in turn. so the first thing is how do we set up the heap at the beginning of prim's algorithm and a preprocessing step, so that both of these invariants are satisfied. well, at the beginning, x consists just of this single arbitrary star vertex s. v minus x contains the other n - 1 vertices. the key value of a vertex other than s at the beginning of prim's algorithm, is just the cheapest edge between that vertex and s if there is one, or plus infinity otherwise. so, the thing to think through and make sure you believe this, is that first of all, with a single scan through the edges, so an o of m time, we can compute the key value for each vertex that needs to go in the heap. and then, we just have to insert those n - 1 vertices into the heap. so that's going to cost us o of m time for an edge scan, and then, m log n for the inserts. in fact, for those of you that really know heaps very well, you might know about a heapify operation which allows you to do a batch of n inserts in o of n time because we can do this even faster in linear time but we're not going to need that in this lectures. and also, i claim that this expression m + n log n is bounded above by the expression m log n, at least an asymptotic notation. to see that, remember two things. first of all we're assuming that the input graph is connected, otherwise there's no spanning trees and it's not interesting to talk about minimum spanning trees. second of all, in any connected graph, the number of edges m is at least n - 1. so asymptotically, m is always at least as big as n and it can be bigger. so you can always replace an n by an m and get a valid upper bound, so that's what we're doing here. the second issue we need to think through is how do we faithfully simulates each iteration of the while loop in prim's algorithm, given that these two invariants halt. so this issue is going to work out beautifully really, by construction. we set up our heap and we set up our definition of keys, so that extracting min from the heap and iteration is a faithful simulation of the brute-force search in the naive implementation of prim's alogrithm. so specifically, assuming that these two invariants hold when we invoke extract-min from this heap, what it provides to us on a silver platter is the next vertex, not yet in x. the next vertex that we should add to x in this iteration. and moreover, the cheapest edge incident to that vertex crossing the frontier is the one that we should be adding to the set t in this iteration. and the way to think about this fact is to think of us as essentially simulating the brute-force search and the naive implementation using a 2-round knockout tournament. so, in the straightforward implementation of prim, the way we think of it is we just do a scan through all the edges crossing the frontier and we remember the winner, we remember the smallest cost of them all. here, with a heap, we're doing it in two steps. so first of all, for each vertex on the right-hand side of the cut, for each vertex in v - x, it locally remembers what is its best candidate so what is the cheapest edge incident on that vertex crossing the frontier. so that's kind of round one, so for an edge to be chosen as the winner, at the very least, it'd better be a local winner. it'd better be the cheapest edge crossing the cut that ends at this particular vertex on the right-hand side of the cut. so that's just in a definition of the key of each vertex and encodes the value of the winner localed in that vertex. and then this extract-min is envoking the second round of this 2, 2-round elimination tournament. it's saying, well, amongst all the proposals from the 1st round, amongst all the crossing edges that are locally minimum given it's endpoint, which of them is the cheapest overall? and that's going to be the cheapest edge crossing this cut, the result of this exact min computation. 
so the third and final issue to think through is we need to make sure that we pay the piper, that we keep these n variance maintained. we know that if they're satisfied than we have this great way of finding the best edge in each iteration, we just do an extractment but how do we make sure that these n variance stay maintained throughout the algorithm? so, to get a feel for the issues that arise in maintaining of the invariants, in specific, invariant number two, and also to make sure we're all on the same page with respect to the definition of key value of the vertices in the heap. let's go through an example. so in this example, i've drawn in the picture a graph that has six vertices. in effect we've already run three iterations of prim's algorithm, so four of the six vertices are already in capital x, the remaining two vertices v and w are not yet an x, they're in v minus x. so, for five of the edges, i've given them a cost labeled in blue. for the other edges, it's not relevant for this question what their edge costs are. so you don't have to worry about it. so, the question is the following. so given our semantics of how we define keys for vertices that are not in x, so in this case the vertices v and w. what are their current key values supposed to be? so those are the first two numbers i want you to tell me. what's the current key value of v and w? and then secondly, after we run one more iteration of prim's algorithm. then what is the new key value of the vertex w supposed to be? so the correct answer is the fourth one. let's see why, so first let's remember the semantics of keys. what's the key supposed to be? it's supposed to be amongst, all the edges. that on the one hand, are incidents to the vertex. and on the other hand, are crossing the cuts. it's the cheapest cost of any of those edges. so, for the node v, there's four incident edges with costs one, two, four, and five. the one is not crossing the cut, the two, four, and five are crossing the cut. the cheapest of those is two. so, that's why v's current key value is two. for the node v, the node w, it has two incident edges, a one and a ten. . the one is not crossing the cut. the ten is. it's the only candidate crossing the cut, so its key value is ten. so the third part of the question says, what about when we execute one more iteration of prim's algorithm? so, what is prim's algorithm going to do? well, it's going to move the edge with the smallest key from the right hand side to the left hand side. v has a key value of two, w has a key value of ten, so, v is going to be the one that gets moved from the right hand side to the left hand side. so, once that happens, we now have a new set capital x with a fifth vertex, v is now a member, so the new value of x is everything except for the vertex w now, the key point is that, as we've changed the set capital x, the frontier has changed. the current cut has changed. so of course, it's a different set of edges that are crossing this new cut. some have disappeared, and some are newly crossing it. the ones that have disappeared are the two and the four and the five. anything between the vertex that got moved that was already spanning, going to the left hand side has now been sucked inside of capital x. on the other hand, the edge vw which was previously buried internal to v minus x, with one of it's endpoints being pulled to the left hand side. it is now crossing the cut. so why do we care well the point is w's t value has now changed it use to have only one incident edge crossing the cut the other across ten now with a new cut it has two incident edges both the one and the ten are crossing the cut. the cheapest of those two edges is of course the edges of cost one and that now determines its key value its dropped from ten to one. so the take away from this quiz is that well, on the one hand, having our heap set up to maintain these two invariants is great, because a simple extract min allows us to implement the previous brute force search in prim's algorithm. on the other hand, the extractions screws things up. so it messes up the semantics of our key values. we may, may need to recompute keys for the vertices. so in this next slide i'm going to show you the piece of pseudocode you'd use to recompute keys in the light of an evolving frontier. fortunately, restoring in varient number two after an extract min is not so painful the reason being is that the damages done by an extract min are local. more specifically, let's think about what are the edges that might be crossing the cut now that were not previously crossing the cut? well the only vertex whose membership in these sets has changed is v so they have to be edges that are incident to v. if the other end point was already in x then we don't care this edge has just been sucked into x, we never have to worry about it. but if the other end points, so if this edge is incident to v if the other end point w is not an x. then with v being pulled over the, the left hand side. now this edge spans the frontier when previously it did not. so the edges we care about are incident to v with the other n point outside of x. and so our plan is just the obvious one, which is for each dangerous vertex. each vertex incident to v where the other endpoint w is not an x. we just follow to the other endpoint w, and we just recompute its key, and we just do that for all of the relevant w's. so that recomputation necessary is not difficult, there's basically two cases. so this other end point w now it has one extra candidate edge crossing the cut. namely the one that's also incident on v. the vertex that just moved. so i did this new edge vw is the cheapest local candidate for w, or it's not. and we just take the smaller of those two options. so that completes the high level description of how you maintain invariants one and two throughout this heap-based implementation of prim's algorithms. so each iteration, you do an extract min, from the extract min you run the pseudocode to restore invariant number two, and you're good to go for the next iteration. so for those of you who want not just the conceptual understanding of this implementation, but really want to get down to the any degree. you want to dot all the i's and cross all the t's. a subtle point you might want to think through is how it is you implement this deletion from a heap. the issue is, is deletion from a heap is generally from a given position. and so here i'm only talking about deleting a vertex from a heap, that doesn't quite tight check. really what you want to see is delete the vertex at position i from a heap. so really pulling this off, the natural way to do it is have some additional bookkeeping to remember which vertex is at which position in the heap. so again, for the detail-oriented amongst you that's something to think through, but this is the complete conceptual description of the algorithm. let's now move on to the final running time analysis. so the first claim is that, the non-trivial work of this algorithm all takes place via heap operations. that is, it suffices to just count the number of heap operations, each of which we know is done in logarithmic time. okay, so let's count up all of the heap operations. one thing we already talked about, but i'll mention it here again for completeness is we do a bunch of inserts just to initialize the heap in a pre-processing step. so after we initialize, we move on to the main while loop. remember, there's exactly n minus one iterations of that while loop. and in each one, we extract min exactly once. so these were the easy steps. what you should be concerned about. are, the, heap operations, the deletions and re-insertions that are triggered by needing to decrease the key avertices that are not in x. indeed, in a single iteration of prim's algorithm, in a single move of a vertex inside of capital x, can necessitate a large number of heap operations. so, it's important to think, to count these operations in the right way, namely in a edge-centric manner and the claim is that a single edge of the graph is only going to trigger a single decrease key pair of. operations a single insertion deletion combo. we can even pinpoint the moment in time at which we're going to have this inser, this deletion and reinsertion. it's going to be when the first of the endpoints, so either v or w, the first iteration at which one of those gets sucked into the left-hand side capital x, that's going to trigger the insert-delete, potentially for the other endpoint. when the second endpoint gets sucked into the left-hand side, you don't care, because the other endpoint has already been taken out of the heap, there's no need to maintain its key. so that means that the number of heap operations is almost twice the number of vertices plus almost twice the number of edges. we're again going to use this fact that the input graph is connected and therefore the number of edges is asymptotically at least the number of vertices. so we can say that the number of heap operations is at most a constant factor times the number of edges, m. as we've discussed every heap operation runs in time logarithmic in the number of objects in the heap so that's going to be log n in this case so we get an overall running time of o of m times log n. so this is now a quite impressive running time for the really quite non-trivial minimum cost spanning tree problem. of course we'd love to do even better. if we could shave off the log n factor and be linear in the input, that would be even more awesome. but we gotta feel pretty good about this running time. right? this is only a log n factor slower than what it takes to read the input. this is the same kind of running time we're getting for sorting. so this actually puts the minimum spanning tree problem into the class of four free primitives. if you have a graph and it fits in the main memory of your computer, this algorithm is so fast. maybe you don't even know why you care about the minimum spinning shaver graph. why not do it? it's basically cost-less. that's how fast this algorithm is. 
so, in these next few videos, we're going to continue our discussion of the minimum cost spanning tree problem. and i'm going to focus on a second good algorithm, good greedy algorithm for the problem, namely kruskal's algorithm. now, we already have an excellent algorithm for computing the minimum cost spanning tree in prim's algorithm. so you might wonder, you know, why bother spending the time learning a second one? well, let me give you three reasons. so the first reason is this is just a, it's a cool algorithm. it's definitely a candidate for the greatest hits. it's something i think you should know. it's competitive with prim's algorithm both in theory and in practice. so it's another great greedy solution for the minimum cost spanning problem. the second reason is it'll give us an opportunity to learn a new data structure, one we haven't discussed yet in this course. it's called the union-find data structure. so, in exactly the same way or in a similar way to how we used heaps to get a super fast implementation of prim's algorithm. we use a unionfind data structure to get a super fast implementation of kruskal's algorithm. so that'll be a fun topic just in its own right. the third reason is, is there's some very cool connections between kruskal's algorithm and certain types of clustering algorithms. so that's a nice application that we'll spend some time talking about. i'll discuss how natural greedy algorithms in a clustering context are best understood as a variance of kruskal's minimum spanning tree algorithm. so let me just briefly review some of the things i expect you to remember about the minimum cost spanning tree problem. so the input of course is an undirected graph, g and each edge has a cost. and what we're trying to output, the responsibility of the algorithm is a spanning tree. that is a subgraph which has no cycles and is connected. there's a path between each pairs of vertices and amongst all potentially exponential many spanning trees, the algorithm is supposed to output the one with the smallest cost, smallest sum of edge costs. so let me reiterate the, the standing assumptions that we've got through the minimum spanning tree lectures. so first of all, in assuming if the graph is connected, that's obviously necessary for it to have any spanning trees. that said, kruskal's algorithm actually extends in a really, just easy, elegant way to the case where g is disconnected. but i'm not going to talk about that here. secondly, remember, we're going to assume that all of the edge costs are distincts, that is there are no ties. now don't worry, kruskal's algorithm is just as correct. if there are ties amongst the edge costs. i'm just not going to give you a proof that covers that case, but don't worry, a proof does, indeed exist. finally, of the various machinery that we developed to prove the correctness of prim's algorithm perhaps the most important and most subtle point was what's called the cut property. so this is a condition which guarantees you're not making a mistake in a greedy algorithm. it guarantees that a given edge is indeed in the minimum spanning tree. and remember, the cut property says, if you have an edge of a graph and you could find just a single cut for which this edge is the cheapest one crossing it. okay? so, the edge e crosses is cut and every other edge that crosses it is more expensive. that certifies the presence of this edge in the minimum spanning tree, guarantees that it's a safe edge to include. so we'll definitely be using that again in kruskal's algorithm when we prove it's correct. so as with prim's algorithm, before i hit you with the pseudocode, let me just show you how the algorithm works in an example. i think you'll find it very natural. so let's look at the following graph with five vertices. so the graph has seven edges and i've annotated them with their edge costs in blue. so here's the big difference in philosophy between prim's algorithm and kruskal's algorithm. in prim's algorithm, you insisted on growing like a mold from a starting point, always maintaining connectivity, and spanning one new vertex in each iteration. kruskal's going to just throw out the desire to have a connected subgraph at each step of the iteration. kruskal's algorithm will be totally content to grow a tree in parallel with lots of simultaneous little pieces, only having them coalesce at the very end of the algorithm. so in prim's algorithm, while we were only allowed to pick the cheapest edge subject to this constraint of spanning some new vertex. in kruskal's algorithm we're just going to pick the cheapest edge that we haven't looked at yet. now, there is an issue, of course, we want to construct a spanning tree at the end. so, we certainly don't want to create any cycles, so we'll skip over edges that will create cycles. but other than that constraint, we'll just look at the cheapest edge next in kruskal's algorithm and pick it if there is no cycles. so let's look at this five vertex example. again, there is no starting point. we're just going to look at the cheapest edge overall. so that's obviously this unit cost edge and we're going to include that in our tree. right? why not? why not pick the cheapest edge? it's a greedy algorithm. so what do we do next? well, now we have this edge of cost two, that looks good, so let's go ahead and pick that one. cool. notice these two edges are totally disjoint. kay.' so we are not maintaining a connectivity of our subgraph at each iteration of kruskal's algorithm. now, it just so happens that when we look at the next edge, the edge of cost 3, we will fuse together the two disjoint pieces that we had previously. now, we happen to have one connected piece. now, here's where it gets interesting. when we look at the next edge, the edge of cost 4, we notice that we're not allowed to pick the edge of cost 4. why? well, that would create a triangle with the edges of costs 2 and 3, and that of course is a no-no. we want to span the tree at the end of the day, so we can't have any cycles. so we skip over the 4 because we have no choice, we can't pick it, we move on to the 5 and the 5 is fine. so when we pick the edge of cost 5, there's no cycles, we go ahead and include it. and now we have a spanning tree and we stop or if you prefer, you could think of it that we do, we do consider the edge of cost 6. that would create a triangle with the edges of costs 3 and 5, so we skip the 6. and then, for completeness, we think about considering the 7, but that would form a triangle with the edges of costs 1 and 5, so we skip that. so after this single scan through the edges in assorted order, we find ourselves with these four pink edges. in this case, it's a spanning tree and as we'll see, not just in this graph but in every graph it's actually the minimum cost spanning tree. so, with the intuition hopefully solidly in place, i don't think the following pseudocode will surprise you. we want to get away with a single scan through the edges in short order. so, obviously in the preprocessing step, we want to take the unsorted array of edges and sort them by edge cost. to keep the notation and the pseudocode simple, let me just, for the purposes of the algorithm, description only, rename the edges 1, 2, 3, 4, all the way up to m conforming to this sorted order, right? so, the algorithm's just going to scan through the edges in this newly found sorted order. so we're going to call the tree in progress capital t, like we did in prim's algorithm. and now, we're just going to zip through the edges once in sorted order. and we take an edge, unless it's obviously a bad idea. and here a bad idea means it creates a cycle, that's a no-no, but as long as there's no cycle, we go ahead and include the edge. and that's it, after you finish up the for loop you just return the tree capital t. it's easy to imagine various optimizations that you could do. so for example, once you've added enough edges to have a spanning tree. so n - 1 edges, where n is the number of vertices, you can get ahead, go ahead and abort this for loop and terminate the algorithm. but let's just keep things simple and analyze this three line version of kruskal's algorithm in this lecture. so just like in our discussion of primm's algorithm, what i want to do is first just focus on why is kruskal's algorithm correct? why does it output a spanning tree at all? and then, the spanning tree that it outputs? why on earth should it have the minimum cost amongst all spanning trees? that's when we're, once we're convinced of the correctness, we'll move on to a naive running time implementation and then finally, a fast implementation using suitable data structures. 
so this algorithm will prove the correctness of kruskal's minimum cost spanning tree algorithm. so to prove this correctness theorem, let's fix an arbitrary connected input graph g. and let's let t star denote the output of kruskal's algorithm when we invoke it on this input graph. so, just like with our high level proof plan for prim's algorithm, we're going to proceed in three steps. we're first going to just establish the more modest goal, the kruskal's algorithm outputs a spanning tree. we're not going to make any initial claims about optimality. so those are the first two steps, one to argue there's no cycles, one to argue that the output's connected. and then, in the third step, relying on the cut property. we'll say, it's not just a spanning tree, it's actually the minimum cost spanning tree. for this proof, i am going to assume you're familiar with the machinery we developed to, to prove prim's algorithm is correct. so the first step of the proof is going to be very easy. so one important property of a spanish [inaudible] there's no cycles and it's quite obvious looking at the pseudocode of kruskal's algorithm that its not going to produce any cycles. every edge that creates a cycle is explicitly excluded from the output. what's a little less obvious is that as long as the, input graph is connected, then kruskal's algorithm will output a connected sub-graph. and therefore a spanning tree. so to argue the output's connected, we're going to have two sub-parts of the proof. the first thing i want you to do is recall what we call the empty cut lemma. so this was a way of talking about when a graph is disconnected or equivalently when a graph is connected. and you'll recall a graph is connected if and only if for every cut there's at least one crossing edge. so to prove this second step of this proof that t star is connected we just have to prove that for every single cut it had at least one edge crossing that cut. so, that's what we're going to show. to get started let's fix an arbitrary cut a, b. using the assumption the input graph is connected, it certainly contains at least one edge that crosses this cut. the question is whether or not t star contains a crossing edge but certainly the original graph g contains a crossing edge. so, here's the key point of the argument. kruskal's algorithm, by definition, it makes a single scan through all of the edges. that is, it considers every edge of the original input graph exactly once. so, what i want you to do is, i want you to think about this cut a, b which has at least one edge of g crossing. and i want you to fast forward kruskals algorithm until the moment that it first considers an edge crossing this cut ab. the key claim is that this first edge seen by kruskal's algorithm, is definitely be, going to be included in the final output, t star. so why is this true? well, let me remind you of the lonely cut corollary. the lonely cut corollary says that if an edge is the sole edge crossing a cut, if it's lonely in a cut, then it cannot participate in any cycle, right? if it was in a cycle, that cycle would loop back around and cross the cut a second time. now, what does that have to do with the picture here? well, if this is the first edge that kruskal sees crossing this cut, then certainly the tree so far t star contains nothing crossing this cut, right? it hasn't even seen anything crossing this cut yet. so at the moment it encounters the first edge, there's no way including that first edge will create a cycle because that first edge is going to be lonely in this cut ab. so again summarising, the first edge crossing a cut is guaranteed to be chosen by a cross because the algorithmic cannot create a cycle. that's why there's at least one edge of kruskal's output crossing this particular cut ab. since ab was arbitrary, all edges, all cuts have some edge of t star crossing them, that's why t star is connected. so this completes the part of the proof where we argue that kruskal's algorithm outputs a spanning tree. now we have to move on and argue that it's actually a minimum cost spanning tree. we're going to argue this in the same way as we did for prim's algorithm. we're going to argue that every edge ever selected by kruskal's algorithm is justified by the cut property. satisfies the hypotheses of the cut property. recall from our correctness proof for prim's algorithm, this is enough to argue correctness. that the output is a minimum cost spanning tree, right? an edge justified by the cut properties, not a mistake. it's got to belong to the minimum cost spanning tree. if you have an algorithm that outputs a spanning tree, and it never made a mistake, that's got to be the minimum cost spanning tree. and that's going to be the case for kruskal. now this statement was easy to prove for prim's algorithm and that's because the definition of prim's algorithm, it selects edges based on being the cheapest in some cut. so it's tailor made for the application of the cut property. not so for kruskal's algorithm. if you look at the pseudocode, nowhere does the pseudocode discuss taking cheap edges across cuts. so we have to show that kruskal's algorithm in effect is inadvertently at every edge picking the cheapest edge crossing some cut. and we actually have to exhibit what that cut is in our proof of correctness. so, that's what we have to do here. so to argue that, let's just sort of freeze kruskal's algorithm at some arbitrary iteration, where it adds a new edge. we need to show that this edge is justified by the cut property. so, maybe this edge has end points u and v, and let's let capital t denote the current value of the edges selected by the algorithm so far. so let's think about what this state of the world looks like, at a, sort of, intermediate iteration of kruskal's algorithm. so kruskal's algorithm maintains the invariant there's no cycles but remember it doesn't maintain any invariant of the current edges forming a connected set so in general in an intermediate iteration of kruskal's algorithm, you've got a bunch of pieces, a bunch of little mini trees floating around the graph. connect the components if you like, with respect to the current edge shed, capital t. and there can be some, you know, isolated vertices floating around as well. what more can we say? well, if the current edge has endpoints u and v and kruskal is deciding to add this edge uv to the current set capital t, we certainly know that capital t has no path between u and v, right? if it did have a path, then this new edge would create a cycle, then kruskal would skip the edge. since it isn't skipping the edge, u and v are currently in different pieces. they're in different components with respect to the current edge set. now remember, ultimately, if we're going to apply the cut property, we have to somehow exhibit some cut from somewhere, justifying this particular edge. so here's where we get the cut from, and it's going to be a very similar argument to when we proved the empty cut limit characterizing disconnectedness in terms of empty cuts. we're going to say look, with respect to the tree edges we have so far, with respect to the green edges, there's no path from u to v. that means we can find a cut such that u is on one side, v is on the other side, and there's no edges crossing the cut. so, here's the cool part of the proof. and this is also the part where we actually use the fact that kruskal was a greedy algorithm. that it considers the edges from cheapest to most expensive. notice that we haven't actually used that fact up to this point and we better use that fact. so, the claim is that not only is this edge we're adding uv, not only does it cross this cut ab, but actually it's the cheapest edge crossing the cut. nothing from the original input graph g that crosses it can be cheaper. so why is that true? well, let's remember how we wrapped up the previous slide when we were arguing that the output of kruskal's algorithm is connected. what did we argue? we said, well, fix any cut you like. any cut of the graph. and freeze kruskal's algorithm the very first time it considers some edge crossing that cut. we noticed that kruskal's algorithm is guaranteed to include that first edge crossing the cut in its final solution. there's no way that, that first edge considered can create a cycle with respect to the edges already chosen. so, it's not going to trigger the exclusion criterion in kruskal's algorithm. and that edge is going to get picked. so what's the significance about being the first edge crossing a cut well because of the gradient criterian because kruskal considers edges from cheapest and most expensive. the first edge it encounters crossing a cut is also necessarily the cheapest edge that's crossing the cut. so here's how we weave these different strands together to complete the correctness proof. alright, so let's remember where we are. we're focusing on a single iteration of kruskal's algorithm. it's about to add this edge u, v to the edges, capital t, that it's chosen in the past. we've exhibited a cut. a, b with a property that no previously chosen edges, no edges of capital t cross this cut, and uv is going to be the first chosen edge crossing this cut. we just argued, that kruskal's algorithm is guaranteed to pick the first edge crossing a cut. so by virtue of their not yet being any chosen edges, crossing the cut ab, it must be the case that this current edge uv is the first one that's ever been seen by kruskal's algorithm that crosses this cut ab. now, in being the first edge it's ever seen crossing this cut. it must also be the cheapest edge of the input graph that crosses this cut. that is the hypothesis of the cut property. that is why this edge uv and this current arbitrary iteration is justified by the cut property. 
so now we understand why kruskal's algorithm is correct, why it always computes a minimum cost-spanning tree. in this video, we'll turn our attention to implementation issues. we'll begin with a straightforward implementation of kruskal's algorithm. that'll give us a polynomial run time bound which is good but we'd like to do better. so then we'll show how deploying a suitable data structure, something you haven't seen before, the union-find data structure, allows us to speed up kruskal's algorithm to be competitive with prim's algorithm. that is we'll get a near linear running time bound of mlogn. so let's just briefly review the very elegant pseudocode for kruskal's algorithm, so it's a greedy algorithm. it considers the cheapest edges first, all the way up to the most expensive. so we begin with a sorting pre-processing step to put the edges in sorted order for notational convenience let's just rename the edges. so, that one is the cheapest edge, and, all the way up to m being the most expensive edge. we then have our single linear scan in this for loop, and we just grab edges whenever we can, okay? so we maintain this evolving set capital t, which at the end of the algorithm will be our spanning tree. now, what forces us to exclude an edge from this set capital t? well if it creates a cycle with the edges was already chosen, obviously that's a no go. we can't have cycles in our final output, but as long as we don't have a cycle from including an edge, we go ahead and optimistically include it. and as we've seen, this is a correct algorithm, it always outputs the minimum cost spanning tree. so, what would be the running time of this algorithm? if we just straightforwardly implement the pseudocode on this slide? well, let's just considered the algorithm step by step. in the first step, we sort the edges, so that's going to take m log n time. now don't forget whenever we're speaking about graphs, we have the convention that m denotes the number of edges and n denotes the number of vertices. so, you might justifiably wonder why i wrote m log n for the running time of the sorting step instead of m log m, since after all what it is we're sorting are the edges and there's m of them. well, what i'm using here is that, in this context we can switch log n and log m interchangeably with each other inside a big-o notation. why is that true? well recall when we first discussed graphs in part one, we noticed that there can't be too many edges. so the number of edges m, is the most quadratic of the number of vertices, it's the most big-o of n squared. so if m is at most m squared, then log m is at most two log n and the two is suppressed in the big-o notation. so log m and log m are interchangeable in this context. notice that for the minimum cost spanning tree problem you may as well assume that there's no parallel edges. you may as well assume that the graphs are simple. if you have a bunch of parallel edges between a given vertices, you can just throw out all but the cheapest one. that's the only one you'll ever need. so, moving on to the main loop, pretty obvious how many iterations we have there. we have m iterations. so all we need to figure out is how much work we have to do in each iteration. so what is it each iteration needs to accomplish? it needs to check, whether or not adding the current edge to the edges we've already chosen creates a cycle or not. so i claim that can be done in time linear in the number of vertices. that is it can be done in the big-o of n time. so how do we accomplish this? well, we need two quick observations. first of all, and this is something we've seen in arguments in the previous videos, checking whether or not this new edge is going to create a cycle. say the edge has end points u and v. checking for a cycle boils down to checking whether or not there's already a path between the end points u and v, and the edges capital t that was chosen so far. if there is a uv path already, adding this edge will close the loop and create a cycle. if there currently is no uv path, then adding this edge will not create a cycle. so the second observation is well, how do we check if there's a path from u to v, in the edges we've already chosen? well, we already know how to do that just. using graph search. so you can use breadth for a search, you can use depth for a search. it doesn't matter. you just start at the vertex u and you see if you have a reach of v or not. if you reach it, there's a path. if you don't reach it, there's not a path. breadth-first-search, depth-first-search, whatever. it takes time linear, in the graph that you're searching. and since we only need to search for the edges that are in capital t. and there's going to be, at most, n minus one of them. linear time in this context means o of n. o of the number of vertices, because that also bounds the number of edges that might be in capital t. the edges that we're searching for are pathing. so, adding up all of this work, what do we have? we have this sorting pre-processing step. that takes time big-o of m log n, then we have these m iterations of the four loop like this is takes o of n times factor. the latter term dominates, so the overall running time is big-o of m times n. so this, coincidentally, is the same running time we got from the straightforward implementation of prim's algorithm, and i'll make the same comments here. this is a reasonable running time, it's polynomial on the input size. it's way better than checking all exponentially many spanning trees that the graph might have. but we certainly would like to do better. we'd certainly love to have implementation of kruskal's algorithm that gets us to a near linear running time bound, and that's the plan. how are we going to do it? well, really the work that we're doing here over and over again, which is kind of a bummer, is these cycle checks. and every single iteration, we're spending time linear in the number of vertices to check for a cycle. and the question is, can we speed that up? and the union-find data structure will actually, believe it or not, allow us to check for a cycle in constant time. so if we actually had a data structure that could implement constant time cycle checks. then we'd have to spend only constant time for each iteration of this while loop. so the loop overall would take only linear time in the number of edges, o of m edges. if we got that, then believe it or not, the sorting pre-processing step would become the bottle neck in the running time of kruskal's algorithm. our running time would drop from n times n down to near linear, down o of n log n. so let me now tell you a little bit about this magical data structure that's going to give us constant time cycle checks. i'm just going to give you the high level picture, and how it connects to kruskal's algorithm on this slide. we'll look at the details of the data structure, in the next video. i also want to warn you that i'm not going to discuss, in this pair of videos, the state of the art for union-find data structures. i'm going to give you a fairly primitive version, but that is nevertheless, sufficient to give us our desired m log n running time of kruskal's algorithm. so if you're interested, there is some optional material about different implementations of union-find that use some super cool ideas, like union by rank and path compression. that give you different, and in some senses, better operation times. but the quick and dirty version of union-find that i'm going to discuss here, is sufficient for our present needs. and so the raison d'tre of a union-find data structure is to maintain a partition of a set of objects. so in this picture, the overall rectangle is meant to denote a set of objects and then c1, c2, c3, and c4 are disjointed subsets that together form the union of the entire set. so that's what i mean by a partition of a group of objects. we're not going to ask too much of this data structure. we're only going to ask it to support two operations. no prizes to guess what those two operations are called. so the find operation, we give it an object from this universe and we ask the data structure to return to us the name of the group to which that object belongs. so for example, if we handed it something in the middle of this rectangle, an object, we'd expect it to return to us the name c3. the union operation by contrast, takes as input, the names of two groups. and what we want the data structure to do is to fuse those two groups together. that is, the objects in the first group, and the objects in the second group should all coalesce, and be, now, in one sole group. so why might such a data structure be useful for speeding up kruskal's algorithm? to see the connection, think of kruskal's algorithm as working conceptually in the following way. so initially, when the algorithm starts in the set capital t is empty, each of the vertices is by it's own, it's on its own isolated component. and then each time kruskal's adds a new edge to the set capital t. what that does is it takes two current connected components and fuses them into a single connected component. so for example, toward the end of kruskal's algorithm that maybe it's included enough edges, that now the tree capital t that is constructed so far has only four different connected components. and maybe it's about to add a new edge u comma v, where of course u and v should be in different connected components with respect to the edges chosen for far. so this new edge addition at this iteration of kruskal is going to fuse the connecting components of u and v into a single one. so that corresponds to taking the union of the groups to which u and v respectively belong. so to be a little more precise about it. so what are going to be the objects contained by the union-find data structure in kruskal's algorithm? we're going to correspond to vertices. it's the vertices coalescing that we want to keep track of. so what are going to be the groups in the partition that we maintain? they're just going to correspond to connected components with respect to the edges that kruskal's algorithm has already committed to. and with these semantics, it's clear that every time kruskal adds a new edge to its set capital t, we have to invoke the union operation to fuse two connected components into one. 
so let me now introduce you to the union-find data structure. let's not lose sight of our goal. our goal is to be able check for cycles in kruskal's algorithm in constant time. so the first and most basic idea behind this union-find data structure implementation, is we're going to maintain a linked structure for each group, that is for each connected component with respect to the edges kruskal has chosen thus far. by link structure i just mean each vertex of the graph is going to have an extra pointer field. in addition, with each group, with each connected component, we're going to designate one vertex. and we don't care which one. just some vertex from this connected component as the leader vertex of that component. all right, so what is the point of these extra pointers at each vertex and what is the point of having these leaders? well a key invariant that we're going to maintain is that a given vertex, with its extra pointer, points to the leader vertex of its connected component. so for example maybe we have two different connected components with three vertices each, one containing the vertices u, v, and w, another containing the vertices x, y and z. any of these three vertices could be the leader of each of these two components, so perhaps u happens to be the leader vertex of the first component and x happens to be the leader vertex of the second component, and then the invariant just says every vertex should be pointing to the leader of its component. so v and w should be pointing to u, u has its own pointer, it should be pointing to itself. similarly in the second component x is pointing to itself, y points to x and z also points to x. so in blue here are the actual edges of the graph and in green there are these sort of extra edge pointers that we've invented where everybody's pointing back to the leader of their component. and so one very simple thing in this setup, which turns out to be a good idea, is each component is in effect inheriting the name of its leader vertex. so we refer to a group, we refer to a component via the object, via the vertex, who happens to be the representative, who happens to be the leader. and what's kind of amazing is even just this very simple scaffolding on the connected components is enough to have constant time cycle checks provided the invariant is satisfied. well, how do we do that. so remember that checking if adding the edge uv is going to create a cycle boils down to checking whether there's already a path between u and v. well, there's already a path between u and v if and only if they're in the same connected component. given two vertices, u and v. how do we know if they're in the same connected component? we just follow the respective leader pointers and we see if we get to the same place. if they're in the same component, we get the same leader. if they're in different components, we get different leaders. so, checking for a cycle just involves for each of u and v, comparing a quality of leader pointers that is clearly constant time. more generally the way you implement the find operation for this flavor of disjoint union data structure is you simply return the leader pointer of the object that you were handed. so if you're given a vertex, you just follow the leader pointer of that vertex, and you return wherever you wind up. so that's pretty excellent as long as in this simple data structure the invariants satisfied we have are desired implementation of constant time cycle checks. but and certainly this is a recurring theme in our data structure discussions. whenever you have a data structure and it needs to maintain an invariant. whenever you do an operation that changes the data structure. so in this case when you do a union fusing two groups together. you have to worry, well does the invariant get destroyed when you do that operation and if it does how are you going to restore the invariant without doing undue work. so in the present context of kruskal's algorithm here's how this plays out. so we're happily doing our constant time cycle checks whenever an edge creates a cycle we don't do anything, we skip the edge, we don't change our data structure, we move on. the issue is when we have a new edge and it doesn't create a cycle, our cycle check fails. now kruskal's algorithm dictates that we add this edge into the set capital t that we're building and that fuses two connected components together. but remember we have this invariant, every vertex should point to the leader of it's component. well if we had component a and we had component b, they are both pointing to the leader vertex of their respective components. now when these components fuse into one, we've gotta do some updating of leader pointers. in particular, there used to be two leaders, now there has to be just one. and we have to rewire the leader pointers to restore the invariant. so to make sure you're clear on this important problem. let me ask you the following question. so consider the case when at some point in kruskal's algorithm, a new edge is added and two connected components fuse into one, now to restore the invariant you have to some leader pointer updates. in the worst case, asymptotically, how many leader pointer updates might be required in order to restore the invariance? so, the answer to this question, somewhat alarmingly, is the third answer. so, it might require a linear number in the number vertices n pointer updates to restore the invariant. maybe one easy way to see that is just imagine the very last edge that kruskal's going to add in the set t, the one which fuses the final two connected components down into one. for all you know, those two components have exactly the same size. they have n over 2 vertices each. you're going down from two leader pointers to one. one of those sets of n over 2 vertices are going to have to inherit the leader pointers from the other side. so one of the two sets is going to have to have n over 2 vertices get their leader pointer updated. so that's a bummer. we were sort of hoping for a near linear time bound, but if every, each one of our linear number of edge additions might trigger a linear number of leader pointer updates that seems to be giving rise to a quadratic time bound. but remember when i introduced the union-find data structure i only said that first idea was idea number one. presumably there's an idea number two and here it is. and it's very natural, if you were coding up an implementation of union-find data structure you would probably very naturally do this optimization yourself. all right, so consider the moment in time in which some component a merges with some component b. each of these two components currently has their own respective leader vertex, and all vertices from that group are pointing to that leader vertex. now when they fuse, what are you going to do? the first obvious thing to do is say, well let's not bother computing some totally new leader. let's either re-use the leader from group a or the leader from group b. that way it, for example, we retain the leader from group a, the only leader pointers that need to be rewired are the ones that come from component b. the vertices in component a can keep their same leader vertex and their same leader pointers as before. so that's the first point. let's just have a new union of two components inherit the leader of one of the constituent parts. now, if you're going to retain one of the two leaders, which one are you going to retain? maybe one the components is 1,000 vertices. the other component only has 100 vertices. well given the choice you're certainly going to keep the leader from the bigger, from the first component. that way you only have to rewire the 100 leader pointers of the second group. right if you kept the leader of the second group you'd have to rewire the 1,000 pointers from the first group and that seems silly and wasteful. so the obvious way to implement a merge is you just keep the leader of the bigger group, and rewire everybody from the second group, the smaller group. so you should notice that in order to actually implement this optimization where you always retain the leader of the larger group, you have to be able to quickly determine which of the two groups is larger, but you can augment the data structure we've discussed to facilitate that. so just with each group, you just keep a count of how many vertices are in that group so you maintain a size field for each group. that allows you to check in constant time what's the population of two different groups and figure out, again in constant time, which one's bigger. and notice that when you fuse two groups together it's easier to maintain the size field, it's just the sum of the sizes of the two constituent parts. so let me know revisit the question from the previous slide. in the worst case given this optimization, how many leader pointers, asymptotically, might you have to rewire when you merge two components together? well unfortunately, this song remains the same. the answer is still the third one, and it's because for exactly the same reason as on the previous slide. it still might be the case that say in the final iteration of kruskal you're merging two components that both have size n over 2, so it doesn't matter. i mean no matter which leader you choose you're going to be stuck updating the leader pointers of n over 2 or theta of n vertices. so while this is clearly a smart practical optimization, it doesn't seem to be buying us anything in our asymptotic analysis of the running time. however, however, what if we think about the work done in all of these leader pointers updates in the following different way. rather than asking how many updates might a merging of two components trigger, let's adopt a vertex-centric view. let's suppose you're one of the vertices of this graph, so initially the beginning of kruskal's algorithm you're in your own isolated connected component, you just point to yourself. you're your own leader. and then as kruskal's algorithm runs it's course, your leader pointer will periodically get updated. at some point you're no longer pointing to yourself. you're pointing to some other vertex. then at some point you're pointer gets updated again, you're pointing to yet some other vertex and so on. how many times over the entire trajectory of kruskal's algorithm, in light of our new optimization, might you as some vertex of this graph have your leader pointer updated? well here's the very cool answer. the answer is the second one. so while it remains true that if you always have the union of two groups inherit the leader pointer of the larger one, it's still true that a given fusion might trigger a linear number of leader pointer updates, each vertex will only see its leader pointer updated a logarithmic number of times over the course of kruskal's algorithm. what is the reason for this? well, suppose you're a vertex and you're in some group and it has maybe 20 vertices, so you're 1 of 20. now, suppose at some point your leader pointer gets updated. why did that happen? well, it meant that your group of 20 vertices merged with some other group that has to be bigger. remember, your leader pointer only gets rewired in a fusion if you were in the smaller group. so you're joining a group at least as big as yours. so the size of the union, the size of your new community, your new connected component is at least double the size of your previous one. so the bottom line is every time you as a vertex has your leader pointer updated, the population in the component to which you belong is at least twice as large as before. now, you started the connecting component of size one. the connecting component is not going to have more than n vertices. so the number of doublings you might have to endure ss at most log base 2 of n. so that bounds how many leader pointers you will see as a vertex in this graph. so in light of that very cool operation we can now give a good running time analysis of kruskal's algorithm using the union-find data structure, using the scaffolding on the connected component structure to do cycle checks. we of course have not changed the preprocessing step. we're still sorting the edges from cheapest to most expensive at the beginning of the algorithm, and that still takes o(m log n) time. so what work do we have to do, beyond this sorting preprocessing step? well fundamentally, kruskal's algorithm is just about these cycle checks. in each iteration of the for loop, we have to check if adding a given edge would create a cycle with those edges we've already added. and, the whole point of the union-find scaffolding, these link structures, is that we can check cycles in constant time. just given a edge, we look at the leader pointers of its endpoints, and a cycle will be created if and only if their leader pointers are identical. so for the cycle checking, we only do constant time for each of the o of m iterations. but the final source of work is maintaining this union-find data structure, restoring the invariant each time we add a new edge to the set capital t. and here the good idea is we're not going to just bound the worst case running time of these leader pointers for each iteration, because that could be too expensive that could be up to linear in just a single iteration. rather we're going to do a global analysis, thinking about the total number reader pointers that ever occur, on the previous slide we observed that for a single vertex, it's only going to endure a logarithmic number of leader pointer updates. so something over all of the n vertices, the total work done for leader pointer updates is only n log n. so even though we might do a linear amount of pointer updates in just one iteration of this for loop, we still have this global upper bound of n log n on the total number of leader pointer updates. very cool. so looking over this tally, we observe the stunning fact that the bottleneck for this implementation of kruskal's algorithm is actually sorting. so we do more work in the preprocessing step, n log n, than we do in the entire for loop, which is only m plus n log n. so that gives us an overall running time down of o(m log n) matching the theoretical performance that we achieved in prim's algorithm implemented with heaps. so just like with our heap-based implementation of prim's algorithm, we get a running time which is almost linear. only a logarithmic factor's slower than reading the input. and just like with prim's algorithm, you should find kruskal's algorithm to be very competitive in practice. 
in this optional video, i'm going to give you a glimpse of the research frontier on the problem of computing minimum cost, spanning trees. we've now seen two excellent algorithms for this problem. prim's algorithm and kruskal's algorithm. and we had suitable data structures both running near linear time. big o of m log n, where m is the number of edges, n is the number of vertices. now going ahead we should be pretty happy with those algorithms. we're only a log factor slower than what it takes to read the input. but again, the good algorithm designer should never be content, never be complacent. should always ask can we do better? maybe we do even better than m log n. well, believe it or not, you can do better than these implementations. at least in principle, at least, in theory. you have to work pretty hard and i'm definitely not going to discuss the algorithms or the analysis that prove this fact. but let me just mention a couple references that give mst algorithms with asymptotic run times even better than m log n. so quite shockingly, if you're happy with randomized algorithms, as we were with say, the quit sort sorting algorithm. then the minimum cost spanning tree problem can be solved in linear time. that's obviously an optimal algorithm. you have to look at the whole graph to compute the minimum cost spanning tree. but you can solve the problem in time a constant factor, larger than what it takes to read the input. this algorithm is much, much newer than prim and kruskal's algorithm, as you might expect. it does date back to the 20th century, but definitely the latter stages of last century. so the names of a couple of the inventors of this algorithm will already be familiar to those of you that paid close attention in part one. so the carver here is the same as the author of the randomized and cut algorithm you studied in part one. tarjan's name is coming up a couple times in these courses, but for example when we discuss strongly connected component algorithms. so, what if you're not happy with the bound just on the bound just on the expected running time, with the expectation over the coin flips of the algorithm? what if you wanted a deterministic algorithm? so, if we think of this randomized algorithm as being analogous to quick sort in the sorting problem, what would be the analog of merge sort? well, it turns out we do not know whether or not there is a linear time deterministic algorithm for minimum spanning trees, that's an open question. you might think, here at this, late date, 2012, 2013 we'd know everything there is to know about minimum cost spanning trees. we do not know, if there exists a linear time deterministic algorithm. we do know that there's a deterministic algorithm, with a running time, absurdly close to linear. specifically, there's an algorithm with running time m, the number of edges times alpha of n. so what, you ask, is alpha of n? what is this function? well, it's something called the inverse ackermann function. so, defining the ackermann function and its inverse actually takes a little bit of work. so i'm not going to do it right now. for those of you that are curious, you should check out the optional material on the union find data structure. which is yet another context where sort of unbelievably, this inverse ackermann function arises. but for the present context, what you should is is that this is a crazy slow growing function. it's not constant. for any number c, i can exhibit a large enough value of n, so that this function values is lease c. so that alpha of n is a lease c. so, it's not bounded by a constant, but it's mind boggling how slow growing this function is. let me actually just give you an incredibly slow growing function which actually goes much faster than the inverse ackermann, just to prove the point. specifically, the inverse ackermann function grows quite a bit slower than log star n. log star n i can define for you easily. we recall our definition of log base two, way back when we first demystified logarithms at the beginning of part one. if you type n into your calculator, and you keep dividing by two, you count the number of times you divide by two until you drop below one, that's log based two of n. for log star, instead of dividing by two, you're going to hit the log button on your calculator, ad you count how many times you hit log, until, the result drops below one. that number, the number of times you hit log, is defined to be log star of n. the log star function as the inverse of the function which is a tower of 2s. the function which takes as input you know, an integer say t and raises two to itself t times. so, to appreciate just how slow growing the log star function is, let alone the inverse ackerman function. what you should do is type in the biggest number you can into your nearest calculator or computer. just keep typing in nines as long as you can. then go ahead and evaluate log star. keep applying log function till it drops below one. probably, the result's going to be something in the neighborhood of five. so log star of the biggest number in your calculator or computer is going to be five. that's how, that's how slow growing this function is. so, the upshot is that the algorithms community has the time complexity of the mst problem almost nailed, but not quite, in the deterministic case. the right answer is somewhere between m and m times inverse ackermann function, and we don't know where. but, you know what? it gets weirder. so check out the following result of pettie and ramachandran. they in a sense solve, the deterministic minimum cost spanning tree problem by exhibiting, an algorithm, who's time complexity is optimal. so they gave an algorithm and they gave a proof, just in the spirit of what we did for sorting. they gave a proof that no algorithm could have running time asymptotically better than theirs. but what they didn't do, is explicitly evaluate what the time complexity of their algorithm is. so they managed to prove optimality, without actually evaluating exactly what's its running time. so it's certainly somewhere between linear and linear times inverse ackermannn. we know that's the true time complexity of the problem. we know an algorithm that achieves an optimal complexity. but to this day we do not know what that optimal time complexity actually is, as a function of the graph size. so those are some of the most advanced things that we do know, about the minimum cost spanning tree problem. let me just mention a couple of things that we still, to this day, do not know. so let me start with the randomized algorithms. now, maybe you're reaction is, there's no open questions in the randomized algorithms world, because we know you need linear time to solve the problem. and i've told you that there is a randomized algorithm with expected running time that is linear. so what more could you want? well, what i want is i want an algorithm that's not just linear time but also simple enough that i can teach it to other people. so ideally, it would be another graduate course like this. but i was actually very happy to have a randomized algorithm, linear time, simple enough to cover in a graduate course. the current linear time algorithms do not have that property. they're more complicated than i can even cover in a graduate course. to accomplish this task, it turns out to be enough to solve a seemingly simpler task, namely, to get a simple randomized linear time algorithm for the mst verification problem. so let me tell you what that means. so in a mst problem you're supposed to optimize amongst all exponentially minimum spanning trees. you got to find me the one with the smallest sum of edge cost. in mst verification, the job seems simpler. i'm actually going to hand you, a candidate mst or i'm going to hand you a spanning tree. it may or may not be the best one and you just need to check, is it the optimal one or not. furthermore, in the event that it's not optimal you should tell me edges that are not in the spanning tree. edges that are too expensive that i should throw out. the reason it's enough to design a linear time algorithm for this seemingly simpler problem, is that the content of the karger-klein-tarjan paper, is a reduction. a randomized reduction from optimizing over spanning trees, to the seemingly simpler problem of mst verification. moreover, all of the novel content in the karger-klein-tarjan algorithm. it's linear time, it's randomized, and it is really simple. i teach this stuff in that paper in my graduate classes. but it needs this mst verification subroutine as a black box. and the only known implementations that are linear time for mst verification are quite complicated. so, find a simple way to do mst verification that runs in linear time. and you're good to go with your simple optimal mst algorithm. for deterministic algorithms, the holy grail is obvious. we'd love to have a deterministic algorithm for mst that runs in linear time. or at the very least, we just need to figure out, you know, what is the best possible time complexity of any deterministic mst algorithm. so one of the takeaways of this discussion is that, you know? for all the amazing advances by computer scientists on the design and analysis of algorithms over the past 50 years or so. there are still totally fundamental things that we do not understand. so there's still great ideas to come in the future. so if you've been intrigued by some of the things that i've said in this video. and you want to learn more about these advanced minimum cost spanning tree algorithms. for further reading, i'd recommend a survey by jason eisner, called state of the art minimum spanning tree algorithms. it's about fifteen years old now. but it's still an amazing resource for learning about this advanced material. 
so, by now we've spent quite a bit of time talking about the minimum cost spanning tree problem. there's a number of motivations why we do that. the first, it's just a uniquely great problem for the study of greedy algorithms. you can propose a bunch of different greedy algorithms, and quite unusually, it's all of them seem to work. so you get correct greedy algorithms, but it's quite subtle what's driving their correctness. you also get a lot of practice, arguing about graphs, and arguing about exchange arguments, and so on. the second reason, it's been worthwhile spending time on these algorithms. it has given us some more practice with data structures and how to use them to speed up algorithms, namely heaps, to speed up prim's algorithm, to union-find data structure to speed up kruskal's algorithm. the third reason that we're talking about them is they have applications in their own rights. that's the subject of this video and the next and we're going to focus on applications to clustering problems. so let me begin by just talking about the goal of clustering informally. and then, i'll let you pin me down to a precise objective function on the next slide. so in a clustering problem, the input is n points that we think of as being embedded in space. and it's actually quite rare that in the underlying problem that we care about is it actually intrinsically geometric? is it actually intrinsically points in space? usually, we're representing something we care about. maybe it's web pages. maybe it's images. maybe it's a database as points in space. and given a bunch of objects, we want to cluster them into, in some sense coherent groups. for those of you coming from a machine learning background, you'll often hear this problem referred to as unsupervised learning, meaning the data is unlabeled. we're looking for just patterns and data where the data is not annotated. this obviously is a fairly wishy-washy description of a problem, so let's be a little bit more precise. we're going to assume that part of the input is what we're going to call a similarity measure. meaning for any two objects, we're going to have a function giving us a number indicating how similar or really rather how dissimilar they are to each other. in keeping with the geometric metaphor, we're going to refer to this function as a distance function. one thing that's cool is we don't need to impose many assumptions on this distance function. the one thing we're going to assume is that it's symmetric, meaning the distance from p to q is the same as the distance from q to p. so what are some examples? well, if you want to really go with the geometric metaphor, if you're representing these points as in a space rm for some dimension m, you can just use the euclidean distance or if you prefer some other norm, like say, l1 or l-infinity. in many application domains, there are widely accepted similarity or distance measures. one example would be for sequences, as we discussed in introductory lecture the penalty of the best alignment between two genome fragments. so now that we have this distance function, what would it mean to have coherent groups? well, things which have small distance from each other which are similar, they should generally be in the same group, and things that are very dissimilar that have large distance between them, you would expect to mostly be in different groups. so how can we evaluate how good a clustering is, how well it's doing with the job of putting nearby points together and dissimilar points in different groups? well, to be honest there's many ways of approaching and formalizing that problem. the approach we're going to take is an optimization-based approach. we're going to positive and objective function on clusterings and then seek out the clustering that optimizes that objective function. i want to warn you that's not the only way to approach the problem. there are other interesting approaches, but optimization is a natural one. furthermore, just like in our scheduling application, there's more than one objective function that people study and that is well-motivated. so one very popular objective function would be, say the k-means objective, which i encourage you to look up and read more about. for this lecture, i'm just going to adopt one specific objective function. it's natural enough, but it's by no means the only one or even the primary one. but it'll serve the purpose of studying a natural greedy algorithm related to minimum spanning tree algorithms which is optimal in a precise sense. so let me develop the terminology needed to state the objective function and the optimization problem precisely. one issue that always comes up in clustering problems is how many clusters are you going to use? so to keep things simple in this video, we're going to assume that part of the input k indicates how many clusters you're supposed to use. so we're going to assume you know a priori how many clusters you want. so in some application domains, this is a totally reasonably assumption. you know, you might know for example that you want exactly two clusters, the k = 2. in some domains you may have you know, good domain knowledge from past experience that you know how many clusters you expect to need that's, all fine and good. also you know, in practice, if you don't really know what k is supposed to be, you can go ahead and run the algorithm we're going to talk about for a bunch of different values of k and then you symmetric or just eyeball it to figure out which is the best solution. so the objective function we're going to look at is defined in terms of separated pairs of points. that is points that are assigned to distinct clusters. now you know, if you have more than one cluster, inevitably there's going to be some pairs of points. some points are going to be one groups, other points are going to be in the different group. so separated points are inevitable and the most alarming separated points are the ones that are the most similar, the ones that have the smallest distance. if points are separated, we want them to be for apart, so we're particularly concerned with nearby points that are separated. so that's going to be our objective function value, called the spacing of a clustering. it's the distance between the closest together pair of separated points. now what do we want from the spacing of a clustering? well, we want all of the separated points to be as far apart as possible. that is, we want the spacing to be big. the bigger the better. so that naturally motivates the formal problem statement. you're given as inputs, the, distance measure. you're told the distance between each pair of points. you're also told the desired number of clusters. amongst all ways of clustering the points into k clusters, find the clustering which makes the spacing as big as possible. so let's develop a greedy algorithm that seeks to make the spacing as big as possible. and to facilitate the discussion, i'll use an example point set with just six black points up here in the upper right part of the slide. now, the good idea behind this greedy algorithm is to not worry about the constraints that, at the end of the day, we can only output k different clusters. we're actually going to be infeasible, we'll have too many clusters throughout the algorithm. only at the conclusion of the algorithm will we be down to k clusters, which will be our final infeasible solution. so that frees us up to initialize the procedure where the degenerate solution, where each point is in its own cluster. so in our example point set, we have these six pink isolated clusters. in general, you're going to have n clusters and we've got to get down to k. now, let's remember what the spacing objective is. in the spacing objective, you go over all of the separated pairs of points. so for this degenerate solution, it's all pairs of points. and you look at the most alarming separated pair, that is those, that are the, the closest to each other. so the spacing is the distance between the closest pair of separated points. now, in a greedy algorithm, you want to increase your objective function as much as possible. but actually, things are pretty cut and dried in this scenario. suppose i give you a clustering and you want to make the spacing bigger, the only way you can do that is by taking the currently closest pair of separated points and making them not separated any more. that is putting them in the same cluster. so, it's in some sense obvious what you want to do to make the objective function go up at all. you gotta look at the pair of points that is defining the objective, the closest pair of separated points, and you have to fuse them. you have to fuse their clusters so that they're no longer separated. in this example, it looks to me like the closest pair of points, which of course are separated, is this pair in the upper right. so, if we want to make the spacing bigger, then we fuse them into a common cluster. [sound] so we started with six clusters. now, we're down to five. so now we reevaluate the spacing again of this new clustering. we ask, what is the closest pair of separated points? so that would seem to me to be this pair in the bottom right. [sound] and again, the only way we can increase the spacing by merging clustering is to merge these two isolated clusters into one. now, we do it again. we say, which pair of points determines the current spacing, which the currently separated pair of points that are nearest to each other? that to me would look like this pair that's on the rightmost part of the picture. the only way to merge two clusters and make the spacing actually go up is to merge the clusters containing the pairs of points determining the current spacing. so in this case, two different clusters with two points, each would collapse into a single cluster with four points. let's assume that we wanted three clusters, clusters anyways, which is where we are. so at this point, the greedy algorithm is going to halt. so let me now spell out the pseudocode of the greedy algorithm more generally, but it's exactly what you'd expect given the discussion so far. all right. so, i'd like you to stare at this pseudocode for ten seconds or however long you need and try to relate this to an algorithm that we've seen in the course. in particular, an algorithm that we've seen quite recently. i hope it reminds you strongly of an algorithm we've already covered. specifically, i hope you see a strong resemblance between this greedy algorithm and kruskal's algorithm for computing a minimum cost spanning tree. indeed, we can think of this greedy clustering algorithm as being exactly the same as kruskal's minimum cost spanning tree algorithm except aborted early. stopped when there's k components remaining, that is before the final k - 1 edge additions. so, just to make sure the correspondence is clear. what is the graph? what are the edges? what are the edge costs? well, the objects in the clustering problem, that is the points. those correspond to vertices in a graph. the other part of the input of the clustering problem are distances, which are given for every pair of points. so those play the role that edge costs were playing in the minimum spanning tree problem. since we have a distance or an edge cost for every single pair of points, we can think of the edge set in the clustering problem as being the complete graph because we have an edge cost or a distance for every single pair. so this type of agglomerative clustering has a name. this idea of fusing components one at a time using mst-lik criterion. it's called single-link clustering. so a single n clustering is a good idea. if you work at all with clustering problems or unsupervised learning problems, it definitely should be a tool in your toolbox. we're going to justify its existence in one particular way in the next video, when we show that it does indeed maximize the spacing over all possible k clusterings. but even if you don't care about the spacing objective function per se, you want to be familiar with single-link clustering. it has many other nice properties as well. 
this video will prove the correctness of our greedy algorithm for clustering. we'll show that it maximizes the spacing over all possible k clusterings. you might have hoped that we could deduce the correctness of this greedy algorithm for clustering immediately from our correctness proofs for various greedy minimum spanning tree algorithms. unfortunately that doesn't seem to be the case. in the minimum cost spanning tree problem, we're focusing on minimizing the sum of the edge cost. here we're looking at different objective, maximizing the spacings. we do need to do a proof from scratch. that's said, you know, the arguments we'll use should look familiar to you not just from the sort of exchange type arguments when we prove the cut property, but also it might remind you even more, going back further, to our greedy algorithms for scheduling. so let's now set up the notation for the proof. as usual, we're going to look at the output of r algorithm. it achieves some objective function value, some spacing. we're going to look at an arbitrary competitor. some other proposed scheduling. we're going to show that we're at least as good, our spacing is, at least, as large. so specifically, we'll denote the clusters in the output of r algorithm by c1 up to ck. our clustering has some spacing, some distance between the near, closest pair of separated points. call it capital s. we're going to denote our competitor, some alternative k clustering by c-hat one of the c-hat k, what is it that we're tryin to show? we want to show that this arbitrary other clustering has spacing no larger than r's, if we can show that, then because this clustering was arbitrary it means the greedy clustering has spacing as large as any other, so it's maximizing the spacing, that's what we want to proof. but differently we want to exhibit a pair of points separated by this cluster and c one-half to c1k, such that the distance between those separated points is s or smaller. so, let me just quickly depose of a trivial case. if the c hats are the same as the c's, possibly up to a renaming, then of course exactly the same pairs of points are separated into each of the clustering, so that the spacing is exactly the same. so that's not a case we have to worry about. the interesting case, then, is when the c hats differ fundamentally from the cs, when they're not merely a permutation of the clusters in the greedy clustering. and the maneuver we're going to do here is similar in spirit to what we did in our scheduling correctness proof. way back in our scheduling correctness proof, we argued that any schedule that differs from the greedy one, suffers from, in some sense, a local flaw. we identified an adjacent pair of jobs that was, in some sense, out of order with respect to the greedy ordering. the analog here is, we're going to argue that, for any clustering which is not merely a permutation of the greedy clustering. there has to be a pair of points which is classified differently in the c hats relative to the c's. by differently, i mean they're clustered together in the greedy clustering. these points, p and q, belong to the same cluster, c sub i. yet, in this alternative clustering, which is not just the permutation of the greedy clustering. they're placed in different clusters. one, maybe p and c hat i, and q and some other c hat j. so i want to now split the proof into an easy case and a tricky case. to explain why the easy case is easy lets, lets observe a property that this greedy clustering algorithm has. now the algorithm's philosophy is that the squeaky wheel should get the grease. that is, the separated pair of points that are closest to each other are the ones that should get merged. so for this reason, because it's always the closest separated pair that get merged, if you look at the sequence of point pairs that get merged together, that determine the spacing in each subsequent iteration, the distances between these sort of worst separated points is only going up over time. at the beginning of the algorithm, the closest pair of points in the entire point set are the ones that get directly merged. then those are out of the picture, and now that some further away pair of points are separated, it determines the spacing, then they get merged. once they've been coalesced, then there is still some further away pair of points, which is now the smallest separated. they get merged, and so on. so if you look at the sequence of distances between the pairs of points that are directly merged by the greedy algorithm, that is only going up over time. and this sequence culminates with the final spacing s of the greedy algorithm. at some sense, the spacing of the output of the greedy algorithm is the distance between the point period that would get merged if we ran the greedy algorithm one more in moderation but unfortunately we're not allowed to do that. okay? so the point is, for every pair of points directly merged by the greedy algorithm, they're always a distance at most s away from each other. so the easy case, then, is when this pair of points, pq, which, on the one hand, lie in a common greedy structure, but on the other hand, in different clusters with c hats. if they were, at some point, not merely in the same cluster, but actually directly merged by the greedy algorithm. if, at some iteration, they determined the spacing, and were picked by the greedy algorithm to have their, clusters merged. then we just argued that the distance between p and q is no more than the space in capital s of the greedy clustering. and since p and q lie in different clusters of the c hats. it's separated by the c hats and therefore they upper bound the spacing of the c hats. maybe there's some even closer separated pair by the c hats. but the very least p and q are separated so they upper bound the spacing of the c hat clustering. so that's what we wanted to prove. we wanted to show that this alternative spacing didn't have better spacing than our greedy spacing. it had to be at most as big. it had to be at most capital s. so in this easy case, when p and q are directly merged by the greedy algorithm, we're done. so the tricky case is when p and q are only indirectly merged, and you may be wondering at the moment, what does that mean? how did two people wind up in the same cluster if they weren't, at some point, directly merged? so let's draw a picture and see how that can happen. so the issue is that two points p and q might wind up in a common greeting cluster, not because the greedy algorithm ever explicitly considered that point pair, but rather because of a path or cascade of direct mergers of other point pairs. imagine, for example, that at some iteration of the greedy algorithm the point p was considered explicitly along with the point a1, where here a1 is meant to be different than q. so that's a direct merger, and p and a1 wind up in the same cluster. their clusters are merged. maybe the same thing happened to the point q at some point a sub l which is different than p. sooner or later maybe, you know, at some other time, some totally unrelated pair of points a2 and a3 are directly merged and then at some point a1 and a2 are considered by the greedy algorithm. algorithm, because the other closest pair of separated points, and, they get merged. and so on. so the edges in this picture are meant to indicate direct mergers, pairs of points that are explicitly fused because they determine the spacing of some point of the greedy iteration. but at the end of the day the greedy clustering is going to have the results of all of these mergings. so in case you're feeling confused, let me just point out that we really saw this exact same exact thing going on when we were talking about minimum spanning trees in kruskal's rhythm. so, at an intermediate point in kruskal's rhythm, after it's added some edges, but before it's constructed a spanning tree. as we discussed, the intermediate state is a bunch of different connected components. and there are vertices that have an edge chosen between them. they, of course, are going to be in the same kinetic component. but then again, a kenetic component could have long paths in it. so you could have vertices that are in the same kinetic component in an intermediate state of kruskal's algorithm, despite the fact that we've haven't chosen an edge directly between them. there's rather, a path of chosen edges between them. it's exactly the same thing going on here. now, what we have going for us is that, if a pair of points, as discussed, was directly merged, we know they're close. the distance between them is, at most, this spacing, capital s. we really don't know anything, frankly, about the distance between pairs of vertices that were not directly merged. they just, sort of, accidentally wound up in a common cluster. but this turns out to be good enough. this is actually sufficient to argue that this competitor clustering with the c-hat has spacing no more than s? no better than ours. let's see why. so given that p and q are in a common greedy cluster it must mean there was a path of direct mergers that forced them to be in the same cluster. so let's let the intermediate points involved in that path denoted a1 of two al. so here's the part of the proof where we basically reduce the tricky case to the easy case. so we've got this pair of points, pq. now, remember, not, not only are they in a common greedy cluster. but they're in different clusters in our competitor in the c hats. so the point p is in some cluster. call it c hat i. and q is in something else. in particular, it's not in c hat i. now, imagine you go on a hike. you start at the point p, and you hike along this path. you traverse these direct mergers toward q. now, you're starting inside c hat i, and you end up outside. so at some point on your hike, you will traverse the boundary. you will, for the first time, escape from c hat i, and wind up in some other cluster. so that has to happen. and let's call ai and ai+1 the consecutive pair of points at which you go from inside this cluster to outside this cluster. and now we're back in the easy case. now we're dealing with a separated pair that would directly merge by the greedy algorithm. remember that we set up this path to be a path of direct mergers so in particular, aj and aj + one were direct mergers, therefore their distance is at most s. and again, by virtue of being direct mergers, their distance is at most the spacing of the greedy clustering and yet as a separated point by the c hats. it's also an upper bound on the spacing of the c hats. this means the spacing s of our greedy clustering is as good as the competitor. is the competitor was arbitrary or optimal. that completes the proof. 
so, welcome to this optional sequence of videos on state of the art implementations of the union to find data structure. now as advanced in optional material, let me make a few comments before we get started. so the first comment is i'm going to assume that you're in a particularly motivated mood. now, no one's forcing you to take these algorithm courses, so i'm always assuming that you're very motivated individual, but that's going to hold even more true than usual in these advanced videos. and the way that plays out is while i'm going to hold myself to the same exacting standards of clarity that i usually do, i'm going to ask a little bit more of you, the viewer. i'm going to perhaps dot a few less is, cross a few less ts than i usually do. so you may find yourself periodically needing to pause and think through some of the details that i'm glossing over. the second comment is i'm going to give is sort of short shrift to applications of the union find data structure. that's not because there aren't any, there's plenty of applications of this data structure but for these videos we're really just going to immerse ourselves in the beauty and the depth of ideas behind the design of the data structures and especially the analysis of their performance. the final comment is to keep in mind that this is some seriously next level material. it is totally normal that the first time you see this stuff, you find it confusing. you find it difficult to understand. so confusion should not discourage you. it does not represent any intellectual failing on your part. rather, keep in mind, it represents an opportunity to get even smarter. so, with those comments in mind, let's go ahead and proceed to a different approach to the union-find data structure via lazy unions. so let's have one slide with a quick review of what we already know about the union-find data structure. recall we discussed this in the context of a fast implementation of kruskal's minimum spanning tree algorithm the raison d'etre of a union-find data structure is to maintain a partition of a universe of objects. so in the context of kruskal's algorithm, the objects were the vertices, and the groups we wanted to maintain were the connected components with respect to the edges we committed ourselves to so far. the data structure should support two operations. no prizes for guessing the names of those two operations. first is the find operation. this is given an object from the universe return the name of that object's group. so for example if x was in the middle of this square that i put on the right representing the universe capital x we would return a c3. the name of the group that contains x. so in the context of kruskal's algorithm, we use find operation to check for cycles. how did we know whether adding a new edge would create a cycle with respect to the edges we've already chosen? well, it would be if and only if the end points of that edge were already in the same connected component. that is, if and only if the two find operations returned the same answer. in the union operation you're given out one object but you call them x and y, and then the responsibility of the operation is to merge the groups that contain x and y. so for example in the picture y was in c4, x was as before in c3. then your job is to fuse the groups c3 and c4 into a single group. in the context of kruskal's algorithm, we needed this operation when we added a new edge. this fused together two of the existing connecting components into one. so that was exactly the union operation. so we all ready went through one implementation of union and find. that was order to get a blazingly fast implementation of kruskal's algorithm. so let's just review how that works. with each group, we associated a linked structure, so each object had one pointer associated with it. and the invariant was in a given group, there was some representative leader object. and everybody in that group pointed to the leader of that group. so for example, on the right i'm showing you a group with three objects, x, y, and z, and x would be the leader of this group. all three of the objects point directly to x, and that would just be the output of the find operation, the leader of the group. we use that as the group name. now the part which is cool and obvious about this approach is our find operations take constant time. all you do is return the leader of the given object. now the tricky part was analyzing the cost of union operations. so the problem here is that to maintain the invariant that every object of a group points to its leader. when you fuse two groups, you have to update a bunch of the objects' leaders. we'd only have one leader for the one new group. the simple, but totally crucial, optimization that we discussed was when two groups merge you update the leader pointers of all objects in the smaller group to point to the leader of the bigger group. that is the new fused group inherits the leader from the bigger of its constituent parts. if we do that optimization, it's still the case that a single union might take linear time fade event time but a sequence of n unions takes only big o of n log in time. and that's because each object endures at most a logarithmic number of leader updates, cause every time it's leader pointer gets updated the population of the group that it inhabits doubles. so in this sequence of videos we are going to discuss a different approach to implementing the union find data structure. and let's just go all in and try to get away with updating only a single pointer in each union. all right, well how are we going to do that? well, i think if we look at a picture, it'll be clear what the approach is going to be. so let's look at a simple example. there's just six objects in the world and currently they're in two groups. one, two, and three are in one group with leader one. four, five, and six are in the second group with leader four. so, with our previous implementation of union find and the two groups have equal size, so we pick one arbitrarily to represent the new leader. let's say four is going to be the new leader. and now we update objects one, two, and three so that they point directly to their new leader, four. and so the new idea is very simple let's just sort of as a short hand for re-wiring all of one, two and three to point to four, let's just re-wire one pointer to point to four and then it's understood that two and three as descendants of one also now have the new leader four as well. so as a result, we do again get a directed tree afterwards, but we don't get one with just depth of one. we don't get as shallow, pushy a tree. we get a deeper one that has two levels below the root. so, let me give you another way of thinking about this in terms of an array representation. and here i also want to make a point that why conceptually it's going to be very useful to think of these union find data structure in terms of these directed trees. you're actually implementing this data structure this is not how you'd do it. you wouldn't bother with actually pointers between the different objects. you'd just have a simply array indexed by the nodes. and in the entry corresponding to a node i you would just store the name of i's parent. for instance if we reframe this exact same example in an array representation before we do the union what do we got? well objects one, two, and three all have parent one. objects four, five, and six all have parent four. so in the array we'd have a one. in the first three entries and a four in the second three entries. so in the old solution we have to update the parent pointers of objects one two and three, they're all going to point directly to four, so that gives us an array entirely of fours. whereas in the new solution the only node whose parent pointer gets updated is that of object one, it gets updated from itself to four. and in a particular objects two and three still point to object one not directly to object four. so that's how union works so in simple example. how does it work in general? well, in general, you're given two objects, each belongs to some group. you can think of these two groups conceptually as directed trees. if you follow all the parent corners, all parent corners lead to the root vertex. we identify those root vertices with the leaders of these two groups. now when you have to merge the two trees together, how do you do it? well, you pick one of the groups and you look at its roots currently it points to itself. you can change its parent pointer to point to the other groups leader. that is you install one of the two roots as a new child of the other trees root. so i hope the pros and cons of this alternative approach to the union find data structure are intuitively clear. the win, the pro, comes in the union operation, which is now very simple. now you might be tempted to just say union takes constant time, because all you do is update one pointer, it's actually not so simple, right? because remember you're given two objects, x and y, and in general you're not linking x and y together. they might be somewhere deep in a tree. you're linking roots together. so what is true is that the union operation reduces two two indications of find. you have to find x's root. r sub 1, you have to y's root r sub 2, and then you just do constant time linking either r1 to r 2 or vice versa. now the issue with this lazy union approach is that it's not at all obvious, and in fact it's not going to be true, that find operation takes constant time. remember, previously when we actually did this hard work of updating all these leader pointers every time we had a union it guaranteed that whenever there is a find boom, we just look in the field, we return the leader pointer, we're done. here, parent pointers do not point directly to roots rather you have to traverse in general a sequence of parent pointers from a given object x to go all the way up to the root of the correspondence tree. so because of these trade offs. these pros and cons, it's really not obvious at all whether this lazy union approach to the union find data structure is a good idea. whether it's going to have any pay offs. so that's going to take really some quite subtle analysis which is the main point of the lectures to come. it's also going to take a couple of optimizations and the next video will start with the first one. you might be wondering, okay, so when you do a union and you have to install one root under the other, how do you make that choice? so the right answer is something called union by rank. that's coming up next. 
so let's start thinking about this alternative lazy union based approach to the union-find data structure in some detail. to get this to work as quickly as we want to we're going to need two optimizations. this video will introduce the first of those optimizations, union by rank. so let's have a quick slide making precise what we learned in the previous video. so in our new implementation it's still the case that each object has one extra field. you can think of it logically as a pointer to some other object. really it's just recording the name of some other object but the semantics have changed. in our first implementation of union find with eager unions, this field we called it a leader pointer and the invariate was that every object's pointer points directly to the leader of its group. in this lazy union implementation, we are relaxing that variant. so i'm going to change the name. i'm not going to call it a leader pointer. i'm just going to call it a parent pointer. and the new relaxed invariant is that the collection of everybody's parent pointers induces a collection of directed trees. each tree in this collection represents one of the groups in the partition that we're maintaining. each of these trees has a root vertex, that is a vertex which points back to itself. it is these root vertices, these root objects that we think of as the leaders of these components. so as before, we're going to refer to a group by the name of its leader vertex and that leader is, by definition, the root of the corresponding directed tree. one way to think about things is that, when we had eager unions, we, again, associated a directed tree with each of the groups, but we also insisted that it was super shallow, bushy tree, that it had depth only one. that is, all you had was a root, the leader, and then everybody else who pointed directly to the leader. in this lazy union implementation we're allowing the trees to grow deeper. we don't insist that they have only one level below the root. so as far as the initialization at birth in a union defined data structure just consists of n degenerate trees each object initially points to itself and is a root at the beginning. in general how do you implement find from some object? well you just have to return the leader of its group and we know you can get to the leader just by traversing parent pointers from x until you get to a root until you get to some object that points back to itself. that's what you return at the end of the find call. how do you do a union? well given two objects x and y you have to find their respective roots so you just invoke the find operation twice once for x to get its root s1 once from y to get its root as two. and then you install either s1 or s2 as a child of the other. so you do one pointer update after the two finds. so for now i'm going to leave union under determined. i'm not going to give you a rule to decide which of s1 and s2 becomes the child of the other. in the next quiz, i want you to think through what would be the worst case running time of these operations if we make that choice arbitrarily, if we just pick s1 s2 arbitrarily to become the child of the other. all right so the correct answer unfortunately is the last one, both find in union can actually blow up to linear time operations if you're not careful about how you implement the union. so why is this true? well first just let me point out that b is definitely not the correct answer. whatever finding union's worst case running time is, it's definitely the same for both of them. remember, union reduces to calls to fine plus constant work. so it's going to tell you that they're both going to have the same operation time in the worst case. now, why is it linear? the reason you're stuck with linear time is because the trees that you build through a sequence of unions can in the worst case become very scraggly. and to see what i mean imagine you just do a sequence of unions and at each time you're unioning a group with just one object into the group that you built up so far. so if you union together a group of size two and group of size one, we'll have you choose arbitrarily. maybe you make the group of size one the new root. so now you just got a chain of the three objects. maybe you union in another singleton group, and arbitrarily you choose the singleton group to be the new leader. that gives you a chain of four objects and so on. so if you do a linear number of unions, and you're not careful about who you make a child of the other, you might well end up with a chain of linear length. and then if you fines for the objects towards the bottom of that chain, they're going to require linear work. and again, union has a subroutine of fines so its going to then take linear work as well. so when you see this example, i hope your immediate reaction is well jeez, we can certainly do better than that. we can just be somewhat intelligent when we do a union which of the old roots do we install as a child of the other one? there's a rough analogy with how we made a natural optimization with our eager union implementation of union find. there we had to make some kind of choice between which of the two old leaders do we retain? we retain the one for the bigger group to minimize the number of leader updates. so here, the natural thing to do is well look at what you did, two trees is already deeper, and we want to keep the root of that deeper tree. that's to avoid it from getting deeper still. so we install the root of the shallower tree as a child of the deeper one. let's make that precise on the next slide with the notion of the rank of an object. so the rank is just going to be a second field along with a parent pointer that we maintain for each object. it's just going to be some integer. let me tell you how i want you to think about the ranks at least for now. okay, so i'm going to give you some semantics, very natural semantics. but i want to warn you, they're only going to hold valid for the next couple of videos. when we introduce something else called path compression these semantics are going to break down. but initially this is how i want you to think about ranks. it's going to be the maximum number of hops required to get from a leaf of x's tree up to x itself. so for example if x is a root, this is going to be the worst case link, the maximum link of any leaf to root path in the tree. so just to make sure that this is clear let me draw a quick example. so here in blue i've shown you one particular union fine data structure that has two groups, now it's very easy to compute the rank of leafs, that's zero, because the only path from a leaf to that node is the empty path. there are two nodes that have rank 1. then the root of the bigger tree has rank 2. and if you think about it, in general, the rank of a node is going to be 1 plus the largest rank of any of its children. for example, if you're in node x, you have a child y and their sum path from a leaf z up to y that requires five pointed traversals while going from z all the way up to u and x is going to require six pointed traversals. and of course, when you initialize the data structure, you want to set everybody's rank to zero, because everybody's just in a tree, by themselves, at the beginning. this notion of rank is going to be a super crucial concept throughout this entire sequence of lectures on union fine so make sure you understand this definition. i encourage you to draw out some of your own examples, do some computations to get a better feel for the concept. for the moment we're just going to use rank to avoid creating scraggly trees. so let's go back to the bad example on the quiz. let's remember what the intuition was. we wound up with this long chain and therefore really bad worse case running time for our operations because we were stupidly taking a tree that was already pretty deep and installing it as a child under a single note under a super shallow tree. thereby producing a tree which is even deeper. so the obvious way to avoid that is when faced with merging together a deep tree and a shallow tree, you install the shallow tree as a child under the deep one. that's going to slow down this deepening process as much as possible. so we can make that precise just to using this rank notion. notice that the rank is exactly quantifying the depth of a tree. so the rank of the root of a tree by this invariant is equal to the depth of the tree. so if you want to make the shallower tree the child of the other one, that means you want to make the smaller rank tree the child of the bigger rank tree. this optimization is what is meant by union by rank. this still remains the case where the two roots have equal rank, that is where the two trees that were fusing have exactly the same maximum path length. and in that case we do just arbitrarily choose one of them. so in the pseudocode i've written here i've arbitrarily chosen y's root to be the one that remains the root, x's root s1 is going to be installed as a child of s2. but again, you can do it either way it's not going to matter. now we are not off the hook yet. remember this is a data structure where we're intending for a invariant to hold these semantics for the ranks that's quantifying worst case path link to the node and we made a modification to the data structure. we rewired somebody's parent pointer. so now it's our responsibility to check does the invariant still hold and if doesn't still hold, then we have to restore it hopefully using minimal extra work. so in this quiz we're going to think through how ranks change when we do a unions. remember the intonation, we're in the middle of a union of the objects x and y. s1 and s2 refer the ranks that is the leaders of the groups of the trees that contain x and y respectively. the first thing i want you to think through and convince yourself of is that for objects other than s1 and s2, other than the pair of objects that are involved in the actual point of re-wiring, nobody's rank changes. so make sure you think that through and convince yourself. ranks are invariant, except possibly for the objects s1 and s2. in this quiz, we're going to drill down on how do the ranks of s1 and s2 change given that we took one of them and rewired it's parent pointer to point to the other. okay so the correct answer to this quiz is the fourth one. so remarkably in many cases there's no change at all to anybody's ranks including s1 or s2. if the ranks were different before the union both of the ranks stay the same. the one exception is when you take the union of two ranks which are equal. then whichever one is the new root and in the pseudo code that i showed you s2 is chosen to be the new root. its rank gets incremented, it goes up by one. i think the easiest way to see this is is just with a couple pictures, couple of examples. so let me just draw two directed trees in the corresponding ranks. and in this one s1 has strictly bigger rank. it's rank is 2 so we install s2 as a child under s1. so re-computing the ranks in the fused together tree, we see that again the rank at the route at s1 is 2., same thing as it was before it didn't go up. so, what's the general principle here? well, in general, suppose you have these two trees and suppose the first ones route s1 has a bigger rank, say rank r. what does that mean? so, that means that there exists a path from a leaf to the root s1 that requires a full r hops. and in the second tree by virtue of its rank being less than r most r minus 1. it says that every single path from a leaf to the root s2 of that tree uses only r minus 1 hops. so when you install s2 as a child under s1 the length of every leaf to root path now the root is s1, it's only gone up by 1. you just have to get to s2 and then 1 more hop you make it all the way to s1. so if all those paths took almost r minus 1 before, all the paths all the way to s minus 1 to s1 now, taken most are hops now. so the worst case path link has not gone up. that's exactly what happens in this example so it's true that when we install s2 under s1 we have yet another path that requires two hops that goes through s2. but we had one previously so the rank doesn't go up. the worst case path length is exactly the same as before. and this same kind of reasoning explains why the rank has to get bumped up by one when you fuse together two trees that had the same rank. so suppose s1 and s2 both have rank r, well then both of them have a leaf to root path that requires a full r hops. and now when you install s1 as a child under s2 that path where you needed r hops to get from the leaf to s1, now to get all the way up to s2 you need one more hop. so now suddenly there exists a path from a leaf to the new root s2 that requires a full r plus 1 hops that's why the ranks gotta go up. 
in this video we'll provide for the first time concrete evidence that the lazy union approach to the union-find data structure is viable. specifically, we'll prove that the worst case running time of both the find and the union operation is logarithmic in n, the number of objects stored in the data structure. we are going to do even better later once, we introduce a second optimization known as path impression. but an important stepping stone is to understand just is why, just union by rank, already gets us to a reasonable algorithmic run-time. so a quick review of the lazy union approach to implementing the union fine data structure. so, with each note we're going to maintain a parent pointer. and it's no longer the case that we insist the parent pointer point directly to the leader of a group. rather we just insist that the collection collection of parent pointers, induces a collection of directed trees. the root of each tree, that is, the node which is it's own parent, we're going to define as the leader of that group. so, given any old object, x, how do you implement find? how do you, figure out what the leader vertex is? well, you just traverse parent pointers, up until you get to the root of that particular group. so for this implementation of the find operation, the worst case running time is just going to be the longest path of parent pointers that you ever have to traverse, to get from an object to some root. so the way we're going to quantify that is using these ranks. so this is again, a field that we maintain for each object. and for now, this will break down later, but for now before we have path -pression, we're going to maintain the invariant that the rank of an object x is exactly the largest number of pointers you have to traverse, from some leaf, to get to x. as a consequence, the biggest rank of any object is the longest path from any leaf to any root. and that's going to be an upper bound on the worst case running time of the find op- operation. so let's move on to the union operation. so here, given 2 objects, x and y, you need to fuse their 2 trees, their 2 groups, so you find the roots of the 2 trees, so you call a find on x, you call a find on y. that gives you their 2 respective roots, and now you install 1 as a new child. of the other. now we saw in a quiz in the last video, if you're not careful about which root you install as a child under the other, you can wind up with these long chains. and be stuck with a linear worse case time for both find and union. so instead we have this union by rank optimization. which says, well, we want to keep our trees from getting scraggly. and the way we're going to do that is. when we have a shallow tree and a deep tree we make the shallow tree shall under the root of the deep one, that prevents the tree from getting even deeper. now there is a situation where the two trees have exactly the same depths that is where the two roots have exactly the same rank, in that case we just proceed arbitrarily. then when we merge two trees that both had a common rank r, its important that in the new tree, the rank is gone up to r+1. so we need the update, we need the incremental rank of the new root to reflect that increase. so that's where we've already been. where are we going to next? well the plan for this video is to show that, with the union by rank, optimization. the maximum rank of any node, is always, bounded above, by log, base 2 (n). where n is the number of objects in the data structure. now we just said, the worst case running time of find, is governed, by the maximum rank. so the logarithmic maximum rank means logarithm run time of find. that also carries over to the union operation. remember union is just 2 finds plus constant work to rewire 1 pointer, so that's going to give us algorithm time value on both operations. so let's see why that's true. so let's begin the analyses with a few simple, but useful properties that follow immediately from our invariant. from the way that we change the ranks of objects as we do finds and as we do unions. so the first simple property is focus on your favorite object. x. and, watch this objects rank change, over the course of the data structure, as we do finds and unions. how can it change? well, when we do a find we don't change anything, all the ranks stay the same. when we do a union, all the ranks stay the same. well, except there is 1 case in the union, where the rank, of a single node, gets bumped up by 1, gets increased. so ranks only go up, over time, for all of the objects, that's property one. so the second property is again pretty much trivial, but really, really useful. so what is the situation in which somebody's rank gets bumped up by 1? we're going to take the union of two trees that have a common rank. and then whichever of the two roots that we pick to be the root of the new bigger tree that's the object whose rank gets bumped up by 1. so new roots of this fused tree. so in particular, the only type of objects that can ever get a rank increase is a root. if you're not a root, your rank will not go up. furthermore, once you're not a root in this data structure, you will never be a root again in the future. there is no process by which, you shed your parent. once you have a parent other than yourself, you will always have exactly that parent. putting those two observations together we find that, as soon as an object x. becomes a non root but as soon as it has a in parent other than itself it rank is frozen for the rest of time forever more. the third and final simple property follows from a formula we mentioned in the last video about computing ranks. so remember the rank of a node in general is going to be one more than the maximum rank of any of its children. so if you have a child and there is some path from a leaf to that child, it takes 5 hops. the path to you from that child is going to take 6 hops. as a consequence as you go from the leaf up to the root you will see a strictly increasing sequence of ranks. the rank of a parent is always strictly more than the rank of all of those children. so that's it for the immediate properties. let's go to a property which is a little less immediate. but still this next lemma, which i'm going to call the rank lemma, it's the best kind of lemma. so on the one hand, it's just not that hard to prove. i'll give you a full proof in the following 2 slides. on the other hand, it's really powerful. it's going to play a crucial role in the analysis were doing right now. a logarithmic run time bound, with a union by rank optimization, and we'll keep using it again as a workforce, once we introduce path compression, and prove better bounds on the operations. so what's the rank limit say? well it controls the population size of objects, that have a given (no period) rank, so we want it to apply at all intermediate stages of our data structure, so we're going to consider an arbitrary sequence of unions. you can throw in some finds as well. i don't care. finds don't change the data structure, so they're totally irrelevant, so think about a sequence of unions, a sequence of mergers. the claim is, for every non-negative integer, r. the number of objects that have rank exactly r at this time is at must n. the total number of objects divided by 2 to the r. so for example, if our rank is 0. it says that at must n objects have rank 0, so it is a trivial statement because only n objects. but at any given time the number of objects that have rank 1 is at most n over 2, the number of objects that have rank 2 is at most no over 4 and so on. and if you think about it, if we succeed in proving the rank lemma, we're pretty much done, showing the efficacy of the union by rank optimization. so in particular, once you take r, the, in this, key lemma, to be log base 2 (n), it says that there's at most 1 object. that has rank log2(n). and there can't be any objects that have rank strictly larger. that is, this limit implies that the maximum rank at all times is bounded above by log2(n). and remember, the maximum rank is the longest path of pointers, traversals, you ever need to get from a leaf to a root. and that means the most amount of work we'll ever do in a find, and therefore, in a union is o (log n) okay, so i've now teased you with the consequences of the rank lemma, assuming that it's true, but why is it true? let's turn to the proof. i'm going to break the proof down into two claims, claim one and claim two. we'll see that the two claims easily imply the rank rank lemma. so claim 1 asks you to consider 2 objects, x and y, that have exactly the same rank r. and the claim asserts that the sub-trees of these 2 objects have to be disjoint. they have no objects in common. and here by the sub-tree of an object, i just mean the other objects that can reach this one by following a sequence. of, parent pointers. so that is the subtree at x, is the objects from which you can reach x. the subtree at y is the objects from which you can reach y. the second claim, is that, if you look at any object that has rank r, and you look at it's subtree, that is, if you look at the number of objects that can reach, this object x by following pointers, there have to be a lot of them. there have to be at least 2 raised to that objects rank are, objects in it's subtree? notice that, if we prove claim 1 and claim 2, then the rank lemma, follows easily. why? well, fix a value for, r. 2, 10, i don't care what. look at all the nodes that have this rank r. by claim 2, each of them has at least 2 to the r objects that could reach them. and by claim 1, these have to be disjoint sets of objects. well, there's only n objects to go around, and if each of these disjoint sets has at least 2 to the r of them, there are going to be at most n over 2 to the r such groups. that is at most n over 2 to the r nodes, objects with this rank r. so we've reduced the proof of the rank lemma to proving claims 1 and 2, i will do them in turn. so for claim 1 let me go via the contra positive, that is, i will assume that the conclusion is false, and i will show that the hypothesis must then also be false. so, lets assume that we have 2 no, objects x and y, and their subtrees are not, disjoint. that is, there exists an object z, from which you can reach x and from the same object z, you can also reach y by a sequence parent pointers. well now let's use the fact that we're dealing with the directed tree, right, so if you start with an object z. there's only a unique parent point, or 2, follow each time. so that is, all of the objects reachable from z, they form a directed path, leading up to the root of z's group. so the only way for both x and y to be reachable from z, they have to both be on this path. if they're both on this path, then 1 has to be an ancestor of the other. so now we're going to use the third of our simple properties that we observed. that is, on every path to the root, ranks strictly go up, each time. so, whichever of x or y is an ancestor of the other, that has strictly higher rank. therefore x and y do not have the same rank. that completes the proof, of claim 1. so lets move on to claim 2. remember, claim 2 is search that, an object of rank r, necessarily has 2 ^ r objects, or more, in its subtree. that's how many objects can actually reach this object x, by following parent pointers. so for this proof we're going to proceed by induction on the number of operations, and again remember fine operations have no effect on the data structure, so we can ignore them. so it's just by induction on the number of union operations that happen. so for the base case, when, before we've done any unions whatsoever, we're doing just fine. every object has a rank of 0 and the sub-tree size of every object is equal to 1. that object itself, also known as 2 to the 0. zero. now for the inductive step, there's an easy case and a hard case. the easy case is where nobody's rank changes, where we do a union, and everybody's rank stays exactly the same. in this case, we're golden. why? well, when you do a union, sub-tree sizes only go up. there's only more. pointers so there's only more objects that can reach any given other objects. so sub-tree sizes go up, ranks stay the same. if we had this inequality of sub-tree size as being at least 2 ^ r before, we have it equally well now. now. so the interesting case is when somebody's ranked actually changes. how can that happen? well it happens in only one particular way that we understand well. looking at a union operation between objects x and y. suppose the roots of these objects are s1 and s2 respectively. it's only when these. two roots have the same rank, let's call that common rank r, that somebodies rank gets changed. in particular, we're going to break ties as we did in the previous video. s2 will be the root of the fused tree, s1 will become a child of it. and, in that case, s2s rank gets bumped up by 1. it goes from r to r + 1. now notice, in this case, we do have something to prove. what are we trying to establish? we're trying to establish that every subtree size is big, as a function of the rank. so, s2's rank has gone up, and therefore the lowerbound, the bar that we have to meet, for the subtree size, has also gone up, it's doubled. so in this case we actually have to scrutinize s2's new sub-tree. so what is its new sub-tree? well, it's really just composed from its old sub-tree, and it inherits s1 and all of its sub-trees. well, in that case, we know that s2's new subtree size, the nubmer of no objects that can reach it, is just, it's old subtree size, plus the old subtree size, of s1. but then w're in good shape because we have the inductive hypothesis to rescue us. so remember, before this union, by the inductive hypothesis, for every object with a given rank, say r, it had at least 2^r objects in its sub tree. so s1, and s2, both had rank r before this. unions, before this union, both of their subtree were at least two to the r. so as two subtree sizes bounded below by two to the r plus two to the r, a quantity also known as two raised to the r plus one. quite conveniently, r plus one is s two's new rank, so s two's new bigger rank, its subtree size is still meeting the lower bound, meeting the target of two raised to, new rank, 2 ^ r + 1. so that completes the inductive step, therefore it completes the proof of claim 2, that objects of rank r have subtree sizes at least 2 ^ r. therefore completes the proof of the rank lemma, that for every rank r, there's at most n / 2 ^ r nodes of rank r. and remember the rank lemma, implies that the maximum rank, at all times, is bounded by log base 2 (n), as long as you're using union by rank. and that implies, that with this first optimization, the worst case running time of union, and find, are both o (log n), where n is the number of objects, in the data structure. 
so i don't blame you if you're feeling a little bit restless. you've now watched three videos about this alternative approach to the union find data structure based on lazy unions, and frankly we don't have many changeable things to show for it. we already had a perfectly fine implementation of this data structure based on eager unions that gave us constant time fines, and yeah, union could be linear in the worst case, but it was logarithmic on average over sequence of unions. so now we have this other implementation. both of our operations are requiring logarithmic time. it's true it's a worst case bound, and it's true union by rank is pretty cool. but still, the bottom line up to this point is not that compelling. but i've still got another trick up my sleeve. now just watch what happens once we employ a second optimization known as path compression. so the optimization is very natural. i suspect many a serious programmer would come up with this on their own if they were tasked with implementing union find with union by rank. so to motivate this optimization let's think about what would be our worst nightmare as someone maintaining such a data structure and hoping for good performance. well, remember the running time of a find is proportional to the number of parent pointers you have to traverse. that is disproportional to the depth of the object at which it's invoked. so, what's the worst case find look like? well it's going to be the leaf. and moreover, it's going to be a leaf, which is furthest away from its corresponding roots. now, on the one hand we're using union by rank, so we know that this depth can't be too bad. it's going to be at most big 0 of log n. however, there will be example there will, in general, be leafs that are theta of log n hops away from their root. and, for all we know, we're just going to get this endless sequence of find operations, where every single one is invoked on a leaf that's a log n number of hops away from from its root. so for example, in the pink tree that is shown in the upper right of the slide maybe someone keeps searching for the object, keeps invoking find from the object one over and over and over again. and then we're going to be suffering a log number of steps with every single operation. but if you think about it, it's totally pointless to keep re-doing the work of previous finds. to keep retraversing the same parent pointers over and over again. so for example, in the pink tree that i've shown in the upper right, imagine that find is invoked from object one. so then we traverse the three parent pointers. we discover the ancestors four and six before terminating at seven. and we discover that seven is the leader or the root corresponding to the object one. now remember, we do not care that four and six are ancestors of one, that is uninteresting. we only visited them to discover what we actually cared about, that seven is the leader, the root corresponding to one. well, but let's just cache that information now that we've computed it. so, this is now basically reconstructing the eager union find implementation. let's just rewire one's parent corner to point straight to seven. we don't need to recompute foreign six in some later find operation. and more generally, after we've invoked find from object one, we may as well rewire four's parent pointer as well, to point directly to its leader, seven. so that then is path compression. when find is invoked from some node, x, and you traverse parent pointers from x up to its root, call it r for every object that you visit along this path from x to r, you rewire the parent corner to point directly to r. so r doesnt have it's parent pointer changed, it still points to itself. the penultimate object on this path doesn't have its parent pointer changed. it already was pointing to the root r, but anything below the root and its immediate descendents on this path from x will have its parent point updated and it'll be updated to point directly to r. now we couldn't get away with this if the tree had to be binary. but it doesn't have to be binary. we couldn't get away with this if we had to satisfy some other constraint like a search tree property, but we don't. so nothing's stopping us from just caching this information about who everyone's leaders are. because that's really the only information we're responsible for exporting from this union find data structure. so pictorially, if you like thinking about the trees, in effect, what path compression does is make the trees shallower and more bushy. so in our example in the upper right, it rips out one and four and makes them immediate descendants of the root seven. prefer to think in terms of the array representation and remember in the array representation, the array index for i is recording the parent of the object i. so, in our pink tree, five, six, and seven all point to seven. they all have parent seven. whereas, two and three all have parent five. four has the parent six and one has the parent four. and after applying path compression following a find invoked at the object one, one and four are redirected to have parent seven. so what are the pros and cons of the path compression optimization? well, the cons side is quite minor. really we're just doing a little multi-tasking on find and that introduces a small constant factor overhead. we're already doing work proportional to the number of hops. on the path from x to it's root and we are still just doing constant work for each node on that path. we're doing an extra constant work for each node after the fact to rewire the parent pointer to point to the root. the pro should be obvious this is going to speed up all subsequent finds operations. you are really making sure you don't redo redundant work. you don't traverse parent pointers over and over and over again. so what's clear is that finds will speed up. what's really not clear is whether they'll speed up by a lot. so this one going to affect the performance of the find operation by say a factor of two, where we get something fundamentally better than the logger than performance we were stuck with without path compression. so, let me spend a slide on the subtle point of how ranks interact with the path compression optimization. so, the plan is we're going to manipulate rank fields with path compression in exactly the same way as we did when we didn't have path compression. so how did we manipulate them previously? well at the birth of the union find data structure everybody has rank zero. ranks never change except possibly when you do a union. when you do a union you look at the roots of the two objects whose groups you have to union. you look at their ranks. if one of them has strictly bigger rank that becomes the root of your new merged tree. the root with the smaller rank is installed as a child underneath. if you union two roots that have exactly the same rank, you pick arbitrarily which of them is the new root, and you bump up its rank by one. that is how we manipulated ranks before, that is exactly how we're going to manipulate them now. so in particular, when we apply path compression, when we rewire parent pointers following a find, we do not touch anybody's ranks. we leave them all exactly the same. so again, ranks only change in one specific case in a very specific way. namely, during a union when we merge two trees that have exactly the same rank. and when we do that kind of union, that kind of merge, whichever of the old roots winds up being the new root, we bump up its rank by one. that is the only modification we ever make to any ranks. now, it might bother you that we don't recompute the ranks when we apply path compression. and in a way i sort of hope it does bother you because by not recomputing ranks, by just manipulating them exactly as before, we're actually losing the semantics of what ranks meant in the previous video, so the key invariant we had that ranks exactly represented worst case search time to that object is now lost. the easiest way to see what i'm talking about probably through an example, so let me redraw the same tree we had on the previous slide, both before and after we apply path compression. following a find invoked at the object one. so what i've done here is, in the top tree, before the path compression's been applied, i've labeled ranks just as they were in the previous video. so for each node in the top tree, its rank is equal to the longest path. from a leaf to that note. then i applied path compression from he object one resulting in the bushier more shallow tree on the bottom. and as i said i did not touch anybody's ranks when i applied this path compression. i didn't do any recomplications. and you'll see now we've lost the semantics for the ranks. just to give a simple example there are now leaves that don't have rank zero, they have ranks strictly bigger than zero. so what we can say is that for every node the rank is an upper bound. not necessarily exactly the same as, but at least as big as the longest path from a leaf to that node. but because of the shortcuts that we've installed we no longer have the equality that we had before. there is good news however. we can definitely salvage a lot of the technology that we developed in last video for the union by rank analysis and apply it fruitfully here to the case with path compression. in particular, any statement that we made last video that talks only about the ranks of objects and not about their parent pointers per se that will be as true with path compression as without. why? well, think about making two copies of a union find data structure. exactly the same set of objects and they'll run them in parallel on exactly the same sequence of union and find operations. so, by definition, we manipulate the ranks of objects in exactly the same way in the two copies. it doesn't matter if there's path compression or not. so at all moments in time, every object will have exactly the same rank in one copy as it does in the other copy. path compression or no, it doesn't matter. exactly the same ranks. what's going to be different in the two copies are the parent pointers. they're, in some sense, going to be further along. they're going to point further up into the tree in the copy with path compression. but no matter. any statement that's purely about ranks, it's going to be equally well true with or without path compression. so in particular, remember the ranking lemma that we proved in the last video. that says there's at most n/2 to the r objects of rank r. that was true without path compression. it's going to be true with path compression. and we're going to use it in the forthcoming analysis. another fact which is still true with path compression, in fact in some sense it's only more true with path compression, is that whenever you traverse a parent corner you will get to a node with strictly larger rank. remember in the last video we said, as you go up toward the root, you're going to see a strictly increasing sequence of ranks. here each pointer with path compression represents a sequence of pointers without path compression. so certainly you will still only be increasing in a rank every time you traverse a pointer. let's now quantify the benefit of path compression by proving new performance guarantees on the operations and the union find data structure. and frankly, the running time bounds are going to blow away the logarithmic bound that we had when we were only doing union by rank without path compression. we're going to do the analysis in two steps, and it's going to be historically accurate in the sense that i'm first going to show you what theorem by hopcroft-ullman, which gives an excellent but not quite optimal bound on the performance of this union find data structure. and then we're going to build on the hopcroft-ullman ideas to prove even better bound on the performance on this data structure. so here is the performance guarantee that hopcroft and ullman proved back in 1973, about 40 years ago. they said, consider a union find data structure implemented as we've been discussing. lazy unions, union by rank, path compression. consider an arbitrary sequence of m, find, and union operations, and suppose the data structure contains n objects. then the total work that the data structure does to process this entire sequence of m operations is m times log star of n. what is log* of n, you ask? well, it's the iterated logarithm operator. so, my analogy, remember when we first demystified the logarithm way back at the beginning of part one, we notice the log base two of a number is well, you just type that number in your calculator, you count the number of times you divide by two until the result drops below one. so, log star, you type the number n into your calculator and you count the number of times you need to hit log before the result drops below one. so it totally boggles the mind just how slowly this log star function grows. in fact, let me give you a quick quiz just to make sure that you appreciate the glacial growth of the log star function. so the correct answer is b. it is five. and you should probably spend a couple seconds just reflecting on how absurd this is. right? so you take this ridiculous number 2 to the 65500. remember the number of atoms in the known universe estimated is way smaller than this. which is 10 to the 80. so, you take this ridiculous number, and you apply the log star function. you get back 5. why do you get back 5? well, take the logarithm once. what do you get back? 65,536, also known as 2 to the 16, so you take log again. you get back 16, also known as two to the four. you take log again. you get back four, also known as two to the two. you take log again. you get back a 2, and then one more time gets you down to 1. so, you can think of log star as the inverse of the tower function, where the tower function takes, think of it as a positive integer, t, and it just raises two to the two to the two to the two to the two t times. and this, in particular, shows that log star, despite its glacial growth, is not bounded by a constant. this function is not. big o of one. for any constant c you can think of i can in principle exhibit a large enough n so that log star of my n is bigger than your c. namely i can just set n to be two to the two to the two to the so on c times. log star of that will be at least c. so that's the hobcroft-owen performance guarantee that on average over a sequence of m union of five operations you spend this only ridiculously small log star n amount per operation. we're going to prove it in this next video. 
in this video, we're going to prove our first performance guarantee on the union-find data structure with path compression. this is the bound first established by hopcroft and ullman. let me briefly remind you about the guarantee. consider a union-find data structure where you're using lazy unions. you're doing union by rank and you're using the path compression optimization. consider an arbitrary sequence, say of m union and find operations. the guarantee is that over the course of this sequence of operations, the total work that you do is at most m, the number of operations, times the glacially growing function log* of n. remember, log* n is defined as the number of times you have to apply the logarithm function to n before you get a result which is below 1. remember, 2 raised to the 65,536, a totally absurd number, log* of that number is merely five. so the theorem is true no matter what m is, no matter how many operations you do, whether you do very few, whether you do a whole lot of them. i'm going to focus on the interesting case where m is at least asymptotically as big as n, where m is omega of n. i'll let you think through in the privacy of your own home why the arguments that we're about to see imply the theorem no matter what m is. so one final comment about this guarantee before we turn to its proof. notice what the theorem is not saying. the theorem is not saying that every single one of these find and union operations runs in big o of log start and time. and that's because that statement in general, which would be stronger, that statement is false. there are going to be operations in general that take more than log* n time. now on the one hand we know no operations going to be slower than logarithmic time. even when we didn't have path compression, no operation was worse than the log n time. and we're not going to do any worse with path compression. so the worst case time bound is log n but some operations may indeed run that slowly. but over a sequence of m operations, the average amount of work we do on a per operation basis is only log* of n. this is exactly the sort of so called amortized analysis that we did in our very first union-find implementation with eager unions. we agreed that particular unions might take linear time, but over a sequence of unions we spent only logarithmic time. so the same thing is here, with the exception that we're getting a radically better on average running time bound of log* n over a sequence. so before i launch into the proof details, let's just spend one slide talking a little bit about the proof plan. and in particular, what is the intuition behind the performance that we're trying to make precise? that we're trying to encode in a mathematical way in the proof that's about to follow? well if we're hoping to prove a bound better than log n, what we were stuck with without path compression it better be the case that installing all these short cuts really qualitatively speeds up finds in unions. and on some level it's sort of clear that things have to be sped up. we're replacing an old chain of pointers with a single pointer so you can only go faster. but how do we keep track of this progress? how do we compile this intuition and make it into a rigorous guarantee. so here's the idea. let's focus on an object x which is no longer a root. at this point it's parent is somebody other than itself. and one thing i want you to remember from our union by rank analysis is that once an object is not a root, its rank is frozen forevermore. now we proved that in the context when there was no path compression. but again remember we manipulate ranks in exactly the same way when there is path compression. so that's still true. if you're an object and you're no longer a root there is no way you're rank will ever change again. now what we're certainly hoping is true is that finds that originate, at say, at this object x are running fast. not only that, they should be getting faster and faster as time goes on. because we're doing more and more path compression. so here's the big idea. the way we're going to reason about the worst case running time of the find operation, or equivalently, the longest sequence of parent pointers we might have to traverse to get to a root, is we're going to think about the sequence of ranks that we observe as we traverse these parent pointers from an object x up to the root. let me give you an example, so what would be the worst case situation? well the worst case would be if we have a data structure, and let's say that the maximum rank is something like 100, the worst case sequence of ranks, the longest sequence we would ever see, would be if we start a find operation at an object with rank zero, we traverse a parent pointer, we get to its parent, it has rank one. we traverse its parent, it has rank two, then three, then four, and so on all the way up to 100. now remember, ranks have to strictly increase whenever we traverse a parent pointer. as we discussed, that was true with or without path compression. so in the worst case situation, to go from 0 to 100, you have to traverse 100 pointers. so that'd be a bummer. wouldn't it be nice if every time we traversed a parent pointer, the rank increased, not just by one, but by a much bigger number. so for example, if we went from 0 to 10, to 20, to 30, to 40, and so on, that would guarantee that we'd get to the max ranked node 100 in only ten steps. so again, the bottom line is, if we can have a better, larger lower bound on the gap in ranks between objects and its parent, that implies more rapid progress through the possible ranks of the nodes that we can see. and it translates to faster finds, to fewer parent pointer traversals. so with that idea in mind, the big gaps between ranks imply rapid progress, i want to propose as a progress measure for a given non-root object x. the gap between x's rank, which again remember is is frozen forevermore, and the rank of its current parent. so this progress measure is a good one for two reasons. first as we just discussed, if you have a handle on this gap, if you can lower bound it then that gives you an upper bound on the search time. secondly, this gap allow us to quantify the benefit of path compression of installing these shortcuts. specifically, whenever you install a new short cut you rewire an object's parent pointer to point higher up in the tree. its new parent is going to have rank strictly bigger than its old one. that means this gap is only going to get bigger. summarizing path compression improves this progress measure. that is if an object x previously had a parent p and then has its parent pointer rewired to a different node p prime the rank of p prime is bigger than the rank of p. and just to make sure this is crystal clear let's just draw a couple of cartoon examples. so first just in the abstract, if you think about an object x. suppose it has some parent p and suppose the root of the tree is some p prime. so some ancestor strictly further up in the tree. remember, ranks always increase as you go up the tree, as you follow the parent pointers. so that means p's rank is strictly bigger than x. p prime's rank is the important thing, is strictly bigger than p. so when you rewire x's parent pointer to point from p no longer but rather to p prime, it acquires a new parent, and its new parent is an ancestor of its old one, ergo it must have strictly bigger rank. for that reason, the gap between x's rank, which is fixed forevermore, and the rank of its new parent. that gap is bigger than the gap between its rank and the rank of its old parent. you can also see this effect in action with the running seven object example we were using in the last video. so i've shown that example, that tree in pink, both before and after path compression. and, again, before path compression, i just defined the ranks as with union by rank so the rank of each node is equal to the longest path from a leaf to that node, and then of course we don't change the ranks when we apply path compression. and what do we observe? where exactly two of the objects had their parent pointer rewired namely objects one and four had their parent pointer rewired to point directly to seven. and as a consequence the gap in rank between object one and its parent has leaped from merely one, the difference between each rank and the rank of four up to three, the difference between its rank and that of its new parent seven. similarly object four, its rank gap has jumped from one to two. its rank is only less than its old parent six but it's two less than that of its new parent seven. so believe it or not we actually have the two crucial building blocks for the hopcraft and ullman analysis. building block number one is the rank lemma that we discussed a couple of videos ago, the fact that with or without path compression you can't have too many objects with a given rank r, you can only have at most n over 2 to vr. the second building block is the one we just covered, that every time you update a parent pointer under path compression, the gap has to grow between the rank of that object and the rank of its new parent. the rest of the proof is just an optimal exploitation of those two building blocks, let me now show you the details. 
i need to begin by setting up some notation and definitions. the first one is that of rank blocks. so if we're given value of n, the number of objects in the data structure we define the rank blocks as follows. 0 gets its own block as does 1. the next block is 2. 3. 4. the next block starts at 5 obviously and the last entry is going to be 2 raised to the fourth. so 2 raised to the final. the rank of the previous block as known as 16. the nest block starts at 17 and goes up to 2 raised to the last rank of previous blocks. so 2 raised to the 16 also known as 65536, and so until we have enough rank blocks that we capture. n. actually, if you think about it, because ranks can only be log n, we only have to go up to log n, but that actually doesn't affect anything by more than a constant. so let's just go ahead all the way up to so, how many rank blocks are there? well, focus just on the largest member of each of the rank blocks. the largest member of a given rank block is the largest member is 2 raised to the largest member of the previous rank block. so, for the t-th rank block, roughly what you have is this largest entry is 2 to the 2 to the 2. raised t times. how many times you need to do that before you get in? by definition, you need log*(n). so, that's how many rank block star, that's where log*(n) enters the analysis. so, i realized this must be a very inscrutable definition, these weird rank blocks. so, let me tell you how you should think of about them. they're meant to encode an intuition that we had on the previous slide where we said well, you know we should be happy if the rank of an object's parent is a lot bigger than the rank of that object itself. why? well, when we traverse that object's parent pointer, we make a ton of progress through the rank space and you can only make a ton of progress through the rank space a limited number of times. so that means a small number of parent traversals, that means fast fines. so these rank blocks are just a very clever way of defining sufficient progress. when we're going to be happy with a gap between the rank of an object and the rank of its parent. specifically we're happy whenever these 2 ranks lie in different. rank blocks. we're not happy if they lie on the same rank block. so, for example, if you're at an object and it has rank 8, and together with his parents, it has rank 18, then we're happy because 18 is in the rank block after the one that contains 8. on the other hand, if you go from rank 8 to rank 15, then we're not happy, okay? we're going to call that not a big gap between the rank of the object and the object's parent. so let's build on that idea to make another definition. so consider a given snapshot in time. that is, we've done some sequence of finds, we've done some sequence of unions. so, we're going to call some objects at this point in time good and some of them bad. here are the criteria by which you are good. so first of all, if you're the root of your tree, you're going to be good. if you're a direct descendant of the root, that is if your parent is the root of your tree, then you're also good, if not though,if you're deeper in the tree, then you're going to be good only if your parents ranked rank is in a strictly larger ranked block, than your own rank. so, that is in the previous example. if your rank is 8, and your parents is 18, than you're good. if your rank is 8, and your parents is 15 and your parent's not a root, then you're going to be bad. the role of this definition is to split the work that we do into two parts. first of all, we do some work visiting good nodes. second of all, we do some work vising bad nodes. the work we do visiting good nodes would be very easy to bout, no problem. we'll have to do a separate global analysis to bound the total work that we do visiting bad nodes. this dichotomy is exactly the same as something we faced when analyzing kruskal's algorithm when we implemented it using the union 5 data structure with eager unions. here, you recall there were some parts of the work in kruskal's algorithm that were easy to balance, just iteration by iteration. for example every cycle check cost only at constant time but then there is this more complicated type of work, namely all of leader pointer updates that we had that bound globally via separate argument. the exact same thing is going to happen here. good nodes will be able to bound operation by operation whereas the bad nodes will have a global analysis control the total work done over all of the operations. so more precisely, i've set up the definitions so that the work done visiting good nodes is bounded by o(log* n) every single operation. indeed, how many good nodes could you possibly visit during a find operation? say, from some object x. so you start at x, and you traverse these parent pointers going all the way up to the roots. well, there's the roots. there's the descendant of the roots, so that's 2. so let's set those aside. what about the other good nodes that you encounter on your path? well, by definition, when you visit a good node. the rank of its parent is on the bigger rank block than the rank of that node itself. that is, every time you traverse a parent pointer from a good node, you will progress to a subsequent rank block. well, assuming that log star n rank blocks, so you can only progress through subsequent log star n times. so, the total number of good nodes you're going to see is o(log*n). so now, let's go ahead and express the total amount of the work that we do overall to find a union operations, and use two quantities. work on the good nodes, which we now know as just log star in each operation. plus, the visits to the bad nodes, which at the moment, we have no idea how big it is. so now, let's proceed to the global bound on the total amount of work that we performed on bad nodes and this is really the crux of the whole theorem. [sound] so, let's just review the definitions real quick. what is that mean that you're a bad node? so, first of all, you're not the root second of all you're not a direct descendant of the root. that is you have a grandparent. and third, it is not the case that your parents' rank is in a later rank block. is in exactly the same rank block as your own rank. that's what it means that your. so how much work do we spend on bad nodes? well let's analyze it one rank block at a time. so, fix an arbitrary rank block, let's say for some integer k, it's smallest rank is k + 1 and it's biggest rank is 2 to the k. now i told you what our two main building blocks were. first 1 is the rank lemma, i am going to ask you to remember that in a second. but first i want to put to use our other building block, which is that path compression increases the rank gap between objects and their parents. that's what we're going to use right now. specifically, consider a fine operation and a bad object x that it visits. by virtue of being bad x is not the root and is not a direct descendant of the root. so, root is a higher up ancestor than its parent. therefore, x's parent will be changed in the subsequent path compression. it will be rewired to point directly to the root, a strict ancestor of its previous parent. therefore, the rank of its new parent will be strictly bigger than the rank of its previous parent. that's going to keep happening every time that x is visited, y looks bad. it keeps getting new parents, and those new parents keep having ranks strictly bigger than the previous one. well, how many times can this happen before the rank of x's parent has grown so big that it lies in a subsequent rank block? the biggest value in x's rank block and remember x is a nonroot its rank is frozen forever. so it's always stuck in this rank block. once its parents rank gets updated let's say at least 2^k times then the rank has to be so big that it lies. in the next rank block. at that point, x is no longer bad. its parent pointer makes so much progress, it goes to another rank block. now, we're going to call it good. and, of course, once x becomes good in this fashion, it would be good forevermore. it is not a root, it will never be a root again. its rank is frozen forever and its parent's rank can only go up. so, once you're good, once your parent's rank is sufficiently large. it will be sufficiently large for the rest of time. alright, so we're almost there. let's just make sure we don't forget anything that we have done. so, the total work we're bounding in this two ways. so, first of all, we visit log star n good nodes for each operation. so, overall m operations at o(m). log*n for the good nodes, plus there's a number of visits over the m operations to the bad nodes. we're going to bound that wrok globally but we're going to proceed rank block by rank blocks. we're fixing one ranks k+1 up to 2^k. what we shown on the last slide is that for every object x whose final frozen rank lies somewhere in this rank block. the number of times it gets visited while it's bad, the number of times it could be visited before it becomes good forever more is bounded above by 2^k. so, if now you used one of our key building blocks, that path compression increases the ranks of parents, now let's use the other building block, the rank lemma. the rank lemma, if you recall, says, that in any given moment in time and for any possible rank r, the number of objects that currently have rank r cannot be any bigger than n/2^r. so, it's used the rank lemma and apply it to the upper bound, how many nodes could possibly have their final frozen ranks lengths somewhere in this rank block. they have their final frozen ranks somewhere between k+1 and 2^5. so, let's just sum over all of the ranks in the rank blocks, so it's starting at 2k+1 going up to 2^k. by the rank lemma, for a given value of i, we noticed the most n/2^i objects that eventually wind up with rank i and by the usual geometrical sum, this whole thing can be upper bounded by n / 2^k. so, this is now just trying to look like a magical coincidence. of course, we made a number of definitions in the analysis. specifically, we structured the rank blocks so that this magic would happen. specifically, the number of inhabitants of a rank block, n/2^k, times the maximum number of visits to an inhabitant in the rank block wile they're bad, times 2^k, that's actually independent of the rank block. we multiply these two things together, the number of visits per object to the k, the number of objects n/2^k and what do we get? we get n. now, this was only counting the visits to bad objects in a given rank block but there aren't many rank block, remember, there's only log*n of them. so, that means the total amount of work spent visiting the bad nodes, some that over the rank blocks is o(m*log*n). combining the balance of the good nodes and the bad nodes, we get (m+n) log*n. at the beginning, i mentioned that the interesting case is when m is big omega of n, in that case, this bound is just o(m*log*n). essentially, if you have a very sparse set of union operations, you can just apply this analysis to each of the directed trees separately. so, that's it, that's the full story of complete analysis of the brillaint hopcroft only analysis. log star and on average operation time, under calf compression. brilliant as it is, you can do even there. that's the subject of the next couple videos. 
the guarantees states that if you implement a union-find data structure using lazy unions, union by rank, and path impression, then you get a pretty amazing guarantee. do any sequence you want of m union and find operations. the tool that i worked on, that is data structures, is bound and above by times log star of n, where n is the number of objects in the data structure. so on the one hand i hope you're suitably impressed by this result. so the proof of it that we gave in the last video is frankly totally brilliant. and secondly the guarantee itself is excellent, what with log star of n being bounded above by five for any imaginable value of n. on the other hand, i do hope there's sort of a little voice inside you wondering if this log star bound could really be optimal. can we do better maybe by a sharper analysis of the data structure we've been discussing, or perhaps by adding further ingenuity, further optimizations to the data structure? for all we know, you might be able to get away with a linear amount of work. so o of m worked to process an arbitrary sequence of finds and unions. it turns out you can do better than the guarantee. there is a sharper analysis of the exact same data structure. that analysis was given by tarjan, and here is the statement of his guarantee. so the improved bound states that for an arbitrary sequence of m union and find operations, the total work done by this data structure union by rank and path compression, is big o of m times alpha of n. now what, you may ask, is alpha of n? that's a function known as the inverse ackermann function. so what then is the inverse ackermann function? well the short answer is that it's a function that, incredibly, grows still more slowly, in fact, much, much, much more slowly, than the log star function we discussed in the bound. the precise definition is non-trivial, and that is the subject of this video. in the next video, we'll prove this theorem and explain how the inverse ackerman function arises in an optimal analysis of union by rank with path compression. so i'll first define the ackerman function, and then i'll describe its inverse. one thing i should warn you is you will see slight variations on these two definitions out there in the literature. different authors define them in ways convenient for their own purposes. that's also what i'm going to do here, i'm going to take one particular definition convenient for the union find data structure analysis. all of the variants that you'll see out there in the literature exhibit roughly the same ridiculous growth. so the details aren't really important for the analysis of data structures and algorithms. so you can think of the ackermann function as having two arguments, which i'm going to denote by k and r, both of these are integers. k should be at least zero, r should be at least one. the ackermann function is defined recursively. the base case is when k equals zero. for all r, we define a sub zero of r as the successor function, that is it takes its input r and it outputs r plus one. in general, when k is strictly positive, the argument r tells you how many times to apply the operator, the function a k minus 1, starting from the input r. that is, it's the r fold composition of a sub k minus 1. and again applied to the argument r. so in some sense describing the algorithm function isn't that hard right? i didn't really need that much of a slide to actually write down its description, but getting a feel for what on earth this function is, takes some work. so the first thing, maybe this is a sanity check, is note that it is a well-defined function. so, you know, you're all programmers, so you could easily imagine writing a program which took as input, k and r, and at least in principle, given enough time, computed this number. it would be a very simple recursive algorithm with a pseudo-code just following the mathematical definition. so it is some function, given k and r, there is some number that's the result of this definition. now, in the next sequence of quizzes, let's get a feel for exactly how this function behaves. and so let's start on the first quiz by fixing k to be one. and now, viewing the ackermann function with k fixed at one, is a function purely of r. so, what function does a1 of r correspond to. all right, so the correct answer is b, at least a1 is quite simple to understand. all it does is double the argument. so if you give it r, it's going to spit out 2r. so, why is that? well, any one of our by definition is just the function a0 applied to r, r times, a0 by definition is the successor function as one, so if you apply the successor function r times, starting from r you get 2r as a result. so let's now move from a1 to a2. so let's think about exactly the same question, fixing k at 2 and regarding it as a function of r only. what function is it? so answer here is c for every positive integer are a2 of r is equal to r times 2 to the r. and the reasoning is the same as before. so remember a2 of r just means you apply a1 r times. in the last quiz we discovered that a1 was doubling. so if you double r times, it has the effect of multiplying by a 2 to the r factor. so let's take it up a notch yet again. let's bump up k to three and let's think about a sub 3. but let's not yet think about exactly what a sub 3 is as a function of r. let's keep things simple. what is just a sub 3 evaluated when r equals 2? so k equals 3, r equals 2, that's a number. what number is it? okay, so the correct answer is the third one. it's 2048, also known as 2 to the 11th. let's see why, well a sub 3 of 2 that just means we apply a sub 2 twice to 2. now in the last quiz we evaluated not only a sub 2 of 2 but a sub 2 of all integers r. we discovered that it's r times 2 to the r. so that means that it's a simple matter to evaluate this application of a sub 2 twice. first a sub 2 of 2 that's just 2 times 2 to the 2 that's just 8. then a sub 2 of 8 is going to be 8 times 2 to the 8, also known as 2 to the 11 or 2048. so what about in general? how does the function a sub 3 behave as a function of a general parameter r? when we answer this question we're going to see for the first time the ridiculously explosive growth that the ackerman function exhibits and this will just be the tip of the iceberg right? this is just when k equals 3. so by definition a sub 3 of r you start from r and you apply the function a sub 2 r times. so let's remind ourselves what the function a sub 2 is. we computed that exactly. that's r times 2 to the r. so to make out life simple, let's just think about the 2 to the r part. so in the real function a sub 2, you also multiply by r, but let's just think about the 2 to the r part. so imagine you applied that function over and over and over again. what would you get? well now you get a tower of 2's where the height of the tower is the number of times that you apply that function. but if you apply it once, you get 2 to the r, if you apply it twice you're going to get two raised to the 2 to the r, 3 times 2 to 2 to the 2 of the r and so on. so all applications of a sub 2 gives you something that's even bigger than a tower of r2's. so, let's move on to a4, and this is the point in which personally my brain begins to hurt. but let's just push a little bit further so that we appreciate the ridiculous growth of the ackermann function. and as with a3, let's punt for the moment on understanding the dependence for a general r. let's just figure out what a sub 4 of 2 is. so, k equals 4, r equals 2. well, so this by definition, you just take 2 and you apply a sub 3 twice. we computed a sub three of 2 in the previous quiz. we discovered that was 2,048. so that's the result of the first application of a3. and now we find ourselves applying a3 to 2048, so in effect r now is 2048. and remember we concluded the last slide by saying well a sub 3, whatever it is, it's at least as big as a tower of r2's. and here r is 2048. so, this of course now is the land of truly ridiculous numbers that we're dealing with. i encourage you to take a calculator or computer and see how big of a tower of 2's you can compute, and it's not going to be anywhere close to 2,000, i can promise you that. and hey that is just a4 of 2, what about the function a4 for a general value of r? well, of course, in general, a4 of r is going to be r applications of a3 starting at r. now, in the last slide, we argued that a sub 3, this function is bounded below by a tower function. so a sub 3 takes in some input, some integer, and it spits out some tower with height equal to that integer, so what happens when apply that tower function a3 over and over again? now you get what's called an iterated tower function. so when you apply the function a sub 3 for the time the r is going to spit out a number which is bounded below by a tower of height r, tower of 2's, all of them. now we apply a3 a second time, it's output is going to be a tower of 2's, whose height is lowerbounded by a tower of 2's of height r, and so on. every time you apply a sub 3, you're just going to iterate this tower function. you will sometimes see the iterated tower function referred to as a yowzer function. you probably think i'm pulling your leg, but i'm not kidding. you can look it up. so that is the ackerman function and a bunch of examples to appreciate just how ridiculously quickly it is growing. so now let's move on to define its inverse. now that ackerman function has two parameters k and r excuse me. so to define an inverse i'm going to fix one of those. specifically i'm going to fix r equal to 2. so the inverse ackermann function is going to be denoted by alpha. for simplicity, i'll define it only for values of n that are at least 4 and it's defined for giving value of n as the smallest k so that if you apply a sub k to 2, to r equals 2, then the result is at least n. so again it's simple enough to kind of write down a definition like this, but you really have to plug in some concrete numbers to get a feel for what's going on. so lets just write out, you know, what are the values of n that will give you alpha of n equal 1. that will give you alpha of n equal 2, alpha of n equal 3, and so on. okay so for starters, alpha of 4 is going to be equal to 1, right. so if you applied a 0 to 2, that's the successor function so you only get 3, so that's not at least as big as 4. on the other hand, if you apply a sub 1 to 2, that's the doubling function, so if you apply it to 2 you get 4. so a sub 1 does give you a number which is at least as big as n when n is 4. so next i claim that the values of n for which alpha of n equals two are the values 5, 6, 7 and 8. why is that true? well, if you start from 2 and you apply the function a sub 1, the doubling function you only get 4. so, a sub 1 is not sufficient to achieve these values of n's. on the other hand if you apply a sub 2 to 2, remember that's the function which given an r outputs r times 2 to the r, so when you apply it to 2, you get 2 times 2 to the 2, also known as 8. so a sub 2 is sufficient to map 2 to a number at least as big as 8. by the same reasoning, recall that we computed a sub 3 of 2 and we computed that to be 2048. so therefore for all of the bigger values of n starting at 9 and going all the way up to 2048, their alpha value is going to be equal to 3 because that is the smallest value of k such that a sub k of 2 is at least those values of n. and then things start getting a little bit ridiculous. so remember we also lower bounded a sub 4 of 2. we said that's at least a tower of 2's of height 2048. so for any n up to that value, up to a tower of 2's of height 2048, alpha of those n's is going to be equal to 4. this of course implies that the inverse ackermann function value of any imaginable number is at most 4. and i don't know about you, but my brain is already hurting too much to think about which of the values of n, such that alpha is equal to 5. so as a point of contrast, let's do the same experiment for the log star function, so that we get some appreciation about how as glacially growing a log star may be, the inverse ackermann function is growing still qualitatively slower. so the log star function remember is the iterated logarithm, so it's starting from n how many times you need to hit log in your calculator, let's say base two before the result drops below 1. so when is log star n going to be equal to 1? well that's when n is going to be equal to 2. then you hit log once and you drop from 2 to 1 and you're done. if n equals 3 of 4 then you need more than one application of logarithm to drop below 1, but you only need two applications, so log star of n is going to be 2 for n equals 3 and 4. similarly for n anywhere between 5 and 16 log star n is going to be equal to 3. right if we start from 16 that's 2 to the 4, so if you fly a log once you get 4, a second time you get 2, a third time you get 1. by analogy, the values of n for which the log star of n equals 4 are going to be starting at 17 and going up to 2 to the 16, also known as 65536. so let's just go one more step for which n is log star of n equal to 5. well obviously we start at 65537, and we end at 2 raised to the largest number of the previous block, so 2 raised to the 65536. so, these numbers are impressive enough on the right hand side, but there is no imaginable number of importance for which log star is going to be bigger than 5. looking at the left and the right hand sides though, we see that there really is a qualitative difference between the rate of growth of these two functions. with the inverse ackermann function, in fact, growing much, much slower than the log star function. so perhaps the easiest way to see that is to look at the right hand side, and on each of the lines in the right hand side, look at the largest value of n, so that log star is a given number. so for example, in the third line, the largest n such that log star of n equals 3, is a tower of 2s of height three, 2 to the 2, to the 2, also known as 16. similarly on the fourth and fifth lines the largest n to give values of log star equal to 4 and 5 are a tower of 2s of height 4 and two and a tower of 2s of height 5. on the other hand look at the fourth line on the left hand side. so i really when we ask for the largest values of n that have inverse ackermann value 4, we're talking about towers of 2s, in fact of height 2048. so that is the n's which give us alpha of n equal to 4. we would have to write down 2048 lines on the right hand side before we had values of n that were equally big. this indicates how the log star function while growing glacially indeed, as it never the less moving at light speed compared to the inverse ackermann function. 
so, in this video, we're going to prove tarjan's inverse ackermann bound in the performance of the union find data structure with union by rank and path compression. frankly, i hope you're kind of excited. this is one of the crown jewels in the entire history of algorithms and data structures. let me remind you what the bound says. you've got a union-find data structure, we're doing lazy unions, we're doing union by rank, we're doing path compression. and for an arbitrary sequence of m find a union operations. the total amount of work you do over the sequence of operations is bounded above by m times alpha of n, where n is the number of objects in the data structure and alpha is the inverse ackermann function that we defined in the previous video. i want to give a shout out here to dexter kozen's beautiful book the design and analysis of algorithms. it is his analysis that i will be following closely here. so, it's true that merely stating, tarjan's bound is non-trivial. we have this entire video defining the ackermann function and it's inverse so we could make sense of the statement. that said, we're well positioned to approve tarjan's bound. in fact, the template of the proof is going to very much mirror what we already did for the log star analysis by hopcroft and ullman. so in that spirit, let's review what were the two basic workhorses, the two building blocks that drove forward the hopcroft-ullman analysis. so the first building block is the rank lemma, which dates back all the way to the videos pre-path compression. and remember the rank lemma gives us control on how many objects can exist with a given rank. and in particular, that upper bound is decreasing exponentially with the rank, it's n/2^r objects at most with the given rank r. but, of course, we need a second building block to exploit the facts that we're using the path compression optimization. so, how do we quantify the progress made by path compression? well, we argue that every time a node has its parent pointer updated in path compression, it acquires a new parent with strictly larger rank. so, you can think of the hopcroft-ullman analysis as a clever and essentially optimal exploitation of these two building blocks. how did we use them? well, we define the notion of rank blocks and each rank block has size to raise to the size of the previous rank block. why were rank blocks useful? well, they allowed us to measure progress as we followed parent pointers. we defined an object to be good, if following it's parent, catapulted you into a subsequent ranked block. because there's only a log star number of ranked blocks, you can only visit log star good nodes on any given find operation. so, that gave us a per operation bound on the log star for visits to good nodes. and then, for visits to bad nodes, we use the second building block to argue that every time you visit a bad node, the rank of its parent increases. and that can only happen so many times before the rank of this object's parent is so much bigger than this object's rank itself that the node has to be good. you have to make a lot of progress by following its parent pointer. and the point is, if we want to do better than the hopcroft-ullman analysis, we're not going to do better by taking these same two building blocks and exploiting them in a better way, that can't be done. so, what we need to do is have a better version of one of the two building blocks. so, the key idea in tarjan's analysis is to have a stronger version of the second building block. we're going to argue that path compression doesn't merely increase the rank of nodes parents by one, but in fact, it increases typically the rank of an object's parents by a lot. and what's kind of amazing is the link of the proof we're about to do is really basically the same as that in the hopcroft-ullman analysis. and the steps match up almost perfectly. so, the bound is even better, the argument is even more ingenious. it's even harder to imagine how one would come up with this idea oneself. but, as far as checking it, as far as understanding it, the complexity level is roughly the same as what we've already done in the log star analysis, so here we go. so, in this slide, i'm going to give you a definition. and the point of the definition is to measure how much progress we make in terms of ranks when we follow a node's parent pointer. so, the definition here is going to play exactly the same role that the definition of rank blocks played in the hopcroft-ullman analysis. so, what i'm going to define is a number, a statistic measuring the difference, the gap, between the rank of an object and the rank of its parent. so, this definition is only going to make sense for non-root objects x. one thing i want you to remember from previous videos is that once an object is not a root, it will never again be a root, and it's rank will never again change. so, non-root objects have rank fixed forevermore. so, we're going to use the notation delta of x for the statistic. and the definition of delta of x is the largest value of k such that a sub k, this is ackermann function remember, such that a sub k applied to the rank of this object x, is bounded above by the rank of x's parent. so, the bigger the gap between the rank of x's parent and x's rank, the bigger the value of delta of x. so, let me mention, talk through some simple examples to make sure this makes sense, and also review the ackermann function a little bit. so, first of all, let me just point out the delta of x is always non-negative for any non-root object x. so, why is this true? well remember, and this goes all the way back to our union by rank discussion, the rank of an object's parent is always at least one more than the rank of that object. and the function a sub 0, recall we defined as the successor function. so, for every object that's not a root, it's always true that it's parent has rank at least one more than it. and that means we can always at least take k to be zero and have the inequality be satisfied. now, when is it going to be true that an object x has a delta value at least equal to one? well, that is equivalent to stating we can take k to be 1 in this inequality, and the inequality holds. so, let's remember what is the function a sub one. in the previous video, we discovered that was just the doubling function. so, we can take k=1 and have this inequality satisfied if and only if the rank of the node's parent is at least double the rank of that object. similarly, an object x has a delta value equal to at least 2 if and only if we can take k=2 on this right-hand side of the definition and have this inequality be satisfied. so, let's recall what the function a sub 2 is. a sub 2 of r is just r*2^r. so, an object x has delta value equal to 2 if and only if its parent's rank is substantially bigger than its own rank, in the sense that if it has rank r, its parent's ranks has to be at least r*2^r. and in general, the bigger the value of delta, the bigger the gap between the rank of the object x and the rank of its parent. and, of course, because the ackermann function grows so ridiculously quickly, the gap between this object's rank and its parent's rank is growing massively as you take delta to be larger and larger. so, now that we understand this better, one thing that should be clear is that the delta value of an object x can only go up over time, right? so remember, we're talking only about non-root objects x, so the x's rank itself, is frozen forevermore. the only thing that's changing is it's acquiring potentially new parents, through path compression, and each new parents rank is bigger than the previous one. so, the gap in the ranks is only going up, and therefore the delta value of this object x can only go up, over time. so one final comment, you recall on the hopcroft-ullman analysis, the way we defined rank blocks ensured that there weren't too many of them, there were only log star. similarly here, the way we're defining the statistic delta, guarantees that there are not too many distinct delta values that objects can take on. so, we've already argued that delta is a, integer that's at least zero, and we can also bound it from above. at least, for any object x that has a rank of at least 2, which will be good enough for our purposes. if an object x has rank at least 2, its value of delta can not be any larger than the inverse acromyn of n, than alpha of n. and this is really just by the way we define the inverse ackermann function alpha, we defined it as the smallest value of k such that if you apply a sub k to 2, that catapult to beyond end. and since ranks of objects are bounded above by n, actually they're even bounded above by log n but in particular by n that means you will never see a delta value bigger that alpha of n for any object x has the rank at least two. so, that completes the definition of the statistic data of x. and, once again, the point of this statistic is to quantify progress through rank space as we traverse a parent pointer, from an object. so, in the hopcroft-ullman analysis, we used rank blocks for that purpose. here, we're using the statistic delta of x. and now, we have to redefine good and bad objects to reflect this different measurement for progress, for gaps between ranks of objects, and ranks of their parents. so, that's what i'm going to provide for you on this slide, the definition of good and bad nodes. and, this definition will play exactly the same role as it did in the hopcroft-ullman analysis. for the good nodes, we'll be able to just get a per operation bound for how many visits we make to them. and then, we'll be stuck with, and this will be the main part of the proof, a global analysis arguing that the total number of visits to bad nodes over a sequence of operations cannot be too big. so, the bad objects will be those that meet the following set of four criteria. if an object fails to meet any of these four criteria, it will be called good. so, the first two 2 criteria will be familiar from the hopcroft-ullman analysis. so, first of all, roots get a pass, and direct descendants of roots also get a pass. so, to be bad, it must be the case you're neither a root nor are you the child of a root. so, what's the motivation for these two criteria? well, it's exactly the same as it was in the hopcroft-ullman analysis. it's just to ensure that bad nodes, after they're visited on a find, get their parent pointer updated. so remember, when you do path compression, after you've done a find from an object x all the way up to its corresponding root, everybody's parent pointer gets updated except for the root and except for the child of the root. those two nodes keep the same parent pointer. so, to make sure we have progress, we want to exclude the root and the directed sentence of the root from being bad, we'll acount for them separately. the third criterion is not conceptually important, its just for technical convenience sort of an out of factor the way i've defined the inverse arithmetic function. we're going to give nodes that have rank zero or one also with free paths. so, in order to bad, we're going to worry about the case where the rank is at least 2. so, the final criterion is really the ingenious one and it's the one that ensures that when you do path compression typically objects parents ranks will increase extremely rapidly, not just by 1. and this criterion says, for an object x to be bad, it has to be the case that is has some ancestor y. so, an object you get to by traversing parent pointers from x, that has exactly the same value of delta. so, if an object x has delta of x equal to 2, for it to be bad, there has to be some ancestor object y that also has a delta value equal to 2. and again, if x fails to meet any of these four conditions, then x gets a pass, x is called a good object. now, i said that the role of this definition is going to be to split the work done into two groups and that we're going to be able to easily bound, even on a per operation basis, the number of visits to a good node. so, the next quiz asks you to think about that carefully. alright. so, the correct answer is b. it's theta of the inverse ackermann function of n. and this is really one of the points, of how we define good and bad nodes. we guarantee that there can not be too many good nodes whenever we do a fine operation. so, to see why this is true, let's just count up the number of nodes along a path that can fail to meet each of the four criteria. well, so the first criterion was we give roots a free pass. so there's at most one node, that's good because it's a root. similarly, on a given path there's at most one object on that path, it's a direct descendant of the root. because ranks always strictly go up when you follow parent pointers that can be at most one object of rank zero and at most one object of rank one. so, we, and most two objects get a free pascals of the third criterion. well, such thing about the interesting case nodes that good be because they failed to meet the fourth criterion. how many can that be? well, let's focus on a particular value of delta. so, let's say, think about all of the objects along a path that have delta of x equal to 3. there may well be lots of objects on this path with delta of x equal to 3, may be there's like 17 of them. call the highest to the closest to the root, such object z. of these 17 objects with delta of x equal to 3, z is going be the only one that is good. the other 16 objects with delta of x equal to 3 are going to be bad. why? well, any object other than z has an ancestor, namely z, with exactly the same delta value. so that, those 16 objects do meet the fourth criterion they will be classified as bad. summarizing for each of the alpha of n possible values, of delta, only the upper-most object, the object closest to the root with that particular delta value, can possibly be classified as good. so, that bounds at alpha of n, the number of objects that fail to meet this fourth criterion. that gives us the total bound of the number of good nodes on any path. 
so just like in the hopcroft ullman analysis we're going to think about the total amount of work done by our union fine data structure as constituting 2 pieces, work done visiting good nodes and work done visiting bad nodes. the previous quiz says that we have a bound on visits to good nodes, even on a per operation basis. at most inverse acronym for n, good nodes are visited every single operation. what remains is to have a global bound on the number of visits to bad nodes. the argument will be to show, that over an arbitrary sequence of m find and union operations, the total number of visits to bad nodes is bounded above by big o of n, times, alpha of n. so here is the crux of the argument. here is why, when you do a visit to a bad node, the subsequent path compression massively increases the gap between that object's rank and the rank of the parent. so, let's freeze the data structure at the moment where we found the operation and makes a visit to a bad object, call that bad object x. let's think about what the world with the data structure must look like in this scenario. so we've got our bad object x, it may or may not have nodes pointing to it. we're not going to care. by virtue of it being bad and meeting the third criterion we know the rank of x is at least 2. it's delta value could be anything. whatever it is, let's call it. okay. so because x is not a root, it has some parent, call that parent p. and not only does x have ancestors, but because it meets the fourth criterion, we actually know it has an ancestor y who has the same delta value as x. that is it has an ancestor y with delta of y equal to k. it is possible that p and y are the same object ir they could be different. it's not going to matter in the analysis. so we're calling that the statistic delta is only definied for non roots, we can conclude that y is also not the root. it must then have some parent p prime. p prime might be a root or it might not, we don't care. [sound] so, now let's understand of the effect of the past compression, which is going to happen subsequent to this find operation. x's parent points is going to be rewired to the root of this tree. the root of this tree is either at p prime, or even higher than that. given that fact let's understand how much bigger the rank of x's new parent p primer higher is compared to the rank of it's old parent p. so the rank of x's new parent is at least the rank of p prime. so if p' is in fact the root, then the rank of x's new parent is just the rank of p'. otherwise, this new parnet is even higher than p prime because its ranks only increase going up the tree, that means it would only be higher than the rank of p prime. how does the rank of p prime compare to its child, that of its child y? well and here is a key point, because delta of y is equal to k. remember what the definition of delta is, it quantifies the gap between the rank of an object, and that of its parent. we are going to use that here for y, and its parent p prime. it means the rank of p prime is so big it's at least the function, a sub k applied to y's rank. so our third and final inequality is the same as the first one. so it could be the case that y actually is the same as p, it actually is x's old parent. in that case the rank of x's old parent is precisely the rank of y. otherwise, y is even higher up x's old parent p and therefore, by the monotonicity of rank, the rank of y is even bigger, than the rank of x's old parent. so now in this chain of inequalities, i want you to focus on the left-most quantity, and the right-most quantity. what does this say, at the end of the day? it says when you apply path compression to x, it acquires a new parent. and the rank of that new parent is at least the a sub k function applied to the rank of it's old parent. again, path compression at the very least applies the a sub k function to the rank of x's parent. so now let's suppose you visit some bad object x, not just once, but the same object x, while it's bad over and over and over again. this argument says that every such visit to the object x while it was bad applies the function a sub k to the rank of its parent. so in particular, let's use r to denote the rank of this bad object x and, again, by virtue of it being bad, r has to be at least 2. imagine we do a sequence of r visits to this object x while it's bad. each of those visits will result in it acquiring a new parent. and that new parents rank is much bigger than the rank of the previous parent. it's at least a sub k, applied to the rank of the previous parent. so, after r visits, that's applying the function a sub k, r times to the rank of x's original parent which forces rank at least that of x, at least r. well we have another name for applying the function a sub k, r times to r. remember this is just by definition of the acronym function a sub k plus 1. applied to r. so what does this mean? well this means, that after r visits, to a bad object x, that has rank r, the rank of x's parent has to have grown, so much, that that growth is reflected in our statistic delta, that measures the gap in the rank, between objects, and the objects parent. so remember how we defined delta of x. it's the largest value of k so that the rank of x's parent is even bigger than a sub k applied to the rank of x. so what this inequality shows is that every r parent pointer updates to x allow you to take k to be even bigger than before. you can bump up k by 1 and still have this inequality be satisfied. that is, every r visits to a bad object of rank r, delta has to increase by at least 1. but there's only so many distinct values of the statistic delta of x can take on. it's always non negative, it's always an integer, it can only increase, and it's never bigger than the inverse ackermann function alpha of n. so therefore, the total number of visits to an object x, while it's bad, over an arbitrary sequence, can not be more than r, the number of visits needed to increment delta of x, times the number of distinct values, which is alpha of n, the inverse function. so now we've done all the hard work. we're almost there, we just need 1 final slide putting it all together. all right, so to see how all of this comes together, let's first recall that all that we need to do is bound the total number of visits to bad objects over this entire sequence of union and find operation. so in the previous slide, we showed that for a bad object x, with rank r, the total number of times it's visited while it's bad, is bounded above by r times the inverse ackermann function of n. so lets sum that over all of the objects to get our initial bound on the total number of visits to bad objects. so now, we have to be a little careful here, because there are n objects x. and ranks can be as large as log n. so a naive-bound would give us something like n times logn times alpha of n, and that's not what we want. we really want n times alpha of n. so we need to use the fact that not all big nodes can have big rank. and that's exactly what the rank lemma says. so to rewrite this, in a way that we can apply the rank lemma, bounding a number of objects with a given rank, lets bucket the objects according to their rank. so rather than summing over the objects, we're going to sum over possible ranks r, and then we're going to multiply times the number of objects that have that rank r. while i'm at it, let me pull the alpha of n factor, which doesn't depend on the object out in front of the sum. for every value of r, the rank lemma tells us there are at most n/(2^r) objects with that rank. so this factor of n is independent of the rank r, so let me pull that out in front of the sum. and when the dust settles we get n times the inverse acronym function of n times the sum of non-negative integer r of r divided by 2 to the r. so, we've seen this kind of sum without the r in the numerator, just a geometric sum 1/2^r. we know that's bounded by a constant, and more generally throwing an r in the numerator, well that's no match for this exponentially decreasing denominator. so, this sum still evaluates to, a universal constant. i'll let you check that in the privacy of your own home. and so that's, give us a bound of o(n) * the inverse ackermann function of n, on the total number of visits to bad objects, since we also have a per operation bound of alpha(n) on the number of visits to good nodes. combining the work of the two cases we get o(n) m+n times inverse ackermann function of n. and again, the interesting case here is when m is omega of n. otherwise, you can just do this analysis separately for each 3 in the data structure. and, there you have it. you now understand in full detail one of the most amazing results in the history of algorithms and data structures. so, tarjans bound is unimagenably close to being linear without actually being linear, it's off by this inverse acromon function factor. now, from a practical perspective for any imagenable value of n, alpha of n is almost 4, so this gives you a linear time bound for imaginable values of n. in fact, even the hopcroft-ullman log starbound, logs stars at most 5, for any imaginable value of n. so that also is in, in essence linear time bound for all practical purposes. but theorotically speaking, we have not proven a linear time bound, on the amount of work done by the union find data structures. so the pavlovian response, of any algorithms researcher worth their salt, would be to ask, can we do better? and, we could ask that question in a couple different senses. the first sense would be; well maybe we could have a sharper analysis, of this exact data structure. maybe, union by rank, and path compression is sufficient to gaurantee a linear amount of work. after all, we didn't need to change the data structure, we only needed to change the analysis, to sharpen the log* bound, to an alpha of n bound. so this question, remarkably, tarjan answered in the negative, in his original paper. he showed that, if you use this exact data structure, union by rank and path compression, then they are arbitrarily large sequences of unions and finds of arbitrarily large collections of objects, so that this data structure actually performs asymptotically, m the number of operations times the inverse ackermann function, n the number of objects amount of work. that is keeping the data structure fixed, this analysis cannot be asymptotically improved. this data structure fundamentally has worst case performance, governed by this insane inverse ackermann function. so this is already a mind boggling fact that indeed tarjan in the conclusion of his paper, notes that it's remarkable to have such a simple algorithm with such a complicated running time. but you could also ask the question, could we do better perhaps by improving or changing the data structure. after all, by adding path compression, we got a qualitative improvement in the average operation time. that dropped from log to alpha of n. perhaps there's yet another optimization out there waiting to be discovered that would drop the amount of work per operation over a sequence down to constant time per operation, linear work overall. tarjan, in his paper made the bold conjecture that there is no linear time method, no matter how clever you are. and remarkably, that conjecture has since been proved. it was proved for certain classes of data structures both in tarjan in his original paper and in and a follow-up paper in '79. but the final version, of this conjecture, was proven by friedman and sachs, back in 1989. no matter how clever you are, no matter what union-find data structure you come up with, there will be arbitrarily long sequences of operations, so that the average amount of work you do per operation is, the inverse ackerman function of, the number of objects, in the data structure. pretty unbelivable, so let me just wrap up with a historical comment. so, it's a full disclosure, i wasn't quite alive when this result came out but, in talking to, in reading about it, and talking to senior researchers about, my sense is that it was really sort of a water-shed moment, for the field of data structures and and algorithms. specifically it confirmed the belief of, people working in algorithms and data structures, that the field posessed surprising depth, and beauty. there had, of course, been earlier glimpses of this, we mentioned in the optional, material in part 1, kanuth's analysis of linear probing, back in the early 1960's. but this was really something for the worst case analysis, of algorithms and data structures. so the fact that such a practical and naturally arising problem, in algorithms and data structures, requires, necessarily, the understanding of the ackermann function and it's inverse. a function, mind you, which was first proposed and defined back in 1920s in the service of recursion theory, almost 10 years before tarjan was doing his work on models of computation. it was a real eye opener and it showed that this field is something that will keep many generations of scientists quite busy for many years to come. 
so this set of lectures will be our final application of the greedy algorithm design paradigm. it's going to be to an application in compression. specifically, i'll show you a greedy algorithm for constructing a certain kind of prefix-free binary codes know as huffman codes. so we're going to spend this video just sort of setting the stage. so let's begin by just defining a binary code. so a binary code is just a way to write down symbols from some general alphabet in a manner that computers can understand. that is, it's just a function mapping each symbol from an alphabet, capital sigma, to a binary string, a sequence of zeroes and ones. so this alphabet capital sigma could be any number of things but, you know, as a simple example, you could imagine it's the letters a through z, say in lowercase, plus maybe the space character and some punctuation. so maybe, for a set of size 32 overall. and if you have 32 symbols you need to encode in binary, well, an obvious way to do it is, there happens to be 32 different binary strings of length five, so why not just use one of each of those for your symbols. so this would be a fixed length code, in the sense we're using the exactly the same number of bits, namely five, to encode each of the symbols of our alphabet. this is pretty similar to what's going on with ascii codes. and of course, it's a mantra of this class to ask, when can we do better than the obvious solution? so in this context, when can we do better than the fixed length codes? sometimes we can, in the important case when some symbols are much more likely to appear than others. in that case, we can encode information using fewer bits by deploying variable length codes. and this is, in fact, a very practical idea. variable length codes are used all the time in practice. one example is in coding mp3 audio files. so if you look up the standard for mp3 encoding, there's this initial phase which you do analog-to-digital conversion. but then, once you're in the digital domain, you do apply huffman codes, what i'm going to teach you in these videos, to compress the length of the files even further. and of course as you know, compression, especially lossless compression, like huffman codes, is a good thing. you want to download a file, you want it to happen as fast as possible. well, you want to make the file as small as possible. so a new issue arises when you pass from fixed-length codes to variable length codes. so let me illustrate that with a simple example. suppose our alphabet sigma is just four characters, a, b, c, d. so the obvious fixed length encoding of these characters would just be 00, 01, 10, and 11. well, suppose you wanted to use fewer bits, and wanted to use a variable length encoding, an obvious idea would be to try to get away with only one bit for a couple of these characters. so, suppose instead of using a double 0 for a, we just use a single 0. and instead of using a double one for d we just use a single one. so that's only fewer bits. so that seems like that can only be better. but now, here's the question. suppose, someone handed you an encoded transmission consisting of the digits 001. what would have been the original sequence of symbols that led to that encoded version? all right, so the answer is d. there is not enough information to know what 001 was supposed to be an encoding of. the reason is is that having passed to a variable length encoding, there is now ambiguity. there is more than one sequence of original symbols that could have led, under this encoding, to the output 001. specifically, answers a and c would both lead to 001. the letter a would give you a zero, the letter b would give you a 01. so that would give you 001. on the other hand, aad would also give you 001. so there's no way to know. contrast this with fixed-length encoding. if you're given a sequence of bits with a fixed length code, of course you know where one letter ends and the next one begins. for example, if every symbol was encoded with 5 bits, you would just read 5 bits. you would know what symbol it was, you would read the next 5 bits, and so on. with variable length codes, without further precautions, it's unclear where one symbol starts and the next one begins. so that's an additional issue we have to make sure to take care of with variable length codes. so to solve this problem, that with variable length codes and without further precautions, it's unclear where one symbol ends and where the next one begins, we're going to insist that our variable length codes are prefix free. so what this means is when we encode a bunch of symbols, we're going to make sure that for each pair of symbols i and j from the original alphabet sigma, the corresponding encodings will have the property that neither one is a prefix of the other. so going back to the previous slide, you'll see that that example was not prefix free. for example, we used zero, was a prefix of zero one, that led to ambiguity. similarly, one, the encoding for d, was a prefix of 10, the encoding for c, and that also leads to an ambiguity. so if you don't have prefixes for each other, and we'll develop this more shortly, then there's no ambiguity. then there's a unique way to decode, to reconstruct what the original sequence of symbols was, given just the zeros and ones. so lest you think this is too strong a property, certainly interesting and useful variable length codes exist that satisfy the prefix-free property. so one simple example, again just to encode the letters a, b, c, d. we can get away with encoding the symbol a just using a single bit, just using a zero. now, of course, to be prefix free, it better be the case that our encodings of b and c and d all start with the bit 1. otherwise we're not prefix free. but we can get away with that, so let's encode a b with a one and then a zero, and now both c and d better have the property that they start neither with 0 nor with 10. that is, they better start with 11, but let's just encode c using 110 and d using 111. so that would be a variable length code. the number of bits varies between one and three, but it is prefix free. and again, the reason we might want to do this, the reason we might want to use a variable-length encoding, is to take advantage of non-uniform frequencies of symbols from a given alphabet. so let me show you a concrete example of the benefits you can get from these kinds of codes on the next slide. so let's continue with just our four-symbol alphabet, a, b, c, and d. and let's suppose we have good statistics in our application domain about exactly how frequent each of these symbols are. so, in particular, let's assume we know that a is by far the most likely symbol. let's say 60% of the symbols are going to be as, whereas 25% are bs, 10% are cs, and 5% are ds. so why would you know the statistics? well, in some domains you're just going to have a lot of expertise. in genomics you're going to know the usual frequencies of as, cs, gs and ts. for something like an mp3 file, well, you can literally just take an intermediate version of the file after you've done the analog to digital transformation, and just count the number of occurrences of each of the symbols. and then you know exact frequencies, and then you're good to go. so let's compare the performance of the obvious fixed length code, where we used 2 bits for each of the 4 characters, with that of the variable length code that's also prefix-free that we mentioned on the previous slide. and we're going to measure the performance of these codes by looking, on average, how many bits do you need to encode a character. where the average is over the frequencies of the four different symbols. so for the fixed-length encoding, of course, it's two bits per symbol. we don't even need the average. just whatever the symbol is, it uses exactly two bits. so what about the variable length encoding that's shown on the right in pink? how many bits, on average, for an average character, given these frequencies of the different symbols, are needed to encode a character of the alphabet sigma? okay, so the correct answer is the second one. it's, on average, 1.55 bits per character. so what's the computation? well, 60% of the time, it's going to use only 1 bit, and that's where the big savings comes from. 1 bit is all that's needed whenever we see an a, and most of the characters are as. we don't do too bad when we see a b either, which is 25% of the time. we're only using 2 bits for each b. now it is true that cs and ds, we're paying a price. we've having to use 3 bits for each of those, but there aren't very many. only 10% of the time is it a c and 5% of the time is it a d. and if you add up the result, that's taking the average over the simple frequencies, we get the result of 1.55. so this example draws our attention to a very neat algorithmic opportunity. so namely, given a alphabet and frequencies of the symbols, which in general are not uniform, we now know that the obvious solution fixed length codes need not be optimal. we can improve upon them using variable length prefix-free codes. so the computation you probably want to solve is which one is the best? how do we get optimal compression? which variable length code gives us the minimum average encoding length of a symbol from this alphabet? so huffman codes are the solution to that problem. we'll start developing them in the next video. 
so we clearly have something to be excited about. there's clearly this opportunity to design binary prefix-free codes which improve over the obvious fixed link solution. so, we'd like to have in some sense, optimal algorithm for this problem and for that, we of course need a crisp problem definition. so, to do that it turns out to be useful to think of codes as binary trees. so, this video will develop that connection concluding with the final formal problem statement. so, the last video introduced us to this very interesting computational problem, namely given characters from an alphabet with frequencies, find the best binary prefix-free encoding, find the code which minimizes the average number of bits needed to encode a character. crucial, the reasoning about this problem is thinking of binary codes as binary trees. so to give you an idea about this correspondence, let's just revisit three of the binary codes we saw in the previous video and see what kind of trees they correspond to. so let's just continue with the four symbol alphabet a b c d. the obvious fixed length code where we encode a, b, c, d was 0, 0, 0, 1, 1, 0 and 1, 1 is just going to correspond to the complete binary tree with four leaves. so let me label this complete binary tree as follows. i'm going to label the leaves a through d from left to right, and i'm going to label each edge of the tree with a 0 if it corresponds to a left-child relationship and with a 1 if it corresponds to a right-child relationship. and now what you see is there's a correspondence between the bits along root to leaf paths and the fixed length encoding. so for example for the symbol c, if we follow the path from the root to the leaf labeled c, we first encounter a 1 because it's a right-child, then we encounter a 0 because it's a left-child. that gives us a 1 and a 0, that's the same as our encoding of the symbol c in this fixed length encoding and the same of course is true for the other three leaves. next, when we first started playing around with variable-length encodings to motivate the prefix-free property, we studied a code where we replaced the double 0 for an a with a single 0 and the double 1 for a d with a single 1. now this code was not prefix-free, but we can still represent it as a binary tree. it's just not going to be a complete one. so i'm going to label the edges of this tree the same way as before. left-child edges will be given a label of 0, right-child edges will be given a label of 1. i'm going to label the left and right children of the root a and d respectively and the two leaves will be given the labels b and c. the reason i labeled the nodes in this way is, because then we have the same kind of correspondence between the encodings that we proposed for the various symbols and the bits that you see along a path from the root to nodes with those symbols. so for example, if you at the node labeled d, the path from the root only has a single bit 1 and that coincides with the proposed encoding of the symbol d. now, remember, this code is not prefix-free and so therefore, as we saw, it had ambiguity. so if you're wondering what some encoded message means and you see a 0, you're not sure that 0 might be representing the symbol a or alternatively it might be the first half of an encoding of the symbol b. so, this ambiguity is also noticeable in the tree. and the property in the tree that tips you off to the ambiguity is that there are symbols at internal nodes of the tree. the symbols are not merely at the leaves as they were with the first tree with the fixed length in coding, but there are also two internal nodes that have symbols. so let's draw the tree for our final example which, was the variable length but prefix-free code that we looked at in the previous video. so this is going to correspond to a tree which is not perfectly balanced, but it will have labels only at the leaves of the tree. so, if you label the edges of this tree the way we've been doing, all the left-child edges get a 0, all the right-left edges get a 1, and we label the leaves of the tree from a to d going from left to right. you'll see we have the same correspondence as in the previous two trees. the sequence of bits from the root to a leaf coincide with the proposed encoding for it. so, for example, if you look at the leaf labeled c, because you traverse a right-child, another right-child and a left-child to get to it, that's the sequence 1, 1, 0 and that is precisely the proposed encoding for the symbol c. so in general, any binary code can be expressed as a tree in this way, with the left-child pointers being labeled with 0's, the right child pointers being labeled with 1's, and the various nodes being labeled the symbols of the given alphabets, and the bits from the root down to the node labeled with the given symbol corresponding to the proposed encoding for that symbol. and what's cool about thinking about codes as trees is that the really important prefix-free condition, which seems like a nuisance to check in the abstract, shows up in a really clean way in these trees, namely the prefix-free condition is the same as leaves being the only nodes that have labels. no internal nodes are allowed to have a label in a prefix-free code. the reason for this is that we've set it up so that the encodings correspond to the bits along paths from the root to the labeled node. so being a prefix of another corresponds to one node being an ancestor of the other, and so, if all the labels are at the leaves, then of course nobody is an ancestor of the other and we have no prefixes. the other things that's really cool about this tree representation of codes is, it becomes pictorially obvious how you do the coding given this sequence of 0's and 1's from a prefix-free binary code, namely, you start at the beginning and you start at the root of your tree, whenever you see a 0 you go left, whenever you see a 1 you go right. at some point you'll hit a leaf, that leaf will have some label and that's the symbol that's being encoded, and after you hit a leaf, you just start all over again back to the root. so for example, if you were using our variable-length prefix-free code for the four letter alphabet, as in our running example, and you were given the sequence of 0s and 1s, 0, 1, 1, 0, 1, 1, 1. what would you do? well, you'd start at the root and you see a 0, so you follow the left-child pointer, and you immediately get to a leaf. it's labeled a, so you're going to output and a as the first symbol. now you start all over. you return to the root, now what do you see? you see a 1, so you go right, you see another one, so you go right, and now you see a 0, so you go left and that gets you to the leaf labeled c. now you start all over again. you see a 1, you go right, you see a 1, you go right again, and then, finally you see yet one more 1 and you wind up at the lead labelled d. so by repeated traversals through the tree, you decode the sequence of 0s and 1s as a c, d. there's never any ambiguity, because when you hit a label, you know you're going to leave, there's no where to go. and every internal note, it's unlabeled, you know to expect another bit, another traversal further down in the tree. so a final important point about this correspondence is that the encoding lengths of the symbols, the number of bits needed to encode the various symbols, are just the depths of the corresponding leaves in the tree that corresponds to the code. so for example, in our running example the symbol a is the only one that needs only one bit to encode and it's also the only leaf in level one of the tree. and similarly b needs two bits and shows up in the next level, the c and the d which need three bits show up in the third level. so this correspondence is, really just by construction, so how do you encode a given symbol. well, it's just the bits on the path from the root, and that the number of such bits is just the number of pointer traversals you need to get from the root down to that leaf, and that's just the depth of that leaf in the tree. so we're now in a great position to really have a crisp definition of the problem. the input of course is just the frequencies for a bunch different symbols i from some alphabet capital sigma. i'm going to use p sub i as notation for the frequency of symbol i. so we know what it is we want to optimize. we want to minimize the expected number of bits needed to encode a symbol, where the average of the expectation is taken over the provided frequencies of the various symbols. well, let's express this objective function using our newfound correspondence with binary trees. in particular, the fact that we can think about encoding lengths as depths of leaves in trees. so, given a tree, t, which corresponds to a prefix-free binary code. that is it should be a binary tree and the leaves of this tree should be in one-to-one correspondence with the symbols of sigma. we're going to define capital l(t) as the average encoding length. it's an average in the sense that we sum over all of the symbols of the alphabet, we weight each symbol by the frequency, and remember, this is part of the input, so whether the provided frequency p survived that symbol i. and then how many bits do we need to encode that symbol i? well, it's just the depth of the leaf which is labelled i in the given tree, capital t. so this is what we want to make as small as possible. so, for instance, using the data from the previous video, the letters a, b, c, d with frequencies 60, 25, 10, and 5%. then if we use the complete binary tree, that is the fixed length encoding, we just get two bits per character. while if we use the lopsided tree optimized, so that each a only takes one bit while suffering three bits for c and d, then the average encoding length drops to 1.55, as we saw in the last video. so what then is the goal, what's the responsibility of our algorithm? well, amongst all binary trees, which have leaves and correspondence to the symbols of sigma, we want to compute the one which makes this average encoding length as small as possible which minimizes our objective function capital l. turns out huffman's greedy algorithm does it. more details to come. [sound] 
so in this video we'll finally discuss huffman's algorithm which is a greedy algorithm that constructs the prefix free binary code minimizing the average encoding length. so, let me just briefly remind you the formal statement of the compositional problem that i left you with last time. so, the input is just a frequency for each symbol i, coming from some alphabet sigma. so the responsibility of the algorithm is to get optimal compression, so to compute an optimal code. so the code has to be binary. we have to use only zeros and ones. we have to be prefix free meaning the encodings of any two characters, neither one can be a prefix of the other, that's to facilitate unambiguous decoding. and finally, the average number of bits needed to encode a character, where the average is with respect to the input frequencies, should be as small as possible. now remember these kinds of codes correspond to binary trees. the prefix free condition just says that the symbols of sigma should be in one to one correspondence with the leaves of the tree. and finally remember the encoding lengths of the various symbols just correspond to depths of the corresponding leaves. so we can formally express the averaging coding length. which given a legal tree capital t. i'm using the notation capital l of t. so what do we do? we just take the average over the symbols of the alphabet weighted by the provided frequencies and we just look at the number of bits used to encode that symbol. equivalently depth of the corresponding leaf in the tree t. so we want the tree that makes this number as small as possible. so this task is a little bit different than any we've seen so far in this course, right? all we're given as an input is an array of numbers and yet we have to produce this actual full blown tree. so how can we just take a bunch of numbers and produce them in a sensible, principled way? some kind of tree whose leaves correspond to symbols of the alphabet. so let's spend a slide just thinking about, you know, at a high level where would this tree come from, how would you build it up given this unstructured input. so there's certainly no unique answer to this question and one idea which is very natural but that turns out to be sub optimal is to build a tree using a top down approach, which you can also think of as an. initiation of the divide and conquer algorithm design paradigm. the divide and conquer paradigm, you'll recall, involves breaking the given sub-problem into usually multiple, smaller sub-problems. they're solved recursively, and the solutions are then combined into one for the original problem. because trees, the desired output here, have a recursive substructure, it's natural to think about applying this paradigm to this problem. specifically you love to just lean on recursive call to construct for you the left sub tree, another sub call constructing the right sub tree. and then you can fuse the results together under a common root vertex. so it's not clear how to do this partitioning of the symbols into two groups, but one idea to get the most bang for your buck, the most information out of the first bit of your encoding. you might want to split them in, the symbols, into groups that have roughly, as close to as possible, of 50% of the overall frequency. so this topped out approach is sometimes called fanno-shannon coding. fanno was huffman's graduate adviser. shannon is the claude shannon inventor of information theory. but huffman in a term paper believe it or not, realized the topped down is not the way to go. the way to go is to build the tree from the bottom up. not only are we going to get optimal codes, but we're going to get the blazingly fast greedy algorithm that constructs them. so what do i mean by bottom up? i mean we're going to start with just a bunch of nodes, each one labelled with one symbol of the alphabet. so, in effect, we're starting with the leaves of our tree. and then we're going to do successive mergers. we're going to take at each step two sub-trees thus far and link them together as sub-trees under a common internal node. so, i think you'll see what i mean in a picture. so imagine we want to build a tree where the leaves are supposed to be just a, b, c, and d. so one merger would be oh, well let's just take the leaves c and d and link them as siblings under a common ancestor. now in the second step let's merge the leaf b with the sub-tree we got from the previous merge, the sub-tree comprising the nodes c, d, and their common ancestor. so now of course we have no choice but to merge the only two sub-trees we have left, and then that gives us a single tree. which is in fact exactly the same lopsided tree we were using in the previous video as a running example. so let me explain what i hope is clear and what is maybe unclear at this juncture. i hope what's intuitively clear is that the bottom of approaches, a systematic way to build trees that have a prescribed set of leaves. so what do we want? we want trees whose leaves are labeled with the symbols of the alphabet sigma. so if we have an alphabet with n symbols, we're going to start with just the n leaves. what does a merger do? well, there's two things. first of all, it introduces a new internal node, an unlabelled node. and secondly, it takes two of our old subtrees and fuses them into one, merges them into one. we take two subtrees, we make one the left child of this new internal node, the other, the right child of this new internal node. so that drops the number of subtrees we're working with by one. so if we start with the n leaves and we do n minus one successive mergers, what happens? well on the one hand, we introduce n minus one new unlabeled internal nodes. and on the other hand, we construct a single tree. and the leaves of this tree that we get are in one to one correspondence with the alphabet letters as desired. now what i don't expect you to have an intuition for is what should we be merging with what and why? forget about, you know, how do we get an optimal tree at the end of the day. i mean, even just if we wanted to design a greedy algorithm. if we just wanted to make a myopic decision, that looks good right now, how would we even do that? what's our greedy criteria that's going to guide us to merge a particular pair of trees together? so we can re-frame this quandary in the same kind of question we asked for minimum cost spanning trees and really more generally with greedy algorithms. when you're making irrevocable decisions which strikes fear in your heart, is that this decision will come back and haunt you later on. you'll only realize at the end of the algorithm that you made some horrible mistake early on in the algorithm. so just as for mst's, we ask, you know, when can we be sure that including an edge irrevocably is not a mistake, it's safe in the tree that we're building? here we want to ask, you know, we have to do a merger, we want to do successive mergers and how do we know that a merger is safe? that it doesn't prevent us from eventually computing an optimum solution. well here's the way to look at things that will at least give us an intuitive conjecture for this question. we'll save the proof for the next video. so what are the ramifications when we merge two subtrees, each containing a collection of symbols? well, when we merge two subtrees, we introduce a new internal node which unites these two subtrees under them, and if you think about it, at the end of the day on the final tree, this is yet another internal node that's going to be on the root to leaf path, for all of the leaves in these two sub trees. that is, if you're a symbol and you're watching your subtree get merged with somebody else. you're bummed out, your like, man that's another bit in my encoding. that's yet one more node i have to pass through to get back to the root. i think this will become even more clear if we look at an example. so naturally, we'll use our four symbol alphabet a, b, c, d. and initially, before we've merged anything, each of these is just its own leaf a, b. c, d. so there's no internal nodes above 'em. in the sense, everybody's encoding length at the beginning is zero bits. so now, imagine we've merged c and d, we introduce a new internal node, which is the common an-ancestor of these two leaves. and as a result, c and d are bummed out. they said, well, there's one bit that we're going to, to have to incur our encoding length, there's one new internal node we're always going to have to pass through enroute back to the root of the eventual tree. now suppose next we merge b with the subtree containing both c and d. well everybody but a is bummed out about, about this merger because we introduce another internal node, and it contributes one bit to the encoding of each of b, c, and d. it contributes an extra one to the encoding of c and d, and it contributes a zero to the encoding of b. so all of their encodings in some sense inc, get incremented, go up by one, compared to how things were before. now in the final iteration, we have to merge everything together. we have no choice, there's only two sub-trees left. so here, everybody's encoding length gets bumped up by one. so, a finally picks up a bit at zero to encode it and b, c, and d each pick up an additional bit of one, which is prepenned into their encodings thus far. and, in general what you'll notice is that the final encoding length of a symbol, is precisely the number of mergers that its subtree has to endure. every time your subtree gets merged with somebody else you pick up another bit in your encoding, and that's because there's one more internal node that you're going to have to traverse enroute to the root of the final tree. in this example, the symbols c and d, well they got merged with somebody else in each of the three iterations. so, they're the two symbols that wind up with the encoding length of three. the symbol b, it didn't get merged with anybody in the first iteration, only the second two. that's why it has an encoding length of two. so this is really helpful. so this, lets us relate, the operations that our algorithm actually does, namely mergers back to the objective function that we care about, namely the average encoding length. mergers increase the encoding lengths of the participating symbols by one. so this allows us to segway into a design of a greedy heuristic for how to do mergers. let's just think about the very first iteration. so we have our n original symbols, and we have to pick two to merge. and remember the consequences of a merge is going to be an increase in the encoding length by one bit, whichever two symbols we're going to pick. now we want to do is minimize the average encoding length with respect to the frequencies that were given. so which pair of symbols are we the least unhappy to suffer an increment to their encoding length, was going to be the symbols that are the least frequent. that's going to increase your averaging poding length by the least. so that's the green merging hiuristic. somethings gotta increase. pick the ones that are the least frequent to be the ones that get incremented. so that seems like a really good idea of how to proceed in the first iteration. the next question is, how are we going to recurse? so, i'll let you think about that in the following quiz. so let's agree that the first iteration of our greedy heuristic is going to merge together the two symbols that possess the lowest frequencies. let's call those two symbols little a and little b. the question is then how do we make further progress? what do we do next? well, one thing that would be really nice is if we could somehow recurse on a smaller subproblem. well, which smaller subproblem? well, what does it mean that we've merged together the symbols a and b? well, if you think about it, in the tree that we finally construct by virtue of us merging a and b, we're forcing the algorithm to output a tree in which a and b are siblings, in which they have exactly the same parent. so what does it mean for the encoding that we compute that a and b are going to be siblings with the same parent? it means that their encodings are going to be identical, save to the lowest order bits. so a will get encoded with a bunch of bits followed by a zero, b will be encoded by exactly the same prefix of bits followed by a1. so they're going to have almost of the same encoding, so for our recursion let's just treat them as the same symbol. so let's introduce a new meta symbol, let's call it ab, which represents the conjunction of a and b. so it's meant to represent all of the frequencies of either one, all of the occurrences of either one of them. but remember the input to the computational problem that we're studying is not just an alphabet, but also frequencies of each of these symbols of that alphabet. so my question for you is when we introduce this new meta symbol a b. what should be the frequency that we define for this meta symbol? all right, so hopefully your intuition suggested answer c, that for this recursion to make sense, for it to conform to our semantics of what this merging does. we should define the frequency of this new meta symbol to be the sum of the frequencies of the two symbols that it's replacing. that's because, remember this meta symbol ab is meant to represent all occurrences of both a and b. so it should be the sum of their frequencies. so i'm now ready to formally describe huffman's greedy algorithm. let me first describe it on an example and then i think of the general code will be, self evident. so let's just use our usual example, so we're going to have letters a, b, c, d, with frequencies 60, 25, 10, and 5. so we're going to use the bottom up approach, so we begin with each symbol just as its own node, a, b, c, d. i'm annotating in red the frequencies 60, 25, 10, 5. the greedy heuristic says initially we should merge together the two nodes that have the smallest frequencies. so that would be the c and the d with their frequencies of 10 and 5. now based on the idea of the last slide when we merged c and d, we replaced them with a meta symbol cd whose frequency is the sum of the frequencies of c and d, namely fifteen. now we just run another iteration of the greedy algorithm meaning we merge together the two nodes, the two symbols that have the smallest frequencies. so this is now b 25, and cd 15. so now we're down to just two symbols, the original symbol a, which still has frequencies 60 and the in some sense, meta, meta symbol bc, d who is now cumulative frequency is 40. so when we're down to two nodes, that's going to be the point in huffman's algorithm where we hit the base case and the recursion begins to unwind. so if you just have two symbols to encode there's pretty much only, one sensible way to do it. one is a zero, and one is a one. so at this point, the final recursive call just returns now a tree with two leaves corresponding to the symbols a and bcd. and now is the recursion on lines we in effect undo the mergers. so for each merge we do a corresponding split of the meta node and we replace that with an internal node. and then two children corresponding to the symbols that were merged to make that meta node. so for example, when we want to undo the merge of the b and the cd, we take the node labeled bcd and we split it into two. the left, left child being b, the right child being cd. so the original outer most call, what it gets from its rehersive call is this tree with three nodes and it has to undue it's merger, it merged c and d. so it takes the leaf labeled with symbol cd and splits it into two. it replaces it with a new unlabeled internal node, left child c, right child d. so how does the algorithm work in general? well just as you'd expect given the discussion on this concrete example. i'm going to take as the base case when the alphabet has just two symbols, in that case the only thing to do is encode one with a zero, the other with a one. otherwise, we take the two symbols of the alphabet that have the smallest frequencies. you can break ties arbitrarily, it's not going to matter, it's going to be optimal either way. we then replace these two low frequency symbols a and b with meta symbol ab, intended to represent both of them in some sense. as we just discussed with those semantics, we should be defining the frequency of the meta symbol ab as the sum of the frequencies of the symbols that it comprises. we now have a well defined, smaller sub problem. it has one fewer symbol than the one we were given. so, we can recursively compute a solution for it. so, what the recursive call returns is a tree who's leaves are in one to one correspondence with sigma prime. that is, it does not have a leaf labeled a, it does not have a leaf labeled b. it does have a leaf labeled ab, so we want to extend to this tree t prime to be one who's leaves correspond to all of sigma. and the obvious way to do that is to split the leaf labeled ab, replace that with a new internal unlabeled node with two children which are labeled a and b. the resulting tree capital t with leaves and correspondents to the original alphabet sigma is then the final output of huffman's algorithm. as always, with a greedy algorithm we may have intuition, it may seem like a good idea. but we can't be sure without a rigorous argument. that is the subject of the next video. 
to make sure that huffman's greedy algorithm is clear, let's go through a slightly larger, more complicated example. so let's work with a six character alphabet. let's call the letters a, b, c, d, e, f, and let's assume that we're given the weights 3, 2, 6 and 8, 2, 6 for these six characters. remember, this problem is well-defined even if the weights don't add up to 1. if you prefer working with actual probabilities, feel free to divide these six numbers by 27. in the first step of huffman's greedy algorithm, we find the letters that have the smallest weights, the smallest frequencies. so in this example, that would be the letters b and e, both of them have weight 2. now what we do is we merge these two letters into a single meta letter, in effect committing right now to having b and e be siblings in the final tree. after this merger, our alphabet is down to five symbols, the symbols b and e being replaced by the merged symbol be. and the weight of be is the sum of the weights of b and e, namely, 4. we can imagine our tree slowly taking shape through these iterations. so after step 1, we know that b and e are going to be siblings and we know that just a, c, d, and f are going to be leaves, that's all we know thus far. in the next iteration, we again look for the two symbols that have the smallest weight. and here the smallest weight symbol is a. it has weight 3. and the runner-up is the merged symbol b sub e. its combined weight is 4, and that's second overall for these five symbols. so in this step, we merge a with b e. now our alphabet is down to four symbols, the merged symbol, a, b, e, which has cumulative weight 7, and then the original symbols, c, d, and f, which have their original weights, 6, 8, and 6. as far as our tree, we've now committed to the symbol a appearing as an uncle of the siblings b and e. and again, c, d, and f, we just know they're leaves somewhere in the final tree. in step three, we're going to again pick the two symbols that have the smallest weights. in this case, the two symbols with the smallest weight are c and f, each of weight 6. in our new alphabet, we still have our symbol abe, it still has weight 7. we still have the symbol d, it still has weight 8. but now we have a new merged symbol cf, and its new weight is 12. as far as our tree, in addition to the information we already knew, we're now committing to having c and f be siblings in the final tree. in step four, we merge the two symbols with the smallest weight. so that would be abe with its weight of seven and d with its weight of eight. so this leaves us with only two symbols, abde and cf. and now we know what both of these subtrees of the root of the final tree are going to look like. now that we're down to two symbols, the only thing we can do is fuse these two symbols into one, fuse these two subtrees into a single one by uniting them under a common root. that gives us the following final output of huffman's algorithm. what prefix-free code does this tree correspond to? well, as usual, let's label all of the left branches with zero and all of the right branches with ones. and now, as usual, the encoding of a character is just the symbols of zeros and ones that you see when you traverse a path from the root to that leaf. so for example, a will be encoded with 000, b with 0010, c with 10, d with 01, e with 0011, and f with 11. 
in this video we'll establish the correctness of huffman's algorithm meaning that that greedy algorithm always computes the prefix free binary code that minimizes the average encoding length. let me also remind you of our expression for the average encoding length in terms of tree capital t. we'll be working with this expression quite a bit in this proof. so we average over the symbols. so we sum over the symbols i and the alphabet capital sigma. we weight the various terms by the frequencies, which are given as part of the input. and remember that the encoding length that we're producing for a given symbol i is exactly the same as the depth of the corresponding leaf in the tree, capital t. so i quite like the proof of huffman's theorem. it's a cool proof, and it will give us an opportunity to revisit the themes that we've been studying and proving the correctness of various greedy algorithms. at a high level, we're going to proceed by induction, induction on the size n of the alphabet sigma. so there's a vague parallel you could draw with how we proved, say, dijkstra's algorithm correct, back in part one, where by induction we showed that each successive iteration of the algorithm is correct, assuming that the previous ones were. but in the inductive step, we're going to use an exchange argument. so to justify each of our mergers, we're going to argue that any optimal solution can be massaged into one that looks more like ours without making it any worse. and that's how we argue that each of our individual mergers is, in fact, a reasonable idea, a good idea. so more precisely, we're going to go by induction on the size n of the alphabet. i'm going to assume, to make the problem non-trivial, that the alphabet size is at least 2. so, as in any proof by induction, we begin with a base case, and then we do the inductive step, wherein we can assume the inductive hypothesis. so, the base case is when we just have a two-letter alphabet. if you go back to huffman's algorithm, you will see that it does the obvious thing in the base case. it just outputs a tree that encodes one of the symbols with the letter zero, and one of the symbols just with the bit one. and that's the best you can do. you need a bit to encode every symbol, and that tree uses exactly one bit for each symbol so huffman's algorithm is optimal in that trivial special case. so for the inductive step, we focus on an arbitrary problem instance, where the alphabet size is at least three. now, of course, whenever you're doing a proof by induction, the thing you've really got going for you is the inductive hypothesis. you assume that the assertion that you're trying to prove, in this case, correctness of huffman's algorithm, is true for all smaller values of n. that is, we're going to assume that if we invoke the algorithm on any smaller input, as, of course, we do, in this recursive algorithm, we get to assume that the algorithm returns the correct solution to that smaller sub-problem. so to understand how we're going to pass from the inductive hypothesis, so this assumption that we're correct on all smaller inputs, to the inductive step, the assertion that we're correct on the current input. we need to take a closer look at how the original input, with its alphabet sigma, can relate to the smaller sub-problem, with the smaller alphabet sigma prime, with the two letters fused into one, which is the one that we solve correctly, by assumption, recursively. so recall our notation from the pseudocode for huffman's algorithm. what the algorithm does is take the two symbols with the smallest frequencies, and let's call those two symbols a and b. and it replaces those two symbols with a single symbol ab, sort of a meta-symbol representing the presence of either a or b. we also, in the quiz, discussed how the sensible frequency for this new meta-symbol ab is the sum of the frequencies of a and b. last video, when we were developing our intuition for what a greedy algorithm for successive bottom-up mergers might look like, we noticed that when you merge two symbols a and b, what you're really doing is committing to outputting a tree at the end of the day, at the end of your algorithm, in which the symbols a and b appear as siblings, in which they have exactly the same parent. therefore, there's an exact correspondence, one-on-one correspondence, between, on the one hand, trees whose leaves are labeled with the symbols in sigma-prime. so that there is no leaf labeled a, there's no leaf labeled b, there's instead one labeled ab. so there's a correspondence between those kinds of trees, labels that the leaves correspond to sigma prime, and trees for the original alphabet sigma, in which the symbols a and b are siblings, in which they have exactly the same parent. given a tree as shown in the left picture, that is, given a tree for t prime, the leaves labelled according to sigma prime, you can, and this is in fact exactly what we do in huffman's algorithm, split the leaf with the metal symbol a, b. make it an internal node, and give it two leaves with labels a and b. that produces a tree of the form on the right. conversely, given a tree of the form on the right, that is, its leaves are labeled according to sigma, and it just so happens that a and b show up as leaves of that tree, you can produce a tree for sigma prime just by contracting a and b together. so, sucking up a and b into their parent, and labeling the parent ab. so you can go back and forth between these two types of trees. one arbitrary tree is for sigma prime and trees of a special kind for sigma, trees in which a and b happen to be siblings. this subset of trees for sigma are important enough to be given its own notation. so let me denote by capital x, sub ab, the trees with leaves labeled according to sigma, in which it just so happens that a and b appear as siblings. so that'll be some trees for sigma, but not all of them, only the ones in which a and b are siblings. so this correspondence between solutions to the smaller sub-problem and solutions of a particular form for the original problem has an important property. namely, it preserves the objective function value, it preserves the average encoding length. okay, that's not quite true, but it's almost true. it's good enough for our purposes. it preserves the average encoding length up to a fixed constant. so let me show you the computation which demonstrates that point. so consider any pair of matched trees, t prime and t. and by matched, i just mean t prime is any old tree whose leaves are labeled according to sigma prime. and t is the tree you would obtain if you split the leaf with meta label ab in the usual way. you replace it with an internal node, and children with labels a and b. so that's going to be the corresponding tree capital t. so take any such matched pair, and let's look at the difference between their average encoding lengths. now, remember the average encoding length of a tree is just a sum of the symbols of the relevant alphabet. and what we got going for us here is that sigma and sigma prime are almost exactly the same, right? so the only difference is sigma prime has the meta symbol ab, whereas sigma has the individual symbols a and b. furthermore, the two trees t and t prime are almost exactly the same. the only difference being that t prime has this leaf with the meta label ab, whereas t has two corresponding nodes, one level down, with the labels a and b. so, when we take the difference of these two sums, everything cancels out, except that the tree t contributes two summands, one for the leaf with label a, one for the leaf with label b. and the t tree prime contributes, with a minus sign, one summand, that corresponding to the leaf with label ab. so, when the dust clears, and we cancel out all the terms, what we're left is the term for the leaf a, and with the frequency of a times the depth of the leaf a in the tree t. a similar term for the leaf with label b in the tree t, and then with a minus sign, the frequency of the meta label ab in the tree times its depth in the tree t prime. but, we are certainly not done with our simplifications. there is a strong relationship between the frequencies that we're staring at and a strong relationship between these various depths. let's begin with the frequencies. how did we define the frequency of the meta symbol ab? or, remember our quiz. it made sense to define it as the sum of the frequencies of a and b. what about the depths? well, you know, this symbol ab is at whatever depth it is in the tree t prime. so, it's a depth 10, something like that. remember, t is obtained from t prime simply by splitting this leaf ab and giving it two new children with symbols a and b. so, if the meta symbol ab was at depth ten in t prime, then the leaves a and b are going to be at depth 11 in t. so, the depth is simply one more than whatever it was previously in the tree t prime. so these relationships are going to result in a second wave of cancellations. so just to make that crystal clear, let's call the the depth of ab in t prime d. so d is what i was calling ten earlier. so a and b are both a depth d plus 1 in the tree t. so the first term becomes the frequency of a times the depth, d plus 1. the second term becomes the frequency of b, plus the depth, d plus 1. and then the third term becomes the sum of the frequencies of a and b times the depth d. and, when the dust settles yet again, we are left with a constant, pa + pb. so, the sums of two frequencies. and one thing i want you to really understand is that the difference between these two average encoding lengths is just some constant. it does not depend on which trees we started with. so if we choose a perfectly balance tree and we do this difference, we get some constant like 0.1. if we choose some totally different pair of trees that are really lopsided and we take this difference, we still get 0.1, exactly the same thing. so, this fulfills the promise i gave you earlier, that not only do we have this natural correspondence between, on the one hand, trees labeled with sigma prime, and trees of a certain type labeled according to sigma, namely those in which a and b appear siblings, but the correspondence preserves average encoding length. okay, it doesn't quite preserve the average encoding length, but it preserves it up to a universal constant, and that's going to be good enough for our purposes, as we'll see in a sec. 
so i know you're probably catching your breath from that last computation. so let's zoom out. let's make sure we don't lose the forest for the trees. and see that we're actually mostly there. we're just one lemma away from finishing the proof of theorem. the proof of correctness of huffman's algorithm. so what do we have going for us. well, first of all we've got the inductive hypothesis. remember and any proof by induction you better rely very heavily on the inductive hypothesis. what does it say? it says when huffman's algorithm recurses on the smaller subproblem with the smaller alpha bit signal prime, it solves it optimally. it returns a tree which minimizes the average encoding length with respects to the alpha bit sigma prime in the corresponding frequencies. so, we're going to call that tree, the output of the recursive call, t prime hat. so let me amplify the power of this inductive hypothesis by combining it with the computation we did in the last slot. so what do we know so far? we know for this smallest subproblem which frankly, we don't care about in its own right. but, nevertheless, for this smaller sub problem, with alphabet sigma prime, the recursive call solves it optimally. it returns to us, this tree t hat prime. and among all trees with leaves inable according to sigma prime. this tree is as good as it gets, it minimizes the average encoding length. but what have we learned in the last slide? we learned that there's a one to one correspondence between feasible solutions, between trees for the smallest subproblem with the alphabet sigma prime, and feasible solutions to the original problem, the one we care about, that have a certain form in which, it just so happens that a and b are siblings, that they share a common parent. moreover, this correspondence preserves the objective function value, the average encoding length or at least it preserves it up to a constant which is good enough for our purposes. so the upshot is, in minimizing average encoding length over all feasible solutions for the smallest subproblem, a recursive call is actually doing more for us. it's actually minimizing the average encoding length for the original problem with the original alphabet sigma over a subset of the feasible solutions, the feasible solutions in which a and b are siblings. now, does this do us any good? well, it depends, what was our original goal? our goal was to get the smallest average encoding if possible over all feasible solutions in the world. and so what have we just figured out? we figured out, well we're getting the best possible scenario amongst some solutions. those in which a and b happen to be siblings. so, we're in trouble if there is no optimal solution in which a and b are siblings. then it doesn't do us any good to optimize only over these crappy solutions. on the other hand, if there is an optimal solution lying in this set xab in which a and b are siblings, then we're golden. then there's an optimal solution in this set, while recursive call is finding the best solution in the sets. so, we're going to find an optimal solution. so that's really the big question. can we guarantee that it was safe to merge a and b in the very first iteration? can we guarantee there is an optimal solution, a best possible tree that also has that property? that also has a and b as siblings. so the key level, which i approve in the next slide in which we'll complete the proof of the correctness of the apartment spherum, is indeed there is always an optimal solution in the set xaba an optimal solution on which a and b, the two lowest frequency symbols are indeed siblings. so i'll give you a full-blown proof of this key lemma in the next slide. but let me first just orientate you and you know, just develop your intuition about why you'd hope this key lemma would be true. the intuition is really the same as that as when we developed the greedy algorithm in the first place, namely when you which symbols do you want to incur the longest encoding lengths? well, you want them to be the lowest frequency symbols, that you want to minimize the average. so if you look at any old tree, there's going to be you know, some bottommost layer. there's going to be some deepest possible leaves. and, you really hope that a and b, the lowest frequency leaves are going to be in that lowest level. now they might both be in the lowest level and not, and might not be siblings. but, if you think about it, amongst symbols at the same level, they're all suffering exactly the same encoding length anyways. so you can permute them in any way you want and it's not going to affect your average encoding length. so once you put a and b at the bottommost level you may as well put them as siblings and it's not going to make any difference. so, the final punchline being, you can take any tree like say an optimal tree. and by swapping. changing the lowest frequency elements a and b to the bottom and to be common siblings, you can't make the tree worse you can only make it better, and that's going to show that there is an optimal tree with the design property where a and b are in fact siblings. all right, so that's a reasonably accurate intuition, but it's not a proof. here is the proof, so we're going to use an exchange argument. we're going to take an arbitrary optimal tree a tree minimizing the average encoding length. let's call it t star, there's may be many such trees, just pick any one, i don't care which. and the plan is to show an even better tree or at least as good tree which satisfies the properties that we want where a and b show up as siblings. so lets figure out where we're going to swap the symbols a and b to. so we've got our tree, t star. and let's look at the deepest level of t star. and let's find our, you know, any pair of siblings at this deepest level, call them x and y. so in this green tree on the right we have two choices of pairs of siblings at the lowest level. it doesn't matter which one we pick. so let's just say i pick the leftmost pair, call them x and y. so a and b have to be somewhere in this tree. let me just pick a couple of the leaves to be examples for where a and b might actually be in t star. and so now we're going to execute the exchange. we're going to obtain a new tree t hat from t star, merely by swapping the labels of the leaves x and a and similarly swapping the labels of the leaves y and b. so after this exchange we get a new valid tree. so once again, you know, the leaves are labeled with the various symbols of sigma. and, moreover, by our choice of x and y, we've now via this exchange forced the tree to conform to the property. we've forced a and b to be siblings. they take the original positions of x and y which were chosen to be siblings. so to finish the proof, we're going to show that the averaging coding length of t hat can only be less than t star. since t star was optimal, that means that t hat has to be optimal as well, since it also satisfies the desired property that a and b are siblings, that's going to complete the proof the, the key lemma and therefore the proof of the correctness of huffman's algorithm. the reason intuitively is that when we do the swap, a and b inherit the previous debts of x and y and conversely x and y inherit the old debts of a and b. so a and b can only get worse and x and y can only get better, and x and y are the ones with the higher frequencies. so the overall cost benefit analysis is a net win and the tree only improves. but, to make that precise, let me just show you the exact computation. so let's look at the difference between the average encoding length given by the 3t star and that after the swap given by the 3t hat. so analogously, to our computation on the previous slide most of the stuff cancels because the trees t star and t hat frankly just all that aren't all that different. the only things that are different are the positions of the symbols a, b, x, and y. so we're going to have four positive terms contributed by t star for these four symbols. and we're going to have four negative terms contributed by the tree t hat again for these exact same four symbols. so let me just show you, what happens after the dust settles and after you take away all the cancellation? you can write the eight terms that remain in a slightly sneaky but you know, clarifying way as follows. so we're going to have one product representing the four terms involving the symbols a and x, and then an analogous term for the four terms involving b and y. so in the first term we look at the product of two differences. on the one hand we look at the difference between the frequencies of x and a. remember those are the two things that got swapped with each other. and then we also look at the difference between their depths in the original tree t star. and then because b and y got swapped we have an entirely analogous term involving them. the reason i re-wrote this difference in a slightly sneaky way, is it now becomes completely obvious, what role the various hypotheses play. specifically, why is it important that a and b have the lowest possible frequencies? we actually haven't used that yet in our proof, but it's fundamental to our greedy algorithm. secondly, why did we choose x and y to be in the deepest level of the original tree t star. well. let's start with the first fare products. the differences between, between frequencies. so the frequency of x minus the frequency of a, and similarly between y and b. well because a and b have the smallest frequencies of them all. these differences have to be non-negative. the frequencies of x and y can only be more. the second pair of differences also must be non-negative and this is simply because we chose x and y to be at the deepest level. their depths have to be at least as large as that of a and b in the original 3t star. since all four differences are nonnegative, that means the sum of their products is also nonnegative. that means the average encoding length of t star is at least as big as t hat. so that's where t hat has to also be optimal. it in addition satisfies the desired property that a and b are siblings. and so that means our recursive call, even though we've committed to merging a and b, we've committed to returning a tree in which their siblings that was without loss, that was a safe merge. that's why huffman's algorithm at the end of the day can return an optimal tree, one that minimizes the average encoding length. we're all possible binary prefix-free codes. so let me just wrap up with a few notes about how to implement huntsman's algorithm and what kind of running time you should be able to achieve. so if you just naively implement the pseudocode i showed you, you're going to get a not particularly great quadratic time algorithm that moreover uses a lot of recursion. so why is it going to be quadratic time? well, you have one recursive call each time. and you always reduce the number of symbols by one. so the total number of recursive calls is going to be linear in the number of symbols in your alphabet in. and how much work did you do in each recursive call, not counting the work done by later calls? well, you have to search to figure out the lowest frequency symbols a and b. so that's basically some minimum computations. so that's going to take linear time for each of the linear recursive calls. now the fact that we keep doing these repeated minimum computations each recursive call, i hope triggers a light bulb, that maybe a data structure would be helpful. which data structure's raison d'etre is to speed up repeated minimum computations? well as we've seen, for example, in dijkstra's and prim's, prim's algorithms, that would be the heap. so of course, we need to find a heap, you have to say what the keys are. but since we always want to know what the symbols with the smallest frequencies are, the obvious thing to do is to use frequencies as keys. so the only question then is what happens when you do a merge? well, the obvious thing is to just to add the frequencies of those two symbols and reinsert into the heap the new meta symbol with its key equal to what? the sum of the keys of the two elements that you just extracted from the heap. so not only is that going to work great, it's going to be n log n time, it's very easy to implement in iterative manner. so that's starting to be a really blazing fast implementation of huffman's algorithm. in fact you can do even better. you can boil down an implementation of huffman's algorithm to a single indication of a sorting sub-routine followed by a merely linear amount of work. now in the worst case you're still going to be stuck asymptotically with an n log n of limitation because you can't sort better than n log n if you know nothing about what you're sorting. but in this implementation the constants are going to be even better, and as we discussed in part one, there are sometimes special cases where you can beat n log n for sorting. so for example if all, all of your data is representable with a small number of bits, then you can do better than n log n. so i'm going to leave this even faster implemtation as a somewhat nontrivial exercise. i'll give you a hint though, which is, if you do sorting as a pre-processing step you actually don't even need to use a heap data structure for this implementation. you can get away with an even more primitive data structure of a queue. you might however, want to use two queues. so, next time you find yourself in line for coffee, that's a nice thing to think about. how do you implement huffman's algorithm merely with one invocation to sorting, so that the symbols are sorted by frequency, followed by linear work using two queues? 
so the time has arrived to begin our study of dynamic programming. so this is a general algorithm design paradigm. as i mentioned at the beginning of the course, it has a number of justly famous applications. however, i'm not going to tell you just yet what it is that makes an algorithm dynamic programming. rather, our plan is, over the next few videos, to develop from scratch an algorithm for a non trivial, concrete computational problem. the problem is finding the maximum weight independent set and a path graph. this concrete problem is going to force us to develop a number of new ideas. and, once we've finished solving the problem, at that point, we'll zoom out, and i'll point out what are the characteris-, characteristics of our solution which make it a dynamic programming algorithm. then, armed with both a sort of formula for developing the dynamic programming algorithms, as well as a concrete instantiation, we'll move onto a number of further, and in general harder applications of the paradigm. indeed, even more than usual, the dynamic programming paradigm takes practice to perfect. in my experience, students find it counterintuitive at first, and they often struggle to apply the paradigm to problems that they haven's seen before. but here's the good news, dynamic programming is relatively formulaic, certainly more so than our recent study of greedy algorithms, and it's something that you can get a hang of. so with sufficient practice, and that's exactly what i'll be giving you in the next couple of weeks, you should find yourself with a powerful and quite widely applicable new tool in your programmer tool box. so let me introduce you to the concave, concrete problem we're going to study over the next few videos. it's a graph problem but a very simple graph problem. in fact, we're going to restrict our attention merely to path graphs. that's graphs that consist solely of a path on some number n of vertices. the only other part of the input is a single non-negative number per vertex. we're going to call these weights. for example here's a path graph on four vertices, and let's give the vertices the weights one, four, five, and four. the responsibility of the algorithm is going to be to output an independent set. what that means is a subset of the graph's vertices, so that no two vertices are adjacent. so in the context of a simple path graph, it just means you gotta return some of the vertices and always avoiding consecutive pairs of vertices. so when you have a path of four vertices, examples of independent sets include the empty set, any single vertex, vertices one and three, vertices two and four, and vertices one and four. you could not, for example, return vertices two and three. because those were adjacent. that is forbidden. now, to make this interesting, we're going to want just, not any old independent set, but the one whose sum of vertex weights is as large as possible. that's the max weight independence set problem. so what i'm going to do next is use this concrete problem as an opportunity to review the various algorithm design paradigms that we've seen so far. along the way we'll see that none of them actually work very well for this problem. and that's going to motivate us to devise a new approach, and that approach is going to turn out to be dynamic programming. . so there's always our standard punching bag, brute force search. this would entail iterating through all of the independent sets and remembering the one with maximum total weights. of course it's correct, no question about that, but as usual this would require exponential time. even in just a path graph, the number of independent sets is exponential in the number of vertices, n. . so what other algorithm design paradigms do we know? well we just finished a big segment on greedy algorithms, we could certainly think about that. you know, pretty much most problems it's easy to propose greedy algorithms, and this one's no exception . i think the most natural greedy algorithm you might try to use to compute a maximally independent set would be well. what's the myopic decision? well, you want to get as much weight overall. so, in each step, you want to just pick the vertex with the highest weight that you haven't already chosen. now, of course you have to worry about feasibility. remember, we're not allowed to output adjacent or consecutive vertices. so, if any vertex is ruled out by adjacency, we ignore it. and amongst those that preserve feasibility, we include the highest weight one in our set so far. well, let me redraw the four node path graph we had in the last slide and let me ask you. what would this greedy algorithm compute on the four node path, and how does that compare to the optimal solution, the independent set with the maximum total weight? so the correct answer is the second one. let's see why. so let's start with the optimal solution, the maximum independence set. remember independence sets are forbidden from choosing adjacent or consecutive vertices. so in this case the only sensible solutions to consider are the first and third vertex, the second and fourth or the first and fourth. of these, the best is the second and fourth for a total of eight. so what about the greedy algorithm? well, you know, we just had this period of time where we got really spoiled with the success of greedy algorithms. especially the minimum span entry problem. but let me remind you, you know, greedy algorithms, you know, they're often good heuristics. they're often not guaranteed to be correct. and so i'm happy to have this opportunity to quickly remind you again, of that drawback of greedy algorithms. they're quite frequently not correct. so this is another such case, so what will the greedy algorithm do? well, it begins by picking the max weight vertex over all. so that would be this vertex with weight five. that unfortunately blocks the algorithm from picking either of the two vertices that has weight four. the only remaining option that preserves feasibility is to pick. the vertex of weight one. so that gives us an indepenedent set of weight six. so this greedy algorithm is not correct. you could of course try to devise other types of algorithms, but i don't know of any greedy approach that will actually solve this problem optimally. so that's a bummer, but we still haven't exhausted our algorithm design paradigms. remember, you know, we learned this quite powerful divide and conquer approach early on in part one of this class. and you know, it seems like it could work here. we had all these successful applications where the input was an array. we broke it into two halves. we were cursed on both sides and combined the results. and here, you know, path graphs don't look so different than an array of numbers. so the obvious approach for divide and conquer is to break the path into two paths, each of half the length of the original, recursively compute a maximum weighted independent set of each, and then somehow combine the results. with the issues with the divide and conquer approach are already apparent, just in our simple four vertex example. so if we recurse on the left half, that is the first two vertices, and we compute a max weight independence set, that's just going to be the second vertex by itself. and if we independently recurse on the right hand side, on the vertices three and four, the maximum weight independence set on right half is going to be the vertex of weight five. and now when we, the recursion's complete and we get our sub-solutions back, we have the second vertex and we have third vertex, but the problem is the union of those two solutions conflicts. right? we cannot simultaneously output the second and third vertices. those are consecutive, those are adjacent, and that's not allowed. moreover, you know, in a four note graph. it's sort of easy to see how to repair this conflict. but in a big graph with say thousands of nodes, if you have a conflict right where the two sub-problems meet, it is not at all obvious how you would quickly fix that and get a feasible and optimal solution to the original problem. now in some sense the divide and conquer paradigm is more powerful or better suited for this problem than the greedy approach, in that i do know of divide and conquer algorithms that could solve this problem optimally that run in quadratic time. but doing better than that in a divide and conquer matter seems quite challenging. and the dynamic programming based algorithm we'll develop will solve the problem in linear time. that's coming up next. 
so, having out iterated through all of our algorithm design paradigms and noticing that none of them seem to work very well for computing the maximum weight independent set of a path graph, we're going to develop some ideas for a new paradigm. and the key approach in this new paradigm is to first reason about the structure of an optimal solution. what i mean by this is we're going to seek out statements of the following form. the optimal solution, whatever it may be, has to possess a certain form. it has to have been built up from an optimal solution to a sub-problem in a prescribed way. so in fact, in much of our discussion of both divide and conquer and greedy algorithms, this kind of reasoning was implicit. with dynamic programming, we're going to make it systematic. for example, implicit in the correctness of many of divide and conquer algorithm is the fact that an optimal solution to the whole problem has to be expressible, has to be constructable in a prescribed way from solutions to smaller sub-problems. so, what's the motivation for doing this thought experiment, trying to understand what the optimal solution could possibly look like? well, the plan is we're going to narrow the possible candidates for the optimal solution down to a relatively small set of candidates. with a small set, we can get away with brute force search to pick the best one. so, one lesson you learn once you get good at dynamic programming, is that it's not at all circular to reason about the very object that you're trying to compute. remember, the goal here is to devise an algorithm to compute an optimal solution. and now, i'm telling you to do a thought experiment as if you had already computed it, as if i had handed it to you on a silver platter. but that kind of daydreaming can be very productive. and thinking hey, what if i did have an optimal solution? what could i say about it? what would it look like? observations of that form can actually light up a trail to the computation of that exact object and we'll see that in the next couple videos. alright so that's enough loft, lofty philosophy for now, lets get concrete. okay, so we've got this path graph, the vertices have weights. we want the max weight independent set. let's again do this thought experiment. what if someone handed to us, what could we say about it's structure? we'll be using the following notation when we reason about this maximum weight independent set. s denotes the vertices in that optimal solution, in that max weight independent set and we're going to let v sub n denote the right most, the final vertex of the input graph. so, here's a content-free statement. this last vertex of the path, v sub n. either it's an s or it isn't. so, that's going to give us two cases, when we reason about the optimal solution. let's start with the case where vn is excluded from the optimal solution capital s. so, let's let g prime denote the path graph you get by plucking off vn, plucking off the right-most vertex from the original graph g. so, let's make a couple of trivial observations. so, first of all, this set, capital s, it's an independent set in g. it doesn't include the last vertex. so, we can regard this set as equally well as an independent set of the smaller graph g prime. if it didn't contain consecutive vertices in g, nor does it in g prime. but actually, we can say more. not only is s any old independent set in g prime. it has to be an optimal, that is, maximum weight independent set in g prime. why? well, if there was something better than s in g prime, we could take that exact same independent set s star regarded as an independent set in g and, of course, it would still be better than s in g. but that contradicts our assumption that s was a max weight independent set in g. so summarizing, if the max weight independent set s of the original path graph g does not include the right-most vertex, it can be very simply described in terms of an optimal solution to a smaller sub-problem. it literally is a max weight independent set of g prime, the path graph with one fewer vertex. alright. so, case one is a warm up for case two, which is similar but slightly more complicated. now, let's assume that the maximum independent s does, in fact, use the right-most vertex, vn. now, by the definition of an independent set, no two consecutive, no two adjacent vertices can be chosen. so, by virtue of choosing, choosing v sub n, the right-most vertex s, in this case, absolutely cannot include the penultimate vertex v sub n - 1. so, we want to know by g double prime, the path you get from g by plucking off both of the right-most two vertices. so now, let's do our best to mimic the argument in case one. in case one, we said well, s has to also be an independent set of g prime. now, here that doesn't make sense, right? here, s contains the last vertex so we can't talk about it even being a subset of any smaller graph. however, if we think about s except for the last vertex of v sub n, s with vn removed is an independent set, in fact, of g double prime, because remember, s can't contain the second to last vertex. and once again, just like in case one we can say something stronger, s with vn removed is not any old independent set of g double prime, it actually must be an optimal one. it must have maximum possible weight. the reasoning is similar. suppose s with vn removed was not the best possible independent set in g double prime. then there's something else called an s star which is even better, has even bigger weight. how do we get a contradiction? well, if we just add vn to this even bigger independent set s star that lies in g double prime, we get a bonafide independent set of the entire graph g with overall weight even bigger than that of s, but that contradicts the optimality of s. so, for example you could imagine that this reported optimal solution capital s had total weight 1,100 in two parts, it had 1,000 weight coming from vertices in the g double prime, but it also had v sub n which had weight 100 on its own. and so now, in the contradiction, you say, well, suppose there was an independent set that had even more than 1,000 weight just in g double prime, say, 1,000 and 50. well then, we just add this last vertex v sub n to that, we get an independent set with weight 1,150 and the original graph g. but that contradicts the fact that s was supposed to be off more with weight nearly 1,100. so, notice that the reason were using the graph g double prime in this argument is to make sure that no matter what s star is, no matter what this independent set of g double prime is. we can just add v into it blively and not worry about feasibility, right? so, the right-most vertex s star could possibly possess is the third to last vertex vn - 2. so, there's no worries about feasibility when we extend it by adding in the right-most vertex v sub n. so, to make sure you don't lose the forest for the trees, let me just point, let me remind you what our high level plan was, and let me point out that we actually just executed that plan quite successfully in this problem. the plan was to narrow down the candidates for what the optimal solution could be. to reason about the form of the optimal solution and argue that it has to look in a particular way. what did we just prove on the previous slide? we said that the optimal solution actually can only be one of two things. either it excludes the final vertex and it is nothing more than the max weight independent set of g prime, or if it includes the right-most vertex than it must be, a maximum weight independent set frp, g double prime augmented with this last vertex v sub n. there's only two possibilities for what the optimal solution could possibly look like, in terms of optimal solutions of smaller sub problems. so, a corollary of this is that if a little birdie told us which case we were in, whether or not v sub n, v sub n was in the optimal solution, we could just complete this by recursing on the appropriate sub-problem. the little birdie tells us the optimal solution doesn't have vn we just recurse on g prime. if the little birdie tells us v sub n is in the optimal solution, we recurse on g double prime and then add v sub n to the result. now, of course, there is no little birdie and we have no idea whether this right-most vertex is in the optimal solution or not. but hey, there is only two possibilities, right? here is an idea. maybe it seems crazy, but why not try both possibilities and just return whichever one is better? why do i suggest that maybe this is crazy? well, if you stare at this and you think about it and you think about the ramifications of trying both possibilities as you recursed down the graph, this may start feeling a little bit like brute force search. and, in fact it is. this is just a recursive organization of a brute force search. nevertheless, as we'll see in the next video, if we're smart about eliminating redundancy, we can actually implement this idea in a linear time. 
so now that we've done some work with our thought experiment, understanding exactly what the optimal solution, the maximum rate independent set had to look like in the path graph, let's turn that work into a linear time algorithm. let me quickly remind you, the bottom line from the previous video. so we argued two things. first of all, if the max weight independent set of a path graph happens to exclude the rightmost vertex, then it has to in fact be a max weight independent set with the smaller graph g prime, obtained from g by plucking off the rightmost vertex. if on the other hand a max weight independent set does include the rightmost vertex v sub n, then if you take out v sub n and look at the remaining part of the independent set, that has to be max weight among the smaller graph g double prime, defined by plucking the two rightmost vertices off of the original graph g. ergo, if we happen to know, if a little birdie told us which of these two cases we were in we could recursively compute the optimal solution of either g prime or g double prime as appropriate and be done. if we recurse on g prime we just return the result. if we recurse on g double prime we augment it by v sub n and return the result. now, there is no little birdie, we don't know which of the two cases, we're in. so we're concluded the previous video by proposing just trying both case. so let's write down what that proposed algorithm would look like before we take a step back and critique it. so, the good news about this algorithm is it is correct. it's guaranteed to return the maximum weight independence set. so, how would you prove this formally? well, it would be by induction, in exactly the same way we proceeded with divide and conquer algorithms. and for the same reason, i'm not going to talk about the details here. if you're interested, i'll leave it as an optional exercise to write this out, formally. but intuitively, the inductive hypothesis guarantees correctness of our recursive calls. so computing the maximum weight solution in either g prime or g double prime, and then, in the previous video, the whole point of that, in effect, was arguing the correctness of our inductive step, given the correct solution to the sub-problem, we argued what has to be the optimal way to extend it to a solution, to the original graph, g. the bad news, on the other hand, is that this algorithm takes exponential time. it's essentially no better than brute force search. so while the correctness of this kind of recursive algorithm, follows the template of divide and conquer pretty much exactly. the running time is blown up to exponential. and the reason for that difference is, in our divide an conquer algorithms, think of merge sort as a canonical example, we made a ton of progress before we recursed. right? we threw out half of the array, 50% of the stuff before we bothered with any recursion. how much progress are we making in this algorithm? well, very little. it's positive progress, but very small. we throw out either one or two vertices out of maybe this graph with say a million vertices before recursing. so we're branching by a factor two and making very little progress before each branch. that's why we give this exponential running time rather than something more in the neighborhood of n log n. so this brings us to the following question. this is an important question. i want you to think about it carefully before you respond. so we have this exponential time algorithm, it makes an exponential number of recursive calls. each recursive call is handed some graph, for which it's responsible for computing the maximum-weight independence set. the question is this. over all of these exponentially many different sub-problems, each of which is passed some graph as a sub-problem, how many distinct, how many fundamentally different sub problems are ever considered across these exponentially many recursive calls? so the answer to this question, and the key to unlock the crazy efficiency of our dynamic programming implementation, is b. so despite the fact that there's an exponential number of recursive calls, we only ever solve a linear number of distinct sub-problems. in fact, we can explicitly say what are the different sub-problems it could solve throughout the recursion. what happens before you recurse? well you pluck vertices from the graph you were given off from the right. maybe you pluck off one vertex that's in the first recursive call, or in the second recursive call you pluck off two vertices, but both from the right. so an invariant maintains throughout the recursion is that the sub-problem you're handed was obtained by successive plucking off of vertices from the right part of the graph, from the end of the, end of the graph. so, however you got to where you are, whatever the sequence of recursive calls led to where you are now, you are guaranteed to be handed a prefix of the graph. the graph induced by the first i vertices, where i is some number between zero and n. so therefore there's only a linear number of sub-problems you ever have to worry about, the prefixes of the original input graph. from this, we conclude that the exponential running time of the previous algorithm arises solely from the spectacular redundancy of solving exactly the same sub-problem from scratch, over and over and over and over again. so this observation offers up the promise of a linear time implementation of this algorithm. after all, there's no need to solve a sub-problem more than once. once you've solved it once you know the answer. so an obvious way to speed up this algorithm, to speed it up dramatically is to simply cache the results of a sub-problem the first time you see it. then you can look it up in some array, constant time, at any point later on in the algorithm. there is a word for this, i won't really use it in this class, but just so you that know what it is, it's called memoization. so in case this is a little vague, what i mean is you would have some array, some global array, indexed one to n or maybe zero to n with the semantics that the ith entry of this array, is the value of the solution of the ith sub-problem. now when you do a recursive call and you're handed the first i vertices of the graph, and again remember that we know that the sub-problem has to look like the first i vertices of the graph for sub i. you check the array, if it's already been filled in, if you already know the answer, great. you just return it and count the time, you don't bother to resolve. if this is the first time you've ever seen. this sub problem then you recurse and you solve it, as as we saw, as we suggested in the previous slot. so with this simple memoization fixed, this action, this algorithm is linear time. the easiest way to see that, and actually in fact a better implementation, is to go away from this recursive top down paradigm, and instead implement the solution in a bottom up way. so solving sub problems in a principled way from the smallest to the original one, the biggest. so a little bit more precisely, let me use the notation g sub i to denote the sub graph of the original graph, consisting of the first i vertices. so we're going to again going to ha-, going to have an array with the same semantics as in the recursive version. the ith entry denotes the solution to the ith sub-problem. that is the max rate independent set of g sub i, and the plan is to populate that bottom up, that is from left to right. so we'll handle the edge cases of the first two entries of, of this array specially g sub zero would be the empty graph, so there's no independent set. so lets define the max weight, independent set of the map graph, to be zero. and, if you graph in g one, where the only vertex is v one, the max weight independent set is obviously that one vertex. remember weights are not negative. so our main four loop just builds up solutions to all of the sub-problems in a systematic way, going from smallest graph, g sub two up to the biggest graph, the original one, g sub n. and when we consider the graph g sub i, how do we figure out what the max weight independence set is of that graph? well now we use, completely directly, the work that we put in the previous video. the previous video told us what an optimal solution has to look like. and it has to have one of two forms. we know, we proved, either, the maximum independent set of g sub i. excludes the last vertex v sub i, and then is merely an optimal solution of the graph g sub i minus one. if it's not that, there is nothing else it can be, other than including the last vertex v sub i. and being an optimal max weighted independent set for the graph g sub i minus two. we know its one of those two things. we don't know which. we do brute force search among the two possibilities, and that gives us the optimal solution for the ith sub problem. crucially, when we need to do this brute force search for the ith sub problem, we already know. we've already computed the optimal solutions to the smaller sub problems that are relevant, those can be looked up in constant time, and that's what makes this, each iteration of this four loop run in constant time. so we've done a fair amount of work to get to this point, but, after seeing that the greedy algorithm design paradigm failed us. the divide-and-conquer algorithm design paradigm was inadequate. brute force search is too slow. with this, as we'll see, dynamic programming algorithm, we now have a one line solution, effectively, to the max weight independent set problem in path graphs. pretty cool. what's the run time? well this is probably the easiest algorithm is run time we've ever had to analyze. obviously, it's linear time, constant time per each iteration of the four loop. why is the algorithm correct? well it's as same as our recursive algorithm. it makes exactly the same decisions. the only difference is it doesn't bother with the spectacular redundancy of resolving sub-problems it's already solved. again if you wanted to prove it by scratch, it would just be a straight forward induction, like in our divide and conquer algorithm, the recursive calls are correct by the inductive hypothesis. the inductive step is justified by the case analysis of the of the previous video. 
so we now have an algorithm, a very elegant one, that solves the weighted independent set problem in path graphs. moreover the algorithm runs in linear o of n time, clearly the best possible. but before we do a victory lap i want to point out that there's something this algorithm doesn't do that you might want it to do, mainly hand you the actual optimal solution, not merely the value of that optimal solution. so the point of this video is to show you how we can reconstruct an optimal solution given the table-filling algorithm from the previous video. so we just write that algorithm back down. it's so short, it won't take me too long. now when this algorithm completes, what do we have in our hands? we have an array, and this array is filled with numbers. in particular, in the last entry in the array is a number like 184 so that's great. that tells us that the maximum weight that any independence set possesses is 184. but, in many applications, we're going to want to know not just that information but actually which vertices constitute that independence set with total weight of 184. so, perhaps the first hack that comes to mind to address this issue is to augment the array, so that each entry stores not just the value of an optimal solution to the sub-problem produced by the graph g sub i, but also an actual set of vertices achieving that value. and i will leave it for you to if you want, rework the previous pseudo codes so that when you fill in a new entry, you fill in not just the value of an optimal solution given solution to the previous sub-problems but infact also the solution itself. this hack however is not generally how things are done in dynamic programming. it unnecessarily wastes both time and space and a much smarter approach is to reconstruct from the filled in table an optimal solution as needed. so if you think about it, it's kind of cool that this is even possible, that our one line algorithm doesn't cover its tracks, that it leaves enough clues for us as detectives to go back and examine and reconstruct what the optimal solution is. the following key point articulates exactly why this is indeed possible. so the starting point of this observation is the correctness of our algorithm, our one line algorithm. which of course we established in the previous video. and by correctness i mean what's guaranteed that this algorithm will populate each entry of your array correctly. the number in the ith entry is indeed the maximum weight of independent set in the graph g i. so remember our thought experiment about what the optimal solution could possibly look like. we concluded it could only be one of two things, and we really wound up wishing we had a little birdie that could tell us whether or not the rightmost vertex v.sub.n was in the optimal solution or not. if we knew which case we were in we could just recursively compute the remainder of the solution from a graph that has either one or two fewer vertices. so here's the point, this filled in table, that's our little birdy. here's what i mean. but what, what's, what's the reasoning that our algorithm goes through to fill in this last entry of this array, and don't forget, we've already proven that our algorithm's correct, we did that in the last video? well, it does a comparison between the two possible candidates vying for the optimal one. on the one hand it commutes the case one solution that looks up the optimal value of a solution for the graph with one fewer vertex and it compares that to the case two solution including v n the last vertex and adding that to an optimal solution with two fewer vertices. and in this max operator in the line of code it's explicitly comparing which is better, case one. the solution which excludes b sub n, or case two, the solution which includes b sub n. so, whichever one of those was the winner, whichever one of those cases was used to fill in that last entry. that exactly tells us whether or not b sub n is in the optimal solution. if we use the first case, that means b sub n is not in the optimal solution, it gets excluded. if the second case was the winner, then we know b sub n is in the optimal solution, because that was the winner. if we have a tie, then there's an optimal solution either way. there's one that includes bn and there's one that excludes bn, so those are the tracks in the mud left for us by the forward direction of the for-loop. we can just go back and look at which case was used to fill in each entry of the array. again, for the ones that used case one, that corresponds to excluding the current vertex; for those that used case two to fill in the entry, that corresponds to including that vertex, in the solution. so the reconnect structure algorithm will take as input the filled in array that was generated by our one line algorithm on the previous slide. and what it's going to do, it's going to trace through this array from right to left. and at each step of the main loop, it's going to say, it's going to look at the current entry. and it's just going to compute explicitly which of the two candidates were used to fill in this array entry. if you want, you can also cache the results of these comparisions on the forward pass. that's an optimization that will be useful later for harder problems. but for now if you want, you can just think about redoing the comparision thing. hey, you know, their were two possible. more ways that we could have filled in this entry, let's just check which of the two were used. so if in fact the preceding array entry is at least as large as the one from two back plus the weight of the current vertex, that corresponds to case one winning, to the sub-solution that excludes the current vertex being better than the one that includes it. so in that case we just skip the current vertex v sub i and we decrease the array index by one in our scan. if the other case wins, that is, if we fill in the current entry, if an optimal solution to the current graph g sub i comprises the current vertex of e sub i plus the optimal solution to the graph with two fewer vertices. in this case we know we'd better include v sub i. that's part of an optimal solution to the current sub-problem. moreover, that's the case where we need to look up the optimal solution with two fewer vertices. so, we include the current vertex and we decrease the array and index by two. so formally we have a correctness claim which is that the final output s return by the reconstructing algorithm is, as desired, a maximum weight independent set of the original graph g. we've already talked about all the ingredients necessary for a formal proof, for those of you who are interested. of course, it precedes by induction and, in the inductive step, you use the same case analysis we've been using over and over again. the optimal solution at any given point, it has only two possible candidates. the algorithm explicitly figures out which of the two it is and that is what. triggers whether or not to include or exclude the current vertex. the running time, it's even easier. we have a wild loop that runs in most an iterations. we do constant work in each of the iterations. so just like the forward pass, this backwards pass is really just a single scan through the away. it's going to be lightning fast linear time. 
hey, so guess what? we just designed our first dynamic programming algorithm. that linear time algorithm for computing the max weight independence set in a path graph is indeed an instantiation of the general dynamic programming paradigm. now i've deferred articulating the general principles of that paradigm until now because i think they are best understood through concrete examples. now that we have one to relate them to, let me tell you about these guiding principles. we will in the coming lectures see many more examples. so the key that unlocks the potential of the dynamic programming paradigm for solving a problem is to identify a suitable collection of sub-problems. and these, sub-problems have to satisfy a number of properties. in our algorithm for computing max weight independent sets and path graphs, we had n plus one sub problems, one for each prefix of the graph. so formally, our ithi sub problem in our algorithm, it was to compute the max weight independent set of g sub i, of the path graph consisting only of the first i vertices. so the first property that you want your collection of subproblems to possess is it shouldn't be too big. it shouldn't have too many different subproblems. the reason being is, in the best case scenario, you're going to be spending constant time solving each of those subproblems, so the number of subproblems is a lower bound than the running time of your algorithm. now, in the maximum independent set example, we did great. we had merely a linear number of subproblems, and we did indeed get away with a mere constant work for each of those subproblems, giving us our linear running time bound overall. the second property you want and this one's really the kicker, is there should be a notion of smaller subproblems and larger subproblems. in the context of independence sets of path graphs this was really easy to understand. the subproblems were prefixes of the original graph and the more vertices you had, the bigger the subproblem. so in general, in dynamic programming, you systematically solve all of the subproblems beginning with the smallest ones and moving on to larger and larger subproblems. and for this to work, it better be the case that, at a given subproblem. given the solutions to all of the smaller sub problems it's easier to confer what the solution to the current sub problem is. that is solutions to previous sub problems are sufficient to quickly and correctly compute the solution to the current sub problem. the way this relationship between larger and smaller subproblems is usually expressed is via recurrence and it states what the optimal solution to a given subproblem is as a function of the optimal solutions to smaller subproblems. and this is exactly how things played out in our independent set algorithm. we did indeed have a recurrence. it just said, that the optimal value, the maxwood independence head value for g sub i. was the better of two candidates. and we justified this using our thought experiment. either you just inherit the maximum independence set value from the preceding sub problem from the i-1 sub problem. or you take the optimal solution from two sub problems back from gi-2. and you extend it by the current vertex, v sub i. that is, you add the [inaudible] vertices weight to the weight of the optimal solution from two sub problems back. so this is a pattern we're going to see over and over again. we'll define subproblems for various computational problems. and we'll use re, recurrence to express how the optimal solution of a given subproblem depends only on the solutions to smaller subproblems. so just like in our independent set example once you have such a recurrence it naturally leads to a table filling algorithm where each entry in your table corresponds to the optimal solution to one sub-problem and you use your recurrence to just fill it in moving from the smaller sub-problems to the larger ones. so the third property, you probably won't have to worry about much. usually this just takes care of itself. but needless to say, after you've done the work of solving all of your sub problems, you better be able to answer the original question. this property is usually automatically satisfied because in most cases, not all, but in most cases the original problem is simply the biggest of all of your subproblems. notice this is exactly how things worked in the independent sets. our biggest subproblem g sub n was just the original graph. so once we fill up the whole table, boom. waiting for us in the final entry was the desired solution to the original problem. so i realize, you know this is a little abstract at the moment. we've only have one concrete example to relate to these abstract concepts. i encourage you to revisit this again after we see more examples and we will see many more examples. something that all of the forthcoming examples should make clear is the power and flexibility of a dynamic programming paradigm. this is really just a technique that you have got to know. now when you're trying to devise your own dynamic programming algorithms, the key, the heart of the matter is to figure out what the right sub problems are. if you nail the sub problems usually everything else falls into place in a fairly formulaic way. now if you've got a black belt in dynamic programming you might be able to just stare at a problem. and intuitively know what the right collection of subproblems are. and then, boom, you're off to the races. but, of course, you know? for white belts in dynamic programming, there's still a lot of training to be done. so rather, in the forthcoming examples. rather than just plucking the subproblems from the sky. we're going to go through the same kind of process that we did for independent sets. and try to figure out how you would ever come up with these subproblems in the first place? by reasoning about the structure of optimal solutions. that's a process you should be able to mimic in your own attempts at applying this paradigm to problems that come up in your own projects. so, perhaps you were hoping that once you saw the ingredients of dynamic programming, all would become clearer why on earth it's called dynamic programming and probably it's not. so, this is an anachronistic use of the word programming. it doesn't mean coding in the way i'm sure almost all of you think of it. it's the same anachronism in phrases like mathematical or linear programming. it more refers to a planning process, but you know for the full story let's go ahead and turn to richard bellman himself. he's more or less the inventor of dynamic programming, you will see his bellman-ford algorithm a little bit later in the course. so he answers this question in his autobiography and he's says, he talks about when he invented it in the 1950's and he says those were not good years for mathematical research. he was working at a place called rand, he says we had a very interesting gentleman in washington named wilson who was the secretary of defense. and he actually had a pathological fear and hatred of the word research. i'm not using the term lightly. i'm using it precisely. his face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. you can imagine how we felt then about the term mathematical. so the rand corporation was employed by the air force, and the air force had wilson as its boss, essentially. hence, i felt i had to do something to shield wilson and the air force from the fact that i was really doing mathematics inside the rand corporation. what title, what name could i choose? in the first place i was interested in planning and decision making, but planning, it's not a good word for various reasons. i decided therefore to use the word, programming. dynamic has a very interesting property as an adjective, in that it's poss, impossible to use the word dynamic in a pejorative sense. try thinking of some combination that will possibly give it a pejorative meaning. it's impossible. thus i thought dynamic programming was a good name. it was something not even a congressman could object to so i used it as an umbrella for my activities. 
so in these next two videos we'll give our second application of the dynamic programming paradigm. it's to a quite well known problem, it's called the knapsack problem. and we'll show how following the exact same recipe that we used for computing independent sets in path graphs leads to the very well known dynamic programming solution to this problem. so let's jump right into the definition of a knapsack problem. so the input is given by n items. so each item comes with a value, v sub i, the more the better for us, and also a size, w sub i. we're going to assume that both of these are non-negative, for the sizes we're going to make additional assumption that their integral. in addition to these two n numbers, we are given one more which is called the capacity, capital w, again we'll assume this is non-negative and an integer. the role of these integrality assumptions will become clear in due time. the knapsack problem, the responsibility of an algorithm is to select a subset of the items. what do we want? we want as much value as possible, so we want to maximize the sum of the values of the items that we select. so what prevents us from picking everything? well, the sum of the sizes of the items that we pick have to total to at most the capacity capital w. now i could tell you some cheesy story about a burglar breaking into a house with a knapsack with a certain size capital w and wants to make away with sort of the best loot possible. but that would really be doing a disservice to this problem which is actually quite fundamental. this problem comes up quite a bit, especially as a subroutine in some larger task. really, just whenever you have sort of a budget of a resource that you can use, and you want to use it in the smartest way possible, that's basically the knapsack problem. so you can imagine how it would come up in a lot of contexts. so let's now execute a recipe for developing a dynamic programming algorithm. remember, the key to any dynamic programming solution is to figure out what is the right set of subproblems. and we're going to arrive at the sub problems for the knapsack problem just as we did for max weight independent sets by doing a thought experiment about the optimal solution, and understanding what it has to look like in terms of solutions to smaller subproblems. the bottom line of this thought experiment, the deliverable will be a recurrence. it will be a formula which tells us how the optimal value of one subproblem depends on the value of smaller subproblems. so to begin the thought experiment, fix an instance of knapsack and let capital s denote an optimal solution, a max value solution. we began our previous thought experiment with a content free statement that the final vertex of a path is either in the optimal solution or it's not. now what is the analog of the right most vertex in this knapsack problem? well, unlike a path graph, there's no intrinsic sequentiality to the items that we're given. they're just an unordered set. but it's actually useful to think of the items as sort literally ordered one, two, three, all the way up to n, and then the analog of the right most vertex is just the final item. so the content free statement we're going to work with here is either the last item belongs to the optimal solution capital s, or it doesn't. we'll again start with the easy case when it doesn't. what we argued in the path graph problem was that the max weight independent set in the analogous case one has to be optimal if we just delete that rightmost edge from the graph. so here, the analogous claim is that this set s should still be optimal if we delete the final item n from the knapsack instance. the argument is the exact same near trivial contradiction. if there was a different solution, s star, amongst the first n minus 1 items with a weight even bigger than that of s, we could regard this equally well as a superior knapsack feasible solution back with all n of the items, but that contradicts the purported optimality of s. so let's go through the slightly trickier case two together using a quiz. so suppose the optimal knapsack solution does in fact make use of this final item n. now we want to talk about this being somehow composed of an optimal solution to a smaller subproblem. so if we're going to delete the last item, then we can't talk about s, because s has the last item, so we need to remove the last item from s before we talk about its optimality. that's analogous to back in the independent set problem, we removed the right most vertex from the optimal solution before talking about its optimality in a smaller subproblem. so the question then is if we take capital s, the optimal solution, we remove item n, in what sense is the residual solution optimal? put differently, for what kind of knapsack instance, if any, is that an optimal solution for? all right, so the correct answer is c. so back in the independent set problem what we said is if we remove the right most vertex, then what's left is optimal for the residual independent set problem we get by plucking off the right most two vertices. here when we remove the nth item from the optimal solution s, the claim is what we get is optimal for the knapsack problem involving the first n-1 items and a residual knapsack capacity of w-w sub n. so the original knapsack capacity with space reserved, or deleted, for the nth item. so before i give you a quick proof, let me just briefly explain why a couple of the other answers are not correct. so first of all, answer b, i hope you could rule out quickly. it just doesn't type check. so capital w, that's the knapsack capacity, so that's in units of size. little v sub n, that's the item's value. so that's in dollars, so it doesn't really make sense to talk the difference between those two. they're just apples and oranges. part d, if you were worried about feasibility at any point. so, if you take capital s and you remove the nth item, what have you done? you have taken a set of items whose size was at most capital w by feasibility of s, and you've removed an item with size wn from it. so, what remains has total size at most capital w minus little w sub n. so, s minus n wouldn't be feasible for this reduced to this residual knapsack capacity w- w sub n. a much more subtle point is part a, that's a very natural one to guess. that turns out to not be correct. so it turns out there might be smarter ways of using the first n-1 items, than s minus item n, if you had a full knapsack capacity of w to work with. so that's a subtler point. and it's a good exercise for you to actually convince yourself that a is wrong. that there's no reason that when you take out item n from s and you still keep using the original knapsack capacity that this has to be optimal. that's not going to be true. all right, so why is capital c correct? well, this is going to have the same spirit of case two of our weighted independent set thought experiment. so let me give you the proof. the proof is going to be the usual contradiction analogous to case 2 of our argument in the weighted independent set problem. so suppose there was something better than s with n removed with the residual capacity, w- wn. call that supposedly better solution s*, so what can we do to get a contradiction? well, let's just take s* which involves only the first n-1 items. let's add item n to it since s* has total size and most w-w sub n and item n has size w sub n, the result has total size of w so it's a feasible solution to take s* and extend it by item number n. and if s* had more value than s with n removed, then s* with n included has more value than s. so for example, if s had total value 1,100, 100 of which was coming from the nth item, then s with n removed had value 1000. if s* was better, it had a value 1,050. well then we just put n back in and it has value 1150 which contradicts the purported optimal value of s* which had total value merely 1100. so notice what's going on here. so in taking away w sub n for the knapsack capacity before we look at the residual problem we're in effect reserving a buffer for item n if we ever need it. that's how we know we're feasible when we stick n back into the solution s*. that's analogous to deleting the penultimate vertex of the path, again as a buffer to ensure feasibility when we include the nth vertex back in independent set problem. so what was the point of this whole thought experiment which we've now completed? well, again the point was to say the optimal solution, whatever it is, it has to have only one of two forms. we've narrowed the list of candidates down to two possibilities. either you just inherit the optimal solution with one less item in the same knapsack capacity, or you look at the optimal solution with one less item and less knapsack capacity by w sub n, and you extend that by item n. but those are the only two possibilities. so again, if we only knew which of these two cases were true. if we only knew whether or not item n was in the optimum solution or not, in some sense we could recursively compute the rest of the solution. so just as that was enough to get us going with a dynamic programming algorithm for weighted independent sets, so it goes with knapsack, as i'll show you on the next video. 
so now that we understand the way in which optimal solutions must be composed of optimal solutions to smaller subproblems. we're well positioned to turn that into a recurrence, and then a dynamic programming solution for the knapsack problem. to compile our discussion from last time into a recurrence, let me just introduce a little notation. so by capital v sub i x, what i mean is the maximum value you can get of a solution that satisfies two constraints. first of all, it should make use only of the first i items. second of all, the total size of the items that you pick should be at most x. the analogue of this notation in the independent set problem was g sub i, the graph with the first i vertices. we're now using two indices instead of one because sub-problems have two different ways in which they can get smaller. recall case two of our though experiments, when we look at a smaller sub-problem there was both one less item and there was also less capacity. so when thinking about a sub-problem we need to keep track of both. we need to keep track of how many items are we allowed to use and we need to keep track of what's the total size that we're allowed to use. so let's express our conclusion to the last video in this notation. what did we figure out last video? we figured out that the optimal solution has to have one of two forms. either you just inherit the optimal solution from that tech instance with one less prop, one less item, that was the first case, or you take the optimal solution to the sub-problem which has one less item and less capacity by the weight of the current item and you extend it to an optimal solution for the current sub-problem using the ith item. so v sub i, x. the optimal value for the current sub problem. it's the better of the two possibilities, so we take a max. the first possibility is just the optimal solution to the, with the same capacity in one pure item, we're using that solution. the second one is you commit to using item i, you gain value vi. and you combine that with the optimal solution using the first i minus one items in the reduced capacity of x minus the weight of the current item. so one quick edge case. if the weight of the ith item is bigger than our permitted capacity x, then of course we can't use it, and then we're forced to use the first case. we're forced to be in case one. this then is our recurrence for the knapsack problem. so that completes step one where we think about the optimal solution structure and use that to devise a recurrence. now, in the step two, we're going to precisely nail down what exactly are the subproblems we have to care about. well in our maximum waited independence set example on path graphs remember the reasoning every time we recursed every time we needed to look up a, a sub problem solution it was obtained by plucking verticies off of the right of the graph and for that reason all we ever cared about was the various prefixes of the graph. in one dimension we got something similar going on here whenever we look at smaller sub problems it's always with one fewer item we're always sort of deleting the last item before we do our look up. so again, we need to worry about all possible prefixes of the items. so sub-problems, of the first i items for all values of i. for the knapsack problem however there's a second sense in which sub-problems can be smaller. and remember case two of our thought experiment, when we want to know the optimal solution that's guaranteed to use the current item i. we have to reduce the capacity before looking up the corresponding optimal solution of a sub-problem. that is we're not just peeling off items, but we're also sort of peeling off parts of the knapsack capacity. now, here is where we're going to use our assumption, that i told you at the beginning, at our input, that sizes are all integers, and that the knapsack capacity is an integer. so, the knapsack capacity starts at capital w. every time we peel off some integer amount of capacity from it. so at all sub problems, we're going to be working with integral knapsack capacities. so therefore, in the worst case, you know, what are the various sub problems that might arise? well, perhaps, each choice of a residual capacity, 012, all the way up to the original knapsack capacity, capital w. so now we're in great shape. in step two, we figured out exactly what subproblems we care about. in step one, we figured out a formula by which we can solve bigger subproblems given the solutions of smaller ones, so all that remains now is to write down a table and fill it in systematically using the recurrence, beginning with the smaller subproblems, ending up with the largest subproblems. so here's this pseudo-code. it's again going to be very simple. in this algorithm, the array a that we're going to be filling in has two dimensions, in contrast to one dimension in the wave independence set problem. that's because our sub-problems have two different indices. we have to keep track of both of which set of items we're allowed to use, and also what capacity we need to respect. so the two independent varables indexing sub problems forces us to have a 2d array that we're going to go through now in a double four loop. so in the init-, initialization step, we fill in all the entries, where i equals zero, where there's no items there, of course, the optimal solution value is zero, and then we just go through the sub problem systematically. when we get to a sub-problem we use the recurrence that we developed to compute the entry in that table using, of course, the entries that we've previously computed in earlier iterations. so for a given sub-problem where you're allowed to use the first i items and the residual capacity is x. what do we know from my thought experiment? this can only be one of two things. we're just going to do brute force search through the two possibilities. the two possibilities where we have case one where we just inherit the optimal solution with one fewer item, and the same capacity. or if we're going to actually use item i, then we compose that with the optimal solution from the first i items. that leaves enough space for item i. and in that case, we get the value v sub i for including this ith item. so this code doesn't quite make sense in the case where the weight of the current item, wy, is strictly bigger than the, the capacity, x. that would give us a negative array index, which, of course, is a no-no. but in that case, you know, amongst friends we just interpret it as ignoring the second case of the max. so, you know? i'm not going to write down the extra code. but what you would write down is. if wi is strictly bigger than x, then you just define a of i, x to be equal to a of ai-1, x. and one crucial point, is that, you know, by the time we need to solve subproblem with a given i and a given x, we have at our disposal the solutions to all of the subproblems that we need. in particular, in the previous iteration of the outer for loop, we computed the solutions to the two relevant subproblems for evaluating this recurrence. they're there waiting for us available for constant time lookup. so after this double four loop completes, sitting there waiting for us in a, in the entry n comma capital w, is the solution that we wanted. that's the max value solution that can use any items that it wants and that can use the entire original knapsack capacity capital w. that's what we want to return. so that's it. that's the whole dynamic programming solution for the knapsack problem. just to make sure this makes sense, in the next quiz, i want to ask you to analyze the running time of this dynamic programming algorithm. alright, so the correct answer is the second one. and the running time analysis is as straightforward as it was for the max weight independent set problem. we just count up the number of sub-problems and look at how much work we do in each. so how many sub-problems are there? where they're indexed by both i and x, there are n plus one choices for i, there are capital w plus one choices for x. so that gives us theta of n times w different sub-problems. and all we do for each is a single comparison amongst previously computed solutions. so we just do constant work per sub-problem, giving us the overall running time of n times capital w. so, a couple other quick notes, which play out in exactly the same way as for the maximum independence set problem. so i'm not going to discuss the details here. so first of all, correctness. again, it's exactly the same template as in the previous dynamic programming algorithm, and in our divide and conquer algorithms. you just go by induction on the problem size. and, you use the case analysis or a thought experiment to justify, formally, the inductive step. so this algorithm suffers from a similar drawback to the max independent set algorithm for path graphs that we looked at. namely it computes the value of an optimal solution not the optimal solution itself. it returns a number, not an actual subset of items. but, just like with the independence set problem, you can reconstruct an optimal solution from the filled in array just by tracing backward. so the intuition is that you begin with the biggest sub-problem, and you look at which of the two cases was used to fill in that entry. if case one was used, you know you should exclude the last item. if case two was used, you know you should include the last item. and also, which of those two cases tells you which sub-problem you should trace back to, to continue the process. so i encourage you to spell out the details of this reconstruction algorithm in the privacy of your own home. that'll give you some nice practice with this reconstruction aspect of the dynamic programming paradigm. 
we've now got two dynamic programming algorithms under our belt. we know how to compute weighted independence sets in path graphs. we also know a dynamic programming solution to the famous knapsack problem. but before i proceed to still more useful and famous dynamic programming algorithms let's pause for a sanity check. let's go through a worked example for that, dynamic programming algorithm for knapsack. just to make sure that everything is crystal clear. so, for reference, let me just, rewrite the key point to the knapsack algorithm. so first, we have a 2d, 2d array a. and for initialization, just whenever i equals 0. that is, you. you can't use any items at all. of course the optimal solution value is 0, and then i'm just going to rewrite the same occurrence that we had in the previous couple of videos. so you'll recall that in the main loop, when you're considering a given item a, and a residual cap-, nap sack capacity x, you take the better of two solutions. either you inherit the solution with i minus one and the residual capacity x that corresponds to not taking item i, or if you do take item i you get a credit of visa buy the value for that item. but the residual capacity drops from x to x minus the weight of item i and you look up the optimal solution for that smaller sub problem. so, for our concrete example, let's just look at a case with 4 items. an initial nap sack capacity capital w equal to 6, and the values and weights of the 4 items as follows. so, i'm going to describe the most straightforward implementation of the dynamic programming algorithm for knapsack. we're literally just going to explicitly form the 2d array a. one index for i will range between 0 and n, the other index for x will range between 0 and capital w, the original capacity. there certainly a lot of optimizations you can apply when filling in these tables. in fact, the future programming assignment will ask you to consider some. but just to make sure the basic algorithm is totally clear, let's just stick with that naive implementation for now. so we begin with the initialization and setting a0x to be equal to 0 for all x, just means we take the leftmost column corresponding to i equals 0 and we fill that in entirely with 0's. now in a real implementation there's no reason to explicitly store these 0's, you know they are there, but again to keep the picture clear let's just fill in the 0's in the left column. now we proceed to the main while loop and recall the outer for loop just increments the index i for the item that we're considering so the outer for loop is considering each of the columns in turn from left to right. now for a fixed value of i for a fixed column, in the inner for loop we consider all values of x from 0 to capital w. so that just corresponds in a given column we're going to fill in the entries from bottom to top. so we begin this double for loop with the second column i equal 1 and at the bottom x equals 0. now in general when we fill in one of these table entries, we're going to take the better of two solutions. either we just inherit the solution from the column to the left in the same row. this corresponds to skipping item i. or we add the value of the current item. to the opitimal solution that we extract from the column one to the left, but also w [unknown] rows down. so, that corresponds to reducing the residual capacity of w sub i. now, for the early subproblems in a column, where you have essentially 0 residual capacity, it's kind of degenerate. right so, if your residual capacity x is actually less than the weight of the current item you're considering, w sub i, you have no choice but to inherit the previous solution. you're actually not allowed to pack the current item i. so concretely in this example, in the first column, notice the weight of the first item is 4. so that means if x equals 0 or 1 or 2 or 3. we're actually not permitted to choose item one, we're going to run out of residual capacity. so in the first, bottommost four rows [inaudible], we're forced to inherit the solution from the column to the left. so it is the first four 0's from the left column get copied over to column with i equals 1. now, in this outer iteration of the, in the first outer iteration of the for-loop, once x reaches 4, now there's actually an interesting decision to make. we have the option of inheriting the 0, immediately to the left. or, we can take item 1, therefore getting a value of 3. and then we inherit the solution from a 0, 0. and now a000 has a 0, but were obviously happy to get this value of three. so that's going to determine the max we're going to except v sub i , plus a of 0, 0. and that gives us a three in this next table entry. by the same reasoning, we're going to fill in the uppermost rows of column one with these threes. we're, we certainly would prefer just taking the item one which we're now allowed to do. we have enough residual capacity. as opposed to just inheriting the 0 from the column one to the left. so that's how we fill in the column of i equal one. moving on to i equal two. again, so here notice that item two has a weight of three. so again, the bottommost three rows when x equals 0, or one, or two. there's nothing we can do. we don't have the option of picking item number two. we don't have enough digital capacity. so we just copy the values from column i equal 1 over. those happen to be 0's, so that gives us three 0's to start column two. now when i equals 2 and x equals 3, now we're, we have enough residual capacity to take item two. and we're certainly, much happier to take the value of 2. which is the value of item number two, as opposed to inheriting the 0, 1 column on the left. so that gives us a two in the column i equals 2, with a residual capacity x equal to 3. so perhaps the first truly interesting table entry to fill in is when i equals 2 and x equals 4, because in this case, we actually have two different non trivial solutions we have to pick from. so one solution, as usual, we can just copy over the number, which is one column to the left, but actually in this case, there's a 3. there's no longer a 0 to the to the left there's a 3 to the left right? a 1,4 is equal to 3. our other option is to take the second item getting us a value of 2 and add that to whatever is in the leftmost column but shifted down by 3, because 3 is the weight of the second item. and you'll note that a of 1,1 is 0. so, picking the current item would just give us 2 plus 0 equals 0. we'd prefer to just inherit the 3, from the column one to the left. that is, we prefer to skip item two, so that we get the value from item one instead. the upper two rows of the second, the column with i equal 2 are filled in with 3s for exactly the same reason. so for example in the top row, what are our options? well we can inherit the three that's one to the left. if we actually use item number two, that knocks a residual capacity down to three and then you have 1,3 equals 0, so again, you'd rather have the three than the two. moving on to the penultimate column, 1i equals 3, for the usual reasons, we have to fill in the two bottommost rows. with a 0. notice that the weight of the third item equals 2. so we need a residual capacity x equal to 2 before we can actually use it. now, when x equals 2, we'd much rather have the value of 4 for the current item than to copy the 0 over. so we're going to fill in the entry a of 3,2 of 4, the value of the third item. similar reasons apply when x equals three or four. so here the alternative starts looking better, right, so in the row where x equals three the value that we'd inherit would be two. in the row where x equals four, the value we'd inherit would be three. but in both of these cases we'd prefer to have the immediate gratification of the value of four from the third item. and just inherit the empty solution with the residual capacity. so when x equals 3 and x equals 4, we're just going to take the third item and get a value of 4. now good things really start happening once x equals 5, because at that point we can both grab the third item and gets it's value of 4. but now, we knocked down the residual capacity only to 5 minus 2 which equals 3, and when you have i equal 2, and x equal 3, you actually get to, a value of 2, in that case. so, we're going to fill in the entry for i equals 3 and x equal 5 with 4, the value of the current item, plus 2, the optimal solution to the corresponding sub-problem. it gets even better when x equals 6. we're again going to take the third item, get its value of 4. but now, in the smaller subproblem, when i equals 3 and x equals 4, we actually get a value of 3, so the value here's going to be 4 plus 3, or 7. moving to the last column when i equals 4, so here the fourth item has weight 3, so that means when x equals 0, or 1, or 2, we don't even have the option of picking it. we have no choice but to copy over the number from the column to the left, so that's going to give us a 0, and a 0, and a 4, for the first three rows of the final column. so now in column 3, we do have the option of picking the fourth item. that'll give us a value of 4. you also have the option of just copying over the value in the column to the left. that will also give us a 4. so, we have a tie. and in some sense it doesn't matter. so we're going to fill in the entry of 4, when i equals 4, and x equals 3. the exact same reasoning applies when x equals 4. we're making the comparison between two thing, two things of equal value, both equal to 4. now, when x equals 5, we let the good times roll. we both can take the fourth item, and we get a value of 4 for the the fourth item. but now, when we subtract out its weight, we get a residual capacity of 2, and when x equals 2 and i equals 3, we also get a value of 4. so in this entry, we can write 4 plus 4, or 8. and that's superior to the alternative, which is just inheriting the six from the column to the left. same story holds when x equals to six we can take the fourth item, get the immediate gratification of four, get four from the smaller subproblem. when i equals 3 and x equals 3, and that eight beats out the alternative, inheriting the seven from the column to the left. so that completes the forward pass of the dynamic programming algorithm, filling in the table systematically using our recurrence. when it completes, of course, the optimal solution value is in the upper right corner. it's the value of the biggest sub-problem. so we know, at this point, that the max value of any feasible solution to this knapsack instance is 8. and as as we've discussed after you've filled in the table in the forward pass, if you want to get your grubby little hands on the optimal solution itself, you can do that with a reverse pass. there's a reconstruction post-processing step. how does that work? well you start with the biggest subproblems, in this case when i equals 4 and x equals 6. and then you ask, by which branch of the recurrence did we fill in this table entry. and that gives us guidance over whether to pick this item or not. so, how did we get this 8? did we inherit it from the column one to the left, corresponding to not taking item 4? or, did we get it from the column to the left, in the sub-problem with decrease, with residual capacity decreased by the weight of item. four. corresponding to taking item four. well, if you look at it, we didn't just inherit the solution in the column to the left which does indeed correspond to taking item four. that was the better of the two options we used to fill in this table entry. so, that means that in the optimal solution, item four will be there. so, having figured that out, we trace back through the table, we say, okay, well, let's look then at the table entry that we used to build up to this optimal solution to the big problem. so, that corresponds to going one column to the left, and a number of columns down equal to the weight of this fourth item, that is three rows down. now we ask exactly the same question. did we inherit the solution from the previous column, corresponding to picking this item, or did we use this item, and build from a solution to a smaller sit problem from the previous column. well, again, you know, where did this 4 come from? it didn't come from immediately to the left. it came from the value of item 3 plus the optimal solution with a decreased residual capacity. so that means that the optimal solution. also includes item number three. so, what do we do then? we trace back, we go one column to the left, and we have to go now two rows down. two rows because the weight of the third item is two. so, that leads us to a(2,1) and at this point, the residual capacity is so small that we have no choice but to inherit the numbers from the left, so we're not going to be able to pack either item one or item two into the optimal solution. so at this point, we just go left during our traceback, and then when we get to i equals 0, we're done. we know we've constructed the optimal solution, which in this case is item number 3 and item number 4. that's how you get an optimal value of 8 in this particular knapsack instance. 
at the beginning of the course, we talked about the sequence alignment problem, a problem which is fundamental to modern computational genomics. and we talked about the need for an efficient algorithm for solving that problem, for finding the best alignment of two strings. i'm pleased to report that at this point we're well prepared to give such an algorithm. indeed, such an efficient solution will readily fall out of the dynamic programming recipe that we now have quite a bit of practice with. so let me briefly jog your memory about the sequence alignment problem. so the goal here is to compute a similarity measure between strings, a similarity measure defined as the total penalty of the best alignment, also known as the needleman-wunsch score. so for example. if you've given as input the strings a g, g, g c t at a g, g c a, a natural candidate alignment would be to stack them on top of the other, inserting a gap in the shortest string after the two gs, that some sense represent the missing g. this is a pretty good alignment that suffers from merely two flaws. so first of all, we did resort to adding a gap in the second string. second of all, there is a mismatch in the final column. the a and the t get mismatched. in general we evaluate the alignment by summing up the penalties of all the flaws and the sum penalty per gap and the sum penalty per mismatch. so, a bit more precisely, as input in this computational problem, we're given two strings. i'm going to call them capital x and capital y. i'm going to use little x and little y to denote the individual characters of these strings. let's say the first string capital x has linked m and the second string y has linked n. in addition to the two input strings we assume we're given as input the values for the various types of penalties. so that we know exactly how much it costs each time we insert a gap. and for each possible mismatch, we need to know exactly what's the cost of that mismatch. in principle you could be given a penalty for matching a letter with itself, but typically that's going to be a penalty of zero. the space of feasible solutions are just the ways of inserting gaps into the two strings so that the results have equal length. i should emphasize that you're allowed to insert gaps into both of the strings. in the example, we only inserted into one of the two strings, but in general, you might have an input where one string is seven characters longer than the other, and it might turn out that in the optimal alignment, the best thing to do is insert three gaps at various places in the longer string, and ten gaps at various places into the shorter string. and the goal, of course is just to compute amongst all of the exponentially many alignments, the one that minimizes the total penalty, where total penalty is just the sum of individual penalties for the inserted gaps and the various mismatches. so let's not be unduly intimidated by how fundamental this problem is, and let's just apply the dynamic recipe, the programing recipe that we have been using all along. now remember, the really key insight in any dynamic programing solution is figuring out what's the right collection of sub problems. and if you're feeling like your up to the black belt level in dynamic programing, you might just want to try to guess what are the right collection of sub problems for sequence alignment? but i don't expect you to be able to do that at this point. and so, as usual we're going to derive the correct collection of sub-problems. and we're going to do it by reasoning about the structure of an optimal solution, narrowing it down to a small number of candidates composed in various ways from solutions to smaller sub-problems. once we've figured out the small number of possibilities for what the optimal solution could look like in terms of solutions to smaller sub problems, we'll be able to drive a recurrence which in effect just does brute force search through the small number of candidates. and from the recurrence we'll be able to back out. we'll be able to reverse engineer what are the various sub problems that we actually care about and that we have to solve. so let's do a thought experiment. what does the optimal solution have to look like? and again, remember, this is exactly what it is that we're trying to compute but that's not going to stop us from reasoning about it. if someone handed to us on a silver platter the optimal solution, what would it have to look like? so consider any old pair of strings, capital x and capital y, and an optimal alignment of them. let's visualize this optimal alignment as follows. let's write down the string x plus whatever gaps get inserted into it on top, and right beneath it we'll write down the string y, with whatever gaps are inserted into it. these two things have exactly the same length. so to figure out the various cases of the structure for this optimal solution, let's reason by analogy with the problems we've already solved. so back when we were looking at independent sets of line graphs, our case analysis was well, either the final vertex, the rightmost vertex of the path, is in the optimal solution or it's not. in the knapsack problem, we said well either the last item is in the optimal solution or it's not. so we always looked at sort of the last part of the optimal solution, in some sense the rightmost position. and happily, staring at this alignment, we see we can once again focus just on the action in the right most, in the final position. so now i have a question for you. so in the independent set problem, there were two cases. the last vertex was either in the optimal solution or it's not. in the knapsack problem, there were also two cases, the final item was either in the optimal solution or it's not. so my question for you is, in the sequence alignment problem: when we focus on what's going on in the final position of the optimal alignment, how many relevant cases do we have to study? so the answer i'm looking for is b, three relevant possiblities for the contents of the final position. let me explain my reasoning, let's start with the upper parts of the final position. observe that if that's a character of the string capital x, it can only be the very last character. it can only be little x sub n. that's because that's where this string ends. now we don't know that little x sub n is in the final position, there might be a gap. similarly, in the bottom part of this final position, there's two possibilities. there's a gap, or, if it's a character of y, it has to be the final character, little y, sub n. so that one seems to suggest four possibilities, two options for the top, two options for the bottom. but the hint of talking about relevant possibilities is that it's totally pointless to have two, a gap in both the top and the bottom. why? well the penalty for gaps is non-negative, so if we just deleted both of those gaps we'd get an even better alignment of x and y. and in studying an optimal solution, we can therefore assume we never have two gaps in a common position. so that leaves exactly three cases. it could be there's no gaps at all, that in fact this alignment matches the character, little x of m, with little y sub n. or it could match the final character of capital x, with a gap. or it could match the final character of capital y with a gap. so the hope behind this case analysis is that we're going to be able to boil down the possibilities for the optimal solution to merely three candidates, one candidate for each of the three possibilities for the contents of the final position. that would be analogous to what we did in both the independent set and knapsack problems, where we boiled the optimal solution down to just one of two candidates, corresponding to whether either the final vertex or the final item, as the case may be, was in the optimal solution. another way of thinking about this is we'd like to make precise the idea that if we just knew what was going on in the final position, if only a little birdy would tell us which of the three cases that we're in, then we'd be done by just solving the some smaller sub-problem recursively. so let's now state for each of the three possible scenarios for the final position, what is the corresponding candidate for the optimal solution, the way in which, it must necessary be composed with an optimal solution to a smaller sub problem. so who are going to be the protagonists in our smaller sub-problem? well, the smaller sub-problem's going to involve everything except the stuff in the final position. so it's going to involve the string's x and y, possibly with one character remaining. so let's let x prime be x, with its final character peeled off. y prime's going to be y, with its final character peeled off. so let me just remind you of how i numbered the three cases. so case one is when the final position contains the final characters of both of the two strings, that is, when there's no gaps. case two is when x, little x of n gets matched with the gap and case three is when little y of n gets matched with the gap. alright, so let's suppose that case one holds. this means that the contents of the final position, includes both of the characters little x sub m and little y sub n. so now what we're going to do is we want to look at a smaller sub problem. and we want to look at the sub problem induced by the contents of all of the rest of the positions. we're going to call that the induced alignment. since we started with an alignment, two things that had to equal length, and we peeled off the final position of both, we have another thing that has equal link so we're justified in calling it an alignment. now what is it an alignment of? well if we're in case one, that means what's missing from the induced alignment are the final characters. little x of m and little y's of n, which means the induced alignment is a bona fide alignment of x prime and y prime. and certainly, what we're hoping is true, is that the induced alignment is in fact, an optimal alignment of these smaller strings x prime and y prime. this would say that when we're in case one, the optimal solution to the original problem is built up in a simple way from an optimal solution to a smaller sub problem. we're of course hoping that something analogous happens in cases two and three. the only change is going to be that the protagonists of the sub-problem will be a little bit different. in case two, the thing which is missing from the induced alignment is the final character of x. so, it's going to be the induced alignment of x prime and y. similarly, in case three, the induced alignment is going to be an alignment of x and y prime. so, this is an insertion, this is a claim, it's not completely obvious, though the proof isn't hard, as i will show you on the next slide. but assuming for the moment that this assertion is true, it fulfills the hope we had earlier. it says that indeed, the optimal solution can only be one of three candidates, that one for each of the possibilities for the contents of the final position. alternatively it says, that if we only knew which of the three cases we were in, we'd be done, we can recurse, we could look up a solution to a smaller sub problem and we could extend it in an easy way to a optimal solution for the original problem. so lets now move onto the proof of this assertion. why is it true that an optimal solution must be built up from an optimal solution to the relevant smaller sub-problem? well all of the cases are pretty much the same argument so i'm just going to do case one, the other cases are basically the same. i invite you to fill in the details. so it's going to be the same type of simple proof by contradiction that we used earlier, when reasoning about the structure of optimal solutions for the independent set in knapsack problems. we're going to assume the contrary, we're going to assume that the induced solution to the smaller subproblem is not optimal, and from the fact that there is a better solution for the subproblem, we will extract a better solution for the original problem, contradicting the purported optimality of the solution that we started with. so when we're dealing with case one, the induced alignment is of the strings x prime and y prime, x and y with the final character peeled off. and so for the contradiction, let's assume that this induced alignment, it has some penalty, say capital p. let's assume it's not actually an optimal alignment of x prime and y prime. that is, suppose if we started from scratch we'd come up with some superior alignment of x prime and y prime, with total penalty p star, strictly smaller than p. but if that were the case, it would be a simple matter to lift this purportedly better alignment of x prime and y prime to an alignment of the original strings x and y. namely we just reuse the exact same alignment of x prime and y prime, and then in the final position, we just match x m with y n. so what is the total penalty of this extended alignment of all of x and y? well, it's just the penalty incurred in everything but the final position, and that's just the old penalty p star, plus the new penalty incurred in the final position. and that's just the penalty corresponding to the match of the characters x m and y n. p star being less than p, of course p star plus alpha x m y n is less than p plus alpha x m y n. but this second term is simply the total penalty incurred by our original alignment of x and y, right? that alignment incurred penalty capital p, just in the induced alignment of x prime y prime, and it's total penalty was just that plus the penalty in the final position, which is this alpha xn, yn. but that furnishes the contradiction that we suppose that we started with an optimal alignment of x and y, yet here is a better one. so with that contradiction, it completes the proof of the optimal substructure claim. 
so, now that we understand that the optimal solution to the sequence element problem has to be only one of three candidates, we're going to be easily able to formulate a recurrence, identify the relevant sub-problems and derive an efficient dynamic programming algorithm for the problem. so, here is the culmination of our work. on the previous video, we thought about an optimal alignment of some pair of strings x and y, and we notice that there are three cases for the contents of the final position. either there's no gaps or there's a gap on top or there's a gap on the bottom. in case one, where there's no gaps, xm and ym get matched. and we proved that the induced alignment which is of the smaller strings x prime and y prime has to be optimal in its own right. in the second case where the character little x sub m gets matched with a gap induced alignment this time of x prime and y. has to be optimal in its own right, and the third case where little y sub n gets matched with the gap, the induced alignment now of x and y prime. must be optimal. so one way to think about this kind of assertion is it says that the optimal solution to a problem, to a sequence of a lot of problem depends only on the solutions to three different smaller sub-problems, one involving x, x prime and y prime with characters peeled off of both of the strings. one involving x prime and y and one involving x and y prime. but in all of the cases, all that we care about are sub-problems in which a single character was peeled off from the right from one or both of the strings that we started with. the situation is very similar to in our previous two examples. we have independent sets on line graphs and the nap sack problem and the independent set problem whenever we, we only cared about sub problems obtained by plucking off either one or two vertices from the given line graph. so all we ever cared about were prefixes of the original line graph. in the nap sack problem we got sub problems by plucking off the last item and perhaps also reducing the nap sack capacity by some interval amount. so there were two dimensions in the nap sack problem for which sub problems could decrease in size, then number of items in the residual nap sack capacity. so we use two parameters to keep track of the sub problems. and what we cared about were all possible prefixes of the items and all possible residual integral capacities, at most the original knapsack capacity. so what's up in the sequence alignment problem? well here, sub problems get smaller by plucking a character off of the first string and or the second string. so again there are two ways in which the sub problem can get smaller, either the first string or the second string. so we'll again use two different parameters, one to figure out how much we've plucked off of the first string, the second one to figure out how much we've plucked off of the second string. right. but all we care about. the only relevant sub problems involved. prefixes of the two original input strings x and y. that is, the only sub problems that we care about have the form x i y j, where x i denotes the first i letters of capital x, some prefix of x, and y j denotes some prefix of y, the first j letters of y. so lets now move from the sub-problems we're going to use in our dynamic programming algorithm to the recurrence that we're going to use. and the recurrence really all it does is compile our understanding of the optimal solution and how it depends on the solution of the smaller sub-problems into an easy to use mathematical formula. so i'll use the notation p sub i j for the value of the optimal solution to the corresponding sub problem, the one involving the prefix x i and the prefix y j so for a given set of positive values for i and j, what is pij? well, there are three possibilities. case one is where the final position of the optimal alignment doesn't have any gaps, so it matches the final character of x sub i, that is little x sub i and the final character of the prefix capital y sub j, that is the character little y sub j. it matches them together and just reuses an optimal alignment for the smaller strings, xi-1 and yi-1 case two is where the last letter of the first string, that is little x of i gets matched with a gap. and that case the penalty of the corresponding alignment is the penalty of a gap plus whatever the optimal alignment is of the first i minus one letter of capital x plus the first j letter of capital y. the symmetrically case three we pay for a gap and then we pay whatever the optimal alignment is of all of the first i letters of capital x with the first j menace one letters of y. this is the case where the last letter of the second string gets matched with the gap in the final position of the optimal alignment. so we know that the optimal solution has to look like one of these three things, we don't know which, so in the recurrence we'll just in effect do brute force search among the three outcomes. we just remember, we just choose the minimum of the three possibilities. so that's the formal recurrence. it's correctness really just follows immediately from the work we already did, understand what the optimal solution has to look like. so, before we state the algorithm where we systematically solve all of the sub problems using this magical formula. let's just make sure we get the edge cases, the base cases where i or j equals zero correctly sorted out. so specifically what is the value of p i,0 and also it turns out p of zero, i where i here is just some non-negative integer. alright. so the answer to this question is the second one, is b. and i hope if you could keep the notation straight then the answer was fairly clear. so let's remember what, what does pij mean? that's the total penalty of an optimal alignment between the first i letters of x, and the first j letters of y. so consider pi zero. so now we're asking about aligning the first zero letters of x with the first zero letters of y. that is with the empty string. well the optimal way to match any string to the empty string is you're just going to insert gaps into the empty string to equalize their lengths. and so if your string has length i, you need to insert i gaps. what's the? penalty of that alignment is just i times the penalty for a single gap, and that's the answer here in b. so we're ready now to give the algorithm, and as with all these dynamic programming algorithms once you know the sub-problems and once you know the recurrence that relates their solutions there's really nothing to do. all you do is systematically answer solve all of the sub-problems moving from smallest to largest. so we're going to use an array a to keep track of the solutions of all of these sub-problems. a is going to have two dimensions. the reason for two dimensions is we have two independent parameters which are keeping track of the sub-problem size. one for how many letters of x we're dealing with, and the second dimension for how many letters of y that we're dealing with. that's analogous to the knapsack problem, where we also had two dimensions to keep track of. the number of items in play, and the residual knapsack capacity. we just figured out what the base case is, so we just solved those in a pre-processing step. so if one of the two indices is zero, then the optimal solution value is just the gap penalty times the non-zero index. and, now we just go to our double four loops. it's double four loops because we have two indices into out array. and whenever we get into a sub problem, we just evaluate the recurrence invoking of these solution to the already computed smaller sub problems. so one sanity check you should always apply when you're writing out the code for a dynamic programming algorithm: when you look at the right hand side of your recurrence, when you look at these purportedly already solved subproblems whose solutions you're using to solve the current subproblem, make sure you have actually already solved those subproblems. so in this case we're good to go because the indices of the subproblems are only less than the entry that we're filling in right now. so indeed all three of the relevant subproblems, a-i - 1 j - 1 a-i - 1 j, and a-i j - 1 they've already been computed in earlier iterations of our double four loop. so they're just hanging out, waiting to get looked up in constant time. and as usual once you've actually figured out the key ingredients for the dynamic programming solution, namely the sub-problems and the recurrence, it's pretty much self evident why the things going to work and it's also self evident exactly what its running time is going to be. so why is the algorithm correct? that is, why does it terminate with every entry aij equal to the true optimal penalty pij of the corresponding sub-problem. well, this just follows because our recurrent is correct, that's where all the hard work was, and then we're just systematically solving all of the sub-problems. so, formally, if you like, it would be proof by induction. so, the running time is completely trivial to evaluate. in each iteration of this double four loop, we do a constant amount of work. we just need to look up three things in constant time and make a couple of comparisons. how many four loops are there? well, m iterations of the outer four loop, n iterations of the inner four loop. so we suffer the product, m times n. that is, the running time is proportional to the product of the lengths of the two strings. so depending on the application, you may be content to just have an algorithm compute for you the nw score, the total penalty of an optimal alignment or perhaps you're actually interested in the alignment itself. and just as we discussed with independent sets of line graphs, by tracing back through the filled in table, you can indeed reconstruct an optimal solution. so let me just give you the high level idea of how it works. it's going to to follow the same template and all you think through the details of why this really works in the privacy of your own home. so assume that you've already run the algorithm on the previous slide. that you've filled in all the entries of this two d array capital a. now we want to trace back. so where are we going to start tracing back this filled in table? well, we are going to start with a problem that we actually care about, namely the largest problem a of m comma n, that's what we want the alignment for. now we know the softball alignment looks, has one of three candidates, we know there's three possible situations for the contents of the final position of that alignment. more over, when we filled in this entry of the table, we explicitly compared the three possibilities to figure out which one was the best. so you know, perhaps on the forward pass we actually cached the result of the comparison, or in the worst case we can just go back and re-compute, and figure out which of those three. pieces was used to fill in this entry, and depending on which one of the three candidates won, that tells us, what should be, the contents of the final position of the optimal alignment. if case one was used to fill in this entry, we should match, little x sub n and little y sub n. if case two was used to fill in this entry, we should match little x sub n with the gap. if case three was used, to fill in this entry, we should match little y sub n with the gap. if there was a tie, we get to pick any of them. arbitrarily, all of them will lead to optimal alignments. then of course, after figuring out what to do in this final position. we have an induced sub problem involving x prime and or y prime. that tells us a, a previous entry of the table to go to. and we just repeat the process. we, again, figure out which of the three cases was used to fill in this entry. that tells us how to fill in the next right most position of the alignment. and we just keep going until we fall off the table. so what do you do when you fall off the table? well, once one of the indices i or j gets all the way down to zero, now you have no choice. so now one of the strings is empty and the other one has some number of symbol. so you should just insert the appropriate number of gaps to equalize the lengths. one thing that's pretty neat is that this trace back procedure is efficient. in fact, it's way more efficient, in general, than the forward pass. for the forward pass, you have to fill in every single one of the m * n entries. but in this trace back procedure, each time you track back. one of the two indices, at least, will get decremented. so that says, you're going to complete this trace back in o of m + n time with an optimal alignment of the original two strings. 
in this next sequence of videos we're going to study a slightly more intricate application of the dynamic programming paradigm, namely to the problem of computing optimal search trees. search trees that minimize the average search time, with respect to a given set of probabilities over the keys. so i'm going to assume in these videos that you remember the basics of the search tree data structure. so if you need to jog your memory, you might want to review the relevant video from part one. so a search tree is they contain objects, each object has a key drawn from some totally ordered sets. and the search tree property states that at each node of a search tree, say it contains some object with a key of value x, it better be the case that everything in the left subtree of that node has keys less than x and everything in the right subtree under the node with key x has to have keys bigger than x. so that has to be true simultaneously at every single node of the search tree. the motivation for the search tree property is so that searching in a search tree just involves following your nose, just like binary search in assorted array. so if you're looking for, say, an object with key seventeen, you know whether to go left or right from the root, just based on the root's key value. if the root has key value twelve, you know where seventeen, if it exists, has to be in the right sub-tree. so you recursively search the right sub-tree. if the root has value 23, you know where seventeen has to be in the left sub-tree. so that's where you recursively search. something we originally discussed in the context of balanced binary search trees. like red black trees. and i'm going to reiterate now. is that, for a given set of keys, there are many, many valid search trees containing those keys. so just to remind you how this works, let's even just suppose there were only three keys in the world, x, y, and z, with x less than, y less than z. one obvious search tree would be the balanced one. so that would put the middle element y at the roots. you would have left child x and right child y. but there's also the two chain search trees containing these three keys, one with the smallest element x at the root, the other with the largest element z at the root. so, given the multiplicity of solutions, all of the different search trees one could use to store objects with a bunch of keys, an obvious question is which one is the best. what's the best search tree to use out of all of the possibilities? so i don't blame you if you've got a sense of deja vu. we did already ask and answer this question in one sense, when we discussed red black trees. there we argued that the best thing to have is a balanced binary search tree that keeps the height as small as possible and therefore the worst case search time, which is proportional to the height, as small as possible, namely logarithmic in the number of objects in the search tree. but now let's make the same kind of assumption that we made when we discussed huffman codes. that is, let's assume that we actually have accurate statistics about how frequently, each item in the tree is going to be searched for. so maybe we know that item x is going to be searched for 80% of the time, whereas y and z will only be searched for 10% each. could we then improve, upon the perfectly balanced search tree solution? so let me make this question more concrete, just by asking you to compare two candidate solutions. on the one hand the balance tree, which has y at the root and x and z as children. on the other hand, the chain which has x as a root, y as its right child and then z as the right child of z, excuse me, z as the right child of y. so what is the average search time in each of these two search trees, with respect to the search frequencies i told you, 80% for x, 10% for y, and 10% for z? and when i say the search time for a node, i just mean how many nodes do you look at on route to discovering what you're looking for, including that last node itself. so the search time for something that's at the root for example, that would be a search time of just one, because you only look at the root to find it. all right so the correct answer to the quiz is the fourth option 1.9 and 1.3. so to see why, let's just compute the average search time in each of the two proposed search trees. in the first one, with y at the root, well, 80% of the time we're going to suffer a search time of two, whenever we look for x we have to look at the root y and then we look at the x. so we pay two 80% of the time and that contributes 1.6. 10% of the time we get lucky, we see a y, that contributes a.1. 10% of the time we see z, that contributes another.2 for a total of 1.9. by contrast think about the chain that has x at the root. here 80% of the time we get lucky, and we only have to pay one. two for every search for x so that contributes only 8. to the total. it is true our worst case search time has actually gone up. when we see its z we suffer a search time of three which never ever happened in the balance case. but we pay that three only ten% of the time, that contributes a.3. the remaining ten% of the time we suffer a search time of two to look for y. that gives us a total of 1.3. and the moral of the story, of course, is that this example exposes an interesting algorithmic opportunity. so the obvious, quote unquote, solution of a perfectly balanced search tree, need not be the best search tree when frequency of access is non uniform. you might want to have unbalanced trees, like this chain if it gets extremely frequent searches, closer to the roots to have smaller search time. so the obvious question is then, given a bunch of items and known frequencies of access, what is the optimal. search tree. which search tree minimizes the average search time? so that brings us to the formal problem definition. we're told n objects that we gotta store in a search tree and we're told the frequency of access of each. so let's just keep things simple and the notation straightforward, let's just say the items are named from one, two, all the way up to n, and that is also the order of their respective keys. so key one is the frequency of searching for the item with the smallest key, and so on. you might wonder where these frequencies come from. how would you know exactly how frequently every possible key will be searched for. it's going to depend on the application. and you know there will be applications where you're not going to have these kinds of statistics. and that's where you'd probably want to turn to a general purpose balanced binary search tree solution, something like a red black tree, which guarantees you that every search is going to be reasonably fast. but it's not hard to think about applications where you are going to be able to learn pretty accurate statistics about how frequently different things are searched for. one example might be something like a spell checker. so if you would implement that by storing all of the legally spelled words in a search tree, and as you're scanning a document, and every time you hit a word, you look it up in the search tree to see if it's correctly spelled or incorrectly spelled. you can imagine that after, you know, scanning through a number of documents, you would have pretty good estimates, about how frequently things get searched for. and then you could use those estimates to build a highly optimized binary search tree for all future documents. if you're in some other kind of application where you're concerned about these frequencies changing over time, so, for example, if they're privy to trends in the industry, you could imagine rebuilding the search tree every day or every week or whatever, based on the latest statistics that you've got. in any case, if you're lucky enough to have such statistics, what you're going to want to do is to build a search tree, which, on one hand is valid. it should satisfy the search tree property and on the other hand, should make the average search time as small as possible. so let me go ahead and write down a formula for the average search time. it's the one that you would expect it to be. and also introduce some notation, namely capital c of t. we'll denote the average search time of a proposed search tree t. so for these lectures, we're going to focus on the case where all searches are successful. the only thing that ever gets searched for is stuff that's actually in the tree. but everything we'll talk about in these lectures and the algorithm is easily extended to accommodate the case where you also have unsuccessful searches and statistics about how frequent the various unsuccessful searches are. but, if there is only successful searches, then we average only over the n elements that are stored in the tree. so we sum over each of the items i, we weight it by the probability or the frequency of it's access p sub i, and then that gets multiplied by the search time required in the tree t to find the item i. and as we discussed in the quiz, the search time for a given key i and a given tree t is just the number of nodes you have to visit until you get to the one containing i. so if you think about it, that's exactly the depth of the node in this tree plus one. so for example, if you're lucky enough that the key is at the root, the depth of the root is zero, and we're counting that as a search time as one. so it's depth plus one. so, one minor point. it's going to be convenient for me to not insist that the pi's sum to one. of course, if the pis were probabilities, they would sum to one. but i'm going to go ahead and allow them to be arbitrary positive numbers. and that, for the same reason, i'm going to sometimes call capital c of t, the weighted search time rather that the average search time, because i won't necessarily be assuming that the pis, sum to one. with that said, go ahead and, you know? think of that as the canonical special case in your mind as we go through these lectures. so for example, in the case where these are probabilities, where the pis sum to one, we could always as a reference point use a red black tree as our search tree. but, as we've seen, when these pis are not uniform, you can generally do better. and so that's the point of this computational problem. exploit the non-uniformities in the given probabilities to come up with the best possibly unbalanced search tree as possible. so i'm sure many of you will have noticed some of the similarities between this computational problem of optimal binary search trees. and one that we already solved back in the greedy algorithm section, namely huffkin, huffman codes, which, amongst all prefix free binary codes, minimize the average encoding length. so, let's just be precise about the similarities and differences between the two problems, and in particular, why we can just reuse the algorithm we already saw off the shelf, to solve the optimal bst problem. so it's, of course, super similar in the two cases, is the format of the output. in both problems, the responsibility of the algorithm is to output a binary tree, and the goal is to minimize the average depth, more or less, where the average is with respect to provided frequencies, over a bunch of objects that we care about. characters from an alphabet in the case of huffman codes, and a bunch of objects with keys from some totally ordered set in the binary search tree case. and it is true that in the optimal bst case, we're not really averaging depths, we're averaging depths plus one but if you think about it that's exactly the same thing. more important is to understand the differences between the problem solved by huffman codes and the computational problem that we have to solve here. in the huffman code case, we had to output of binary code and the key constraint was that it be prefix-free. and in the language of trees what that meant is that the symbols that we're encoding had to correspond to the leaves of the tree. symbols could not correspond to internal nodes of the tree that we output. now in the optimal binary search tree problem we do not have this prefix free constraint, so we're going to have a label that is an object at every single node of our tree, whether it's a leaf or not. but, we have a different, seemingly quite a bit harder constraint to deal with, namely the search tree property. so remember, back in the huffman code case we didn't even have an ordering on the symbols of the, of the alphabet. there wasn't a sense that one of them was less than another, it wouldn't even make sense to talk about the search tree. brought in that context. here by contrast, we're given these keys and there's this total lowering on them. and we'd better satisfy the search tree property in the tree that we output, that is in every single node in the tree that we output, it better be the case that all keys in the left sub-tree are less than the key at that node, and all keys in the right sub-tree are bigger than the key at that node. that's a constraint that we have no choice but to satisfy. this constraint is harder in the sense that no greedy algorithm, huffman's algorithm or otherwise, solves the optimal binary search tree problem. rather we're going to have to turn to the more sophisticated tool of dynamic programming to design an efficient algorithm for computing optimal binary search trees. that's the solution we'll start developing in the next video. 
so now that we have motivated and formally defined the optimal binary search tree problem, lets think about how to solve it. after settling on dynamic programming as the paradigm we are going to try to use we're going to proceed in the usual way, turning to the optimal solution for clues, asking in what way is it composed of optimal solutions to smaller sub-problems. so let me just remind you of the formal problem statement. there's n objects we got to store in a search tree, and let's just name them in the order of their keys, and let's name them one, two, three, all the way up to n, for simplicity, and we're also given. frequencies or weights reflecting how often the different objects are searched for. so that's p1 up to pn positive numbers. canonically we think of these summing to one, being probabilities, but actually won't use that fact so in general they're just arbitrary positive numbers. the goal is to output a search tree. it should satisfy the search tree property, it should contain all of these objects one through n, and amongst all such search trees it should minimize the weighted search time. so the sum over all of the items i of the probability of i times the search time for i, namely its depth in the tree plus one. now in case you're feeling cocky about the fact that the greedy algorithm works to solve the seemingly similar optimal prefix-free binary code problem in the form of huffman's algorithm, i want to spend a little time pointing out that greedy algorithms are not sufficient, are not correct, to solve the optimal search tree problem. so if we were to design a greedy algorithm what kind of intuition would motivate a particular greedy rule. well, staying at the objective function it's very clear that we want the objects that high frequency of access to be at or near the root and we want items of low frequency access to be in the bottom levels of the tree, like the leaves. so what are some ways we can compile this intuition into a greedy algorithm? well one, perhaps motivated by the success of huffman's algorithm, is we could think about a bottom up approach. now i'm not going to define what i mean here precisely, but informally we want to start with the bottom most levels, with the leaves and the nodes you want to put there are the objects that are accessed the least frequently. any reasonable way of implementing this kind of body of greedy rule is not going to work. let me show a simple counter example. so, let's just assume we have four objects, one, two, three, four. what i'm showing here on the right in pink is two possible search trees valid for those four keys. and let's assume that, the frequencies are as followed. object one is searched for two% of the time. object two for 23% of the time. object three, the bulk of the time 73%. an object for only one% of the time. any greedy algorithm which insists on putting the lowest frequency objects in the very bottommost level of the tree is not going to produce this tree on the right, which has the two% object below, at a lower level than the !% object. instead, such a greedy algorithm would presumably output a searchtree like the one on the left, which has the two as the root, and then the object four, at the lowest level, at depth two. but you should be able to easily convince yourself that, for these probabilities, it's the tree on the right which is the one you want, that's optimal, because the object three is the one that's searched for the bulk of the time, that's the one you want at the root as opposed to the object two. so i realize i'm being a little informal here but i hope you get the idea that a naive bottom-up implementation of a greedy algorithm, which if you think about it is really what we did in huffman's algorithm, is not going to work here. the same can be said about a top-down approach. perhaps the simplest top-down approach would be just to take the most frequently searched for object and put that at the root and then recursively develop an appropriate left and right sub-trees under that most frequently accessed element. so let me show you again and, formally, just the same kind of counter-example. we're going to use the exact same four objects, the exact same two trees. i, i will however, change the numbers. now, let's imagine that object one is searched for almost never, just one percent of the time and each of the other three objects are searched for roughly a third of the time each. but, let me sort of break ties, so that the object number two is the most frequently one. searched for 34%. so that, in that case the greedy algorithm will put the 34% node up in the roots when really what should happen is you want a perfectly balanced sub-tree for the objects two, three and four because each accounts for roughly a third of the searches. so let's give object three 33% of the searches and object four 32% of the searches. and again i'll leave it for you to convince yourself that this is indeed a counter example the tree's spit out by the greedy algorithm on the left, we have an average search time of roughly two, where as the search tree on the right we have an average search time of roughly five thirds. so we'd like to produce the tree on the right but the greedy algorithm proposed here will spit out the tree on the left. this of course doesn't exhaust the list of potential greedy algorithms. you could try others but it's not going to work. there's no known greedy algorithm that successfully solves the optimal binary search tree problem. so in particular if we focus on the top down approach. the choice of the route. the choice of what to do at the uppermost level. has very hard to predict repercussions, for what the two different sub-problems look like. so this is what stymie is, not only the top down gritty approach, but also a naive divide and conquer approach. so for example if we just wanted to split the keys into the first half, and the second half, recursively compute and optimal b is i need on each of those two half's, and then put them back together. the search tree property would say that we have to, unite those two sub-solutions under a root which is the median, in between the two sub-problems. and who is to say that the median is a good choice for the root. again, because the ramifications further down the tree, maybe that's a stupid root. but, boy, is it tempting to try to solve this problem recursively, right? we're trying to output this binary tree. it has recursive structure. if only we knew which root we should pick. we would love to recurse twice. once to construct an optimal left subtree. once to construct an optimal right subtree. okay, so if only we knew the right route. this is starting to sound familiar, actually. how did it work in all our dynamic programming solutions? we always said, oh. if only a little birdie told us this one little piece of the solution. well, then, you know? then we could, sort of, look up or recursively compute the rest of the solution. and extend it back to one for the original problem, easily. so maybe the same thing's true here. maybe, if only a little birdie told us what the root was. then we could look up or recursively compute optimal solutions to smaller subproblems. and paste everything back together, and be done. that would be great. so as usual we want to make this precise with an optimal substructure lemma. we want to understand the way in which an optimal solution to an optimal bst problem must necessarily be constructed from optimal solutions to smaller sub-problems. so in the following quiz i'm going to ask you to guess what the appropriate optimal substructure lemma is and then after that quiz once we've identified the right statement at that point i will show you the proof. okay, so the answer i'm looking for is the fourth one, is d. which is the strongest statement of all. so the first point is that each of the trees t1 and t2, now as subtrees of a binary search tree these of course are themselves binary search trees, valid for the trees that they contain. and not only can they be viewed as search trees on the keys they contain, but the claim which we'll prove on the next couple slides is that they are indeed optimal. they minimize the weighted search time over all possible search trees for the objects contained in those two trees. so that gets us down, it rules out a, it rules out b. we can say something stronger than that, but we can even say something stronger than c, that each of the two trees is optimal for the items that they contain. we actually know exactly which items are in t1 and which items are in t2. and this is by the search tree property. search tree property said that every node and in particular here at the roots everything to the left of the root is less than that of everything to the right of the node is bigger than it. so the root being r by assumption we know the objects one through r minus one, but they got to be somewhere. the only way they can be is in the left sub-tree, t1. so that's exactly the contents of t1. similarly, the contents of t2 are precisely the objects r+1 through n. so the two sub-trees are optimal. and we know exactly which keys they are, it's everything less than r on the left and everything bigger than r on the right. okay, so here's where things stand at the end of this quiz. we've identified a statement that we're really hoping is true. we're really hoping that an optimal bst, binary search tree, must necessarily be composed in this way of optimal binary search trees for the key sets to the left of the root and the right of the root. if that's true, hopefully with the experience we now have, we can sort of envision what a dynamic programming algorithm might look like. i'll just fill in the details in the next video. if this weren't true, if we didn't have this optimal substructure, honestly, i have no idea how we'd get started. it's really not clear what an algorithm would look like if this weren't true. so the next couple slides, i'm going to prove this to you. the format, you know, will not be radically different than the ones we've already seen. i don't think there'll be any big surprises, but it's so important, this really is the whole reason why the algorithm is going to work, i'm still going to give you a fool proof of this optimal substructure lemma. 
so let's take any old optimal binary search tree for keys one through n with frequencies p1 through pn. the thing we're trying to prove asserts that the left subtree t1 should be optimal for its keys, one through r minus one, and the right subtree t2 should be optimal for its keys, r plus one through n. so we're going to proceed by contradiction, we're going to assume that's not true. what would that mean? that means for one of these two subproblems, either one through r - 1 or r + 1 through n, there's an binary search tree with even smaller weighted search cost, search time, then t1 or t2 respectively. the two cases are totally the same whether. in our contradiction, we assume t1 is not optimal or the t2 is not optimal. i'm just going to prove it in the case the t1 is not optimal. so if t1 is not optimal, there's gotta be a search tree on its keys, one through r - 1 which is better. call that purportedly better solution t star one. now as usual, the thing which we're going to try to contradict to get the final proof is we're going to exhibit a search tree on all of the keys, one through n which is even better than t. the t was supposed to be optimal, so that would be a contradiction. so how do we get our superior search tree for all of the keys? well, we're just going to take t and we're going to do cut and paste. we're going to do surgery on the tree t, ripping out it's left subtree t1 and pasting in this subtree t star one. call the resulting tree t star. so, to complete the contradiction and therefore the proof of the optimal substructure lemma, all we have to show is that the weighted search cost of t star is strictly less than that of t, that would contradict the purported optimality of t. so that's precisely what i'll show you on this next slide and it's going to be evident if we do a suitable calculation. here it is. so let's begin just by expanding the weighted search time of our original tree, capital t, the purportedly optimal one. let's expand its definition. so you have one sum n for each of the items i and the weight we give to a given item is just its frequency p sub i. and we multiply that by the search time of for i n t so let me now pause to tell you the point of this calculation we're about to do. so we start with the weighted search time in t, and that's, of course expressed in terms of search times in this tree t. what i want to show next is that can equally be, be well expressed in terms of search times in the subtrees t1 and t2. that is, there's a simple formula to compute the search time in t if i told you the search times in t1 and t2. that's what's going to allow us to easily analyze the ramifications of the cut and paste and to notice that in cutting and pasting in a better tree for the subproblem, we actually get a better tree for the original problem. so the right way to search time in t in terms of the weighted search time in t1 and t2, it's going to be convenient to bucket the items into three categories. those that are in the left subtree t1, i.e. one through r minus one, those in the right subtree t2, r plus one through n, and then of course left over is the root r itself. so let's just break down the sum into its three constituent parts. so the sum and corresponding to the root r contributes the frequency of r times the search time for r, namely one because it's the root. and then we have our sum, over just the items up to r - 1 of their frequencies times their search times and similarly those r plus one up to n, their frequencies times their search times nt. so for the next step, let's observe it's very easy to connect search times in t with search times in the subtrees t1 and t2. suppose you were, you, example, you were searching for some key in t that was in t1. so some key that was less than r. what happens when you search for this item in the tree t? well, first you visit r, it's not r, it's less than r, so you go left, and then you just search as appropriately through the search tree t1. that is, the search time in the big tree t is simply the search time in the subtree t1 + 1, 1 because you had to visit the root r before you made it in to the subtree t1. so that is, for any object i other than the root, we can write its search time in the big tree t as simply one plus the search time in the appropriate subtree. so now, let me expand in groups and terms just to clean up this mess. so now that the dust has settled, let's inspect each of these three sums. we actually have a quite simple understanding of each of them. so, the first sum is just the sum of the pi. so for example, in the conical case where we're dealing with actual probabilities, this is just one. the point is, whatever the pis are, this first sum is just a constant, meaning it's independent of the tree t, with which we started. what's the second sum? so it's the sum of the objects from one to r minus one, the frequency of ithe times search4i time for i in the searchtree t1. well, we have a much better, shorter nickname for that sum. it's the weighted search time of the search tree t1, for the objects that contains one through r minus one. similarly this last sum, sum from i over r plus one to n of the frequency of i times the search time in t2, that's just the weighted search time in the search tree t2. so, we did this computation with the purportedly optimal search tree capital t in mind. but if you think about it, you look at this algebra. this, it applies to any search tree you like. for any search tree, the weighted search cost can be expressed as the sum of the pis plus the weighted search cost of the left subtree of the root plus the weighted search cost of the right subtree of the root. in particular, that's true for this tree t star we got by cutting and pasting the better left subtree t star one and for t1. so applying this reasoning to t star, we can right its weighted search cost as the sum from micro one to n of all the pis plus the weighted search cost of its left subtree of its root, which remember by construction is t star one, and then it has the same right sub tree as the tree t does, namely t2. and the point of all this algebra is that we now see in a very simple way what are the ramifications of cutting and pasting in this better left subtree, we get a better tree for the original key set contradicting the purported optimality of the tree t that we started with. that completes the proof of the optimal substructure lemma for optimal binary search trees. 
so now that we understand the structure of optimal solutions for this optimal binary search tree problem. that is, we understand how an optimal solution must be one of a relatively small number of candidates. let's compile that understanding into a polynomial time dynamic programming algorithm. let me quickly remind you of the optimal substructure lemma that we proved in the previous video. suppose we have an optimal binary search tree for a given set of keys, one through n, with given probabilities. and suppose this binary search tree has the root r. well then it has two sub-trees, t1 and t2. by the search tree property, we know exactly the population of each of those two sub-trees. t1 has to contain the keys one through r - 1. as usual we're sorted, we're assuming these are in sorted order. whereas the right sub-tree t2, has to contain exactly the keys r + 1 through n. moreover, t1 and t2 are, in their own right, valid search trees for these two sets of keys. and finally, and this is what we proved in the last video they're optimal for their respective sub-problems. t1 is optimal for keys one through r minus one and the corresponding weights or. abilities and t2 is optimal for r plus one through n and their corresponding frequencies. so let's now execute our dynamic programming recipe. so now that we understand the way in which an optimal solution must necessarily be composed in a simple way from solutions to smaller subproblems. let's take a step back, and ask, well. given that, at the end of the day, we care about the optimal solution to the original problem. which subproblems are relevant? which subproblems are we going to be forced to solve? for example, with independent sets in line graphs we observed that to solve a subproblem we needed to know the answers to the subproblems where we pluck either one or two vertices off of the right hand side. so overall what we cared about was subproblems corresponding to prefixes of the graph. in the knapsack problem we needed to understand subproblems that involved one less item and possibly a resus, reduced residual knapsack capacity, so that led to us caring about solutions to subproblems corresponding to all prefixes of the items and all integer possibilities for the residual capacity of a knapsack. in sequence alignment, when we looked at subproblems. as we were plucking a character off of one or possibly both of the strings. so we cared about subproblems corresponding to prefixes of each of the two strings. now, here's one of the things that's interesting about the binary search tree problem which we haven't seen before. is that, when we look at a subproblem. in the optimal structure lemma, there's two that we might consider. we don't just pluck off from the right. we care about both the subproblem induced by the left subtree. and that induced by the right subtree. in the first case, we're looking at a prefix of the items we started with. and that's like we've seen in our many examples. but in the second case, the sub problem corresponding to t sub two. that's actually a suffix of the items that we started with. so put differently, the sub-problems we care about are those that can be obtained by either throwing away a prefix from the items that we started with or throwing away a suffix from the items that we started with. so in light of this observation, that the value of an optimal solution depends only immediately on sub problems that you obtain by throwing out a prefix with a, or a suffix of the items, what i want you to think about on this quiz is, what is the entire set of relevant sub problems? that is, for which subsets s of the original items one through n is it important that we compute the value of an optimal binary search tree on the items only in s? so before i explain the correct answer which is the third one, let me talk a little bit about a very natural but incorrect answer, namely the second one. indeed, the second answer seems to have the best correspondence to the optimal substructure lemma. the optimal substructure lemma states that the optimal solution must be composed of an optimal solution on some prefix and an optimal solution on some suffix, united under a common root r. so we definitely care about the solutions to all prefixes and suffixes of the items but we care about more than just that. so maybe the easiest way to see that is to think about the recursive application of the optimal substructure lemma. and again relevant subproblems at the end of the day are going to correspond to all of the different distinct subproblems that ever get solved over the entire trajectory of this recursive implementation. so, i mean just think about one sort of example path in the recursion tree, right? so in the outermost level recursion you've got the whole item set, let's say there's 100 items one through 100, you're going through and trying all possibilities of the root. so at some point you're trying out root number 23 to see how it does. you have to recurse once on items one through 22 to optimally build a search tree for them, and similarly for items 24 through 100. now let's sort of drill down into this first recursive call where you recurse on item just one through 22. now here again, you're going to be trying all possibilities of the route, those 22 choices. at some point you'll be trying route number seventeen. there's again going to be two recursive calls. and the second recursive call is going to be on items eighteen through 22. it's going to be the items that were passed through this recursive call. a prefix of the original items. and then the second recursive call here, locally is going to be on some suffix of that prefix. so in this case, the items eighteen through 22. a suffix of the original prefix, one through 22. so, in general, as you think through this recursion multiple levels. at every step, what you've got going for you is, you're either deleting a chunk of items from the beginning, a prefix. or you're deleting a chunk of items from the end. but you might be interleaving these two operations. so it is not true that you're always going to have a prefix of a suffix of the original set of items. but. what is true is that you will have some contiguous set of items. it's going to be. if you, if you have i as your smallest item in the subproblem and j is the biggest, you're going to have all of the subproblems in between. and that's because you were only plucking off items from the left or from the right. so that's why c is the correct answer. you need more subproblems than just prefixes and suffixes. alright, so that was a little tricky, identifying the relevant sub problems but now that we've got them in our grubby little hands the dynamic programming algorithm as usual is just going to fall in to place, the relevant collection of sub problems unlocks the power in a very mechanical way of its entire paradigm. so let's now just fill in all the details. the first step is to formalize the recurrence. that is, the way in which the optimal solution of a given subproblem depends on the value of smaller subproblems. this is just going to be a mathematical formula which encodes what we already proved in the optional substructure lemma. and then we're going to use this formula to populate a table in a dynamic programming algorithm to solve, systematically, the values for all of the subproblems. so let's have some notation to put in our recurrence, in our formula. we're going to be indexing sub-problems with two indices i and j and this is because we have two degrees of freedom where the continuous interval of item starts i, and where the continuous interval of items ends, j. so for a given choice i and j, where of course i should be the most j. i'm going to denote by capital c sub ij, the weighted search cost of an optimal binary search tree just in the contiguous set of in, items from i to j. and of course, the weights or the probabilities are exactly the same as in the original problem they're just inherited here, pi through pj. so now let's state the recurrence. so, for a given sub problem cij, we're going to express the value of an optimal binary search tree in terms of those of smaller sub problems. the optimal sub structure lemma tells us how to do this. the optimal substructure lemma says, that if we knew the roots, if we know the choice of the root r which here is going to be somewhere between the items i and j, then in that case, the optimal solution has to be composed of optimal solutions to the two smaller sub-problems united under the root. now we don't know what the root is. there's a j - i + one possibilities. it could be anything between i and j inclusive. so as usual, we're just going to do brute force search over the relatively small set of candidates that we've identified. so brute force search we encode by just explicitly having a minimum. so i choose some route r somewhere between i and j inclusive. and given a choice of r we're going to inherit the weighted search cost of the optimal solution on just the prefix of items i through r minus one. so on our notation that would be c of i. r minus one. similarly we pick up the weighted search cost of an optimal solution to the suffix of items r plus one through j. and if you go back to our proof of the optimal substructure lemma you'll see we did a calculation which gives us a formula for what, how the weighted search cost of a tree depends on that of its subtrees. and in addition to the weighted search cost contributed by each of the two search trees we pick up a constant, namely the sum of all of the probabilities in the items we're working with. so here that's sum of. p sub k, where k ranges from the first item in the sub problem i to the last item in the sub problem j. so one extra edge case we should deal with is if we choose the root to be the first item, then the first recursive term doesn't make sense, then we'll have c, i, i minus one, which is not defined. similarly, if we choose the root to be j, then this last term would be c of j plus 1j which is not defined. remember indices are supposed to be in order. so in that case, we'll just interpret these capital c's as zero. and so why is the recurrence correct? well all of the heavy lifting was done and our proof of the optimal substructure lemma. what did we prove there? we proved the optimal solution has to be one of just j minus i plus one possible things. it depends only on the choice of the root. given the root, the rest is determined for us. the recurrency is by definition doing brute force search through the only set of candidates. so therefore, it is indeed a correct formula for the optimal solution value, in terms of optimal solutions to smaller sub problems. 
so now that we have our magic formula, our recurrence, all that's left to do is to systematically solve the subproblems. as usual, it is crucial that we solve the subproblems in the right order, from smallest to largest. how should we measure the size of a subproblem in the optimal binary search tree problem? the natural way to do it is the number of items in the subproblem. so if you're starting at i and you're going till j, the number of items in that subproblem is j - i + 1 that, and that's going to be our measure of subproblem size. so let's bust out our [inaudible] array. the number of dimensions of this array is going to be two and that's because we have two different degrees of freedom for annexing subproblems, one for the start of the contiguous interval, one for the end. so the outer for loop is going to control the subproblem size. it's going to ensure that we solve all smaller subproblems before proceeding to larger subproblems. specifically, we'll be using an index s s and in the iteration of this outer for loop, whatever the current value of s is, we're only going to consider subproblems of size s + 1. so you should think of s as representing the difference between the larger index j and the earlier index i. the inner for loop controls the first item in the contiguous interval that we're looking at, so that's just i. and now, all we have to do is rewrite the reccurrence in terms of the array entries and with this change variable where s corresponds to j - i. that is for a given subproblem, starting with the item i and ending with the item i + s. we just by by brute force pick the best route, so the route here is going to be similar to i and i + s. regardless of the choice of the route, we pick up the constant, the sum of the pks where here, k, is ranging from the first item i to the last item i + s. and then we also look at the previously computed optimal solution values for the two relevant subproblems, one starting in i, ending in r - 1. the other starting in r + 1 and ending at i + s. so a couple of quick comments about the two array lookups on the right-hand side of this formula. so first of all, if you choose i to be, the root to be the first item i, then the first lookup doesn't make sense. if you choose it to be the last item, the second array lookup doesn't make sense. in that case, it's understood we're just going to interpret these lookups as zero. of course, in an actual implementation, you'd have to include that code but, i'll let you take care of that on your own. so the second comment is just our usual sanity check and again, you should always do this when you write out a dynamic programming algorithm. when you write down your formula to populate the array entries, make sure that on the right-hand side, whenever you do an array lookup, that is indeed already computed and available for constant time lookup. so in this case, whatever our choice of the root is, the two relevant subproblems are going to involve strictly fewer items than what we started with. and therefore, the two subproblem lookups on the right-hand side will indeed have been computed in some previous iteration of the outer for loop. remember, the outer for loop is ensuring we solve some problems from smallest number of items up to largest number of items. and of course, after the two for loops complete, what we really care about is the answer in a of one comma n, that is the optimal binary search tree value for all of the items that's the eventual output. some students like to think about these double for loops pictorially. so let's imagine a, the 2-d array is laid out as a grid. so imagine the x-axis correspond to the index i, that is the first item in the set of items we're looking at and the y-axis corresponding to j, the last item in the current set. and let me single out the diagonal of this grid, so these are subproblems corresponding to i = j, that is subproblems with a single element. now, we only ever solve problems where j is at least as large as i, so that means we're really only filling in the upper left or northwestern part of this table. so we never bother to fill in the southeastern, the bottom right part of this table. we just sort of think of it all as zero. now, in the first outer iteration. so, when s0. = 0, that's when our dynamic programming algorithm solves, in turn, each of the n single item problems. so, in the first iteration of this double for loop, it's going to solve the subproblem a11. in the next iteration of the inner for loop, it's going to proceed the a22, then a33, and so on. in each of those, both of the array lookups are going to just correspond to zero and we're just going to fill in this diagonal with the base cases, where aii is just the probability of item i. then, as the dynamic programming algorithm proceeds, we're going to be filling in the upper left portion of this table diagonal by diagonal. each time we increment s, the index in the outer for loop, we're going to march up to the next northwesternmost diagonal, and then as we step through the possible values of i, we're going to fill in that diagonal one at a time moving from southwest to northeast. when we're filling in the value of a subproblem on one of these diagonals, all we need to do is lookup the value for two subproblems on lower diagonals. lower diagonals correspond to subproblems with strictly fewer items. so that's it. that's the dynamic programming algorithm that computes the value of an optimal binary search tree given a set of items with probabilities. i'm not going to say anything about correctness. it's the, the same story as we've seen in the past. all the heavylifting is improving the optimal substructure lemma, that gave us the correctness of our occurrence given that our magic formula is correct and we're just applying it systematically, correctness of the dynamic programming algorithm follows in a straightforward way, just by induction. let me, however, make some comments about the running time. so, let's just follow the usual procedure. let's just look at how many subproblems got to get solved and then how much work has to get done to solve each of those subproblems. so as far as the number of subproblems, it's all possible choices of i and j, where i is at most j, or in other words, it's essentially half of that n by n grid. so this is roughly n squared over two, let's just call it theta of n squared, so a quadratic number of subproblems. now, for each of the subproblems, we have to evaluate this recurrence, we have to evaulate the formula, which conceptually is a breadth-first search through the number of candidates that we've identified. and a disctinction between this dynamic programming algorithm and all of the other ones we've seen recently, sequence allignment, knapsack, computing independent set of the line graphs, is that it's actually kind of a lot of options for what the optimal solution can be. that is, our breadth-first search, for the first time, is not over a merely constant number of possibilities. we have to try every possible route, each of the items in our given subproblem is a candidate route and we try them all. so, given a start item of i and an end item of j, there's j minus i plus one total items and we have to do constant work for each one of those choices. so there will be subproblems, some subproblems that we can evaluate quickly and only say constant time if i and j are very close to each other, but for a constant fraction of the subproblems we have to deal with, this is going to be linear time, theta of n time. so over all, that gives us a cubic running time, theta of n cubed. alright, so i would say this running time is sort of okay, not great. so it is polynomial time, that's good. that's certainly way, way, way faster than enumerating all of the exponentially many possible binary search trees, so it blows away breath-first search. but it's not something i would call blazingly fast or for free primitive or anything like that. so you're going to be able be to solve problem sizes with n in the 100's, but probably not n in the 1000's. so that will cover some applications where you'd want to use this optimal binary search tree algorithm, but not all of them. so it's good for some things, but it's not a universal solution. on the other hand, here's a fun fact. and the fun fact is you can actually speed up this dynamic programming algorithm significantly. you can keep with the exact same 2-d array with the exact same semantics. again, each index is going to correspond to the subproblem with the optimal binary search tree between items i and j inclusive. but, you can actually fill up this entire table all n squared entries using only a total of n squared time. that is on average, constant work per subproblem. so this fun fact, it's very clever. it's really more intricate than what we discussed in this video here, but it, it's not impossible to read. so if you're interested, i encourage you to go back to the original papers or search the web for some other resources on this optimized speed up version of this dynamic programming algorithm. i mean, at a very high level, sort of from 30,000 feet, the goal is to avoid doing this breadth-first search over all possible routes in every single subproblem. and it turns out there's structure, nice structure in this optimal binary search tree problem that allows you to piggyback on the work done in smaller subproblems. so, in smaller subproblems, you already searched over a bunch candidate roots and it turns out using the results of those previous breadth-first search is, you can make inferences about which subset of the current set of roots might conceivably be the ones that determine the recurrence. and so, that lets you avoid searching over all of the possible candidates for the roots, instead focusing just on a very small set. in fact, the average, on average, constant number of possible roots over all of the subproblems. and needless to say, this speeding up of the running time from cubic to quadratic really significantly increases the problem sizes that you can now apply this algorithm to. so now, instead of being stuck in the hundreds, you'd certainly be able to solve problem sizes in the 1000's, possibly even in the 10,000's using this quadratic time algorithm. very cool. 
algorithms are everywhere. whether you are writing software, analyzing a genome, predicting traffic jams, producing automatic movie recommendations, or just surfing the internet, you're dealing with algorithms. every single branch of computer science uses algorithms, so a course on algorithms and data structures is an essential part of any cs curriculum. >> it's important that the algorithms we use are efficient as users want to see the search results in a blink of an eye even if they search through trillions of web pages. a poorly thought out algorithm could take literally centuries to process all the webpages indexed by a search engine or all the facebook posts. and thus, algorithmic improvements are necessary to make these systems practical. that's why tech companies always ask lots of algorithmic questions at the interviews. >> in data science problems, like ranking internet search results, predicting road accidents, and recommending movies to users, advanced algorithms are used to achieve excellent search quality, high prediction accuracy, and to make relevant recommendations. however, even for a simple machine learning algorithm like linear regression to be able to process big data is usually a challenge. when advanced algorithms such as deep neural networks are applied to huge data sets they make extremely accurate predictions. recently starting to even outperform humans in some areas of vision and speech recognition, but getting those algorithms to work in hours instead of years on a large dataset is hard. and performing experiments quickly is crucial in data science. >> algorithms are everywhere. each of trillions of cells in your body executes a complex and still poorly understood algorithm. and algorithm are the key for solving important biomedical problems such as what are the mutations that differentiate you from me and how is it they relate to diseases. in this specialization you will learn the theory behind the algorithm. implement algorithm in the programming language of your choice and apply them to solving practical problems such as assembling the genome from millions of tiny fragments, the largest jigsaw puzzle ever assembled by humans. >> to conclude, algorithms are everywhere. and it is important to design your algorithms and to implement them. to turn you into a pro in algorithm design we will give you nearly 100 programming assignments in this class. your solutions will be checked automatically, and you will learn how to implement, test, and debug fast algorithms solving large and difficult problems in seconds. we look forward to seeing you in this class. we know it will make you a better programmer. >> algorithms are everywhere. in fact you just saw five algorithms solving the fundamental sorting problem in computer science, and they all have different running times. in fact while four of them are about to finish one will take a much longer time. in this specialization you'll be able to implement all of these algorithms, and master the skill of answering both algorithmic and programming questions at your next interview. 
hi, in this short video, we will solve a code problem together. the problem itself is quite elementary. so the main purpose of this video is to show you the general pipeline of solving code problems in this class. the problem is called a plus b. in this problem, we are given two digits on the standard input and our goal is to output their sum on the standard output. you can see two sample tests here on the page. for example, if the input for your program consists of the integers 3 and 2, then your program should output 5. if, on the other hand, the input consists of two integers 7 and 9, then your program is supposed to output 16, of course. the next section consists of three starter files. which in this particular case are actually solutions, not just starter files. so the files are for programming languages python, java, and c++. finally the last section contains an instruction consisting of four steps on how to solve this problem. so let's go through these four steps together. the first step is to download one of the starter files. for this, let's select the c++ starter file, and let's download it. okay. now the file is here and let's take a look inside. so, this is a very simple solution which, first, creates three variables. then it reads a and b from the standard input. then it computes the sum. and then finally it outputs the sum on the standard output. so to run this program we first need to compile it. so this is the second step of our instruction. for this we highly recommend to use the same compiler flags that are shown on this page. this will ensure that your program behaves in the same way on the testing system and on your local machine. so in this case let's just copy the flags and use it to compile our program. so this produced an executable file a.out and we can now run it. let's give this program for example 2 and 3. so the output is 5. well, one more test. 9 and 4, the output is 13 as expected. so far, so good. and now the next step is to actually submit this solution to this problem. let's go to my submission tab. press create submission, then replace a file by this .cpp file and press the submit button. so after a while, the testing system shows that our solution passed all the tests, which is quite satisfactory. in this particular case, the grader output is empty, meaning that there is no error message in this case. to illustrate it one more time, let me repeat the whole procedure quickly for the python programming language. so we first download the starter python file. let's take a look at what is inside. so the program is again very simple as expected. so we just take a and be from the standard input and we output the sum of a and b. now we need to run this program for this you might want to go to this available programming languages page. again just to check how we run python scripts so we just use python, python 3. so let's do this. for example 4, 4 and the output is 8 which is as expected. so let's just go ahead and submit this solution. so you go to the my submission tab, you press the create submission button and then you replace this file by aplusb.py. so when the file is uploaded, you finally press the submit button. so in a few seconds, this solution will be accepted by the testing system. well this wasn't very challenging, right? in the next video we will see a much more interesting example of a computational problem. for this problem we will start with a naive solution. we will submit this solution to a testing system to figure out that it is buggy actually. that there is a bug. so we will fix this bug, submit it again, to find out that our solution is slow. we will get a time limit exceeded feedback message from the testing system. meaning that for large data sets it works in more than one second, for example. so this will require us to come up with a much, more faster solution. we will implement it, submit it to the system, again, to find out that it is still buggy. we will use stress testing to locate the bug, to fix it, and to finally submit a correct solution to the system which will pass all the tests. 
in this video, we will consider a more interesting code problem. this problem will require us to implement a faster than a naive solution first, then to debug it, then to stress test it before we actually come up with a correct solution that will be able to pass all the tests in the testing system. so the problem is called the maximum prioritized product problem. in this problem, we're given an array, or a sequence of n numbers. and our goal is to find a number which can be obtained by multiplying some two numbers from this sequence. so recall that our problem is given a sequence of n non-negative integers to find the maximum pairwise product. so the input to this problem consists of two lines. the first line contains just a single number n, which is at least 2 and at most 2 multiplied by 10 to the 5. the next line contains the sequence of n numbers which are non-negative integers not exceeding 10 to the 5. so our goal is to output a single number, the maximum pairwise product. the page contains also two sample tests. for example, if an input to our program is a sequence of length 3 consisting of numbers 1, 2, 3, then the output should be, of course, 6, because this is just 2 multiplied by 3. for a slightly longer sequence of size 10, the output should be 140, and this is 10 multiplied by 14. so as usual there is also a section that contains starter files and let's begin, choose c++ as our programming language for this problem. this is how the starter c++ solution file looks like. so we start by reading the number n from the standard input. we then create an array, or a vector of size n. we then fill in this vector by elements of our sequence, element by element. then we read from the standard input. we then pass the real numbers to the function maxpairwiseproduct that computes the answer to our problem. let's see what is going on inside the function maxpairwiseproduct. so we start by initializing the variable result by 0 and we then go through all possible pairs of different elements. that is, we go through all possible i and j, where i is less than j. we do this by two nested loops. and for each such pair, we check whether the product of the corresponding two numbers is greater than our current result. and if it is, we update the result. finally, we return the result. so this algorithm is correct for an obvious reason. the goal in our problem is to find the maximum pairwise product, right? so to do this we just go through all possible pairs and select the maximum product. okay, to check that it is indeed correct, let's, as usual, compile it. so for this we use. the compiler flags for c++. okay, and then, we test it. for example, for the sequence of length 3, consisting of numbers 1 through 3, then say 6. let's test it one more time. for example, this sequence of length 5. consisting of numbers 3, 4, 5, 1, 2. so the answer is 20 as expected, because this is the product of 4 and 5. okay, so it seems that this solution is correct and let's just go ahead and submit it as a solution to the testing system. so i've just submitted our current solution to the grading system, but unfortunately it hadn't passed all the tests. in particular, there is an error message shown in the grading system. let's take a look. so it says that our solution failed test case number three, which consists of just two numbers. the first one is 10 to the 5, and the second one is 90,000. let's check whether we can reproduce this behavior on our local machine. so let's call our program with just two numbers, 10 to the 5 and 90,000. so indeed, as a result, instead of getting nine billions as we would expect, we got some random number. so this usually happens when there is an integer overflow. and indeed, so we use the standard integer type for storing the result of multiplying two numbers. while nine billion is too much for the integer type, so it doesn't fit into the integer type. so what we are supposed to do is actually to use a long, long c++ type. so let's do this and check whether this will fix our problem, our solution. so for this we need to change all the places where we compute and store the results. so first of all we change the declaration of this variable, we change also the type of the return observe returns result. when computing the product of this to numbers we first cast. for example, the first one to the long, long type. okay, and also here, great. let's now compile it once again. yes, and let's run it to check whether it correctly processes our previous data set. so 10 to the 5 and 90,000. great, so it correctly outputs nine billion. so now, we must be completely sure that our solution is correct. because, well, first of all, to find the maximum pairwise product, we'd just go through all possible pairs and select the maximum product. so what can we do else? and also, we now use the right type for storing the result. so let's go ahead and submit our new solution to the grading system. unfortunately our solution is still incorrect. so in this particular case, the output of the grading system is the following: failed case 4, time limit exceeded. so it means that our solution is quite slow, that for large datasets it runs in more than one second. so let's try to understand why it so happens. first of all, the maximum size of a data set is the maximum size of a sequence for this problem, in our case, it is roughly 10 to the 5. so what is the number of operations performed by our algorithm on such a sequence? so recall that in our algorithm we just go through all possible pairs of input numbers. so this is roughly n squared. so when n is equal to 10 to the 5, this is 10 to the 10. and this is more than one billion. and one billion is roughly the number of basic operations that modern computers can perform in one second. so we conclude that we need a faster algorithm to solve this problem. in particular, we realize that we need to find the maximum pairwise product, without actually going through all possible pairs of numbers. so in search of an inspiration, let's take a look at the problem page. in particular let's review the second sample case. in this case, our input consists of ten integers, and the output in this case, is 140. and why is that? well, this is just because in our sequence there are two numbers, 14 and 10. these two numbers are two maximal numbers actually. so for any other two numbers, their product are less than 140. so this gives us an idea: that to find the maximum product of two numbers from our sequence, it is actually enough to find two maximal numbers in our sequence. this is because all of our integers in this sequence are non-negative. so let's just implement this idea in our c++ file. so this is already implemented, and it looks as follows. so the new function is called maxpairwiseproductfast. so it already uses the long.long type for storing the result and we do the following. just by scanning the input array two times, we find two maximal numbers. so in the first block, we find the first maximum and we store its index in the max_index variable, and in the second block, we find the second maximal element. so the difference in the second block is that we also need to skip the first maximal element while scanning the array. so what we do is, we do this in this check. okay, and then we just return the product of two maximum numbers in our array, right? let's, as usual, compile it and see that everything works correctly. so, this is our new file, max_pairwise_product_fast.ccp. we compile it and we first run it on some small sequence. for example, size 3, for example, 2, 3, 4 it gives us 12 as expected. great, now let's do the following. let's check what is the running time when a huge data set. so for this, let's instead of reading the data from the standard input, let's just generate a huge array here. let's use vector of int of size 10 to the 5, and let it be an array filled by zeroes, let's compile it and let's run it. so it outputs zero immediately which means that it, well, it is much faster then the previous algorithm. so now our solution just must be completely correct. so it is fast, it uses the right type for storing the solution and it is also obviously correct because, well it selects two maximal numbers, right, and multiplies them. no other product of two numbers can be larger because we selected two maximal numbers. however, unfortunately, even this solution is not correct. so you can submit it to the testing system and see that the output will be wrong answer for some test case. in the next video, michael will tell you about stress testing and will show you how to fix this solution. 
the previous video ended on an unsuccessful attempt to submit our solution to the max pairwise product problem. even worse, we don't know what is the test on which our program fails because the system doesn't show it to us. this is actually a pretty standard situation when solving algorithmic programming assignments. in this, and the next few videos, we will learn how to overcome this situation and to even avoid it in the first place. i will explain to you and show you in action a powerful technique called stress testing. by the end, you will be able to implement a stress test for your solution of an algorithmic problem, use it to find a small and easy test on which your program fails, debug your program, and make sure it works correctly afterwards with high confidence. also, you will be able to list and apply the set of standard testing techniques, which should be always applied when solving algorithmic programming assignments. so what is stress testing? in general, it is a program that generates random tests in an infinite loop, and for each test, it launches your solution on this test and an alternative solution on the same test and compares the results. this alternative solution you also have to invent and implement yourself, but it is usually easy, because it can be any simple, slow, brute force solution, or just any other solution that you can come up with. the only requirement is that it should be significantly different from your main solution. then you just wait until you find a test on which your solutions differ. if one of them is correct and another is wrong, then it is guaranteed to happen, because there is some test for which your wrong solution gives the wrong answer, and your correct solution gives correct answer, and so they differ. if, however, both of your solutions are wrong, which also happens often, they are still almost guaranteed to have some test on which one of them gives wrong answer and another one gives correct answer because they're probably wrong in different places. when you find a test on which your solutions' answers differ, you can determine which one of them returns wrong answer and debug it, fix it, and then repeat the stress testing. now let's look at the practical implementation. i've already implemented the stress test for this problem. it is in the file called stress_test.cpp. let's look into that. so it is almost the same as the solution that we've sent in the previous video, but i've added some things. first, we add this #include <cstd.lib>. and this include just allows us to use a part of standard library to generate some random numbers. and we will use it to generate some random tests automatically. then we have the same code of two functions, maxpairwiseproduct and maxpairwiseproductfast, which we used in our last solution which was submitted in the system. but now in the main function, we have a whole additional while loop. here it is, and this is where the stress test itself is. so what do we do in principle is we generate some random tests, then we launch both solutions, maxpairwiseproduct and maxpairwiseproductfast on this random test, and we compare the results. and the idea is if you have a correct solution and another correct solution and the correct answer for your problem is the only correct answer, then any two correct solutions will give the same answers for any test. and if you have some wrong solution and some correct solution, then on some tests, their answers will differ. and also if you have two wrong solutions, then probably they're wrong in a different way, and then there will also be some test, hopefully, on which their answers differ. if you generate a lot of tests, then with some probability, at some point, you will generate a test for which the answers of the solutions differ. you can detect that situation, and then look at the input test and at the answers. and you can determine which of the algorithms was right, and which was wrong, maybe both were wrong. but anyway, you will find at least one of the algorithms which are wrong because if their answers are different, then at least one of them gives wrong answer. and then you will be able to debug that algorithm, fix the bug, and then run the stress test again. and either, you will again find some difference, or you won't find any difference anymore, and then hopefully, you fixed all the bugs in all the solutions, and you can submit it. so how it works in practice. first, we need to generate the test for our problem. we'll start with generating number n, the number of numbers. and our problem states that n should be at least 2. so we first generate a big random number using function rand. then we take it modulo 10, and it gives us some random number between 0 and 9, and then we add 2. and so we get a random number between 2 and 11. why is it so small? well, we first want both our solutions to work fast enough. and also if we create a big random test, it will be hard for us to debug it, so we start with some relatively small value of n. we immediately output it on the screen, so that if we find some tests for which our solution is wrong, we immediately see it on the screen. after generating n, we should generate the array of numbers, a, itself. so we iterate n times, and we add random numbers from 0 to 99,999 to the end of array a. so these are the numbers in the range which is allowed. and then we also output all these numbers in one line, separated by spaces and a newline character. so by this time, we've already output the whole input test on the screen. now what we need to do is actually need to launch both our solutions on this input test and get two results, the result of the main solution and the result of the fast solution. after that, we compare those two results. if they are different, it means that at least one of the solutions was wrong. so we output words wrong answer on the screen, and we also output the first result, a space, and the second result and a newline character. after that, we break our loop. we can notice that our while loop is a so-called infinite loop, but we actually end it as soon as we find some test for which our solutions differ. if we don't find the different sum test, we just output the word ok on the screen, to denote that actually both solutions have already computed the answers, but the answers are the same. and then we continue. so we continue our infinite loop, in the search of the test that will save us, that we can use to debug our solution. and we wrote that code just in front of all the code to read the numbers from the input, to compute the answer, and to output it to the screen. so we basically inserted this code in our regular program. but instead of reading numbers from the input, it will first try to find the test for which our two solutions differ. and in the next video, we will launch this stress test to find a test in which our solutions differ, debug our fast solution, and fix it. 
in this video we will launch the stress test to find a test on which our main solution and alternative solutions differ, use this test to debug our main solution and fix it. so we save the file. now we will compile this file because we will want to run it. and to compile this file, we go to the section solving programming assignments. we look at the c++ flags, copy them, and then use them to compile our file. also, of course, specify which file want to compile and also the name for the output file will be stress_test, without .cpp. so we compile our stress test and then we launch it and see what happens. so we see, our screen was filled with a lot of lines. actually, those lines were words, ok, and some numbers, like number n from 2 to 11, and input numbers, which are big. and there are a lot of tests for which our solutions didn't differ, and that all went pretty fast. and then in the end, we have number 11, which is n, and then 11 big numbers, which is the input test, and then the words, wrong answer and two very big numbers which are different, which are the answers correspondingly of the first and the second solution we implemented. so now, we already have a test on which at least one of our solution runs incorrectly. but it is very hard to work with this test because we have 11 numbers and they are very big and the answer to this problem is even bigger. so let us try to find a smaller test on which our solution still fails. usually we can create a very small test with this technique. so let's go back to our code, stress_test.cpp. and let's decrease the restrictions on n and on the numbers. so, for example, we want our number n to be no more than 5, then we first generate a random number modulo 4 so it will be from 0 to 3. and then, after adding 2, it will be from 2 to 5. and also, we want our numbers to be less than 10. so we just generate numbers modulo ten. and that's everything we change. now we save the file. we recompile with the same options. and we again launch our stress test. now, we found a test on which solutions differ even faster. only maybe five or seven tests were ok. and now we have a test with five numbers, which are 2, 9, 3, 1, and 9, on which our solutions differ. and the answer of the first solution is 81. and the answer of the second solution is 27. which one of those is correct? of course, the first one is correct, because we have numbers 9 and second 9, which are the biggest in the input, and their product is 81. so our first naive solution gives a correct answer, but our second fast solution gives an incorrect answer. and let's think why that can be. well, that can be if it multiplies wrong numbers from the array. so, let's go back in the code and check which are those numbers. we can go into the code and we look into the code for function maxpairwiseproductfast. it's very convenient that we have a separate function for this solution because the error should be localized somewhere here. so to simplify our goal of debugging, let's output the indices of the maximum number and of the second maximum number that at least this function thinks are two maximums in the array. so we output them to cout. first the first index, then a space to separate them, then the second index, and then newline character. so we save our program. we again recompile it, and we again launch the stress test. so note that although we generate random tests, it is safe to launch our stress test again because those are pseudo random. the sequence of generated random values is the same. so this is a reproducible test. so now, again, we have the same test in which two solutions differ with five numbers, the same five numbers. and now, under the line with five numbers, we have another line which says 1 2. this is actually our debug output, which means that the index of the first number, which is maximum, is 1 and it corresponds to number 9 because we enumerate our numbers from 0 in our c++ array. and the second number has index 2, and this is number 3, again, because we enumerate from 0. so, our fast solution thinks that the second maximum number in the array is the number 3 instead of number 9, which is in position four. so let's go back to the code and see why it happened. again, we open the file with the code. we go to the function maxpairwiseproductfast. we ignore the code for the max_index1 and we're very interested in the code for max_index2. so now let's look at this complex if expression, where we try to determine whether a number at position j may be a new maximum, which is different from the maximum we've already found. and let's look more closely at this particular first condition. so here, what we do is we check that number at position number j is different from the maximum we've already found. but that's exactly the problem. what we need is instead, that j is different from max_index1 because we don't want to find the same number, but we can find number which is equal to the first found maximum. so, instead of this comparison, what we actually need is to compare j, to max_index1. if we fix our code this way, then we will be at least able to find the second number 9 in our input array 2 9 3 1 9, which we saw. so let's see how it goes after we fix this. so we save the file, we again recompile it with the same options and we launch our stress test. so now what we see is a screen which is very quickly filling with some numbers, some words ok, and then some numbers again and words ok. and we should wait for some time, for maybe 10 seconds or even 30 seconds to be sure that we cannot find a test in which our solutions differ. but, by now, it seems that that is the case. and in the next video, we will do more testing and then submit our main solution to the system and see how it goes. 
in this video, we will do more testing first, and then submit our main solution to the system, and see how it goes. by the end, you will be fully prepared to test your programming assignments in the future. for some small tests with a small n and small numbers, we cannot actually find any tests in which our solutions differ. that doesn't yet mean that both our solutions work. maybe they will break for bigger values of n, or for bigger values of the input numbers. let's check for that. so we stop our problem with a control-c. we go into the code, again. and, we increase back the restrictions on n, on the numbers. for example, n will be up to 100. and the numbers will be up to 100,000, the maximum value permitted in the problem statement. we save the code, recompile, and launch the stress test again. and now we see that, again, our screen is filling with numbers and sometimes words ok. but, there are many more numbers in each test, and the numbers are bigger. and still we're waiting and waiting and nothing happens. our program just proceeds filling the screen with oks. so, maybe it works. or maybe we can test for even bigger values of n, because the numbers we used for testing are already big, but n is not that big yet. so, let's estimate what is the value of n for which we still can test. our slow solution works in quadratic time. so, testing on 100,000 numbers would be probably too long. so maybe we can test for a value of n up to 1,000. let's do that, recompile, launch the stress test again. so now our screen is almost completely filled with numbers. but we will know if we find a test in which our program breaks, because it will then stop. and as soon as it doesn't stop and proceeds filling our screen with numbers, it means that for all those tests, everything works correctly. so by now, we're pretty sure that our solution should work correctly. we stop the stress test. and now we want to make it back a solution to the initial problem on the stress test. and to do that, we go to the main function, and we comment out the while loop, which implements the stress test. and now, everything else in the main loop is the solution of the initial problem. so, that's it, we can probably already submit this file. so let's save it, and just in case, recompile it. and test it on some simple test case. for example, we input number two as the two numbers. and then three and five as the input numbers. what we see is, instead of just number 15, on the screen, we also see some number one and zero. what is that? well, disregarding what is that, if we submitted this exact solution to the system, we would get an incorrect result on the first test we encounter in the system, because we only have to output the correct answer to the problem and nothing else to their output. and that is a very frequent mistake that people do in their first programming assignment, so please beware of that. please only output what is stated in the problem statement and nothing more than that, nothing less than that. so in this case, we only need to output number 15. so let's fix this additional output. now we probably already guessed that this additional output is this debug output of indices, which our fast solution finds. so let's comment this also out. save the file, recompile, and launch our program again. and then input the same test. now we see that it only outputs 15, which is the correct answer. so now, we're pretty safe. we can try to submit that solution to coursera, actually. so, let's go and do that. so we create a new submission, and we upload the file, the file is stress_test.cpp. that is the file with which we worked, right now. we wait for it to upload. and then, we submit. now we have to wait for the grader to grade our solution. and, great, it passed in the system. this illustrates the stress test technique. but let us give you some additional advice. first, you notice that the stress test magically gave us the test in which our main solution and alternative solution differed very quickly. but that's not always the case. for example, in this problem, max pairwise product, the only test cases on which our main and alternative solution differed were the test cases where the two biggest numbers are the same. and that's a rare event, especially when you generate big random numbers from zero to 100,000. so we were actually lucky to quickly generate such a test. and that's true in the general case. often, the cases when there are equal numbers are some kinds of corner cases. also, the same happens when you are working, for example, with strings of latin letters and, the cases where the strings only contain one letter, a, or only contain two different letters, for example a and b, as opposed to strings which can contain arbitrary latin letters. those cases with only a few different letters are sometimes corner cases. so, when you want to do stress testing and you cannot find quickly a test on which your main solution and alternative solution differ, try to generate tests in a more focused subspace of full possible tests. for example, if you are working with graphs, try generating a disconnected graph, a full graph, a bipartite graph. another important point is that if you found a test in which your solutions differ, don't hurry to debug something. first try to generate the smallest and easiest test on which your solutions differ. this will simplify your debugging a lot. don't expect stress testing to be a silver bullet. for example, it probably won't find out that your main solution is too slow because your alternative solution will probably be even slower, some brute force solution. and you will have to generate only small random tests to compare them. in other cases, integer overflow. even if you generate random tests with big numbers, both of your solutions could potentially have integer overflow problems. and then you will not notice that your main solution has them in the stress test. so first, test for those cases, and then apply stress testing. however, if you've done manual tests, integer overflow testing, and max test, and then apply stress testing, you're almost guaranteed to have success with it. because the authors of the problem do basically the same when they make the test sets for you. they make some manual tests, they make tests with big numbers for integer flow, they make big tests for checking time limit, and then they generate some random tests and maybe some focused random tests. but that's all. so, you will probably find all the traps they've set up for you with just testing. in conclusion, know that it is very important to write programs that work on all of the allowed test cases, not just on most of the allowed test cases. in practice, if you write programs with bugs that only rarely reveal themselves, still some of your users will encounter those bugs, and their experience will deteriorate significantly. also, sometimes in the start of a project, you want to test some hypothesis and you write some code to check it. then you get some results and base your decisions on that result. only to later learn that you had a bug in the code and all your decisions were wrong. that's why we've prepared thorough test suites to check all the aspects of your problems. and here, stress testing will be very helpful when your solution fails on some test and you don't even know what is the test on which your program fails. we hide the tests on purpose. in real life, you often also won't know what are the exact conditions under which your program fails. but now, you are fully prepared to test your programs, debug them, and fix them. 
hello everybody. i'm daniel kane. welcome to the data structures and algorithms specialization. for this very first lecture, we're going to start at the very beginning and talk about why do you need to study algorithms in the first place. so the basic goal in this lecture is to sort of talk about what are the sorts of problems that we're going to be discussing in this algorithms class and why they're important. and in the context of doing this, we're also going to discuss some problems that you might run into when writing computer programs that might not actually require sophisticated techniques that we'll be discussing in this course. and on the other hand, we'll discuss some other sorts of problems that you might want to solve that go beyond the sort of material that we will be talking about here. so, to begin with, suppose that you're writing a computer program. there are a lot of tasks that you might want to perform that you don't really need to think about very hard. these are things like displaying a given text on the screen, or copying a file from one location to another, or searching a file for a given word. each of these algorithms has essentially a linear scan. you go through every word in the file, one at a time and you do the appropriate thing. and for each of these problems there's essentially a linear scan that you really can't do much better than. in order to do whatever task it is you're doing, you have to go through all the data one at a time and process it appropriately. and so when you do more or less the obvious thing, you have a program that works. it solves the problem that you need. and it does so approximately as efficiently as you could expect. so for these sorts of problems you might not have to think very hard about what algorithm you are using. on the other hand, there are some other problems, actual algorithms problems, where it's not so clear what it is you need to do. for example, you might be given a map and need to find the shortest path between two locations on this map. or you might be given, you might be trying to find the best pairing between students and dorm rooms given some sort of list of preferences, or you might be trying to measure the similarity of two different documents. now, for these problems it's a lot more complicated, it's not immediately clear how to solve these problems. and even when you do come up with solutions, often the simple solutions to these problems are going to be far too slow. you could end up with some simple algorithm, you could try all possible pairings between people and dorm rooms and return the one that optimizes some function that you're trying to deal with. on the other hand, if you did that, it would probably take a very, very, very long time. and you might not have enough time to wait, and so you might need to do something better. and then even once you have a reasonably efficient algorithm for these problems, there's often a lot of room for further optimization. improve things so that things run in an hour rather than a day. or a minute rather than an hour. or a second rather than a minute. and all of these improvements will have a large effect on how useful this program you've written is. now, on the other hand, there are some things that you might want to try and do with your computer that go a little bit beyond the sort of things we're discussing in this course. we might want to call these artificial intelligence problems. and these are problems where it's sort of hard to clearly state what it is that you're trying to do. an example of this might be, to try and write a computer program to understand natural language. that is, write a program where i can type something in, some english sentence, asking it, you know, what's the price of milk at the local food store today? and you want the computer to then take this sentence that i wrote, and figure out what it means, figure out some way to parse it. and then do an appropriate lookup and return a useful answer to me. and the problem with doing this isn't so much that anything involved here is actually difficult to perform, but the problem is that fundamentally we don't really understand what it means to interpret an english sentence. now, i mean we can all speak english, hopefully, if you're listening to this lecture, but we don't sort of really fundamentally understand what it means. it's hard to put it into precise enough language that you can actually write a computer program to do that. now you have similar problems, like if you want to identify objects in a photograph. you've got a picture, with maybe with a dog and tree and a cloud and you want the computer to identify what's what. then once again, this is a thing that our brains have gotten very good at doing, and we understand what the question is. however, it's hard to really put into words how you identify that this thing's a dog and this thing's a tree. and this sort of business makes it very difficult to teach a computer to do the same thing. another thing that you might want to do is teach a computer to play games well like play chess effectively. and, once again, this is a thing where we can sort of identify what it means to do this. but, actually how you want to do it, there's a lot of sort of very vague, intuitive things that go on there. it's not a clearly defined problem that you're trying to solve. and so, for all of these problems sort of the difficulty is not so much that it's hard to do things quickly. but it's hard to even state what it is that you're trying to do and figure out how to approach it. now, these are problems that we're not going to really cover in this class, we're going to focus on algorithms, how to do things quickly and efficiently. but if you do want to get into ai and want to try and solve these problems, it will be very important that you have a solid grounding in algorithms, so that once you have some idea of what does it mean to identify trees in pictures, you will have an idea of what sort of algorithms can actually support these ideas, which sort of ideas you can actually implement in a reasonable amount of time. and so, what we're going to focus on in this course are the algorithms problems. so, we want problems that are cleanly formulated, like clear mathematical problems. and some of the things we looked at maybe aren't immediately clear, like if you want to find the shortest route between two points on a map, that's not immediately a math problem. but, pretty quickly you can interpret it as such. you can say, well, i want some sequence of intersections, i'm traveling between such that each pair is connected by a road, and the sum of the lengths of the roads is as small as possible. and so, pretty quickly this just becomes a problem where we can very clearly state what it is that we're trying to do but, for which it is still nontrivial to solve it. and so, that's the sort of thing we're going to be talking about in this class. and, hopefully, by the end of it you will have a good idea of how to solve these problems, how to write programs that will solve them very quickly and very efficiently. and that's what we'll be talking about. i hope you enjoy the rest of the class. 
hello everybody, welcome back to the data structure and algorithms specialization. here i just want to give you a brief heads up, about what we're going to be talking about in the next two lectures. so, in the next two lectures, we're going to dive right in. we're going to look into a couple of algorithms problems, in particular we're going to talk about algorithms for computing fibonacci numbers and algorithms for computing greatest common divisors. now these might be, feel a little bit weird for the very first algorithms that we're going to talk to in a class. they're kind of number theoretic and numerical, and they won't actually be that similar to what we'll be talking about in the rest of the class. so you might wonder why we're looking at them, and what you should be paying attention to. and that's really what i  want to talk to you today about. in particular, these two topics were chosen, because they were the clearest examples we could think of for why algorithms are critically important. why coming up with a good algorithm is really so important on so many problems. and in particular, both of these problems have the following very interesting properties. to begin with, both of these problems have a pretty straightforward algorithm. one where you sort of take the definition of the problem that you're trying to solve. and from that you more or less immediately extract an algorithm. you sort of take the words that you have, and you sort of interpret them as an algorithm. and more or less immediately you have some code, and it works. and it computes the things that you want it to compute. unfortunately, in both cases, these very straightforward algorithms are far, far too slow. you end up with algorithms that take thousands of years to run, even on very modest inputs. and this is not acceptable for practical purposes, you don't have millenia to wait for your computation to finish. and so you need something better, and it turns out in both of this cases there is something better. there is going to be a slightly more complicated algorithm, something that maybe requires one or two bright ideas to get it to work. this slightly more complicated algorithm, works fine and is actually incredibly fast. you can handle sort of any reasonable instance of these problems in the blink of an eye. and so, the whole point of this is to really show with these very concrete examples, that at least in these instances, and in many others that we'll see throughout the course, finding the right algorithm can make all the difference in the world. it's the difference between having an algorithm that takes more time than you will have in your life to finish, and an algorithm that is done before you even knew it started. and so really this is what you should be paying attention to. why it is so, so important that we find good algorithms. so that's what's coming up, i hope you enjoy the next couple of lectures. 
hello everybody! welcome back. today we're going to start talk about fibonacci numbers and algorithms to compute them. in particular, in this lecture we're just going to introduce the sequence of the fibonacci numbers and talk a little bit about their properties. so to begin with the fibonacci numbers is a fairly classically studied sequence of natural numbers. the 0th element of the sequence is 0. the first element is 1. and from thereon, each element is the sum of the previous two. so 0 and 1 is 1. 1 + 1 is 2. 1 + 2 is 3. 2 + 3 is 5. and the sequence continues, 8, 13, 21, 34, and so on. so it's a nice sequence of numbers defined by some pretty simple recursive rule and it's interesting for a number of reasons. it has some interesting number theoretic properties, but originally this sequence was developed by an italian mathematician as a mathematical model. and it's a little bit weird. you might try and wonder what sorts of things this could be a model for. well, it turns out that, originally, this was used as sort of a mathematical model for rabbit populations. there was some idea that if you had a pair of rabbits, it would take them one generation to mature and every generation thereafter, they'd produce a pair of offspring. and if you work out what this means, then you find out the fibonacci numbers, tell you how many pairs of rabbits you have after n generations. now, because rabbits are known for reproducing rather quickly, you might assume that the sequence therefore grows quickly, and in fact it does. it's not hard to show that the nth fibonacci number is at least 2 to the n over 2 for all n at least 6. and the proof can be made by induction. you prove this directly for n 6 or 7 just by computing the numbers and showing that they're big enough. and after that point, fn is the sum of fn-1 and fn-2. by the inductive hypothesis, you bound that below and do a little bit of arithmetic. and it's bounded below by 2 to the n/2. so that completes the proof. in fact, with a little bit more work you can actually get a formula for the nth fibonacci number as roughly 1 point square root of 5 over 2 to the n. these things grow exponentially quickly. and to sort of drive that home a little bit more, we can look at some examples. the 20th fibonacci number is 6765. the 50th fibonacci number is approximately 12 billion. the 100th fibonacci number is much, much bigger than that. and the 500th fibonacci number is this monster with something like a 100 digits to it. so these numbers do get rather large quite quickly. so the problem that we're going to be looking into for the next couple of lectures is, how do you compute fibonacci? so, if you want to use them to model rabbit populations or because of some number theoretic interest. we'd like an algorithm that as input takes a non negative integer n and returns the nth fibonacci number. and we're going to talk about how you go about doing this. so, come back next lecture and we'll talk about that. 
hello everybody. welcome back. today we'll talk a little bit more about how to compute fibonacci numbers. and, in particular, today what we're going to do is we're going to show you how to produce a very simple algorithm that computes these things correctly. on the other hand, we're going to show that this algorithm is actually very slow, and talk a little bit about how to analyze that. so let's take a look at the definition again. the zero'th fibonacci number is 0. the first fibonacci number is 1. and from there after each fibonacci number is the sum of the previous two. now these grow pretty rapidly, and what we would like to do is have an algorithm to compute them. so let's take a look at how we might do this. well, there's a pretty easy way to go about it, given the definition. so if n is 0, we're supposed to return 0. and if n is 1, we're supposed to return 1. so we could just start with a case that says if n is at most 1, we're going to return n. otherwise what are we supposed to do? otherwise, we're supposed to return the sum of the n- 1, and n- 2 fibonacci numbers. so we can just compute those two recursively, add them together, and return them. so, this gives us a very simple algorithm four lines long that basically took the definition of our problem and turned it into an algorithm that correctly computes the thing it's supposed to. good for us. we have an algorithm and it works. however, in this course, we care a lot more than just, does our algorithm work? we also want to know if it's efficient, so we'd like to know how long this algorithm takes to run, and there's sort of a rough approximation to this. we're going to let t(n) denote the number of lines of code that are executed by this algorithm on input n. so to count this is actually not very hard. so if n is at most one, the algorithm checks the if case, goes to the return statement, and that's two lines of code. not so bad. if n is at least two, we go to the if case. we go to the else condition, and then run a return statement. that's three lines of code. however ,in this case we also need to recursively compute the n-1, and n-2 fibonacci numbers. so we need to add to that however many lines of code those recursive calls take. so all in all though, we have a nice recursive formula for t(n). it's two as long as n is at most one. and otherwise, it's equal to t(n) minus one plus t(n) minus two plus three. so a nice recursive formula. now, if you look at this formula for a little bit, you'll notice that it looks very similar to the original formula that we used to define the fibonacci numbers. each guy was more or less the sum of the previous two. and in fact, from this you can show pretty easily that t( n) is at least the n'th fibonacci number for all n. and this should be ringing some warning bells because we know that the fibonacci numbers get very, very, very large, so t(n) must as well. in fact, t(100) is already 1.77 times 10 to the 21. 1.77 sextillion. this is a huge number. now, suppose we were running this program on a computer that executed a billion lines of code a second. it ran it at a gigahertz. it would still take us about 56,000 years to complete this computation. now, i don't have 56,000 years to wait for my computer to finish. you probably don't either, so this really is somehow not acceptable, if we want to compute fibonacci numbers of any reasonable size. so what we'd really like is we'd like a better algorithm. and we'll get to that next lecture. but first we should talk a little bit about why this algorithm is so slow. and to see that, maybe the clearest way to demonstrate it is to look at all of the recursive calls this algorithm needs in order to compute its answer. so, if we want to compute the n'th fibonacci number, we need to make recursive calls to compute the n-1,and n-2 fibonacci numbers. to compute the n-1, we need the n-2 to the n-3. to compute the n-2, we need the n-3, and n-4, and it just keeps going on and on. from there we get this big tree of recursive calls. now if you'll look at this tree a little bit closer, it looks like we're doing something a little bit silly. we're computing fn-3, three separate times in this tree. and the way with our algorithm works, every time we're asked to compute it, since this is a new recursive call, we compute the whole thing from scratch. we recompute fn-4, and fn-5, and then, add them together and get our answer. and it's this computing the same thing over and over again that's really slowing us down. and to make it even more extreme, let's blow up the tree a little bit more. fn-4 actually gets computed these five separate times by the algorithm. and as you keep going down more and more and more times, are you just computing the same thing over and over again? and this is really the problem with this particular algorithm, but it's not clear immediately that we can do better. so, come back next lecture and we'll talk about how to get around this difficulty, and actually get a fairly efficient algorithm. 
hello, everybody, welcome back. we're still talking about algorithms to compute fibonacci numbers. and in this lecture, we're going to see how to actually compute them reasonably efficiently. so, as you'll recall, the fibonacci numbers was the sequence zero, then one, then a bunch of elements, each of which is the sum of the previous two. we had a very nice algorithm for them last time, which unfortunately was very, very slow, even to compute the 100th fibonacci number say. so we'd like to do better. and maybe you need some idea for this new algorithm. and one way to think about it is what do you do when you compute them by hand. and in particular, suppose we want to write down a list of all the fibonacci numbers. well, there's sort of an obvious way to do this. you start off by writing zero and one because those are the first two. the next one going to be zero plus one, which is one. the next one is one plus one which is two, and one plus two, which is three, and two plus three, which is five. and at each step, all i need to do is look at the last two elements of the list and add them together. so, three and five are the last two, i add them together, and i get eight. and, this way, since i have all of the previous numbers written down, i don't need to do these recursive calls that i was making in the last lecture, that were really slowing us down. so, let's see how this algorithm works. what i need to do is i need to create an array in order to store all the numbers in this list that i'm writing down. the zeroth element of the array gets set to zero, the first element gets set to one, that's to set our initial conditions. then as i runs from two to n, we need to set the ith element to be the sum of the i minus first and i minus second elements. that correctly computes the ith fibonacci number. then, at the end of the day, once i've filled out the entire list, i'm going to return the last element after that. so, now we can say, this is another algorithm, it should work just as well, but, how fast is it? well, how many lines of code did you use? there are three lines of code at the beginning, and there's a return statement at the end, so that's four lines of code. next up we have this for statement that we run through n minus one times, and each time we have to execute two lines of code. so adding everything together we find out that t of n is something like 2n plus two. so if we wanted to run this program on input n equals 100, it would take us about 202 lines of code to run it. and 202 is actually a pretty small number even on a very modest computer these days. so essentially, this thing is going to be trivial to compute the 100th or the 1,000th or the 10,000th fibonacci number on any reasonable computer. and this is much better than the results that we were seeing in the last lecture. so in summary, what we've done in this last few lectures, we've talked about the fibonacci numbers, we've introduced them. we've come up with this naive algorithm, this very simple algorithm that goes directly from the definition, that unfortunately takes thousands of years, even on very small examples to finish. on the other hand, the algorithm we just saw is much better, it's incredibly fast even on fairly large inputs and it works quite well in practice. and so, the moral of this story, the thing to really keep in mind is that in this case and in many, many others the right algorithm makes all the difference in the world. it's the difference between an algorithm that will never finish in your entire lifetime and one that finishes in the blink of an eye. and so, that's the story with fibonacci numbers. next lecture we're going to talk about a very similar story that comes with computing greatest common divisors. so i hope you come back for that. until then, farewell. 
hello everybody. welcome back. today, we're going to be talking about computing greatest common divisors. so, in particular, what we'd like to do this lecture, is we're going to define the greatest common divisor problem. and, we're going to talk about an inefficient way to compute them. and, next lecture we'll talk about how to do better. so, okay. what are gcds? so, suppose that you have a fraction, a over b. and, you want to put it in simplest form. now, the standard way of doing this, is we want to divide the numerator and denominator both by some d, to get some equivalent fraction, a/d / b/d. fair enough. now, what d do we want to use for this? well, it needs to satisfy two properties. firstly, d had better divide both a and b, since the new numerator and denominator are both integers. but, subject to that, we would like this d to be as large as possible. so, that we can reduce the fraction as much as we possibly can. so, turning this into a definition, we say that for two integers, a and b, their greatest common divisor, or gcd, is the largest integer d that divides both a and b. okay, so this is a thing that you use to reduce fractions. however, it turns out that gcds are a critically important concept in the field of number theory. the study of prime numbers, and factorization, and things like that. and, because it's so important to number theory, it turns out that being able to compute gcds is actually very important in cryptography. and, the fact that you can perform secure online banking is, in part, due to the fact that we can efficiently compute gcds of numbers in order for our cryptographic algorithms to work. so, because of this importance, we're going to want to be able to compute gcds. so, we'd like an algorithm that, given two integers, a and b, at say, at least 0, we can compute the gcd of a and b. and, just to be clear as to what kinds of inputs we care about, we'd actually like to be able to run this on very large numbers. we don't just want something that works for gcd of 5 and 12, or 11 and 73. we'd like to be able to do things like the gcd of 3,918,848 with 1,653,264. in fact, we'd also like to be able to compute much bigger numbers, 20, 50, 100, 1000 digits. we'd still like to be able to get gcds of numbers of those sizes pretty quickly. well, let's get started. let's start by just finding an algorithm that works. what we'd like is the largest number that divides both a and b. so, one thing we can do, is we can just check all of the numbers that are candidates for this, figure out which ones divide a and b, and return the largest one. so, there's an easy implementation for this. we create a variable called best, and set it to 0. this just remembers the biggest thing we've seen so far. we then let d run from 1 to a + b, since this is the range of numbers that are valid. now, if d divides a, and d divides b, well, since d is increasing, this has to be the new best that we've seen. so, we set best equal to d, and then, at the end the of the day, we return back the best thing we've seen. so, that's a perfectly good algorithm. it works. unfortunately, it's a little bit slow, because we need to run through this for loop a + b many times. and, this means that, even once a and b are, say, 20 digit numbers, it's already going to be taking us at least thousands of years in order to run this computation. and, so that's not sufficient for the sorts of applications that we care about. we're going to need a better algorithm. so, come back next lecture, and we'll talk about how to find a better algorithm for this problem, and what goes into that. 
hello everybody, welcome back. today we're going to be talking a little bit more about computing greatest common divisors. in particular today, we're going to be talking about a much more efficient algorithm than last time. this is know as the euclidean algorithm, we'll talk about that and we'll talk a little bit about how its runtime works. just to recall for integers, a and b, their greatest common divisor is the biggest integer d that divides both of them. what we'd like to do is we'd like to be able to compute this, given two integers we want to compute their gcd. we found a bad algorithm for this and we'd like a better one. it turns out that in order to find a better algorithm, you need to know something interesting. there's this key lemma that we have, where suppose that we let a' be the remainder when a is divided by b, then the gcd(a,b) is actually the same as the gcd(a',b), and also the same as the gcd(b, a'). the proof of this, once you know what to prove, is actually not very difficult. the idea is that because a' is a remainder, this means a is equal to a' plus some multiple of b plus b times q, for some q. from that you can show that if d divides both a and b, that happens if, and only if, it divides both a' and b. because, for example, if d divides a' and b, it divides a' plus bq, which is a. from this statement, we know that the common divisors of a and b are exactly the same as the common divisors of a' and b. therefore, the greatest common divisor of a and b is the greatest common divisor of a' and b. this is the idea for the algorithm. basically, we have the gcd(a,b) is the same as the gcd(b,a'), but a' is generally smaller than a. if we compute that new gcd recursively, hopefully that will be an easier problem. now, we do need a base case for this, so we're going to start off by saying if b is equal to zero, everything divides zero, so we just need the biggest thing that divides a. we're going to return a in that case. otherwise, we're going to let a' be the remainder when a is divided by b, and we're going to return the gcd(b,a'), computed recursively. by the lemma that we just gave, if this ever returns an answer, it will always give the correct answer. at the moment, we don't even know that it will necessarily terminate, much less do so in any reasonable amount of time. let's look at an example. suppose that we want to compute the gcd(3918848,1653264). so b here is not zero, we divide a by b, we get a remainder that's something like 612000, and now we have a new gcd problem to solve. once again, b is not zero, we divide a by b, we get a new remainder of 428,000 some. we repeat this process, gives us a remainder of 183,000 some, 61,000 some. divide again we get a remainder of zero. and now b is 0, so we return the answer, 61232, and this is the right answer. you'll note though, this thing took us six steps to get to the right answer. whereas, if we'd used the algorithm from last time, we would've had to check something like 5 million different possible common divisors to find the best one. this it turns out is a lot better, and to get a feel of how well this thing works, or why it works well, every time we take one of these remainders with division, we reduce the size of the number involved by a factor of about 2. and if every step were reducing things by a factor of two, after about log(ab) many steps, our numbers are now tiny or zero, and so, basically after log(ab) many steps, this algorithm is going to terminate. this means that, suppose that we want to compute gcds of 100-digit numbers, this is only going to take us about 600 steps to do it. each of the steps that we've used here is a single division with remainder, 600 divisions with remainder is something you can do trivially, on any reasonable computer. this algorithm will compute quite large gcds very quickly. in summary, once again, we had this computational problem. there was a naive algorithm, one that was very simple, came right from the definition, but it was far too slow for practical purposes. there's a correct algorithm which is much, much better, very usable. once again, finding the right algorithm makes all the difference in the world. but here there was this interesting thing that we found. in order to get the correct algorithm, it required that we actually know something interesting about the problem. we needed this key lemma that we saw today. this is actually a theme that you'll see throughout this course, and throughout your study of algorithms. very often, in order to find a better algorithm for a problem, you need to understand something interesting about the structure of the solution, and that will allow you to simplify things a lot. in any case, that's all for today, come back next lecture, we'll start talking about how to actually compute runtimes in a little bit more detail. until then good bye. 
hello everybody, welcome back to data structures and algorithms specialization. today, we're going to be talking about what really goes into computing runtimes and really understanding how long it takes a program to work. so in particular, today we're really going to dive in. up to this point we're using this sort of rough number of lines of code executed count. and today we're going to talk about how accurate this is and what sorts of complications come in. and in particular we'll see that if we actually want something that's sort of fundamentally an accurate measure of runtime, it's going to be a huge mess. we're going to have to bring in all sorts of extra data that aren't really convenient for us. and so, we're really sort of talking about the problem that comes in with computing runtimes in algorithms. something that we're not going to resolve really until the next lecture. so to start with, let's look at this algorithm that we had for computing fibonacci numbers. remember we created an array. we assigned the 0th element to 0. the first element to 1. then, we have this big for loop where we set the i'th element to the sum of the i minus first and i minus second elements and then at the end of the day we return the nth element. so we determined that when we ran this program we executed about 2n + 2 lines of code. but we should really ask ourselves, is this number of lines of code executed really sort of an accurate description of the runtime of the algorithm? and, i mean, somehow, implicitly, this measure of lines of code assumes that, sort of, any two lines of code are sort of comparable to each other. they're sort of one basic operation. and so, let's actually look at this program in some detail and see what goes into some of these lines of code and see how valid this assumption is. so to start with, we create this array. and what happens when you try to initialize an array? well, this depends a lot on your memory management system. fundamentally, all you have to do is find some space in memory and then get to pointer to the first location. on the other hand, how exactly you find this, maybe you need to shuffle some other things around to make room for it, maybe after you allocate the array, you then want to zero out all of the entries so that you don't have junk sitting in there. and so, it's not entirely clear. it depends a little bit on how exactly your program is being interpreted. but it could be pretty fast. it could actually take a while, depending on circumstances. let's look at the next line. this is just a simple assignment, we set the 0 entry to 0. however, if you really look at this, at the machine level, you're doing a bit more work, you need to load up the pointer to the 0th element to the array, you maybe then have to do some pointer arithmetic, you then need to store this literal 0 into that spot in memory. it could actually be not one operation but a few operations. similarly when we set the first element to one, you have to do this very similar set of things. next there's this for loop and with the for loop, again every time you you have to do a few things, you need to increment the value of i. you then need to compare i to n to see if you need to break out of the loop and if it is, you need to branch, you need to move to another instruction in your program after the for loop. next up there's this addition and here we have to do some things, we have to look up two values in the array, we have to write to a third value in the array. all of this involves the same sort of pointer arithmetic, and memory lookups, and writes that we were talking about before, but we also have to do this addition. and if it were just a normal addition, maybe it wouldn't be such a big deal. however, this is addition of two fibonacci numbers, and if you'll recall from a couple of lectures ago, we found that fibonacci numbers were pretty big, in fact, so big, they probably don't fit in a single machine word. so adding two of them together actually takes a non-trivial amount of time. so somehow, not only do you have to do all of these, array arithmetic things but, the actual addition of the fibonacci numbers is actually a pretty non-trivial operation. and then we do this return stuff where we have to do an array lookup which involves all the sorts of things we talked about already and then have to do a return which sort of is going to operate with the program stack and pop it up a level and return an answer. so in conclusion, this program has six lines of code to it but the amount of work being done in various lines of code is very, very different. exactly what goes into each line of code is not sort of at all the same thing. maybe we want to reconsider the fact that this count, that the number of lines of code, is sort of our runtime. maybe we need to measure something else. so what else should we do? well, if you want to be totally correct about what we actually care about, what you need to say is, well, we're going to take this program, we're going to run it on some real life computer. and we'd like to know how much actual time it will take for this program to finish. that is fundamentally what we want to know. unfortunately, in order to figure that out we need to know all kinds of messy details. we need to know things like the speed of the computer that we're running it on. if you run it on a big supercomputer, it'll take a lot less time than if you run it on your cell phone. the system architecture of the computer will matter. exactly what operations your cpu supports and exactly how long they take relative to one another, those are all going to have an effect on your runtime. the compiler being used is also going to make a difference. in practice, what you'll do is, you'll write this program in some high-level language, in c or java or python or something, and then you'll run it through a compiler to turn it into machine code. and then the compiler, though, isn't just sort of doing something completely obvious. it's performing all kinds of interesting optimizations to your code. and which optimizations it performs, and how they interact with exactly what you've written. that's all going to have an impact on the final runtime. finally, you're going to care about details of the memory hierarchy. if your entire computation fits into cache, it will probably run pretty quickly. however, if you have to start doing lookups into ram, things will be a lot slower. ram lookups actually take a fair bit of time. if, on the other hand, you run out of memory in ram and having to start writing some of these memory operations to disk, things are going to go a lot slower. lookups to hard disk can take milliseconds which are forever in computer time. and so, exactly how much memory is stored in these various levels of the hierarchy, and exactly how long the lookups take, and how good the algorithms about to predict what things you're going to look up in the future are. those are all going to affect your runtime. and so, putting it all together, we found basically a problem. figuring out accurate runtimes is a huge mess. you need to know all of these details and you need to figure out how everything interacts. and we're going to be talking about a lot of algorithms in the class. and we're going to need to tell you about runtimes for all of them and we don't want to have to do this huge mess every single time we have a new algorithm that we want to analyze. and this is an issue. and another issue it was just that, in practice i mean, this is all assuming that you did know these details. in practice, you don't know a lot of these details, because you're writing a program, it's going to be run on somebody else's computer, and you've got no idea what their system architecture looks like on that computer, because you don't know what the computer is. you don't know who's running it. in fact, there'll be several people running it on different computers with different architectures and different speeds, and it'll be a mess. and you really don't want to compute the runtime separately for every different client. so we've got a big problem here and we're not going to solve it today but next lecture we're going to talk about how we get around this. and what we really want is we want a new way to measure runtime that allows us to get some reasonable answer without knowing these sorts of details. and one of the key tricks that you should be looking at, that we'll be using to solve this problem, is we're going to be getting things that really give us results for very large inputs. they tell us, not necessarily how long it actually takes in terms of real seconds, minutes, and hours, but tell us sort of how our runtime scales with input size. and in practice this is a very important thing, because oftentimes, we really do care what happens when we have huge inputs, millions of data points that we need to analyze, how long does it take? and so, come back next lecture, we'll talk about how we resolve some of these issues, and talk about some very useful notation that we will be using throughout the rest of this sequence. so i hope you come back then, and i'll see you there. 
hello, everybody. welcome back. today we're going to start talking about asymptotic notation. so here we're going to sort of just introduce this whole idea of asymptotic notation and describe some of the advantages of using it. so last time we ran into this really interesting problem that computing runtimes is hard, in that if you really, really want to know how long a particular program will take to run on a particular computer, it's a huge mess. it depends on knowing all kinds of fine details about how the program works. and all kinds of fine details about how the computer works, how fast it is, what kind of system architecture it is. it's a huge mess. and we don't want to go through this huge mess every single time we try to analyze an algorithm. so, we need something that's maybe a little bit less precise but much easier to work with, and we're going to talk about the basic idea behind that. and the basic idea is the following. that, there are lots of factors that have an effect on the final runtime but, most of them will only change the runtimes by a constant. if you're running on a computer that's a hundred times faster, it will take one-one hundreth of the time, a constant multiple. if your system architecture has multiplications that take three times as long as additions, then if your program is heavy on multiplications instead of additions, it might take three times as long, but it's only a factor of three. if your memory hierarchy is arranged in a different way, you might have to do disk lookups instead of ram lookups. and those will be a lot slower, but only by a constant multiple. so the key idea is if we come up with a measure of runtime complexity that ignores all of these constant multiples, where running in time n and in running in time 100 times n are sort of considered to be the same thing, then we don't have to worry about all of these little, bitty details that affect runtime. of course there's a problem with this idea, if you look at it sort of by itself, that if you have runtimes of one second or one hour or one year, these only differ by constant multiples. a year is just something like 30 million seconds. and so, if you don't care about factors of 30 million, you can't tell the difference between a runtime of a second and a runtime of a year. how do we get around this problem? well, there's a sort of weird solution to this. we're not going to actually consider the runtimes of our programs on any particular input. we're going to look at what are known as asymptotic runtimes. these ask, how does the runtime scale with input size? as the input size n gets larger, does the output scale proportional to n, maybe proportional to n squared? is it exponential in n? all these things are different. and in fact they're sort of so different that as long as n is sufficiently large, the difference between n runtime and n squared runtime is going to be worse than any constant multiple. if you've got a constant multiple of 1000, 1000n might be pretty bad with that big number in front. but, when n becomes big, it's still better than n squared. and so, by sort of only caring about what happens in this sort of long scale behavior, we will be able to do this without seeing these constants, without having to care about these details. and in fact, this sort of asymptotic, large scale behavior is actually what you care about a lot of the time, because you really want to know: what happens when i run my program on very large inputs? and these different sorts of scalings do make a very large difference on that. so suppose that we have an algorithm whose runtime is roughly proportional to n and we want it to run it on a machine that runs at about a gigahertz. how large an input can we handle such that we'll finish the computation in a second? well if it runs at about size n, you can handle about a billion sized inputs, before it takes more than a second. if instead of n, it's n log n it's a little bit slower, you can only handle inputs the size about 30 million. if it runs like n squared, it's a lot worse. you can only handle inputs of size about 30,000 before it starts taking more than a second. if the inputs are of size 2 to the n, it's incredibly bad, you can only handle inputs of size about 30 in a second. inputs of size 50 already take two weeks, inputs of size 100 you'll never ever finish. and so the difference between n and n squared and 2 to the n is actually really, really significant. it's often more significant than these factors of 5 or 100 that you're seeing from other things. now just to give you another feel of sort of how these sort of different types of runtimes behave, let's look at some sort of common times that you might see. there's log n, which is much smaller than root n, which is much smaller than n, which is much smaller than n log n, which is much smaller than n squared, which is much smaller than 2 to the n. so, if we graph all of these, you can see that these graphs sort of separate out from each other. if you just look at them at small inputs, it's maybe a little bit hard to tell which ones are bigger, there's a bit of jostling around between each other. but if we extend the graph outwards a bit, it becomes much more clear. 2 to the n starts after about 4 really taking off. really just 2 to the n just shoots up thereafter and becomes 20 or 30, it just leaves everyone else in the dust. n squared keeps up a pretty sizable advantage though against everyone else. n log n and n also are pretty well separated from the others. in this graph, root n and log n seem to be roughly equal to each other, but if you kept extending, if you let n get larger and larger, they'd very quickly differentiate themselves. square root of 1 million is about 1,000. log of 1 million is about 20. and so really as you keep going out, very quickly the further out you go the further separated these things become from each other, and that's really the key idea behind sort of asymptotics. we don't care so much about the constants, we care about what happens as your inputs get very large, how do they scale. so that's it for today. come back next lecture. we'll talk about in sort of detail what this actually means and how to actually get it to work. so until next time. 
hello, everybody. welcome back. today, we're going to be talking about big-o notation, which is the specific, sort of asymptotic notation that we will be using most frequently here. so, the idea here is we're going to introduce the meaning of big-o notation and describe some of its advantages and disadvantages. so to start with the definition. the idea is that we want something that cares about what happens when the inputs get very large and sort of only care about things up to constants. so we are going to come up with a definition. if you've got two functions, f and g. g(n) is big-o of g(n). if there are two constants, capital n and little c, such that for all n, at least n. f(n) is at most c*g(n). and what this means is that at least for sufficiently large inputs, f is bounded above by some constant multiple of g. which is really sort of this idea that we had from before. now, for example, 3n squared plus 5n plus 2 is o of n squared, because if we take any n at at least 1, 3n squared plus 5n plus 2 is at most 3n squared plus 5n squared plus 2n squared, which is 10n squared. some multiple of n squared. so, and in particular if you look at these two functions, they really in some sense do have the same growth rate. if you look at the ratio between them, sure it's large, its 10n equals 1, but as n gets large it actually drops down to about 3. and once you're putting in inputs, at n equals 100, n squared is a million 3n squared + 5n + 2 is a little bit more than 3 million. so, they're not the same function. one of them is distinctly larger than the other, but it's not larger by much, not by more than a factor of about three. throughout this course, we're going to be using big-o notation to report, basically, all of our algorithms' runtimes. and, this has a bunch of advantages for us. the first thing that it does for us is it clarifies growth rate. as i've said before, often what we care about is how does our runtime scale with the input size. and this is sort of an artifact to the fact that we often really care about what happens when we put really, really, really big inputs to our algorithm. how big can we deal with, before it starts breaking down? and, if you gave me some sort of complicated expression in terms of the input, with lots of terms, then it might be hard given two algorithms to really compare them. i mean, which one's bigger would depend on exactly which inputs i'm using. it requires some sort of annoying computation to determine where exactly one's better than the other. but, if you look at things asymptotically what happens as n gets large? it often becomes much more clear that, once n is very, very large, algorithm a is better than algorithm b. the second thing it does for us is that it cleans up notation. we can write o(n), instead of 3n + 5n + 2. and that's a lot cleaner and much easier to work with. we can write o(n) instead of n + log(n) + sin(n). we can write o(n log(n)) instead of 4n log(n) + 7. and note, that in the big o, we don't actually need to specify the base of the logarithm that we use. because log(n), and log(n), and log(n), and log(n), they only differ by constant multiples. and up to the constant multiples, this big o that we have really doesn't care. another consequence of this is that because our notation is cleaner, because we have fewer lower order terms to deal with, this actually makes the algebra that we have to do easier. it makes it easier to manipulate big o expressions because they're not as messy. and the final thing this does is that this big o notation really does solve these problems we were talking about a couple of lectures ago. in order to compute runtimes in terms of big o, we really don't need to know things like how fast the computer is, or what the memory hierarchy looks like, or what compiler we used, because, by and large, although these things will have a big impact on your final runtime, that impact will generally only be a constant multiple. and if two things are only off by a constant multiple, they've got the same big o. that's all there is. now, i should say that there's a warning. big-o is incredibly useful, we are going to be using it for basically everything in this course, but it does lose a lot of information about your runtime. it forgets about any constant multiples. so, if you have two algorithms, and one of them's a hundred times faster, they have the same big-o. but, in practice, if you want to make things fast, a factor of 100 is a big deal. even a factor of two is a big deal. and so, if you really want to make things fast, once you have a good asymptotic runtime, you then want to look into the nitty-gritty details. can i save a factor of two here? can i rearrange things to make things run a little bit smoother? can i make it interact better with the memory hierarchy? can i do x, y and z to make it faster by these constant factors that we didn't see beforehand? the second thing that you should note along these lines is that big o is only asymptotic. in some sense, all it tells you about are what happens when you put really, really, really, really, really big inputs into the algorithm. and,well, if you actually want to run your algorithm on a specific input. big o doesn't tell you anything about how long it takes in some sense. i mean, usually the constants hidden by the big o are moderately small and therefore you have something useful. but sometimes they're big. sometimes an algorithm with worse big o runtime, that's worse asymptotically on very large inputs, actually, for all practical sizes, is actually beaten by some other algorithm. and there are cases of this where you find two algorithms where a works better than b on really, really, really big inputs. but sometimes really, really, really big means more than you could ever store in your computer in the first place. and so, for any practical input you want to use algorithm b. in any case, though, despite these warnings, big o is incredibly useful. we're going to be using it throughout this course. and so, next lecture, we're going to be talking a little bit about how to deal with big o expressions, how to manipulate them, how to use them to compute runtimes, but once you have that we'll really be sort of ready to do some algorithms. in any case, that's all for this lecture, come back next time and we'll talk about that. 
hello everybody. welcome back. today we're going to be talking about using big-o notation. so the basic idea here, we're going to be talking about how to manipulate expressions involving big-o and other asymptotic notations. and, in particular, we're going to talk about how to use big-o to compute algorithm runtimes in terms of this notation. so recall, we said that f(n) was big-o of g(n). if for all sufficiently large inputs f(n) was bounded above by some fixed constant times g(n). which really says that f is bounded above by some constant times g. now, we'd like to manipulate expressions, we'd like to, given expressions write them in terms of big o in the simplest possible manner. so there's some common rules you need to know. the first rule is that multiplicative constants can be omitted. 7n cubed is o of n cubed. n squared over 3 is o of n squared. the basic premise that we had when building this idea was that we wanted to have something that ignores multiplicative constants. the second thing to note is that you have two powers of n. the one with the larger exponent grows faster, so n grows asymptotically slower than big-o of n squared. root n grows slower than n, so it's o of n. hopefully this isn't too bad. what's more surprising is that if you have any polynomial and any exponential, the exponential always grows faster. so n to the fifth is o of root two to the n. n to the 100 is o of 1.1 to the n. and this latter thing is something that should surprise you a little bit. because n to the 100 is a terrible runtime. two to the 100 is already so big that you really can't expect to do it ever. on the other hand, 1.1 to the n grows pretty modestly. 1.1 to the 100 is a pretty reasonable-sized number. on the other hand, what this really says, is that once n gets large, maybe 100 thousand or so, 1.1 eventually takes over, and starts beating n to the 100. and it does so by, in fact, quite a bit. but it doesn't really happen until n gets pretty huge. in a similar vein, any power of log n grows slower than any power of n. so log n cubed is o of root n. n log n is o of n squared. finally, if you have some sum of terms smaller terms in the sum can be omitted. so n squared plus n. n has a smaller rate of growth. so this is o of n squared. 2 to the n + n to the 9th. n to the 9th has a smaller rate of growth, so this is o(2 to the n). so, these are common rules for manipulating these expressions. basically these are the only ones that you'll need most of the time to write anything in terms of big-o that you need. okay, so let's see how this works in practice. if we actually want to compute runtimes using big-o notation. so let's look at this one algorithm again. so we created an array. we set the 0th element to 0 and the first element to 1. we then went through this loop, where we set each element to the sum of the previous two. and then returned the last element of the array. let's try computing this runtime in terms of big-o notation. so, we're just going to run through it operation by operation and ask how long it takes. first operation is we created an array, and let's for the moment ignore the memory management issues, assume that it's not too hard to allocate the memory. but let;s suppose that what your compiler does is we actually want to zero out all of these cells in memory and that's going to take us a little bit of work. because for every cell, basically what we have to do, is we need to zero out that cell, we then need to increment some counter to tell us which cell we're working on next and then maybe we need to do a check to make sure that we're not at the end. if we are at the end, to go to the next line. now for every cell we have to do some amount of work. we have to do something like do a write, and the comparison, and an increment. and it's not entirely clear how many machine operations this is. but it's a constant amount of work per cell in the array. if there are n plus 1 cells. this is o of n time, some constant times n. next we set the zeroth elements of the array of zero. and this might just be a simple assignment. we might have to load a few things into registers or do some pointer arithmetic, but no matter whether this is one machine operation or five or seven, that's still going to be a constant number of machine operations, o(1). similar is setting the first element to one again, o(1) time. next we run through this loop, for i running from two to n, we run through it n minus one times, that's o(n) times. the main thing we do in the loop is we set the ith element of the array to the sum of the i minus first and i minus second. now the lookups and the store, those are all of the sorts of things we had looked at, those should be o of 1. but the addition is a bit worse. and normally additions are constant time. but these are large numbers. remember, the nth fibonacci number has about n over 5 digits to it, they're very big, and they often won't fit in the machine word. now if you think about what happens if you add two very big numbers together, how long does that take? well, you sort of add the tens digit and you carry, and you add the hundreds digit and you carry, and add the thousands digit, you carry and so on and so forth. and you sort of have to do work for each digits place. and so the amount of work that you do should be proportional to the number of digits. and in this case, the number of digits is proportional to n, so this should take o(n) time to run that line of code. finally, we have a return step, which is a pointer arithmetic and array lookup and maybe popping up the program stack. and it's not quite clear how much work that is, but it's pretty clear that it's a constant amount of work, it doesn't become worse as n gets larger. so, that's o of one time. so, now we just have to add this all together. o of n plus o of 1 plus o of 1 plus o of n times through the loop times o of n times work per time through the loop plus o of 1, add it all together, the dominant term here, which is the only one we need, is the o of n times o of n. that's o of n squared. so this algorithm runs in time o of n squared. now, we don't know exactly what the constants are, but o of n squared means that if we want to finish this in a second, you can probably handle inputs of size maybe 30,000. now, depending on the computer that you had and the compiler and all of these messy details, maybe you can only handle inputs of size 1,000 in a second. maybe you can handle inputs the size of million in a second. it's probably not going to be as low as ten or as high as a billion but, i mean, 30,000's a good guess and well, it takes work to get anything better than that. and so, this doesn't give us an exact answer but it's pretty good. okay, so that's how you use big-o notation. it turns out that occasionally you want to say a few other things. big o really just says that my runtime is sort of bounded above by some multiple of this thing. sometimes you want to say the reverse. sometimes you want to say that i'm bounded below. and so there's different notation for that. if you want to say that f is bounded below by g, that it grows no slower than g, you say that f(n) is omega of g(n). and that says that for some constant c, f(n) is at least c times g(n), for all large n. now instead of saying bounded above or bounded below, sometimes that you actually want to say that they grow at the same rate. and for that you'd see f is big-theta of g(n). which means, that f is both big-o of g, and, big-omega of g. which says, up to constants, that f and g grow at the same rate. finally, sometimes instead of saying that f grows no faster than g, you actually have to say that it grows strictly slower than g, and for that you say f(n) is little-o of g(n). and that says that, not only is the ratio between f(n) and g(n) bounded above by some constant, but actually this constant can be made as small as you like. in particular this means that the ratio f(n) over g(n) goes to zero as n goes to infinity. so, these are some other notations that you'll see now and then. you should keep them in mind. they're useful. big-o is the one that usually shows up, because we actually want to bound our runtimes above. it's sort of the big, important thing, but these guys are also useful. so, to summarize the stuff on asymptotic notation. what it lets us do is ignore these messy details in the runtime analysis that we saw before. it produces very clean answers that tell us a lot about the asymptotic runtime of things. and these together make it very useful. it means we're going to be using it extensively throughout the course. so you really ought to get used to it. but, it does throw away a lot of practical useful information. so if you really want to make your program fast, you need to look at more than just the big-o runtime. but, beyond that, we're going to use it. with this lecture, we basically finished the sort of introductory material that we need. next lecture i'll talk to you a little bit about sort of an overview of the rest of the course and some our philosophy for it. but after that, we'll really get into the meat of the subject. we'll start talking about key important ways to design algorithms. so, i hope you enjoy it. 
hello everybody, welcome back to the data structures and algorithm specialization and the algorithmic toolbox course within it. this is the last lecture in the introductory unit and here we're going to give sort of an overview of the course. and in particular, what we're going to do is we're going to talk about sort of the philosophy of the course, and how it fits into the what we're going to be teaching you within the rest of this course. so, there's a problem. algorithm design is hard, and in particular it's hard to teach. and by this i actually mean something pretty specific. now, algorithms solve many, many different problems. you can use them to find paths between locations on a map, or find good matchings with some property, or identify images in a photograph. many, many different sort of unrelated sounding problems can all be solved by algorithms. and because the sorts of things that an algorithm problem might ask you to do are so varied, there's no unified technique that will allow you to solve all of them. and this is different from what you see in a lot of classes, when you're learning linear algebra they talk about how do you solve systems of linear equations. and they teach you some technique, like row reduction, and then you're sort of done. you just sort of need to practice it, and you can solve any system of linear equations. they give you a system of linear equations, you turn the crank on this row reduction technology and out pops an answer. for algorithms there isn't that sort of thing. there's no general procedure where i give you an algorithms problem and you sort of plug it into this machine and turn a crank and out pops a good algorithm for it. and this makes it hard to teach. if there was such a thing, we could just teach you, here's this thing that you do. you do this, and you'll have a good algorithm for any problem you might run into. and it's harder than that. i mean, sometimes, in order to find a good algorithm, it requires that you have a unique insight. you're working on some problem that no one's ever looked at before. in order to find a good algorithm for it, you need to come up with some clever idea that no one else has ever come up with before. this is why sort of algorithms are so well studied, why they're such an active field of research. there are still so many different new things yet to be discovered there. and we certainly can't teach you things that haven't been discovered yet. and we also can't teach you things custom tailored to the problems that you are going to run into in your life as a programmer. so since we can't teach you everything you need to know about how to solve all of your algorithm problems, what can we teach you? well, there are sort of two things. one thing that we can definitely give you is practice designing algorithms. we're going to have lots of homework problems with lots of things for you to work on, and this will give you practice, how do you, given a problem you haven't seen before, come up with a good algorithm for it? once you have the algorithm, how do you implement it and make sure everything works and runs reasonably well? that's something you can practice. and it turns out that for the type of problems where they're sort of very general and can be many different things, i mean, it's possible to solve a lot of them, and one of the ways to be able to solve them is practice. but we're also going to do more. we're not just going to throw you in the deep end and say, try to swim, try to program all of these algorithms. there is something useful. we can't teach you a generic procedure that will solve any algorithms problem for you. but what we can do is we can give you some common tools. some very useful tools for algorithm design. and especially in this first course in our specialization we're really going to focus on helping to build up your algorithmic toolbox. and in particular, this course is going to focus on three of the most common and most generally applicable algorithmic design techniques. the first of these is greedy algorithms. this is something where you're trying to construct some big object, and the way you do it is you sort of make one decision in the most greedy, locally optimal way you can. and once you've made that decision you make another decision in the most greedy, locally optimal way you can. and you just keep making these decisions one at a time until you have an answer. and surprisingly somehow making these locally optimal decisions gives you a globally optimal solution. and when this happens it gives you very clean algorithms and it's great. that's the first thing we'll talk about. next, we'll talk about divide and conquer, which is a technique where you've got some big problem you're trying to solve. what you do is you break it into a bunch of little pieces, you solve all the pieces, and then you put their answers together to solve the original thing. finally we'll talk about dynamic programming. this is a little bit more subtle of a technique. this is what you get when you've got some sort of large problem, that has sort of a lot of, not sub-problems, but sort of related problems to it. and this sort of whole family of related problems, their solutions sort of depend on one another in a particular type of way. and when you have it there's this great trick that you have, where you sort of start at the small problems at the bottom of the pile. and you solve all of them. and you sort of keep track of all of your answers. and you use the answers to the small problems, to build up to obtain answers to the larger and larger problems. so these are what we're going to talk about. each of the techniques we're going to talk about, how you recognize when it applies, how do you analyze it when it applies, and some practical techniques about how to implement, how to use them. all that good stuff. so there's one other thing before we let you go into the fun world of greedy algorithms that you should keep in mind throughout this course, and that's that there are these, maybe, different levels of algorithm design. there's sort of different levels of sophistication that go into it. at sort of the very lowest level, or top of this slide, i guess, there is the naive algorithm. this is sort of a thing where you take the definition of a problem and you turn it into an algorithm, and we saw this for fibonacci numbers and greatest common divisors. you sort of interpreted the definition of the thing you wanted to compute as an algorithm, and you were done. now, these things are often very slow, as we saw. often they look like in order to find the best way of doing something, we enumerate all ways to do it, and then figure out which one's the best. on the other hand, these are slow, but it's often a good idea to first come up with a naive algorithm, just make sure you have some algorithm that works. sometimes this works well and often you can just be done with it. other times, it's too slow, but at least you made sure that you understood what problem you were working on and have something that runs. but after that, the next thing that you want to do, if this naive algorithm is too slow, is you try and look at your tool box. you say, are there any standard techniques that i know that apply here? maybe there's a greedy algorithm that solves this problem, or maybe i have to use a dynamic program. but if you can find one of these standard techniques that work, often that doesn't involve too much effort on your part, and gives you something that works pretty well. now once you have something that works, you often want to optimize it. and there are lots of ways to improve an existing algorithm. reduce the runtime from n-cubed to n-squared or n-squared to n. and to do this, there are just a whole bunch of things. maybe sometimes you could just sort of rearrange the order in which you do the operations to cut out some of the work that you do. sometimes you have to introduce a data structure to speed things up. there are a bunch of ways to do this. we'll talk a little bit about how this works. and these three levels are things that you should be comfortable with and able to apply pretty well by the end of this course. however, sometimes these three are not enough. sometimes a naive algorithm is just too slow, the standard tools don't apply, there's nothing that you can really optimize to improve things. sometimes in order to get a workable algorithm, what you need is magic. you need some unique insight that no one else has ever had before. you need some sort of clever new idea and these, there's only so much we can do to teach you how to produce magic. we will show you some examples of things that really did have clever ideas that maybe you can't reproduce the thought process like, how do you come up with this crazy idea, that just happens to make this work? you should at least be able to appreciate the sort of thought that goes into this sort of thing. in any case it's something to keep in mind when looking on that, when thinking about our problems, and what sort of things are expected of you. in any case, that is basically it for the introductory segment. we've talked a lot about sort of why algorithms are important and given you some examples. we've talked about asymptotic notation, but now it's time to let you go to the rest of the course. the rest of the course will keep giving you exercises to hone your skills, and each unit of this course will cover one of these major techniques. after i leave you with the end of the introduction, michael will pick up and talk to you about greedy algorithms. next off, neil will talk to you about divide and conquer. finally, pavel will have a unit on dynamic programming. each of these, they will talk to you about where the technique applies, how to analyze it, how to implement it, all that good stuff. but this is where i leave you, i hope you enjoyed the introduction, and i will put you in michael's very capable hands to start learning about greedy algorithms starting in the next lecture. so, until then, farewell. 
hi, i'm michael levin and together we will study greedy algorithms. they are some of the simplest algorithms that exist and they usually seem a pretty natural way to try solving a problem. we will start with a few examples of problems that can be solved by greedy algorithms, just to illustrate how they work. and our very first problem is a problem about largest number. and for this problem, you will probably be able to come up with a greedy algorithm yourself, because that is really a very natural thing to do. so imagine this situation. you are trying to get a job at a company and you already had a few interviews, and everything went well. now you have your final interview with the boss, and he tells you that he will give you an offer. but instead of negotiating your salary with you, he will give you a few pieces of paper with digits written on them. and your task will be to arrange those digits in a row, so that when you read the number from left to right, that will be your salary. so basically, you have a problem. given a few digits, to arrange them in the largest possible number. and there are a few examples of such numbers on the slide. so what do you think will be the correct answer to this problem? and of course, the correct answer is 997531. and it's pretty obvious that you should arrange numbers from the largest to the smallest, from left to right. but let's look at how the greedy algorithm does that. so, the greedy strategy is start with a list of digits then find the maximum digit in the list. append it to the number and remove it from the list. and then repeat this process until there are no digits left in the list. so on the next step we will find the largest digit, which is again 9. we'll append it to the number to the right and remove it from the list. then we will find 7 as the largest digit appended to the right and remove it from the list. and then we'll continue with 5, 3, and 1. and then we get the correct answer, 997531. and in the next video, we will design an algorithm to find the minimum number of refuels during a long journey by car. we will see similarities between these two problems, largest number problem and car refueling problem. and we will define how greedy algorithms work in general. 
hi. in this video, we will consider the problem to find the minimum number of refills during a long journey by a car. you will see the similarities between this problem and the largest number problem from the previous video. by the end, you will be able to describe how greedy algorithms work in general and define what is a safe move and a subproblem. consider the following problem. you have a car such that if you fill it up to full tank, you can travel with it up to 400 kilometers without refilling it. and you need to get from point a to point b, and the distance between them is 950 kilometers. of course, you need to refill on your way, and luckily, there are a few gas stations on your way from a to b. these are denoted by blue circles, and the numbers above them mean the distance from a to the corresponding gas station along the way from a to b. and you need to find the minimum number of refills to get from a to b. one example of such route is to get from point a to the first gas station, 200 kilometers, then to get from first station to the third gas station, 350 kilometers distance. then from third gas station to the fourth gas station, 200 km, and then from the fourth gas station to b, 200 kilometers. but that's not optimal. we can do better. here is another route, which only uses two refills. we get from a to the second gas station, less than 400 kilometers, then we get from the second gas station to the fourth gas station, again less than 400 kilometers. and then, from the fourth gas station to b, only 200 kilometers. and this route uses only 2 refills, and it turns out that in this problem, the minimum number of refills is exactly 2. more formally, we have the following problem. as the input, we have a car which can travel at most l kilometers, where l is a parameter if it's filled up to full tank. we have a source and destination, a and b, and we have n gas station at distances from x1 to xn in kilometers, from a along the path from a to b. and we need to output the minimum number of refills to get from a to b, not counting the initial refill at a. we want to solve this problem using a greedy strategy, and greedy strategy in general is very easy. you first make some greedy choice, then you reduce your problem to a smaller subproblem, and then you iterate until there are no problems left. there are a few different ways to make a greedy choice in this particular problem. for example, you can always refill at the closest gas station to you. another way is to refill at the farthest reachable gas station, and by reachable, i mean that you can get from your current position to this gas station without refills. another way is, for example, to go until there is no fuel and then just hope that there will be a gas station in there. so what do you think is the correct strategy in this problem? and of course, the third option is obviously wrong. the first option is also wrong, if you think about it, but the second option is actually correct. it will give you the optimal number of refills. we will prove it later. for now, let's define our greedy algorithm as the whole algorithm. so we start at a and we need to get to b with the minimum number of refills. we go from a to the farthest reachable gas station g so that we can get from a to g with full tank without any refills in the middle. and now, we try to reduce this problem to a similar problem. we make g the new a, and now our problem is to get from the new a to b, again with the minimum number of refills. and by definition, a subproblem is a similar problem of smaller size. one example of subproblem is from the previous video. when we need to construct the largest number out of a list of digits, we first put the largest digits in front, and then we reduce our problem to the problem of building the largest number out of the digits which are left. in this problem, to find the minimum number of refills on the way from a to b, the first refill at the farthest reachable gas station g. and then solve a similar problem which is a subproblem to get from g to b with the minimum number of refills. another important term is safe move. we call a greedy choice a safe move if it is consistent with some optimal solution. in other words, if there exists some optimal solution in which first move is this greedy choice, then this greedy choice is called a safe move. and we will prove a lemma that to refill at the farthest reachable gas station is a safe move. let us first prove it visually. let's consider some optimal route from a to b, and let the first stop on this route to refill b at point g1. and let g be the farthest gas station reachable from a. if g1 and g coincide, then our lemma is proved already. otherwise, g1 has to be closer to a than g, because g is the farthest reachable from a, and g1 is reachable from a. now, let's consider the next stop on the optimal route, and that would be g2. and the first case is that g is closer to a than g2, then the route can look like this. in this case, we can actually refill at g instead of g1, and then we will have another optimal route because it has the same number of refills and g is reachable from a. and g2 is actually reachable from g, because it was reachable from g1, but g is closer to g2 than g1. so this is a correct route, and in this case, our lemma is proved. and the second case is when g2 is actually closer to a than g, and then the route can look like this. but in this case we can avoid refilling at g1 at all and refill at g2 or even refill at g in the first place. and then we will reduce the number of refills of our optimal route, which is impossible. so the second case actually contradicts our statement that we are looking at an optimal route, and we've proved our lemma. to recap, we consider the optimal route r with a minimum number of refills. we denote by g1 the position of the first refill in r, and by g2, the next stop was r, which is either a refill or the destination b. and by g we denote the farthest refill reachable from a, and we considered two cases. in the first case, if g is closer than g2 to a, we can refill at g instead of g1, and it means that refill at g is a safe move. otherwise, we can avoid refill at g1. so this case contradicts that the route r is the route with the minimum number of refills. so there is no such case, and we proved our lemma. and in the next lecture, we will implement this algorithm in pseudocode and analyze its running time. 
hi, in this video you will learn to implement the greedy algorithm from the previous video, and analyze its running time. here we have the pseudocode for this algorithm, and the procedure is called minrefills. and the main input to this procedure is array x. from the problems statement, we know that the positions of the gas stations are given by numbers from x1 to xn. and those are measured in kilometers in terms of distance from a to the corresponding gas station along the path from a to b. for convenience, we actually add to array x positions of point a which is x0 and is the smallest value in the array. and point b, which is xn plus 1, and it is the largest value in the array x. along our route from a to b, we will visit some points. of course we will start from point a. and then we'll probably go to some gas station, refilll there. and then go to another gas station and to another gas station and then to another and then at some point we will get to the point b or point x n plus 1. so we see that we only need to store the positions in the array x. we don't need to consider any positions between the elements of array x. and so, we will store in the variable currentrefill, the position in the array x where we're currently standing. and we will initialize it with 0. because we start from point a, which is the same as x0, and has index 0 in the array x. and later currentrefill will store the index in the array x, where we're currently standing. we'll also store the answer to our problem in the variable numrefills. at each point in the execution of the algorithm, it will contain the number of refills we have already made. and we initialize it with zero because the problem statement asks us to count the minimum number of refills we need to do. not counting the initial refill at point a. so when we are standing at point a, we consider that we haven't made any refills yet. then the main external while loop goes. and it goes on while we're still to the left from point b, because then we need to go right to reach our destination b. and we check this condition with this inequality, that currentrefill is at most n. this means that the position or index in the array x is at most n, and so we're to the left from point b currently. in this case we still need to go to the right. and first we save our current position in the array x in the variable lastrefill. this means that we made our lastrefill in the position currentrefill. and now we need to go to the right from there, and either get to destination b or get to the rightmost reachable gas station and refill there. and the next internal while loop does exactly that. it gradually increases our currentrefill position in the array x until it reaches the rightmost point in the array x which is reachable from the lastrefill position. so first we check that currentrefill position is at most n because if it is n plus 1 already it means that we reached our destination b. and there is no point increasing it further. if it's still to the left from b, then we'll look at the next position to the right, x currentrefill plus 1. we need to check whether it's reachable from lastrefill position or not. and first we can build the distance from the lastrefill position to the currentrefill plus one position by subtracting the values of the array x. and if this distance is at most l, then it means that we can travel this distance with full tank, without any refills. and of course, at the lastrefill position, we could fill our tank up to the full capacity. and then we'll be able to travel for l kilometers. so, this inequality checks if actually position currentrefill plus 1 is reachable from the lastrefill position. if it is, we increase the value of currentrefill and go on with our internal while loop. when we exit this internal while loop we're already maybe in the point b, or we may be in some point which is the farthest reachable gas station. now we compare it with our lastrefill position. and if it turns out that it is the same, it means that we couldn't go to the right. we don't have enough fuel even to get to the next gas station. and then, we cannot return the minimum number of refills that we need to do on the way from a to b, because it is impossible to get from a to b at all. and so we return this result impossible. otherwise, we moved at least a bit to the right, and then we need to see. if we are already in the point b, we don't need to do anything else. otherwise, we need to refill there. so, we check that we're to the left from point b with this inequality. and if it's true then we're at some gas station and we need to refuel. so we increase the numrefills variable by one. and then we return to the start of our external while loop. and there we again check if we're to the left from point b we need another iteration. and if currentrefill is already n plus 1, then we've reached point b and we need to exit the external while loop. and in that case, we just return the answer which is number of refills we've made so far. we've implemented the greedy algorithm from the previous lecture. now let's analyze its running time. from the first look it can seem that it works in n squared time because we have the external while loop which can make n iterations and internal loop which can make n iterations. so, n by n is n squared, but actually we will show that it only makes o of n actions for the whole algorithm. to prove that let's first look at the currentrefill variable. we see that it only changes one by one here. and it starts from zero. and what is the largest value that it can attain? of course, the largest value is n plus 1 because this is the largest index in the array x, and currentrefil is index in the array x. so, variable currentrefil starts from zero, changes one by one. and the largest value it can have is n plus 1. it means that it is increased at most n plus 1 times which is big-o of n. but that's not all we do. also, we increase variable numrefills here. but we also increase it always one by one. it also starts from zero. and what is the largest number that this variable can attain? well, of course, it is n because when we have n gas stations. there is no point to refuel twice at the same gas station. so we can refuel at most n times. so variable numrefills goes from 0 to n and changes one by one. so it is only changed at most n times. and so, it is also linear in terms of n. and so, we have at most n plus 1 iterations of the external while loop. everything but the internal while loop takes there constant time. this assignment, this if, and this if with assignment. and the external loop and internal loop combined also spend at most linear time of iterations. because they change variable currentrefill and it changes at most linear number of times. so all in all our algorithm works in big-o n time. let's go through this proof once again. the lemma says that the running time of the whole algorithm is big o of n. and we prove this by first noticing that the currentrefill variable changes only from zero to at most n plus 1. and the change is always one by one. that the numrefills variable changes from zero to at most n. and it also changes one by one. so, both these variables are changed big-o of n times. and everything else that happens is constant time. each iteration of the external loop, and there are at most n plus 1 iterations of the external loop. thus, our algorithm works in linear time. in the next video, we will review what we've learned about greedy algorithms in general. 
hi. in this video, we will briefly review the main ingredients of greedy algorithms and the first of them is reduction to a subproblem. basically, when you have some problem, you make some first move and thus reduce your problem to a similar problem, but which is smaller. for example, you have fewer digits left or fewer gas stations left in front of you and this similar problem, which is smaller is called a subproblem. another key ingredient is a safe move and the move is called safe if it is consistent with some optimal solution. in other words, if there exists some optimal solution in which the first move is the same as your move, then your move is called a safe move and not all first moves are actually safe. for example, to go until there's no fuel is not a safe move in the problem about car fueling. and often, greedy moves are also not safe, for example, to get to the closest gas station and refuel at it is not a safe move while to get to the farthest gas station and refuel there is a safe move. now the general strategy of solving a problem goes like this. first, you analyze the problem and you come up with some greedy choice and then the key thing is to prove that this greedy choice is a safe move and you really have to prove it. because, otherwise, you can come up with some greedy choice and then come up with a greedy algorithm and then even implement it and test it and try to submit it in the system. only to learn that the algorithm is incorrect, because the first move is actually not a safe move and there are cases in which this first move is not consistent with any optimal solution. and in that case, we will have to invent a new solution and implement it from scratch. all the work you've done before will be useless. so please prove your algorithms and prove that the first move is a safe move. when you prove that, you reduce a problem to a subproblem. and hopefully, that is a similar problem, problem of a same kind and then you start solving this subproblem the same way. you make your greedy choice and you reduce it to subproblem, and you iterate until there are no problems left or until your problem is so simple that you can just solve it right away. and in the next lessons, we will apply greedy algorithms to solve more difficult problems. 
hi, in this lesson we will discuss the problem of organizing children into groups. and you will learn that if you use a naive algorithm to solve this problem, it will work very, very slowly, because the running time of this algorithm is exponential. but later in the next lesson, we will be able to improve the training time significantly by coming up with a polynomial time algorithm. let's consider the following situation. you've invited a lot of children to a celebration party, and you want to entertain them and also teach them something in the process. you are going to hire a few teachers and divide the children into groups and assign a teacher to each of the groups this teacher will work with this group through the whole party. but you know that for a teacher to work with a group of children efficiently children of that group should be of relatively the same age. more specifically age of any two children in the same group should differ by at most one year. also, you want to minimize the number of groups. because you want to hire fewer teachers, and spend the money on presents and other kinds of entertainment for the children. so, you need to divide children into the minimum possible number of groups. such that the age of any two children in any group differs by at most one year. now, let's look at the pseudo code for the naive algorithm that solves this problem. basically, this algorithm will consider every possible partition of the children into groups and find the partition which both satisfies the property that the ages of the children in any group should differ by at most one and contains the minimum number of groups. we start with assigning the initial value of the number of groups to the answer m and this initial value is just the number of children. because we can always divide all the children into groups of one, and then of course each group has only one child so the condition is satisfied. then we consider every possible partition of all children into groups. the number of groups can be variable, so this is denoted by a number k, and we have groups g1, g2 and up to gk. and then we have a partition, we first need to check whether it's a good partition or not. so, we have a variable good which we assigned to true initially, because we think that maybe this partition will be good. but then we need to check for each group whether it satisfies our condition or not. so, we go in a for group with index i of the group from 1 to k, and then we consider the particular group gi, and we need to determine whether all the children in this group differ by at most 1 year, or there are two children that differ more. to check that, it is sufficient to compare the youngest child with the oldest child. if their ages differ more than by one, then the group is bad. otherwise, every two children differ by at most one year, so the group is good. and so we go through all the groups in a for loop. if at least one of the groups is bad, then our variable good will contain value false by the end. otherwise, all the groups are good, and the variable good will contain value true. so, after this for loop, we check the value of the variable good, and if it's true, then we improve our answer. at least try to improve it. with a minimum of its current value and the number of the groups in the current partition. and so, by the end of the outer for loop which goes through all the partitions, our variable m will contain the minimum possible number of groups in a partition that satisfies all the conditions. it is obvious that this algorithm works correctly because it basically considers all the possible variants and selects the best one from all the variants which satisfy our condition on the groups. now, let us estimate the running time of this algorithm. and i state that the number of operations that this algorithm makes is at least 2 to the power of n, where n is the number of children in c. actually, this algorithm works much slower and makes much more operations than 2 to the power of n, but we will just prove this lower bound to show that this algorithm is very slow. to prove it, let's consider just partitions of the children in two groups. of course there are much more partitions than that. we can divide them in two, three, four, and so on. much more groups. but, let's just consider partitions in two groups and prove that even the number of such partitions is already at least two to the power of n. really, if c is a union of two groups, g1 and g2, then basically we can make such partition for any g1 which is a subset of such c of all children. for any g1, just make group g2 containing all the children which are not in the first group. and then all the children will be divided into these two groups. so, now the size of the set of all children is n. and if you want to compute the number of possible groups g1 then we should note that each item of the set, or each child, can be either included or excluded from the group g1. so, there can be 2 to the power of n different groups g1. and so there are at least 2 to the power of n partitions of the set of all children in two groups. and it means that our algorithm will do at least 2 to the power of n operations because this considers every partition. among all the partitions, there are all the partitions into two groups. so, how long will it actually work? we see that the naive algorithm works in time omega (2n), so it makes at least 2 to the power of n operations. and for example for just 50 children this is at least 2 to the power of 50 or the larges number which is on the slide. this is the number of operations that we will need to make and i estimate that on a regular computer, this will take at least two weeks for you to compute this if this was exactly the number of operations that you would need. so, it works really, really slow. but in the next lesson we will improve this significantly. 
hi, in this lesson you will learn how to solve the problem of organizing children into groups more efficiently. most specifically, we will come up with a polynomial time algorithm for this problem as opposed to the exponential type algorithm from the previous lesson. but in order to do this, we first need to do a very important thing that you should probably do every time before solving an algorithmic problem. you should reformulate it in mathematical terms. for example, in this problem we will consider points on the line instead of children. for example, if we have a child of age three and a half years we will instead consider a point on the line with coordinate 3.5 and if we have another child of age 6, we will instead consider a point with coordinate 6 on the line. now, what do groups of children correspond to? if we have a group of children, it consists of several children and several points on the line correspond to this group and the fact that the age of any two children in the group differs by at most one, means that there exists a segment of length one on this line that contains all those points. now the goal becomes to select the minimum possible number of segments of length one, such that those segments cover all the points. then, if we have such segments, we can just take all the points from that segment in the same group, and any two children in the group who differ by, at most, one year. now let's look at an example. we have a line with a few points on it and we want to cover all the points with segments of length one. here is one example of such covering. all the segments on the picture are of the same length and we consider that this is length one of this line. this is not the optimal solution because below there is another example of covering and we have only three segments and they still cover all the points. now we want to find a way to find the minimum possible number of segments to cover all the points in any configuration. we want to do that using greedy algorithm and you probably remember from the previous lessons that to come up with a greedy algorithm, you need to do a greedy choice and to prove that this greedy choice is a safe move. i state that in this problem, safe move is to cover the leftmost point with a segment of length one which starts or has left end in this point. to prove that this is really a safe move, we need to prove that there exists an optimal solution with the minimum possible number of unit length segments such that one of the segments has its left end in the leftmost point. let's prove that. to do that, let's consider any optimal solution of a given problem with a given point. let's consider the leftmost point colored in green. it is covered by some segment. colored in red. now, let's move this red segment to the right until it's left end is in this leftmost point. i say that we didn't miss any of the points in the process, because this green point is the leftmost point so there are no points to the left from it and while we are moving the segment to the right, we didn't miss any of the points. it means that what we have now is still a correct covering because all of the points are still covered and the number of segments in this covering is the same as in some optimal solution from which we started and that means that it is also an optimal covering. so we have just found an optimal solution in which there is a segment which starts in the leftmost point. so, we proved that covering the leftmost point with a segment which starts in it is a safe move. now that we have a safe move, let's consider what happens after it. we have the leftmost point covered, and also maybe some other points covered. so we don't need to consider these points anymore. we are not interested in them and we need to cover all the other points with the minimum possible number of unit length segments. so this is the same kind of problem which we started with, so this is a subproblem. basically it means that we have a greedy algorithm. first, make a safe move. add a segment to the solution with the left hand starting in the leftmost point. then remove all the points which are already covered by the segment from the set and if there are still points left, repeat the process and repeat this process until there are no points left in the set. 
now let us consider the pseudocode that implements this algorithm. for the sake of simplicity we assume that the points in the input are given in sorted order from smallest to largest. we'll start with an empty set of segments denoted by r and we start with index i pointing at the first point which is the leftmost because the points are sorted. now we go through the points, and we find the leftmost point. currently i is pointing to the leftmost point in the set. and at the start of the while loop i will always point to the leftmost point which is still in the set. now we cover it with the segment from l to r which has unit length, and the left end in the point xi, so this is a segment from xi to xi+1. we add this segment to the solution set, and then we need to remove all the points from the set which already covered. instead of removing the points, we will just move the pointer to the right and forget about the points, which are to the left from the pointer. so the next while loop, does exactly that. we know that for any i that is larger than the current i, xi is to the right from the left end of the segment, because the points are sorted. so if xi is also to the left from r, then it is covered by the segment. so we just go to the right, and to the right with pointer i. and while xi is less than or equal to r, we know that the point is covered. and as soon as we find some xi which is bigger than r, it means that this point is not covered and all the points further in the array are also not covered. so we stop. and then we repeat again the iteration of the outer while loop. or maybe our pointer i is already out of the array of the input points. and then we stop and return r, which is the set of segments that we've built in the process. now let's prove that this algorithm works in linear time. indeed, index i changes just from 1 to n. and we always increase it by one. for each value of i, we add at most one segment to the solution. so overall, we increase i at most n times and add at most n segments to the solution. and this leads to a solution which works in big-o of n time. now, we had an assumption that the points in the input are already sorted. what if we drop this assumption? then we will have to sort the points first, and then apply our algorithm pointscoversorted. later in this module, you will learn how to sort points in time n log n. combining that with our procedure pointscoversorted will give you total running time of n log n. now let's look at our improvement. we first implemented a straightforward solution, which worked in time at least 2 to the power of n. and it worked very, very slowly. so that even for 50 children, we would have to spend at least 2 weeks of computation to group them. our new algorithm, however, works in n log n time. and that means that even if we had 10 million children coming to a party, it would spend only a few seconds grouping them optimally into several groups. so that's a huge improvement. now let's see how we went to this end. first, we've invented a naive solution which worked in exponential time. it was too slow for us so we wanted to improve it. but to improve it, the very first important step was to reformulate it in mathematical terms. and then we had an idea to solve it with a greedy algorithm. so, we had to find some greedy choice and prove that it will be a safe move. in this case, the safe move turns out to be to add to the solution a segment with left and in the leftmost point. and then we prove that this is really a safe move. it is very important to prove your solutions before even trying to implement them. because otherwise, it could turn out that you implemented the solution, tried to submit it, got wrong answer or some other result, different from accepted. and then, you've made a few more changes, but it still didn't work. and then, you understand that your solution was wrong, completely from the start. and then you need a new solution, and you will have to implement it from scratch. and that means that you've wasted all the time on implementation on the first wrong solution. to avoid that, you should always prove your solution first. so after we've proved the safe move, we basically got our greedy solution. which works in combination with a certain algorithm in time n log n. which is not only polynomial, but is very close to linear time, and works really fast in practice. 
hello. in this lesson, you will learn an algorithm to determine which food items and in which amounts should you take with yourself on a really long hike so that to maximize their total energy value. so, you're planning a long hike. it will take a few days or maybe a few weeks, but you don't know exactly how long will it take. so, to be safe, you need to get enough food with you. and you have a knapsack which can fit up to 15 kilograms of food in it. and you've already bought some cheese, some ham, some nuts, and maybe some other food items. you want to fit them all in the knapsack, so as to maximize the amount of calories that you can get from them. of course you can cut the cheese. you can cut the ham. you can select only some of the nuts. and then fit all of that into your knapsack. to solve this maximization problem, we again need to first reformulate it in mathematical terms. and then it becomes an instance of a classical fractional knapsack problem, which goes like this. we have n items with weights w1 through wn and values v1 though vn. and a bag of capacity big w. and we want to maximize the total value of fractions of items that fit into this bag. in this case, weights are also weights in the real life and values are the energy values of the food items that you've bought. so, here's an example, and we will denote by dollars the value of the item, and the weight just by numbers. so, for example, the first item has value $20 and has weight 4, the second item has value $18 and weight 3, and the third item has value $14 and weight 2. and we have a knapsack of capacity 7. there are a few ways with which we can fill this knapsack. for example, of them is put the whole first item and the whole second item in the knapsack. then the total value is the sum of the values of the first item and the second, which is $38. we can improve on that. for example, take the whole first item, the whole third item, and only one third of the second item for a total value of $40. we can do even better by taking the whole third item, the whole second item, and only half of the first item, and that will give us $42. and actually it turns out that this is the optimal thing to do. so now we want to create a greedy algorithm that will solve this maximization problem, and we need to get some greedy choice and make a safe move. and to do that, we have to look at the value per unit of weight. so, for example for the first item, value per unit of weight is $5, for the second item, it's $6 per unit, and for the third one it's $7 per unit. so although the first item is most valuable, the third item has the maximum value per unit. and of course there is an intuition that we should probably fit first the items with the maximum value per unit. and really, the safe move is to first try to fit the item with the maximum value per unit. and there's a lemma that says that there always exists some optimal solution to our problem that uses as much as possible of an item with the maximum value per unit of weight. and what do we mean by as much as possible? well, either use the whole item, if it fits into the knapsack, or, if the capacity of the knapsack is less than how much we have of this item, then just fill the whole knapsack only with this item. let's prove that this is really a safe move. we will prove looking at this example. so, first let's suppose we have some optimal solution, and let's suppose that in this optimal solution, we don't use as much as possible of the best item with the highest value per unit of weight. then take some item which we used in this solution and separate its usage into two parts, one part of the same size of how much we have of the best item, and the second part is everything else. then we can substitute the first part with the best item. so, for example, in this case, we substitute half of the first item with second item. of course, in this part, the total value will increase, because the value per unit of weight is better for the best item than for the item currently used. and in the general case, this will also work. so, either we will be able to replace some part of the item already used by the whole best item, or we can replace the whole item that is already used by some part of the best item. and in any case, if we can make such a substitution, of course the total value will increase, because the best item just has better value per unit of weight, so for each unit of weight, we will have more value. so this gives us a greedy algorithm to solve our problem. what we'll do is while knapsack is still not full, we will do a greedy choice. we will choose the item number i which has the maximum value of vi over wi, which is the value per unit of weight. and then if this item fits into knapsack fully, then take of all this item. otherwise, if there is only few space left in the knapsack, take so much of this item as to fill the knapsack to the end. and then in the end, we'll return the total value of the items that we took and how much did we take of each item. 
hi. in this lesson you will learn how to implement the greedy algorithm for the fractional knapsack. how to estimate its running time and how to improve its asymptotics. here is the description of the greedy algorithm from the previous lesson. while knapsack is still not full, we select the best item left. the one with the highest value per unit of weight. and either fit all of this item in the knapsack or if there is only few space left in the knapsack, cut this item and fit as much as you can in what's left in the knapsack, and then repeat this process until the knapsack is full. in the end return the total value of the items taken and the amounts taken. we've proven that the selection of best item is a safe move. then after we've selected the best item what we've got left is a knapsack with a capacity which is less, but the problem is the same: you have some items and you have a knapsack of some capacity and you should fill it optimally so as to maximize the total value of the items that fit. so this greedy algorithm really works. now let's implement it. here we have a procedure called knapsack. it starts with filling the array a with amounts of items taken with 0 and the total value we also initialize to 0 and then, as we said on the slide, we repeat for n times the following iterations. if the knapsack is already full than in the variable w, we will have 0 because in the start we have in the variable w the total capacity of the knapsack, but each time we will put something in the knapsack, we will update w will decrease it by the amount of weight that we put already in. and so in the end, when the knapsack is full, w will be 0. if w became 0, it means that we should just return the total value and the amounts of the items that we took. otherwise we should select the best item. the best item is the item which is still left so, wi is more than 0 and out of such items, it's the item with the maximum value per weight, so the one which maximizes vi over wi. when we've selected the i, we determine the amount which will it take, it is either the whole wi, the whole of this item if it fits in the knapsack. otherwise, if the capacity of the knapsack is already less, then we just fill it up to the end. so a is minimum of wi and big w. after we select the amount, we just update all the variables. so we update wi by decreasing it by a, because we took already a of this item. we're also increased the amount a of i corresponding to the item number i by the value a. and we'll also decrease the capacity left, because we just decrease it by a. by putting it a of item i. also we increase value v by this formula: a multiplied by vi and divided by wi. why is that? because we took a of item number i, we took a units and one unit brings us amount of value equal to vi over wi. so if you take a units, the total value by these items, a multiplied by vi and divided by wi. after we do n iterations, or maybe less, if the knapsack is full before we do all n iterations, we'll return the total value and the amounts in the array. now the running time of this algorithm is big-o of n squared. why is that? well, first we have the inner selection of best item, which works in linear time. because basically, we have to go through all the items to select the best one. and we have the main loop, for loop, which is executed n times at most, maybe less. so in each iteration we do some linear time computation and we do this at most n times. that means that the total running time is big-o of n squared. now we can't improve on that because if we sort the items by decreasing value of vi over wi, then it will be easier to select the best item which is still left. let's look at this pseudo code. let's assume that we've already sorted the input items, size that v1 over w1 is, more than or equal to v2 over w2 and that is greater, or equal to the fraction for the next item and up to vn over wn. and we can start with the same array of amounts and the same total value filled with zeroes. but then we make a for loop for i going from 1 to n. and on each iteration i will be the best unit which is still left. so on the start of the iteration we check whether we still have some capacity in the knapsack. if it is already filled we just return the answer. otherwise we know that i is the best item because we didn't consider it previously and it is the item with the maximum value per unit out of those which we didn't consider before. so we determine the amount of this item with the same formula and we update the weights, the amounts, the capacity, and the total value in the same way as we did in the previous pseudocode. the only change is that we change the order in which we consider the items. and this allows us to make each iteration in constant time instead of linear time. so, this new algorithm now works in linear time, because it has at most n iterations, and each iteration works at most in constant time. so, if we apply first some sorting algorithm to sort items by decreasing value of vi over wi. and then apply this new knapsack procedure. total run time will be n log n, because sorting will work in n log n. and the knapsack itself will work in linear time. 
hi, in this lesson, we will review what we saw in this module about greedy algorithms and specify what is common and important to all greedy algorithms. the main ingredients of any greedy algorithm are greedy choice and reduction to a subproblem. you have to prove that your greedy choice is a safe move. and also, you have to check that the problem that is left after your safe move is really a subproblem. that is, a problem of the same kind but with fewer parameters. after you have both, you have a greedy algorithm. then, you need to estimate its running time and check that it's good enough for you. safe moves in different problems are different. basically, you have to invent something each time you have a new problem. in the first problem it was, select the maximum digit and put it first. in the last problem it was select the item with the maximum total value per weight. and you see that in every safe move, there's something like maximum, or minimum, or first, or leftmost, or rightmost. so, always safe move is greedy, but not all greedy moves are safe. so, you really have to prove every time that the move that you invented is really safe. also, you can notice that sometimes we can optimize our initial greedy algorithm if we sort our object somehow. so, you can maybe try to solve problem, assuming that everything is sorted in some convenient order. and if you see that, because of that, your greedy algorithm can be implemented asymptotically faster, then you can just apply sorting first and then your greedy algorithm. the general strategy is when i have a problem, you can try to come up with some greedy choices, and then for some of them, you'll be able to prove that they're really safe moves. and if you've proven that this is a safe move, then you've reduced your problem to something. and then you have to check that this something is a subproblem. that is, the problem about the same thing, optimizing the same thing with the same restrictions. and then, this is a subproblem. and then you can solve it in the same way that you solved your initial problem. and you have this loop from problem to subproblem and back to the problem, always reducing it by the number of parameters. and in the end of this loop, you will have a problem so simple that you can solve it right away for one object or zero objects. and then you have your greedy algorithm. 
hi, i'm neil rhodes. welcome to the divide and conquer module. in the last module, you learned about how to use greedy algorithms to solve particular classes of problems. in this module you'll learn about ways of solving problems using divide and conquer algorithms. the term divide and conquer is quite old, and when applied to war, suggests that it's easier to defeat several smaller groups of opponents than trying to defeat one large group. in a similar fashion, divide and conquer algorithms take advantage of breaking a problem down into one or more subproblems that can then be solved independently. just as not all problems can be solved with a greedy algorithm, not all problems can be solved using divide and conquer. instead, these are both techniques that are part of a toolbox of strategies to solve problems. as you're designing an algorithm, you'll need to consider whether or not a greedy algorithm might work. if not, would a divide and conquer algorithm work? let's look at the general structure of a divide and conquer algorithm. here, we have a problem to be solved represented abstractly as a blue rectangle. we break the problem down into a set of non-overlapping subproblems. represented here, by colored rectangles. it's important that the subproblems be of the same type as the original. for example, here's a way to break down the original rectangle problem into a set of subproblems that are not of the same type. these subproblems are triangles. thus this does not represent the divide and conquer algorithm. in this case, we've broken down the original rectangle problem into a set of subproblems that are themselves rectangles. the difficulty is that these subproblems overlap with one another. thus it too does not represent the divide and conquer algorithm. we return now to breaking down our problem into a set of non-overlapping subproblems of the same original type. we break it apart, then we go ahead and solve each subproblem independently. we solve the first problem, represented by a check mark. we then continue solving each problem, in turn. once we've successfully solved each of the subproblems, we combine the results into a solution to the original problem. one question that comes up, how do we solve each subproblem? since each subproblem is of the same type as the original, we can recursively solve the subproblem using the same divide and conquer strategy. thus, divide and conquer algorithms naturally lead to a recursive solution. in practice, while you can program a divide and conquer algorithm recursively, it's not uncommon to rewrite the recursive program into an iterative one. this is often done both because some programmers aren't as comfortable with recursion as they are with iteration, as well as because of the additional space that a recursive implementation may take in terms of additional stack space. this can be language and implementation dependent. in summary, the divide and conquer algorithm consists of one: breaking the problem into non-overlapping subproblems of the same type. two: recursively solving those subproblems. and three: combining the results. in the next video, we'll see an extremely simple example of divide and conquer. 
we're going to start our divide and conquer algorithms with what might be considered a degenerate form of divide and conquer: searching in an unsorted array using linear search. here's an example of an array. to find a particular element of the array, we look at the first element, if it's not there, we look at the second element. we continue until we either find the element we're interested in, or until we reach the end of the array. this same type of search is also used to free elements that are stored in a linked list. let me describe a real-life use of linear search. twenty years ago i did consulting for a company developing software for one the first hand-held computers, the apple newton. the application translated words between any two of the languages: english, french, italian, german, or spanish. its data was stored in five parallel arrays. so for example, car was in english at the second position. in spanish, car is auto, so the second position of the spanish array contained auto. the program would take the user's input word along with from and to languages. then it would search through the corresponding from array, english for example with trying to translate car from english to spanish. if it found a match, it returned the element at the same index location and target language. with a small dictionary of three words, as in this example, this linear search is quick. however, i was brought in as a consultant to speed up the application. when users clicked on the translate button, it'd take seven to ten seconds to retrieve the translated word, an eternity as far as the user was concerned. there were about 50,000 words in the dictionary, so on average it took 25,000 word checks in order to find a match. the next video, i'll show you how we sped up this application using binary search. the problem statement for linear search is as follows: given an unsorted array with n elements in it and a key k, find an index, i of the array element that's equal to k. if no element array is equal to k, the output should be not_found. note that we say an index rather than the index, to account for the fact that there may be duplicates in the array. this might seem pedantic, but it's important to be as careful as possible in specifying our problem statement. the well known solution to this problem is a linear search. iterate through the array until you find the chosen element. if you reach the end of the array and haven't yet found the element, return not_found. we can construct a divide and conquer recursive algorithm to solve this problem. our recursive function will take four parameters: a, the array of values; low, the lower bound of the array in which to search; hgh, the upper bound of the array in which to search; and k, the key for which to search. it will return either: an index in the range low to high, if it finds a matching value; or not_found, if it finds no such match. as with all recursive solutions, we'll need to accurately handle the base case. in particular, base cases for this problem will be either: be given an empty array, or finding a match on the first element. the subproblem is to search through the sub array constructed by skipping the first element. we'll recursively search through that smaller sub array, and then just return the result of the recursive search. although this is a recursive routine that breaks the problem into smaller problems, some would argue that this shouldn't be called divide and conquer. they claim that a divide and conquer algorithm should divide the problem into a smaller subproblem, where the smaller subproblem is some constant fraction of the original problem. in this case the su-problem isn't 50%, or 80%, or even 95% of the original problem size. instead, it's just one smaller than the original problem size. i don't know, maybe we should call this algorithm subtract and conquer rather than divide and conquer. in order to examine the runtime of our recursive algorithm it's often useful to define the time that the algorithm takes in the form of a recurrence relation. a recurrence relation defines a sequence of values in terms of a recursive formula. the example here shows the recursive definition of the values in the fibonacci sequence. you can see that we defined the value for the n'th fibonacci as the sum of the preceding two values. as with any recursive definition, we need one or more base cases. here, we define base cases when evaluating f(0) and f(1). from this recursive definition, we've defined values for evaluating f(n) for any non-negative integer, n. the sequence starts with 0, 1, 1, 2, 3, 5, 8, and continues on. when we're doing run-time analysis for divide and conquer algorithms, we usually define a recurrence relation for t(n). where t stands for the worst time taken for the algorithm, and n is the size of the problem. for this algorithm, the worst-case time is when an element isn't found because we must check every element of the array. in this case we have a recursion for a problem of size n which consists of a subproblem of size n minus one plus a constant amount of work. the constant amount of work includes checking high versus low, checking a at low equals key, preparing the parameters for the recursive call, and then returning the result of that call. thus the recurrence is t(n) equals t(n-1) plus c, where c is some constant. the base case of the recursion is in an empty array, there's a constant amount of work: checking high less than low and then returning not_found. thus t(0) equals c. let's look at a recursion tree in order to determine how much total time the algorithm takes. as is normal, we're looking at worst-case runtime, which will occur when no matching element is found. in a recursion tree, we show the problem along with the size of the problem. we see that we have an original problem of size n which then generates a subproblem of size n-1, and so on all the way down to a subproblem of size zero: an empty array. the work column shows the amount of work that is done at each level. we have a constant amount of work at each level which we represent by c, a constant. alternatively, we could have represented this constant amount of work with big theta of one. the total work is just the sum of the work done at each level that's a summation from zero to n of a constant c. which is n plus one times c, or just big theta of n. this analysis seems overly complicated for such a simple result. we already know that searching through n elements of the array will take big theta of n time. however, this method of recurrence analysis will become more useful as we analyze more complicated divide and conquer algorithms. many times a recursive algorithm is translated into an iterative one. here we've done that for the linear search. we search through the elements of array a from index low to index high. if we find a match, we return the associated index. if not, we return not_found. to summarize, what we've done is one: created a recursive solution; two: defined a corresponding recurrence relation, t; three: solved t of n to determine the worst-case runtime; and four: created an iterative solution from the recursive one. what you've seen in this video, then, is an example of a trivial use of our divide and conquer technique in order to do a linear search. in our next video we'll look at a non-trivial use of the divide an conquer technique for searching in a sorted array: the well known binary search. 
hi, so let's talk now about binary search. a dictionary is a good example of a ordered list. okay, basically where every word is in order. and that makes finding words much easier. you can imagine how difficult it would be to search a dictionary if the order of the words was random. you'd have to just search through every single page, and in fact, every word on every page. it'd take quite a long time. so let's look at the problem statement for searching in a sorted array. so what we have coming in is, a, an array, along with a low and upper bound that specify the bounds within the array in which to search. what's important about the array is that it's in sorted order. what we mean by that is if we look at any index i at an element. and then the next element, that this first element is no more than the next element. we don't say less than because we want to allow for arrays that have repeated elements. so officially this is called a monotonic non-decreasing array. the other input is the key to look for. the output for this is an index such that the element at that index in the array is equal to the key. we say an element and not the element just as we did in linear search. because of the fact that there may be more than one element-- more than one element that matches because there may be duplicates in the array. if we don't have a match, instead of returning not_found as we did in the linear search case, we're going to actually return somewhat more useful information, which is where in the array would you actually insert the element if you wanted to insert it? or where would it have been, if it were there? so what we're going to return is the greatest index, such that a sub i is less than k. that is, if the key is not in the array, we're returning an index such that if you look at the element at that index, it's less than the key but the next element is greater than the key. and we do have to take account of the fact that what if every element in the array is greater than the key? in that case, we're going to go ahead and return low- 1. so look at an example. we've got this array with 7 elements in it, and the element 20 is repeated in it. so if we search in this array for 2, we want to go ahead and return 0, saying that every element in the array is larger than this. if on the other hand, we look for 3, we're going to return 1. if we look for 4, we're also going to be returning 1. which really signifies between 1 and 2. that is, it's bigger than 3 but it's less than 5. if we search for 20, we return 4. or we might also return 5. either one of those is valid because 20 is present at each of those indexes. and if we search for 60, we'll return 7. but if we search for 70, we'll also return 7. so let's look at our implementation of binarysearch. so we're going to write a recursive routine, taking in a, low, high and key, just as we specified in the problem statement. first our base case. if we have an empty array, that is if high is less than low, so no elements, then we're going to return low-1. otherwise, we're going to calculate the midpoint. so we want something halfway between low and high. so what we're going to do is figure the width, which is high- low, cut it in half, so divide by 2, and then add that to low. that might not be an integer because of the fact that high- low divided by 2 may give us a fractional portion, so we're going to take the floor of that. for example, in the previous case, we had 1 to 7, it'll be 7- 1 is 6, divided by 2 is 3 + our low is 1 is 4, so the midpoint would be 4. we'll see an example of this shortly. and now we check and see is the element at that midpoint equal to our key. if so, we're done, we return it. if not, the good news is of course, we don't have to check all the other elements, we've ruled out half of them. so if the key is less than the midpoint element, then all the upper ones we can ignore. so we're going to go ahead and now return the binarysearch in a from low to mid- 1, completely ignoring all the stuff over here. otherwise, the key is greater than the midpoint, and again, we can throw away the lower stuff and go from midpoint + 1, all the way to high. let's look at an example. so let's say we're searching for the key 50 in this array with 11 elements. so we'll do a binary search on this array, from 1 to 11, looking for 50. low is 1, high is 11. we'll calculate the midpoint, the midpoint will be 11- 1 is 10, divided by 2 is 5, add that to 1, the midpoint is 6. and now we check and see is the midpoint element equal to 50? well, no. the midpoint element is 15 and the element we are looking for, the key we're looking for, is 50. so we're going to go ahead and ignore the lower half of the array and now call binary search again, with the low equal to 7, so one more than the midpoint. so now we've got a smaller version of the problem. we're looking for 50 within the elements 7 to 11, we'll calculate the midpoint. 11- 7 is 4 divided by 2 is 2, so we'll add that to 7 to get a midpoint of 9. we check, is the element at index 9 equal to our key? the element at index 9 is 20, our key is 50, they're not equal. however, 50 is greater than 20, so we're going to go ahead and make a new recursive call with midpoint + 1, which is 10. so, again, we do our binary search from 10 to 11. we calculate the midpoint. high- low, 11- 10 is 1, divided by 2 is one-half + 10 is 10 and a half, we take the floor of that, we get 10 and a half, so our midpoint is 10 and a half. i'm sorry, our midpoint is 10. and now we check. is the value at element 10 equal to our key? well the value at element 10 is 50, our key is 50 so yes. we're going to go ahead and return that midpoint which is 10. in summary then, what we've done is broken our problem into non-overlapping subproblems of the same type. we've recursively solved the subproblems. and then we're going to combine the results of those subproblems. we broke the problem into a problem of size half (slightly less than half). we recursively solved that single subproblem and then we combined the result very simply just by returning the result. in the next video, we're going to go ahead and look at the runtime for binary search, along with an iterative version. and we'll get back to actually discussing that problem that i discussed with the dictionary translation problem. we'll see you shortly. 
hi, in this video we're going to be looking at the run time of binarysearch along with looking at an iterative version of it. so here's our binarysearch algorithm again. we look in the middle, if it's not found, then we either look in the lower half or the upper half. so whats our recurrence relation for the worst-case runtime? well, the worst case is if we don't find an element. so were going to look at t(n) is equal to t of roughly n over 2 + c. we have a floor there of n over 2 because if n is odd, let's say there are five elements, then the question is: how big is the problem size on the next call. so if we have five elements we're going to either end up looking in the upper half of the array. those two elements or the lower half of the array, those two elements because we skipped the midpoint. we already checked them. plus some constant amount of work to add together, to calculate the midpoint. as well as checking the midpoint against the key. and then our base case is when we have an empty array. and that's just a constant amount of time to check. so what's the runtime look like? we got our original size n, and we're going to break it down, n over 2, n over 4. all the way down. how many of these problems are there. well, if we're cutting something in two over and over again. it's going to take log base two such iterations until we get down to 1. so the total here, is actually log base two of n + 1. the amount of work we're doing is c. so at each level, we're doing c work. so the total amount of work if we sum it, is just the sum from i=0 to log base 2 of n of c. that is just log base 2 of n + 1, that is log base 2 of n, that quantity, plus one times c. and that is just theta of log based two of n, but really what we'd normally say is theta of log n, because the base doesn't matter. that's just a constant multiplicative factor. all right, what's the iterative version look like. the iterative version has the same parameters low, high, and key. and we have a while loop that goes through similar to the base case so in the base case of the recursive version we were stopping if high is less than low. here, we have a while loop where the while loop stops if high is less than low. we calculate the midpoint and then again check the key. if it matches the element at the midpoint we return the midpoint. otherwise, if the key is less than the element, we know we're in the first half of the array and so instead of making a new recursive call like we did in the recursive version we have the original array. and we want to look at the first half of it so we're going to change the value of high and that will be mid minus one because we already checked mid. otherwise, we want to look in the upper half of the array so we move low up. if we reach the end of the while loop. that is if we drop out of the while loop because high is less than low. that meant we have nothing more to search. we have an empty array. and therefore, we didn't find the element in the array. we're going to return low minus 1. so the same result as the recursive version. the difference is we won't be using the stack space that the recursive version uses. you remember we talked two videos ago about this real-life example where we had five languages and we were translating words between any two of those languages. the way we had that represented was parallel arrays, so that at any given index, each of the element in the arrays represented words that were the same in all those languages. so for instance, chair in english is at index two, and in spanish that's silla and in italian it's sedia. the problem was it took a long time to look, right? we had 50,000 elements in our arrays, and it took like ten seconds for searching, because we had to really search through all of them if it wasn't there, on average, half of them, just 25,000. so one question might be, why didn't we use a sorted array? right? you could imagine, for instance, sorting these arrays. here they're sorted. the good part is, it's easy to find a particular word in a particular language. so i can find house in english, for instance, and find what index that is at very quickly, using binary search. the problem is, i no longer have this correspondence, because the order of the words that are sorted in english is different from the order of the words sorted in spanish. so if i look at chair, for instance, in english, it no longer maps to silla. so instead, if i look at chair and that's to casa. so although we can find a particular word in our source language, we don't know the corresponding word in the target language. so the solution was to try and find some way we could do sorting and yet still preserve this relationship where everything at an index meant the same translated word. the way to do that was an augmented set of arrays. so what we really did was keep these augmented arrays which were pointers back into the original arrays in sorted order. so we're having a kind of level of indirection. so if i look at english for example, the order of the words in english is chair, house, pimple. well, what order is that in the original array? it is first element 2, and then element 1, and then element 3. so if you want to do a binary search, you can use this sorted array. whenever you want to look at what an element is in that represented sorted array. so for instance, if we looked at the middle element, which in the sorted array is 2, it has the value 1 and that says go find house. so we basically, say house is sort of at element 2 and chair is at element 1 and pimple's at element 3. the spanish, of course, has different mapping, so in spanish, the first sorted word happens to be the first word in the array. the second sorted word is the third word in the spanish array; and the third sorted word, silla, is the second element. so what happened when we ran this? well what happened, we had a space time trade off. we had to pay extra space. and there were, of course, not only just english and spanish sorted but also french, italian, and german. so,  five arrays, extra arrays. each array, had 50,000 entries in it and what was the size of each element of the array? well, it represented a number from one to 50,000 that can be represented in 16-bits which is two bytes. so we had 50,000 elements times 2 bytes, that is 100,000 bytes times 5 is 500,000 bytes. so about a half a megabyte, which today is almost nothing. and even then, was certainly doable 20 years ago. that's the cost we have in space. what is the benefit that we get. well, instead of having to do let's say 50,000 look ups in the worst-case. instead, we have to do log base two of 50,000 lock ups. so log base 2 of 50,000, that's about, let's see, log base of 1,000 is about ten because two to the ten equals 1024, so we have another factor of 50 to go. log base 2 of 50 is around, let's say six because i know that 2 to the 5th is equal 32, 2 to the 6th equals 64. so, what that means is, we have 16 references we have to do the array instead of 50,000. that's almost a factor of a thousand, so what that ended up meaning is that when the user clicks translate, instead of taking ten seconds, it was what appeared to be instantaneous. it was well under a tenth of a second. so in summary, what we've seen is that the runtime of binary search is big theta of log n. substantially quicker than the big theta of n that linear search takes. so sorted arrays really help. in the next lesson we're going to be looking at a more complicated application of divide and conquer, where we actually have multiple subproblems instead of just one subproblem. 
in this lecture we're going to talk about a more complicated divide-and-conquer algorithm to solve polynomial multiplication. so first we'll talk about what polynomial multiplication is. so polynomial multiplication is basically just taking two polynomials and multiplying them together. it's used in a variety of ways in computer science. error correcting codes, if you want to multiply large integers together. right, so if you've got thousand digit integers and you want to multiply them together, there's a quicker way than doing it the normal way you learned in elementary school. and that uses the idea of multiplying polynomials. it is used for generating functions, and for convolution. let's look at an example. so let's say you have polynomial a, which is 3 x squared + 2x + 5, and polynomial b, which is 5 x squared + x + 2. if you multiply them together you get 15 x to the fourth + 13 x cubed + 33 x squared + 9x + 10. why is that? well, let's look, for instance the 15 x to the fourth comes from multiplying 3 x squared times 5 x squared, that's 15x to the fourth. the 10 comes from multiplying 5 by 2. the 13 x cubed comes from 3 x squared times x, which is 3 x cubed, plus 2x times 5 x squared, which is 10 x cubed. for a total of 13 x cubed. so let's look at the problem statement. so we're going to have two n- 1 degree polynomials, all right? a sub n-1 is the coefficient of the x to the n-1 all the way down to a0 which is the coefficient of the x to the 0 term or the one term. and then we similarly have a b polynomial as well. now first you may wonder what happens if you actually want to multiply polynomials that don't happen to have the same degree? what if you want to multiply a degree three polynomial times a degree two polynomial? right, where the degree is just the exponent of the highest term. well in that case, what you you could do is just pad out the smaller polynomial, the lower degree polynomial, to have zeros for its earlier coefficients. i'll give an example of that in just a second. and then the product polynomial is the result that we want to come up with so that's a higher degree polynomial, right? if our incoming polynomials, are degree n- 1, then we're going to get a term of the x to the n- 1 in a, times x to the n- 1 in b, and that's going to give us an x to the 2n- 2 in the c term. so, the c sub 2n-2 term, comes about from multiplying the a sub n-1 term and the b sub n-1 term. the c sub 2n-3 term comes from the a sub n-1, b sub n-2, and a sub n-2, b sub n-1. so it's got two terms that multiply together. the c sub 2n-4 term would have three terms that multiply together. and we have more and more terms that get multiplied and summed together, and then fewer and fewer back down. so c sub 2 has three pairs which get added together, c sub 1 has two pairs and c sub 0 has one pair. so here's an example. this is actually the same example we had before. so n is three and all we need, notice, are the coefficients. we don't actually need to have the x's written out. so 3, 2, and 5 means 3 x squared plus 2x plus 5. 5, 1, 2 means 5 x squared plus x plus 2. what if b were only a degree one polynomial? it was just x plus 2. well then we would set b equal 0, 1, 2. that is, b's x squared term is 0 x squared. so a(x) is this, b(x) is that. when you multiply them together, we get the same result we got before. and now we just pluck off the coefficients here, so the 15, the 13, the 33, the 9, and the 10. and that's our resulting answer: those coefficients. so let's look at a naive algorithm to solve this. the naive algorithm basically just says, well first off, let's create a product array. this is basically going to be the c, the result, and it's going to be of highest degree 2n-2. so it's going to have 2n-1 terms all the way from the 0 term up to the 2n-2 term. so we'll initialize it to 0, and then we'll have a nested for loop. for i equals 0 to n-1, for j equals 0 to n-1. and at every time, what we'll do is we will calculate a particular pair. so we'll calculate the a[i], b[j] pair, multiply them together and add them into the appropriate product. which is the appropriate product to put it in? it's the i + j case. as an example, when i is 0 and j is 0, we calculate a at 0 times b at 0 and we add that to product at 0. so that says the two zero-degree terms in a and b get multiplied together to the zero-degree term in c. at the other extreme, if i is n-1 and j is n-1, we take a at n-1 times b at n-1 and we store that in the product of 2n-2. as you can see, the intermediate values in product are going to have more terms added to them than the edges. and, of course, then we return the product. how long does this take? well, this takes order n squared. clearly, we've got two for loops, one smaller for loop that's from 0 to 2n-2, so that's order n. and then a nested for loop, where i goes from 0 to n-1, j goes from 0 to n-1, so those each go through the first one n times, the second one n squared times. so our runtime is o(n squared). in the next video, we're going to look at a divide and conquer algorithm to solve this problem. although, we'll see that it too will be somewhat naive. 
so let's look at a naive divide and conquer algorithm, to solve polynomial multiplication problem. the idea is, we're going to take our long polynomial and we're going to break it in two parts. the upper half and the lower half. so a(x) is going to be d sub one of x ,times x sub n over 2, plus d sub 0 of x, the bottom half. d sub 1 of x, since we've pulled out x sub n over 2 terms, it's lowest term is actually, just a sub n over 2. so we have two parallel sub polynomials, the high and the low. we do the same thing for b. so we break that into e sub 1 of x, and e sub 0 of x. again, where e sub 1 of x is the high terms, e sub 0 of x is the low terms. when we do our multiplication, then, we just multiply together d 1, x sub n over 2 plus d 0 and e 1 times x sub n over 2 plus e 0. and that then yields for terms, d sub 1 e sub 1 times x sub n, d sub 1 e sub 0 + d sub 0 e sub 1 times x sub n/2 + d sub 0 e sub 0. the key here is that, we now just need to calculate d1 e1, d1 e0, d0 e1, and d0 e0. those are all polynomials of degree n over 2. and so, now we can go ahead and use a recursive solution to solve this problem. so it gives us a divide and conquer problem. its run time is t of n, equals 4 t of n over 2. why 4? 4, because we're breaking into 4 subproblems. each of them takes time t of n over 2 ecause the problem is broken in half. plus, then, in order to take the results and do our addition that's going to take order n time. so some constant k, times that. let's look at an example. so we have, n is 4, so we have degree three polynomials. and we're going to break up a of x into the top half, 4x plus 3, and the bottom half, 2x plus 1. similarly, we're going to break up the top half of b of x. x cubed plus 2 x squared just becomes x plus 2. and 3x plus 4, stays at 3x plus 4. now, we compute d1 e1. so multiplying together, 4x + 3, times x plus 2, gives us 4 x squared + 11x + 6. similarly, we calculate d1 e0, d0 e1, and d0 e0. now we've done all four of those computations, ab is just d1 e1, 4 x squared + 11x + 6 times x to the 4th, plus the sum of d1 e0 and d0 e1, times x squared, plus finally d0 e0. if we sum this all together, we get 4 x to the 6th, plus 11 x to the 5th, plus 20 x to the 4th, plus 30 x cubed, plus 20 x squared, plus 11x plus 4. which is our solution. now, how long's this take to run? we're going to look at that in a moment. let's look at the actual code for it. so we're going to compute a resulting array, from 0 to 2n-2, so is all the results coefficients. and our base case is that if n of size 1, we're going to multiply together a at a sub l, plus b at b sub l. let's look at those parameters again. so a and b are our arrays of coefficients, n is the size of the problem, a sub l is the first coefficient that we're interested in. and b sub l is the coefficient in b, that we're interested in. so we're going to be going from b sub l, b sub l plus one, b sub l plus two, etc. and for n times. first thing we'll do, is multiply together the d sub one and e sub one. so basically what we're doing, i'm sorry, the d sub zero and e sub zero. so, what we're doing is taking a and b, we're reducing the problem size by 2 and we're starting with those same coefficients. and we're going to assign those to the lower half of the elements in r. then we're going to do something similar, where we take the upper halves of each of a and b. so again, the problem size becomes n/2, but now we're moving the lower coefficient we're interested in from a sub l to a sub l + n/2 and b sub l to b sub l + n/2. and we're going to assign those to the high coefficients in our result. then, what we have to do is calculate d sub 0 e1, and d1 e0. and then, sum those together. when we sum those together, we're going to assign those to the middle elements of the resulting array. and we'll then return that result. now the question comes up, how long does it take? so we have an original problem of size n, we break it into four problems of size n over 2. so, level 0 we have size n, level 1 we have size of n over 2, at level i, our problems are of size n over 2 to the i. and all the way down to the bottom of the tree is at log base 2 of n, and each of the problems are of size 1. how many problems do we have? at level 0, we have 1 problem. we have then 4 problems. if we go to the i'th level, we have 4 to the i problems. and at the very bottom, then we have 4 to the log base 2 of n problems. how much work is there? well, we just need to multiply together the number of problems times the amount of work, so we have kn here, and 4 times, 4 because there are 4 problems, kn over 2, because the problem size is n over 2 and the amount of work we're doing at each level is k times n over 2 per problem. so 4 kn over 2 just equals k times 2n, at the ith level for the i problems, each problem takes k times n over 2 to the i to deal with, we multiply together, k 2 to the i times n. and at the very bottom, we have k amount of work, we have a problem size of 1, times 4 to the log base 2 of n. well four the log base two of n, is just n squared. so we have k n squared. our total as we sum up all the work is going to be summation from i equals zero to log base two of n of four to the i k times n over two to the i. and that just gets dominated by the very bottom term which is big theta of n squared. so that's what our runtime takes. this is kind of weird. we went through all this work to create a divide and conquer algorithm. and yet, the run time is the same run time as it was with our naive original algorithm. we're going to see in the next video, a way to redo our divide and conquer algorithm, so we have less work to do at each level, and so we actually get a better final run time. 
in this video we'll look at creating a faster divide and conquer algorithm in order to solve the polynomial multiplication problem. this problem, this approach was invented by karatsuba in the early 1960s. so he was a graduate student of komolgorov, a famous russian mathematician. and komolgorov theorized that n squared was the best that one could do. so there was a lower bound of n squared, doing polynomial multiplication. karatsuba, a grad student, heard the problem, went away, came back a week later with a solution. so let's look at what is involved. so if we look at a(x) it's just a very simple polynomial, a1x + a0. and b(x) = b1x + b0, and then c(x) is, what would match in there? a1b1x squared + (a1b0 + a0b1)x + a0b0. so we'll notice here we need four multiplications. we need to multiply a1 times b1. we need to multiply a1 times b0, a0 times b1, and a0 times b0. this is how we did the divide and conquer in fact in our last video. so we need four multiplications. karatsuba's insight was that there was a way to re-write c(x), so that you only needed to do three multiplications. so basically what he did is he re-wrote that inner term, a1b0 + a0b1 as something slightly more complicated. so he added together, (a1 + a0) (b1 + b0). so (a1 + a0) (b1 + b0) is just a1b1 + a0b1 + a1b0 + a0b0. and then he subtracted out the a1b1 and the a0b0, so he's left with a1b0 + a0b1. which is exactly what's there to begin with. the key here though, is how many multiplications are needed. it only needs three multiplications. we need to compute a1 b1, even though we use it twice. we need to compute a0 b0, even again, though we use it only twice. and then we need to multiply together (a1 + a0) and (b1 + b0). so we do have some extra additions. but the key is, when we have three multiplications instead of four. why does this matter? well, why it matters is because we are reducing the number of problems at each level. but let's first look at an example. so here we've got a(x). we're going to have 4 x cubed + 3 x squared + 2x +1. b(x) = x cubed + 2 x squared + 3x + 4. we're going to go ahead and pull out d1 and d0 like we did before. in our divide and conquer. the key is what we're going to actually do in terms of the subproblems. so we have d1 and d0. we have e1 and we have e0. we're going to compute d1 e1, again, just like we did before. we're going to compute d0 e0, again just like we did before. but now we won't compute d1 e0 and d0 e1. instead we're going to sum together d1 and d0. sum together e1 and e0. so (d1 + d0) is going to be (6x + 4). (e1 + e0) is going to be (4x plus 6). and then we multiply those two polynomials together, yielding 24 x squared + 52x + 24. so, so far, how many multiplications have we done? three. and then, our final result for a(x) b(x) is d1e1 times x to the fourth +, now what do we do here? we take that (d1 + d0) (e1 + e0). (24x squared + 52x + 24), okay? add that in the second term. and then subtract out d1 e1. subtract out d0 e0. and then our final term will be d0 e0. if we simplify that middle portion, and all of it. we just end up with 4 x to the sixth + 11 x to the fifth + 20 x to the fourth + 3 x cubed + 20 x squared + 11x + 4. which is the exact same result we got doing it in the more naive divide and conquer. and also the same way we'd do it if we did a straight naive problem, okay? so we get the same result, three multiplications instead of four multiplications. that extra multiplication makes a big difference. let's look at our runtime. so our initial problem is of size n. when we break it down, we have three problems of size n over 2, again, rather than 4. so level 0, problem size n. level 1, a problem of size n over 2. at level i, our problems are of size n over 2 to the i, just like they were in the other divide and conquer problem. and we have the same number of leaves. so at log base 2 of n level, all the problems are of size 1. and the number of problems that we have, 1 of them at level 0, 3 instead of 4 at level 1, 3 to the i. instead of 4 to the i, at level i. and 3 to the log base 2 of n, instead of 4 to the log base, 2 of n at the bottom level. how much work? we'll multiply together, so we'll figure out for each problem how much it takes. in this case at level 0 it's kn. at level 1, each problem takes k(n/2) work. and there are three of them. so it's k(3/2) n. at the ith level, we end up with k times (3/2) to the i times n. and at the bottom level, k times 3 to the log base 2 of n. a to the log base b of c, is the same thing as c to the log base b of a. so therefore this is the same as kn to the log base 2 of 3. we sum those, summation from i = zero to log base 2 of n of 3 to the i times k times n over 2 to the i. this is bounded, it's this geometric series bounded by the last term. which is big theta of n to the log base 2 of 3. log base 2 of 3 is about 1.58. so, we now have a problem where our solution is big theta of n to the 1.58. compared to our original problem, which had a big theta of n squared solution. so this makes a huge difference as n gets large, in terms of our final runtime. it's not uncommon for divide and conquer algorithms sometimes to require sort of a way of looking at it in terms of breaking up a problem. so that you have fewer subproblems. and because of the compounding of the fact that the more subproblems at a level, you have more, and more, and more. reducing the number of subproblems, reduces the final runtime. 
here we're going to talk about the master theorem. we'll describe what the master theorem is and how to use it. and we'll reserve to the next video a proof. so we've had many occasions where we have had to write a recurrence relation for a divide and conquer problem. this is an example of one for binary search. we break a problem down into a problem half as big and we do a constant amount of work at each level. and this gives us a solution t(n) = o(log n). the problem is for each one of these we have to create a recurrence tree, figure out how much work is done at each level, sum up that work. that's a lot to do to solve each recurrence relation. here's an example that we used for the polynomial multiplication. so we broke a problem into four sub-problems, each half the size, and did a linear amount of work. and the solution was t(n) = o(n squared). when we had the more efficient algorithm, where we had only three sub-problems instead of four, we then got a solution of o(n to the log base 2 of 3). sometimes we break a problem into only two subproblems and there the solution is o(n log n). so, wouldn't it be nice if there was a way that we just had a formula to tell us what the solution is rather than having to create this recurrence tree each time? and that's what the master theorem basically does. so, the master theorem says if you have a recurrence relation t(n) equals a, some constant, times t( the ceiling of n divided by b) + a polynomial in n with degree d. and that ceiling, by the way, could just as well be a floor or not be there at all if n were a power of b. in any case, the a is a constant greater than 0. b is greater than 1 because we want to actually make sure the problem size gets smaller. and d is greater than equal to 0. well, in that case, we have a solution for t of n. there are three sub cases. case number 1, and all of these cases depend on the relationship between d, a, and b. in particular, is d greater than log base b of a? if so, the solution is just this polynomial in n, o(of n to the d). if d is exactly equal log base b of a, then the solution is big o of n to the d with an extra factor of log n. and finally, if d is less than log base b of a, then the solution is big o of n to the log base b of a. so let's look at some applications of this theorem. so here's one where we go back to the polynomial multiplication. here a is 4, b is 2, and d is 1. because o(n) is just o(n to the 1). and we look at the relationship between d, which is 1, and log base b of a, which is log base 2 of 4 or 2. well clearly d is less than log base b of a, so we're in case three. therefore t(n) = o(n to the log base b of a), or just o(n squared). if now we change the 4 to a 3, a is 3, b is 2, d is 1. now d is still less than log base b of a because log base 2 of 3 is greater than 1, and so again we're in case three. t(n) equals o(n to the log base b of a), which equals o(n to the log base 2 of 3). if we reduce the 3 down to a 2 what happens? well here, a is 2, b is 2, d is 1. log base b of a is log base 2 of 2, which is just 1. so now d is equal log base b or a. we're in case two now. and so, t of n equals o(n log n). and now this shows an example also of case two. so this is the binary search example. a is 1, b is 2, d is 0. well the log base two of one, log base b of a, is equal to zero. so d is equal to log base b of a. we're in case two, t(n) = o(n to the d log n), which is in the 0 log n, which is just o(log n). and a final example where we are actually in case one. so here a is 2, b is 2, and d is 2. so log base b of a is log base 2 of 2, which is one. so d is now greater than log base b of a. we are now in case one, t(n) equals o(n to the d), which is o(n squared). so what we've seen now is that we have this master theorem that allows us, for most recurrences, when you do a divide and conquer which fit into this general formula, allows us to easily figure out which case we are based on the relationships between a, b, and d. and then figure out the result quite quickly. in our next video we'll look at a proof of why the master theorem works. 
in this video, we'll look at a proof of how the master theorem works. so a reminder, the master theorem states that if t(n) equals a t of ceiling of n over b plus a polynomial, then we have these three cases. so let's do as we normally do with a recurrence relation and let's create a recurrence tree. so we'll have our recurrence at the top to just remind ourselves what that is. let's assume for the sake of argument that n is a power of b. that's a reasonable assumption since we can always just pad n to be larger, right, if we increase it by no more than b we can get to the next closest power of b and then this will be a simpler analysis. so we have our problem n. at the next level, we break the problem down into a copies of a problem n over b large. so level zero. we have a problem of size n. level 1 we have problems of size n/b. at the general level i we have problems of size n over b to the i. at the bottom level, which is level log base b of n, we have problems of size 1. how many problems are there? at level 0 there's of course one problem. at level 1, a problems. and in general at the ith level, a to the i problems. at the log base b of n level, it's a to the log base b of n. how much work do we have to do? well work is just a function of how many problems we have and the amount of work for each problem. so at level zero we have just o(n to the d) work. there's one problem and it takes o(n to the d) time. and level one we have a problems. and each of them takes o(n over b to the d) work. okay, we can pull out the a and the b and the d to be all together, and that's just o(n to the d) times a over b to the d. at the ith level we have a to the i problems and each one is o(n over b to the i to the d). again, we can pull out the a to the i, the b to the i, and we're left with o(n to the d) times a over b to the d to the i. and finally, at the bottom level it's just a to the log base b of n because the problems are all size 1. it's just o(n to the log base b of a). so the total amount of work is the summation from 0 to the log base b of n. o(n to the d) times the quantity a over b to the d, all that to the i. so let's look at what seems like a slight digression, and that is geometric series. so a geometric series is a series of numbers that progresses by some multiplicative factor. i'll give you an example. if we take 1 + 2 + 4 + 8 + 16 + 32 + 64, that's a geometric series where our factor is a factor of 2 at each time. just as well, we could have a geometric series that goes down. so we could have, for instance, let's say 10,000, 1,000, 100, 10, 1. where we're going down by a constant factor of ten at each increment. now it turns out, our multiplicative factor, let's call that r, as long as r is not equal to one we have a simple closed form for this. this is just a times (1-r) to the n over 1 minus r. and it turns out that big o notation, what happens is we care about the largest term. so our sum is going to be bounded by a constant times our largest term. so, if r is less than 1 then our largest term is the first element a and therefore our solution is o(a). okay, because it's our largest term, it gets smaller, smaller, smaller, smaller, smaller. and as long as it's by this multiplicative factor, then all that really matters is this first term, because the rest of it sums to no more than a constant times that first term. if on the other hand, r is greater than 1, then what matters is the very last term, because that's the biggest term and all the previous ones are smaller and smaller. so it's smallest, larger, larger, larger, largest. and so that largest term is a r to the (n-1). so in a geometric series we care about either the first term or the last term, whichever one is bigger. now if we take that back to the case of our recurrence tree, we notice our summation here. this is the same summation we had from our recurrence tree and we see that we have a geometric series. a is taking the place of big o then to the d and r is taking the place of a over b to the d. so our multiplicative factor is a over b to the d. and there are three cases. you remember as we stated the solution to the master theorem. case one is d is greater than log base b of a. well it's equivalent to saying a over b to the d is less than 1. so now we have our multiplicative term is less than 1. so it's getting smaller and smaller and smaller. that means that the largest term is the first term. and that's the one that we have an order of. so this is big o of, officially big o of big o of n to the d, which is just the same as big o of n to the d. case 2, where d equals log base b of a and equivalently, a over b to the d is equal 1. well, if a over b to the d is equal to one, remember our geometric series formula didn't hold, so we're going to just have to calculate this. but if a over b to the d is 1, then a over b to the d to any power is still 1. so that means, that our summation is just a summation from i equals 0 to log base b of n of o(n to the d). and that's just 1 plus log base b of n, because that's the number of terms in our summation times o(n to the d). well the 1 is a low order term we don't care about, and log base b of n can just be treated as log n, because a base change is just some multiplicative factor, and that disappears in our big o notation. so we end up with, as we see in the theorem, o(n to the d times log n). and then our final case, is d is less than log base b of a, which is equivalent to saying a over b to the d is greater than 1. so here, our multiplicative factor is greater than 1. so our smallest term is the first term and our largest term is the last term. so in this case, this is big o of our last term is o(n to the d) times a over b to the d to the log b of n. so, i is log base b of n. this is a bit of a mess. let's see whether we can fix this a little bit. so let's go ahead and apply the log base b of n power separately to a and b to the d. so we have, in the numerator, a to the log base b of n. and then the denominator, b to the d times log base b of n. well, b to the log base b of n is just n. so, that's going to disappear down to n to the d in the denominator. in the numerator, a to the log base b of n, by logarithmic identity is equal to n to the log base b of a. so we can swap those other two. and now, if we compare big o of n to the d and n to the d, we know big o of n to the d is bounded by some constant, k times n to the d. so we have k n to the d divided by n to the d, which is just some k. and that constant can go away because we're still talking about big o notation. so we're left just with big o of n to the log base b of a, which is what we have for the final case. so the master theorem is a shortcut. our master theorem again as a restatement is here. i have a secret to tell you, however. i do not remember the master theorem and i don't actually even look up the master theorem. here's what i do. when i have a recurrence of this rough form, i look at the amount of work done at the first level and at the second level (which is a very easy calculation) and then i just say to myself is that the same amount of work? if it's the same amount of work it's going to be the same amount of work all the way down and so we're going to be in case two. so it's going to be the amount of work at the first level, which we known is o(n to the d), times log n because there are that many levels. on the other hand, if the first term is larger than the second term i know the first term is going to dwarf all the other terms. and so, we're left with just o(n to the d). and finally, if the first term is less than the second term, i know they're going to keep increasing and it's the bottom term that i need. and that is just going to be the number of leaves which is n to the log base b of a. the master theorem is really handy to use whether you memorize it or you have it written down and use it or in my case you sort of recreate it every time you need it. thanks. 
hello, and welcome to the sorting problem lesson. as usual, we start with a problem i'll review. so sorting is a fundamental computational problem. your input in this problem consists of a sequence of elements, and your goal is to output this element in, for example, non-decreasing order. the formal statement of this problem is as follows. you are given a sequence of finite elements. we will usually denote the sequence by a throughout this lesson. and your goal is to output these same elements in non-decreasing order. once again, sorting is an important computational task used in many efficient algorithms. for some algorithms, it is just as important to process given elements in non-decreasing order, going from smaller ones to larger ones. in some other algorithms, just by sorting your input data, you gain a possibility to perform your queries much more efficiently. a canonical example of such situation is a search problem. in this problem, we are given a sequence of finite elements. and your goal is to check whether a particular element is present in your sequence. a simple way to solve this problem, is of course, just to scan your input sequence from left to right and to check, whether your element is present in this sequence. this gives you a linear kind algorithm. and you know already that if you input data, if you input sequences you sorted, then you can do this much more faster. basically, in time, in logarithmic time, in the size of your input sequence. so ou first compare your element to the middle element. if it is just few element, then you are done, if it is not, you continue with the left half of your sequence or the right half of your sequence. so in logarithmic number of comparison, and the worst case, you will be able to say whether your element is present in this sequence or not. so, if you are given a sequence and you are expecting many such queries. you're expecting to be asked to check whether a given object is present or not. for me such objects, then it just makes sense to first sort your input data and only then perform all these queries. this will give you a much more efficient algorithm in general. all right. and this is only a small example. we will see many other situations, where sorting your data first helps to perform queries much more efficiently. so in the subsequent videos of this lesson, we will study many efficient sorting algorithms. 
in this video, we will study one of the simplest sort of algorithms called selection sort. so it's main idea is quite simple, we just keep growing the sorted part of our rate. so let me illustrate it on a toy example, assume we're given a sequence of links. five consistent of five integers, eight four two five and two. so we start just by finding one of the minimum elements in this array, in this case it is two. now lets just do the following, lets just swap it with the first element of our array. after swapping, two stays in its final position, so two is the minimum value of our array and it is already in its first position. now let's do the fun one, let's just forget about this element. it is already in its final position and let's repeat the same procedure was the remaining part of our array. namely, we began first find the minimum value, it is again two. we'll swap it with the first element of the remaining part and then we'll just forget about this element. so again, we find the minimum value which is now four with what was the first element of the remaining part which is now the sole element of our array. and then, we just forget about first three elements and we continue with only remaining parts. so once again, we just keep growing the sorted part of our array. in the end, what we have, is that the whole array is sorted. the pseudocode shown here on the slide, directly implements the idea of the selection sort algorithm that we just discussed. so here we have a loop where i ranges from 1 to n. initially, i is equal to 1. inside this loop, we compute the index of a minimal value in the array, from, within the list from i to n. we do this as follows, so we create a variable, minlndex which is initially equal to i. and then we go through all the remaining elements inside this part, i mean through elements from i + 1 to n. and if we find a smaller element we update the variable minlndex. so in the end of this for loop, what we have is that minindex is a position of a minimal element inside the array from i to m. then we swap this element with the element ai. namely, when i is equal to one, what we've done, we've found the minimal element in the well array and we've swapped it with the first element. so now, the first element of our array is in its final position. then under second iteration of our loop, we do the same actually. we find the minimum value, the position of a minimum value inside the remaining part of our array and put it on the second place. on the sort loop we find the minimum value in this remaining part and put it on the place and so on. so we keep growing the sorted part of our array. so when it would be useful to check the online visualization to see how it goes, so let's do this. this visualization shows how selection sort algorithm performs on a few different datasets. namely on the random datasets, on a sequence which is nearly sorted. also on a sequence which is sorted in reversed order. and on a sequence which contains just a few unique elements. so let's run this algorithm and see what happens. so you can see that indeed this algorithm just grows the sorted region, the sorted initial region of our array. so another interesting property is it is revealed by this visualization is the following. so the running time of this algorithm actually does not depend on input data. so it only depends on the size of our initial sequence. the other [inaudible] time of how algorithm is quadratic and this is not difficult to see right? so what we have is two nested loops. in the outer loop, i ranges from 1 to n. in the inner loop, j ranges from i plus 1 to n, to find a minimum inside the remaining part of our array. so in total we have quadratic number of iterations. at this point however, we should ask ourselves whether our estimate was right in time of the selection, so our algorithm was too pessimistic. and this is whar i mean by this. so recall that we have two nested loops. in the outer loop, i ranges from 1 to n. in the inner loop, g ranges from i + 1 to n. so when i is equal to 1, the number of iterations of the inner loop is n- 1. however, when i is equal to 2, the number of iterations of the inner loop is n- 2, and so on. so when i increases, the number of iterations of the inner loop decreases. so a more accurate estimate for the total number of iterations of the inner loop would be the following, (n- 1) + (n- 2) + (n- 3) and so on. so it is definitely less than n-squared. however we will show this it is equal to n-squared. namely, this is xx n-squared, and this it is roughly equaled n-square divided by two. the sum is that we need to estimate is called an arithmetic series, and there is a known formula for this for this sum. namely 1 + 2 + 3 +, and so on, n, is equal to n(n+1)/2. and this is how we can prove this formula. let's just try it, all our n integers in a row, 1, 2, and so on, n. below them let's write the same set of integers, but in the reverse order. so, n, then n minus 1, and so on, 2, and 1. then what we get is a row of size 2 by n. having n columns, and in each column, the sum of the corresponding two integers is equal to n plus 1. great, so in the first column we have n and one, and in the second column we have two and minus one and so on and in the last column we have n and one. so the sum in each column is equal to n plus one and zero n columns. which means that the sum of all the numbers in our table is equal to n, when supplied by n plus one. so since this table contains our sum, the sum of the integers from 1 to n twice, we conclude that the sum of all the numbers from 1 to n is equal to n(n+1)/2. another possibility to find this formula, to see why this formula is correct is to take a rectangle of size n, of dimensions n multiplied by n plus 1. so it's area is equal to n multiplied by n plus one. and to cut it into two parts such as it's shown in the slide, such as the area of each of these two parts is equal to 1 + 2 + and so on n. we're all ready to conclude. so we've just discussed the selection sort algorithm. this algorithm is easy to implement, easy to analyze, and it's running time is n squared, where n is the size of the input sequence. so it sorts the input sequence and array in place. meaning that it requires almost no extra memory. i mean, all extra memory which is required by this algorithm is only for storing indices, like i, j and m index. there are many other quadratic algorithms, like insertion sort and bubble sort. we're not going to cover them here, and instead, in the next video we will proceed, to do a faster, a faster sort algorithm. 
in this video, we will study the so-called merge sort algorithm. it is based on the divide and conquer technique, which main idea is the following. to solve a given computational problem, you first split it into two or more disjoint subproblems, then you solve each of these subproblems recursively. and finally, you combine the results that you get from the recursive calls to get the result for your initial subproblem. and this is exactly what we're going to do in merge sort algorithm. so let's show a toy example. we're given an array of size eight, and we are going to sort it. first, we just split this array into two halves of size four, just the left half and the right half. then we make two recursive calls to sort both these parts. these are two results in arrays. now what remains to be done is to merge these two arrays into one, these two arrays of size four into one array of size eight. well, let's think how this can be done. first of all, i claim that it is easy to find the minimal value in the resulting array. indeed, we know that the minimum value in this case in the first array is two, and the minimum value in the second array is one. which means that the minimum value in the result in merge array must be one. so let's take one from the right side of array, put it in the resulting array and forget about it. it is already in its right place. what remains is an array of size four and an array of size three that still need to be merged. well, again, it is easy to find the minimum value of the result of merging these two arrays. in this case, it is two, because the minimum value in the array of size four is two, and the minimum value in the arrays of size three is six. so two is smaller than six, so we get two out of our left array, put it into the resulting array after one, and press hit. in the end, we get the following sorted array. again, the pseudocode of the merge sort algorithm directly implements this idea. so this pseudocode takes an input array a of size n as an input. and if n is equal to 1, then in this case, just nothing needs to be done, we can just return the rate a itself. if n is greater than 1, on the other hand, then we split the rate a into two roughly equal parts and sort them recursively. we call them b and c here. then the only thing that needs to be done is to merge these two sorted arrays. so this is done in the procedure merge, which we will present on the next slide. and finally, we just return the result of this merging procedure. the pseudocode of the merging procedure is also straightforward. assumes that we are given two sorted arrays, b and c, of size p and q respectively, and we would like to merge them into a sorted array of size p + q. so the first thing we do is create an array of size p + q in array d. it is initially empty. then we keep doing the following thing. so what is the minimum value among all the values stored in the arrays b and c? well, it is easy to find. we know that the first element in the array b is its smallest element, and the first element in the array c is its smallest element. so the smallest one among these two is the smallest element inside the unit of these two arrays. so we just find the minimum of these first elements and move it from one of these arrays to the results in array d, and forget about this element completely. now what is left is essentially the same problem. we're left with two sorted arrays, and we still need to merge them. so we do it exactly the same. we take the first two elements, we compare them and move the smaller one to the resulting array. and we keep doing this while both of these arrays are empty. i mean, we need this to be able to take their first elements. when one of them becomes empty, we just copy the rest of the other array to the resulting array d. i mean, where rest to the resulting array d. well, it is not difficult to see that this procedure is correct, and the trying time is p + q, namely, the size of the array p plus the size of the array q. and this just because we just can both of these arrays from left to right in the run of this merging procedure. this is how sorting our initial array of size eight by the merge sort algorithm looks like. so the merge sort algorithm first splits the initial array of size eight into two arrays of size four. each of these arrays of size four in turn is split into two arrays of size two, and each of them is split into two arrays of size one. then merge procedure starts merging these arrays of size one into arrays of size twos and into, then these arrays of size two into a size four. and finally, it merges the result into arrays of size four, into the resulting array of size eight. we are now going to prove that the running time of the merge sort algorithm, on a sequence containing n elements, is big o of n log n. know that this is significantly faster than a quadratic selection sort algorithm. for example, it is perfectly okay to sort the sequence of size 1 million, for example, 10 to the 6th, on your laptop using merge sort algorithm. while for the quadratic time selection sort algorithm, sorting a sequence of size 10 to the 6th, 1 million, will take roughly 10 to the 12th operations, which is too much for modern computers. okay, so to prove this lemma, to prove the upper bound on the running time of the merge sort algorithm, first know that to merge two parts of size n over 2 of our initial array, takes the linear time. namely, big o of n, because while the left part has size n over 2, the right part has size n over 2. and for merging, we basically just combo these parts from left to right. so it takes just a linear amount of work to do this. which, in turn means, that if we denote by t of n the running time of our merge sort algorithm, then it satisfies the following recurrence. t(n) is at most 2t(n / 2) + big o(n). here 2t(n / 2) could response to two recursive calls. so we denote it by t(n), the running time of our algorithm on input of size n. so when we sort two sequences of size n / 2, we spend time twice t(n / 2). so the big o of n term corresponds to what we do before we make recursive calls and what we do after recursive calls. so what we do before is just split the input array into two halves. what we do after is merging the results of two arrays into one array of size n. so it is not difficult to see that all of this can be done in linear time. so we get this recurrence, and on the next slide, we're going to show that this recurrence implies that the running time of our algorithm is bounded from above by n log n. to estimate the running time of this algorithm, let's consider its recursion tree. namely, at the top of this tree, we have one array of size n. so for this array of size n, we make two recursive calls for arrays of size n over 2. each of these arrays of size n over 2 in turn is split into two arrays of size n over 4. so we get four arrays of size of n over 4 and so on. so in this tree, we have log n levels. now let's estimate the work done at each of the levels of these three separately, namely, once again, to solve a problem of size n. to sort an array of size n, we first prepare to make recursive calls. in this case, we just split the array into two halves of size n over 2. then we do make recursive calls, and then we need to combine the results. so all the work now inside recursive calls will be accounted for on the lower levels of this tree. so now what we are going to do is to account for only the work done before the recursive calls and after the recursive calls at each separate level. and we know already that it takes linear time to do this. i mean, if we have an array of size n, it takes linear time to split it into two halves. and then it takes linear time to combine the results of recursive calls into one array. so let's just denote this time by cn, i mean let's denote the hidden constant inside big o by c. then what we can say is that on the top level we spend time cn. then on the next level, for each subarray, we spend time c times n over 2, because the size of array is n over 2. however, we have 2 arrays, so the total work that we do at this level is 2 multiplied by c, multiplied by n over 2, which is again just cn. on the next level, we spend time 4 because we have 4 arrays multiplied by c, multiplied by n over 4, because the size of the array is now n over 4. this is a cn again, and so on. so we have log n levels. at each level, we do roughly cn operations. so the total number of operations in our algorithm is cn log n, which proves our lemma. so again, what we've just proved is that the running time of the merge sort algorithm is big o of n log n. so in the next video, we will show that actually no algorithm, no comparison based algorithms, to be completely formal, can sort a given sequence of n elements asymptotically faster than in n log n time. which actually means that the merge sort algorithm is asymptotically optimal. 
in the previous video, we proved that the running time of the warshall algorithm on a sequence consisting of n elements is big log of n. in this video we will show that this bond is essentially optimal. we will do this by showing that any correct sorting algorithm that sorts an object by comparing pairs of them. must make a clear stand log in operation such as particularly in the worst case. once again we say that the sorting algorithm is comparison based if it sorts the given objects just by comparing pairs of them. we can imagine the following situation, we have add objects, that look the same, for example in walls, but have different weights. and we also have pen balance. and this pen balance is, our only way to compare pairs of these balls. and our goal is to rearrange these balls, in order of increasing weights. so, for example, the two source and algorithms that we'll already consider it, namely the selection sort algorithm and the merge sort algorithm are both comparison based algorithms. so for example, the selection sort algorithm at each region finds the minimum value in the remaining part of the array. and it does so exactly by comparing pairs of objects, right? also, the merge sort algorithm is also comparison based algorithm. so it first splits an array into halves. and then it needs to merge the two results in arrays. and when merging the results in arrays, it also uses comparisons, right? so we take the first two elements of two sorted arrays. we compare them, and based on this comparison we take one of these elements out of one of those two arrays and put it to the result in the array. so this as a formal statement that we're going to prove. it says that any comparison based algorithm that sorts an object has running time. at least big and n log n in the worst case. so but in otherwise we can say the following. assume that you have an algorithm that sorts an object by comparing pairs of them. it can be the case that for some given both sequences of an object, your algorithm performs less than analog operations. say, linear number of operations. however, it cannot be the case that your algorithm always sorts in time, asymptotically less than n log n. meaning that, there must exist, sequence of objects, on which your algorithm will perform at least performing a login comparison to sort such sequences. any comparison based algorithm can be shown as a huge tree that contains all possible sequences of comparisons that can be made by this algorithm. for example, here on the slide. we show a simple algorithm that sort three object. three objects. so it starts by comparing a1 and a2. if a1 happens to be smaller than a2, then we proceed to comparing a2 and a3. if a2 is smaller than a3, then we already know the permutation of the input three objects in non-decreasing order. namely, we know that a1 is smaller than a2, and we know that a2 is smaller than a3. so we can just output the following permutation. right. if on the other hand, a2 happened to be at least a3, then at this point we already know that a2 is greater than a1. and a2 is no smaller than a3. so at this point, we know that a2 is the maximum element among our three elements. however, we still need to compare a1 and a3, so we do this comparison, and based on its result, we output either this permutation or this permutation. well this was the case when a1 happened to be small as an a2. however we need also to consider the case when a1 happened to be at least a2. so we proceed similarly in this case. so this is just a toy example for an algorithm for a comparison based algorithm comparing three objects, sorting three objects. however, such a huge tree can be drawn for any comparison based health algorithm. so at the root of this tree we have the first comparison. and its children will label just the next comparison that is made based on the result of the first comparison and so on. so each internal node is labeled with some comparison. and each leaf is labeled with a permutation of m input objects. a simple but crucial for this argument observation is that in this tree, we must have at least n factorial leaves. and this is because we have n factorial different permutations of n input objects. where n factorial is defined to be the product of n and minus one and minus two and so on. so why is that? why we must have any possible permutation as a leaf in our tree? well, this is just because it is possible that this permutation is a lead output, is the right output of our algorithm. so for example on our previous slide, on our toy example, we have three objects and there are six possible permutations of these three objects, and there are six leaves in our tree. for example one of them is 213 and it says that the second element is the smallest one, then goes the first element, and then goes the third element. and indeed there are cases when this is the right answer. right? so when the input data consists of three objects, such that the second element is the smallest one, the first one is the next one, and the third element is the largest one. right? so once again, you have a huge tree which carries a comparison based algorithm. there must be at least n factorial leaves, because each possible permutation must be present as a leaf in our tree. so on the other hand the maximal number of comparisons made by our algorithm corresponds to the depths of our tree. so the depths is defined as the maximal number of edges run away from the root to the leaf, to some leaf of our tree. so and this is exactly the maximal possible number of comparisons which our algorithm makes. so now we would like to show that d must be large in our case, must be at least be big o omega of analog n. and we know already that our tree contains many, many leaves. mean n factorial is a function that grows extremely fast. okay so, intuitively we would like to show that if a tree has many, many leaves, then it has a large depth. and at least intuitively this clear. if you have a tree of very small depths then it must just a few leaves, right? but, we know that it has many, many, many leaves, in fact at least ten factorial leaves. to formally show this we need the following, we need the following estimate. the depths of a binary tree is at least a binary algorithm of its number of leaves or equivalently 2 to the depths is at least its number of leaves. well this can be proved formally, but let me just show you this informally. let's concede a tree for example of depth 1. so in this case, d is equal to 1. and it is clear that the maximal possible number of leaves in a tree of depth 1 is equal to 2. so now, let's try to understand what is the maximal possible number of leaves in a depth of in a tree of depth 2. for example, this is a tree of depth 2. this is another tree of depth 2, it has has three leaves. and this is a tree of depth 2 that has maximal possible number of leaves, in this case it is 4. it is 2 to the d indeed. and intuitively it is clear that to have a tree of depth d that has maximal possible number of leaves. we need to take a tree which has a full binary tree of depth d, right? and this tree has exactly 2 to the d leaves. so the maximal number of leaves in a tree of depth d is 2 to the d. which proves that 2 to the d is at least l. okay, so the last step that we need to show is that if we have n factorial leaves, then the depths of our tree is at least big log n again. and we will show this on the next slide. it remains to estimate log of n factorial. we're going to show here that log of n factorial is at least c times n log n. which means as it works that log of n factorial is big log of n log n. to do this, we express n factorial as a product of 1, 2, 3. and so on n minus 1, and then right algorithm of product of these numbers as a sum of their algorithm. so, log of n factorial is equal to log of 1 plus log of 2 plus log of 3 and so on plus log of n. so, this is a sum of an object. let's just throw away the first half of this and elements, and leave only the second half. so in this second half, we have n over two elements and each of them is at least log of n over two, right? so this has algorithms of numbers which are at least n over two. so we have n over two. elements, each of them is at least algorithms of n over 2. this allows us to conclude that log sum is at least 10 over 2 times log of n over 2. and this in turn be big log of n for a simple reason. so log n over 2 is equal to log n minus 1. well, this grows like log n, right? because log n is a growing function and one is a constant so again minus one goes as log n. and over grows as n, right? so, this is up to constant factors, this is just n. so, n over two times log n over two grows like n log n. okay so this concludes our proof, and this concludes the proof of the fact that any comparison based algorithm must make at least n log n adorations in the worst case. once again, another conclusion is that when merged sort algorithms that we considered in the previous lecture e is asymmetrically optimal. in the next video we will see an algorithm that actually sorts n given objects in time less than n log n. actually in time just in linear time. in time big log of n however, it will sort the n given objects, knowing something about these objects. it will only sort the given objects if the subject has small integers. and we will sort them without actually comparing them to each other. 
in this last video we will show that there are cases when we can sort the n given objects without actually comparing them to each other. and for such algorithms, our lower bound with n log n does not apply. well, probably the most natural case when we can sort the n given objects without comparing them to each other is the case when our input sequence consists of small integers. we will illustrate it with a toy example. so consider an array of size 12 which consists of just three different digits. i mean each element of our array is equal to either 1, 2 or 3. then we can do the following, let's just go through this array from left to right. i mean by a simple count and count the number of occurrences of 1, 2 and 3. just by scanning this array you will find out that 1 appears two times, 2 appears seven times, and 3 appears three times. and this information is enough for us to sort these objects, so we can use this information to fill in the resulting array, a prime. so we put 1 two times, then we put 2 seven times, and then we put 3 three times. and this gives us the resulting sorted array a prime, right? so what just happened is that we sorted this array, these n objects, without comparing these objects to each other. we just counted the number of occurrences of each number, and for this we used, essentially, the information that this array contains small integers. the algorithm that we just saw is called counting sort algorithm. its main ideas are the following. i assume that we're given an array a of size n, and we know that all its elements are integers in the range from 1 to m. then, we do the following. we create an array count of size m, and by scanning the initial array a just once from left to right, we count the number of occurrences of each i from 1 to m, and we store this value in the cell count of i. so, we scan the array a from left to right, and whenever we see an element equal to i, we increment the value stored in the cell count of i. then when this array is filled, we can use this information to fill in the resulting array a prime, as we did in our toy example. so this is a pseudocode of the count and sort algorithm. here we're given an array a of size m and we assume that all the elements of this array are integers from 1 to m. so we introduce the recount of size m which is initially filled in by zeroes. then by scanning our initial array we fill in this array. namely, whenever we see an element k in our initial array, we increase the cell count of k. so after the first loop of this algorithm, we know exactly the total number of occurrences of each number k from 1 to m in our initial array. so for example in our toy example two slides before, we counted that the number 1 appears two times in our initial array, the number 2 appears seven times in our initial array, and number 3 appears three times. so at this point, we know that in the resulting array, the first two elements will be occupied by the number 1, the next seven elements will be occupied by the number 2, and the next three elements will be occupied by the number 3. now we would like, instead of having just the lengths of these three intervals, we would like to compute the starting point of each interval. so we do this in a new loop. and for this we introduce a new array pos. so pos[1] is equal to 1, meaning that number 1 will occupy a range starting from the first index. and the starting point for each subsequent range is computed as a starting point of each previous range, plus the length of this previous range. so pos[j] is computed as pos[j -1] + count[j- 1]. so at this point we know the starting point for each range. namely, k in the resulting array, number k will occupy a range starting from pos[k]. then we just count our initial array and whenever we see an element, we always know where to put it in the initial array. so then let me remind you that we do not just fill in the array with numbers from 1 to m, but we copy elements from our initial array. this is because what we are looking for in this certain problem is a permutation of our initial n given objects. because what we have is probably not just number, not just integers from 1 to m, but these numbers are keys of some probably complex object. okay, so the running time of this algorithm can be easily seen to be big o of m plus m. this is just because here we have three loops. so the first loop has n iterations, the second loop has m iterations, and the last loop also has n iterations. well, so, this is the formal statement. the running time of count and sort algorithm is just n + m. and the final remark about this algorithm is that if m grows no faster than n, namely, for example, if our array is filled by integers from 1 to n, or if this array is filled just by integers which are upper bounded by some constant, then the running time of our count and sort algorithm is just linear in n. i will now summarize the last three videos. so we first covered the merge sort algorithm. so this is a divide and conquer based algorithm that proceeds as follows. given an array of size n it first splits it into two halves, both roughly equal size, then it sorts them recursively and then it merges them into the resulting array. we then, and we showed that the running time of this algorithm is big o(n log n), which is quite fast actually. almost teeny. we then showed that no other comparison based algorithm can sort n given objects asymptotically faster than an n log n. so we did this by showing that any comparison based algorithm must distinguish between too many cases. between n factorial possible permutations. for this, in the worst case, a comparison based algorithm must perform at least big o(n log n) interpolations. we then showed that it can be actually done faster and in certain problems, can be solved in time less than n log n, in some cases. for example, in the case when our input array contains small varied integers. 
hello, and welcome to the next lesson in the divide-and-conquer model. this lesson is going to be devoted to the quick sort algorithm, which is one of the most efficient and commonly used in practice sorting algorithms. well, as usual, we start with the overview of this algorithm. the algorithm is comparison based, meaning that it sorts the n given elements by comparing pairs of them. its running time is also asymptotically n log n, but not in the worst case, as was with the merge sort algorithm, for example, but on the average case. this is because this algorithm is randomized, so it uses random numbers to sort the given n objects. well, we will explain later in this lesson what this means. finally, as i said before, this algorithm is very efficient in practice and, at the same time, not so difficult to implement. this is a toy example explaining the main idea of the quick sort algorithm. so given an array, in this case of size 11, let's take its first element. in this case it is 6. and let's do the following. let's rearrange all the elements in this array such that the element 6 stays in its final position. all the elements that go before it are actually at most 6. and all the elements that go after 6, after this element, are greater than 6. well, we will show that this can be done by a single scan of the initial array. this is how the resulting array looks like. so once again, 6 stays in its final position. all the elements before it are at most 6. all the elements after it are greater than 6. so we do not need to move 6 anymore. it is already in its final position. so what remains to be done is to sort all the elements that go before 6 and all the elements that go after 6. and this can be done just with two recursive calls to the same algorithm, to the quick sort algorithm. so we do this, and immediately after these two recursive calls, we have a sorted array. well, in the next video we will explain all the details of this algorithm. 
in this video, we'll provide the full outline of the greek word algorithm. so as you remember that algorithm is recursive, for this reason we pass through this procedure [inaudible] a and also doing this is l and r in this array for left and right and this procedure saw the sub array inside r within this is form l to r. well we first check whether l is at least r. and if yes then this means that they can respond in sub array contains at most one element. and this in turn means that nothing needs to be done so we just return. otherwise we call the partition procedure with the same parameters. it returns an index m between l and r. so it rearranges all the elements inside this sub array with the following property. after the call to this procedure, a of m stays in its final position, meaning that all the elements to the left of an element a of m are at most a of m. and all the elements to the right are greater than a of m. well once again, after the call to the partition procedure, a of m stays in its final position. so what remains to be done is to sort all the elements that are at most a to m. they stay to the left of a of m, and all the elements that stay to the right. so we do this just by making two recursive calls. so this is how the wall algorithm looks pictorially. again, we are given an array a, with two indices l and r, and we are going to sort the sub array inside from indices l to r. so with first call the participation procedure which parameter a, l and r. and it gives us an index m between l and r was the following property. all elements to the left of them are at most the element a of m. all the elements to the right are great as an a of m. then we make two recursive calls to sort the left part within this is from l to m- 1 and to solve the right part within this is from m + 1 to r. and immediately after these two recursive call, we have a sorted array. so before showing the actual of the partition procedure, we explain it's main ideas again, on a toy example. so first of all, we will take is the element a[l] and denoted by x. this will be called our pivot element. so what pivot is exactly is the element with respect to which we're going to partition our sub array. so x will be placed in its final position. so our goal now is to rearrange all the elements inside our current sub array so that x stays in its final position and all the elements to the left of x. at most x and all the elements to the right of x are greater than x. so we will do this gradually increasing the region of already discovered elements. so for this we will use a counter i, and we will maintain the following invariant. so i will go from l + 1 to r, and at each point of time when we have already have the i element we will keep to region in sizes these region from l + 1 to i. in the first region from l + y to j, we will keep all elements that are at most x. in the second adjacent region within this is from j + 1 to i we will have all elements that are greater than x. let's see it for example. so i assume that we are somewhere in the middle of this process. in this case, x is equal to 6, and we need to partition all the elements with respect to x. we already have two sub regions so in the red region, we keep all elements that are at most x. there are at most 6 in the blue region we have holds elements that are greater than 6. okay, now we move i to the next position and we discover the element 9. so this element is greater than 6, so we just need to extend the second region, the blue region. the region of elements is at greater than 6. so in this case we just do nothing. well the next case is more interesting, we move i to the next position, and we discover the element 4. in this case, we need to somehow move this element to the red region, to the region of elements which at most 6. so to do this we just swoop it to currently first element of the blue region, in this case was 9. so if we do this 4 will be the last element of currently red region and 9 will go to the blue region. so we do this and now, we increase also the just to reflect the fact that our red region had just been extended. then we will find to the next element so we discover element 7 which is greater than 6 which means that we can just extend the blue region, then we discover another element which is 6. 6 is at most 6 and it is actually equal to 6, so we need to move it to the red region. again, we swap it with the first element of the blue region and then we extend the red region. we increase g to reflect the fact that the red region has just been extended, then we discover another element, which is at most 6. we move it to the end of the red region. and finally, what we also need to do in the very end is to move the pivot element which is 6 in this case to its final position. and its final position actually can easily be found in this case. so we have red region and we have blue region. in red region, all the elements are at most 6, and in blue region, all the elements are greater than 6. so we can just swap 6 with the last element of the red region. in this case it is 1, so if we swap these two elements then you can see that all the elements in the blue region are indeed greater than 6. all the elements in the red region are smaller than 6. so we are done with this partition procedure. where now ready to present the soutacot of the petition procedure. we called it what we're going to do is to place some element x, which is called the pivot, into it's final place so that all the elements before x are at most x and all the elements after x is greater than x. so as the pivot element in this procedure, we are going to use just the first element of the correspondence of rate, so x is assigned a of l. we're also going to remain the following subregions. so first of all, we will readily increase the region of discovered elements. so i goes from l +1 to r and inside this region of [inaudible] elements, we will maintain two sub regions. in the first region with indices from l +1 to j, we will keep all of the elements at most x. in the second region with indices from j+1 to i, we will keep all of the elements that are greater than x and we will gradually and freeze the value of i. so when i is increased, so i assumed that i has just been increase so we discovered a new element of a of i. so if a of i is greater than x then just the second of region of elements that are greater than x, is extended automatically and we do not need to do anything in this case. however, if the newly discovered element is at most x, then we need to move it to the first region. so we do this as follows. so we just increase the value of j to indicate the fact that the first region has just been increased, and then swap the elements. a[j] and a[i], so this way, we just maintain our invariant each time when i is increased. so in the very end, when i reaches the value of r, we also need to place our initial element that states that at the beginning our pivot between our two regions. so for this we just swap elements a[l], so this is our pivot with element a[j]. and we then return the value j as an index of our pivot element. 
now when the algorithm is present, we need to estimate the running time. for the quicksort algorithm, the running time analysis is a little bit tricky. so before stating, and proving the theorem about it's running time, let's be allowing partition. first of all, let's consider a pathological case, when somehow, it always happens that we select the minimum value of our current subarray as our pivot element. well in this case, well let's see what happens with our current subarray. say of size n. and we select its minimum value as a pivot. and partition, the subarray with respect to the sum. since this is the minimum value, it's final position is just the first position, is the resulting array, right? which means that we partition into two parts. the first part is just empty, we have no elements smaller than our pivot. and the second part, contains n- 1 elements, because all the remaining elements are greater than our current element. okay, so in this case, if this happens at this iteration, i mean at this call to partition procedure, then we can write the running time of our quicksort algorithm, satisfies the following relation t of n is equal to n plus t of n minus one. the term n here, the response to the running time of the petition procedure. well, it is actually big often. but just to simplify let's put n here. let me also recall you that, well if we have an array of size n, then the partition procedure indeed works in time, big often, because it just becomes the subarray? so now let's see what is the solution for this recurrence relation. well, we can just unwind this recurrence relation term by term. so we have n plus t of n minus 1. let's replace t of n minus 1 by n minus 1 plus t of n minus 2. then we replace t of n minus 2 by n minus 2 plus t of n minus 3. and we keep doing so. so what is left is the following sum, n + (n- 1) + (n- 2) and so on. so what we know is this sum already, so this is arithmetic series. and we know that it grows quadratically. which give us something strange, i mean our quicksort algorithm works in quadratic time. which means that it is not quick, actually, right? we'll resolve this issue later. now, let's consider a slightly different case. assume that somehow, we always partition into two parts, such that one of them has size, for example, n- 5. and the other one has the size four. well i claim that even in this case. first of all, note that both these cases correspond to very unbalanced partitions. in the first case, we have two parts one of size 0 and one of size n-1. in the second case, we have two parts one of size five. and one of size four and one of size n minus five. so the size of stuff parts are very unbalanced. they are very different. okay, so i claimed that in this case the running time, is also going to be quadratic. and this can also be shown, just be unwinding this recurrence relation. so let's just throw away this t(4), and leave only t(n- 5). okay, so t(n) is equal to n plus t(n-5). let's replace t(n-5) with (n-5)+t(n)-10. let's then replace t of n minus ten with t of n with n minus ten plus t of n minus 15 and so on. so this leaves us with the following sum. n plus n minus five plus n minus ten and so on and this is also an arithmetic progression. the only difference with the previous arithmetic progression is that now, we have step five. the difference between neighbors is five, but not one. well, still, this arithmetic progression has a linear number of terms. which means that it sums rows quadratically. with the only difference that the hidden constant inside this set up is smaller than in the previous case. now let's consider another pathological case. assume that it somehow so happens for some unknown reasons that at each iteration at each call there's a partition procedure. it partitions the array into roughly equal sizes. well in this case we can write the following reference relation on the running time of our algorithm. t of n is equal to t of n over 2 plus the linear term. and we know this reference already. it is exactly the reference the running time of the satisfies. right? and we, proved that in this case t of n grows as n increases vertically. let me remind you, how we prove this. we analyzed the. so, in this three of the route we have one array of size n. at the next level we have two arrays of size n over two n, at the next level we have four rays of size n over four, and so on. so the height of this tree is log base two, well it is basically logarithmic. at each level the sum of the sizes of of full arrays is equal to n. so we have array of size n at the top, two arrays of size n over two at the next level, and four arrays of size n over four at the next level, the size is still n, and so on. at each level we spend a linear amount of work. this is essential. we spend a linear amount of work at each level, and we have a logarithmic number of levels, which means we spent an n log n in time total. okay, let's consider another, again very pathological case. i assume that we alway split an array of size n into two parts. one of size n over 2, n over 10. i'm sorry. one of size 9n over 10. so in this case the recurrence is the following. t of n is equal to t of n, over 10. plus t of 9n over 10 plus a linear term. i claim that even in this case we can prove [inaudible] again on the running time of how well and this is how we can do this. well, lets again consider the [inaudible] because the [inaudible] of [inaudible] algorithm. in this case, it is not balanced. right? because when we go to the left branch, we reduce the size of the current subproblem by 10. and when we go to the right branch, we reduce the size of the current subproblem only by factor of 10 divided by 9. which means that in our 3, the size of the left branch is of the left most branch, is actually log based ten. of n while is the size of the right most branch is log based ten over nine over m. well, still the height of this of this three is logarithmic. but the previous case is that nouns are based on the algorithm is different but it's still constant. it is log based, log based 9 of m. and also, but also, the previous property is true. the sum of the sizes of all arrays at each level is still equal to n. it is at most n, actually. at the root we have one array of size n. at the next level we have two arrays, one of size n/10, and the other one is of size 9n/10. right? so the size is still n. at the next level it is the same, and so on. so we have a logarithmic number of levels, and at each level we spend a linear amount of work which gives us an n log n upper bound once again. okay, all this analysis of what about only pathological cases if we always split in a balanced way or in an unbalanced way. in reality, or just when we run a greek algorithm on some array, well some of the partitions are going to be balanced. some of the partitions are going to be unbalanced. so will still do not know what is the actual running time of the greek algorithm. we still need to determine this. however, we already get an important message. so running time of algorithm of the greeks are depends on how balanced our partitions. what we know know is the following, if all our politicians are balanced does that make improved that the running time is at most n log n hypothetically. at the same time if all of our partitions are unbalanced then the running time is quadratic. this means that we would like to have a way of selecting a pivot element such that it always guarantees a balanced partition. at the same time it is not clear at all how to do this. how to guarantee that we can always peek quickly. the pivot element with respect to this pivot, the rate is partitioned in a balanced way. so instead of this we will use the following elegant solutions, so let's just select the pivot element from the current subarray randomly. to implement this solution, we do the following. before following the partition procedure. we just select a random index between l and m, and we swap elements a[l] and this random element. okay, then we call partition, and then we proceed in a usual way. let me explain intuitively why selecting a random partition is going to help us to prove a good upper bound on the running time of the quicksort algorithm. well, for this, consider array a's that we're going to partition with respect to random p and consider it sorted version. assume for the moment that all the elements inside our array are different. in the sorted version, consider the middle half elements. well we can see that n/2 elements that stay exactly in the middle. well an important property of all these elements is the following: for each of these n/2 elements there are at least n/4 elements that are greater than them. and at least n over four elements that are smaller. well this means that if we select any of these elements inside our array a, then the partition with respect to this element is going to be balanced. right? in both parts there will be at least n over four elements. well these n over two elements stay somewhere in the initial array. so they stay in the middle in the sorted array and they stay somewhere in the initial array. it doesn't matter for us. what is important for us is that there are at least n over two elements with respect to which the partition is going to be balanced. which means that with probability one half we will have a balanced partition. and this happens to be enough to prove it with upper bound. so we're going to show that the randomized quicksort algorithm is actually very fast. well first of all it is fast in practice and we will prove it's theoretical analog out upper bound when it's running time. this is a formal statement of an upper bound on the running time of the quicksort algorithm that we are going to prove in the next video. so i assume that we are given an array a, and assume for the moment that all the elements of this array are pairwise different. then the average running time of the quicksort algorithm on this array consisting of n elements, is big o of n log n. while the worst case running time of this algorithm is n squared. well let me explain the word on average here. well, this means that for any fixed array. so if we are very unlikely with the random beats, the running time potentially could be higher as an algorithm. however, on average, and average is over all possible random beats. the running time of the quicksort algorithm is n log n. and this is true for any input. so this theorem doesn't say well, for quicksort algorithm. for some arrays, the running time is large, for some arrays the running time is low. but on average, the running time is good enough. so it says that for any fixed rate, the average running time is then n log n. okay, so we are going to prove this theorem in the next video. 
in this video we are going to prove formally an upper bound on the running time of the randomized quick sort algorithm. namely we're going to prove that the running time on average of the randomized quick sort algorithm is n log n. that is it must be go for n log n. and that is, that, in the worst case, the running time is big o of n squared. well, before going into the details of the proof. let's again, intuition. first of all, let's know that what we need to estimate is the number of comparisons. why is that? well because a quick sort algorithm contains of first the call to the partition procedure and then to recursive calls. each of these two recursive calls is going to be unwinded into the call to partition procedure and another to small recursive calls. so what is going on inside the quick sort algorithm is essentially many calls to the partition procedure. while inside the partition procedure, what we do actually is to compare all the elements of the curve and separate who is the pivot element, right? so what we estimate is a total number of comparisons. which is not surprising because the quick sort algorithm is a comparison based algorithm. well, now let me also explain why balanced partitions are better for us. i will explain this intuitively. so consider this story example. this array of size seven. assume that we selected one as the pivot element. so we partitioned the rate is shown here on the left. now let's see what we now know. we can pair it all the elements in this array to the pivot element which is one in this case. so now we know that one, the final position of the pivot element is just the first position in the array. so we known that 1 is the minimum value. however, we know nothing about pairs of other elements, right? so we only learn that 1 is the minimum value. now consider another possibility, consider the following balanced partition shown on the right. so assume that we selected 4 as the pivot element. i claimed that in this case, this partition is much better for us because we saved many subsequent comparisons. so look, in this case, in the subsequent trends of the partition precision, we are not going to compare elements 3, 1, 2 with element 6, 5, 7. because we already know that all the elements to the left are for a. as in all the elements to the right are formed. well the left part will stay in a separate recursive code and the right part well stay in a separate recursive code. so once again, balanced partitions save us a lot of comparisons that we do not need to make in the subsequent calls to partition procedure. another thing i would like to discuss with you before growing and know details of the proof is the following. our algorithm is randomized, so its running time and its number of comparisons depends on the random beats used inside the algorithm,. in particular for any two elements there is some probability that they are going to be compared. and using this toy example shown on the slide, i would like to just build an intuition on how to estimate this probability on which factors this probability depends. so consider this small example. so this is an array of say it's nine containing all the digits. and i would like to estimate the probability that elements 1 and 9 are going to be compared if we call randomized quick sort physics already. so, let's see what happens. assume that in the very first quarters of partition procedure, we select the elements 3 for example as the pivot element, so what happens? in this case, 1 will go to the left of 3 and 9 will go to the right side. to the right of three, i'm sorry. so in this case 1 and 9 will be in different parts and they will never be compared in as a partician procedure just because they are already in different parts. okay, for the ways it means, that we already know that 1 is smaller than 9, because 1 is smaller than 3, and 3 is smaller than 9, right? we do not need to compare them. well then this happens if we select as our pivot element, any of the elements, 2, 3, 4, 5, 6, 7 or 9, 8, i'm sorry. if on the other hand we select 1 and 9 as our first pivot element, then 1 and 9 will become pivot. just because, well, if we select, for example, 9 as the pivot element, we can pivot with all the elements of our array, in particular with 1. so there are two cases when 1 and 9 are compared. and this is how exactly the case is when either 1 or 9 are selected as a first pivot. in all other seven cases there are not compared. this means that the probability that they are compared are 2 over 9. okay, makes sense? now let's try to estimate the probability that the elements three and four are compared. well i claimed that in this case this probability is equal to 1. and the explanation is the following, there is no element inside rre that can help the randomized weak sort algorithm to understand that 3 is smaller than 4 without comparing them. i mean for 1 and 9, there are seven such elements. all they are is elements. i mean, if we partition with respect to any of the elements, we already know that 1 is smaller than 9 because they go to other parts. different parts with respect to this pivot. for three and four there is no such element. so algorithm just must compare these two elements to be sure that 3 is smaller than 4. so in this case the probability is 1. well this shows that the probability of comparing two elements depends on how close they are in the sorted array. in particular if they're very far apart of each other than the probability is small and if they are close to each other than the probability is high. we will use this observation in the formal proof of our statement. we now start to formally prove an upper bound on the running time of the randomized quicksort algorithm. for this, we introduce the following random variable. let i and j be different indices from 1 to m. we define xi of ij to be equal to 1 if two elements, a'[i] and a'[j] are compared in the [inaudible] quick sort algorithm and to be equal to 0 otherwise. once again, to estimate the running time of the quick sort algorithm, we would like to estimate the total number of comparisons made, so we would like to estimate, for any pair of elements, what is the probability that they are compared? as we discussed on the previous slide, the probability that two elements are compared depends on how close they are in the sorted version of our array. for this reason, we define c of ij dependent on the sorted array. we do not have this sorted array, right? we are only constructing this in the quick sort algorithm but we use it just for the analysis, okay? the next thing to note is the following. for any two elements of our initial array assorted array doesn't matter, so for any two elements they are either compared just once or they are not compared at all. so, why is that? why just once? well, if two elements are compared at some point, this means that at this point one of these elements is because in the partition procedure we can put a with all of the elements of the current summary. so, if two elements are compared that one of them is a pivot. this also means that right after the call of this partition procedure, we are not going to use this pivot element. we will put the pivot element into its final place, and we are not going to touch it in any of the subsequent calls. this immediately implies the quadratic upper bound on the worst case right in time with final algorithm. once again we have quadratic number of possible pairs of element, and each pair of element is as it compared once or not compared at all. right so right in time with the worst case is quadratic. now comes the most important observation of this proof. i claim that the elements a'[i] and a'[j] are compared if and only if the first pivot selected in the subrange of the solitary a prime from either side to index j is either a prime of a, of i, or a prime of j. well let's see why. first of all, when we select it pivot the random pivot which is not in their sub range, and then all the elements from this sub range in this sort of element goes either to the left or this to the right. so, they all stay together in the same branch of three, okay. so before we select a pivot which stays inside this range, all these elements stay together in the same sub-array. now, assume that we selected a pivot from this sub-range, and assume that it is not a'[i] or a'[j], for example. in this case a prime of a and a prime of j will be splitted apart. they will go into different parts with respect to this pivot, right? at this point i must humor that all the elements in my summary are different, and in duality are different, okay? so once again, if the first selected element from this subrange is not prime of a or a prime of j then these two elements are not going to be compared. because right after the partition procedure uses this pivot from this range a prime of a and a prime of j will go to different parts, right? if, on the other hand, the first selected pivot from this subrange is either a prime of a or a prime of j, then these two elements are going to become paired, right? so this is the most important observation in this proof. everything else is just calculations. so if this is clear, let's then estimate the probability that second respondent to elements are compared. so we know that they're compared if and only if the first selected pivot in this sub range is one of these two elements. this helps us to estimate the probability of not the fact that c of i j is equal to one. well this is equal to two. i mean because we have only two choices. i mean either a prime of a, or a prime of j divided by the total number of choices, i mean the total number of elements in this subrange. and this is j minus i plus 1. so the probability that z of ij is equal to 1 equals 2 divided by g minus i plus 1. for example, if j and i differ by 1. so j is equal to y plus 1. so neighboring element in the. then this probability is equal to 2 divided by 1 plus 1. this is 2 by 2, this is 1. [inaudible] just reflects the fact that if there are two neighboring elements inside. this sorted array, then the algorithm just must compare them, to understand that one of them is smaller. there is no other element that can help our algorithm to realize that one of these element is smaller than the other one, okay. this in turn helps us to estimate the expected value of this random variable. so recall that if we have a random variable, which takes only values zero and one, then its expected value is one multiplied by the probability that it takes value one plus zero multiplied by the probability that it takes the value zero. well zero multiplied by something is zero. so what is left is just probability. that it takes multiplied by one. so the expected value of cij is equal to 2 divided by g minus i plus one. the final step in our proof is estimating the sum random variables to see they all possible i and j. so, once again the expected value of average value of the sum of the number of comparisons made is the expected value of the sum of all possible x adjacent, or for all i js. so the expected value of their sum is the sum of their expected values. so we can write the following. the average running time is equal to the sum overall possible different of the expected values cij, and we know this expected value already. so this is a some overall possible different ij, in where j is greater than i of 2 divided by g minus i plus one. well we can take this constant two out, and consider all the possible. and consider a fixed i. for this i what we have j ranges from i+1 to n. so what we have for the specified time in this sum is a subset of the following sum, 1/2+1/3+1/4 and so on. and this is actually a known sum. this is called harmonic series and it is known that it grows arithmetically. once again, 1 over 2 plus 1 over 3 plus, and so on, 1 over n, is theta of logarithm of n. well, this means that, for each they correspond in sum, which ranges over all j from i plus 1 through n, grows, at most, logarithmically. this means since we have m traces for i from one to m that the grows vertically as m. okay, and this concludes our proof. 
in this video we address the issue of equal elements in the. so recall that we proved the upper bound on the running time of the render greek algorithm, in the assumption that all the elements inside the given array are prioritized different. and actually, we used essentially these assumptions. so, recall that we estimated the probability that two elements, a prime of i and a prime of j are comparative. and we argued that if any element between them is selected, the appearance is that they will not become period. however, if they are equal, so if a prime of a is equal to a prime of j, this means actually that all the elements in this range are equal. so if we select any element inside this range, in the middle of this range, as a pivot, then it is not true that these two elements will go into different parts with respect to this element, this is just because all of the elements inside this range are equal, which means that if we partition with respect of this element, all of the elements in this range will be [inaudible] this element. so, they all will be in the left part with respect to this element, okay? so our analysis doesn't work for equal elements but let's see what happens in real life. what happens if we run the greek sort algorithm for the array in which there are equal elements. for this, let's use the following online visualization. this visualization shows how different selection, different certain algorithms that are formed on different datasets. so there are eight certain algorithms here where we are now interested in this quicksort algorithm. and there are four different types of datasets. so the field datasets is just random sequence. the next one is in sorted sequence, the next one is a reversed sequence and the next one which is most interesting to us at the moment is a sequence that contains a few unique elements. so let's see how the greek sort algorithm performs on the last dataset. so for this let's just run all the algorithms, on all data sets. so let's see what happens here. so you, you may notice now that, for example, have already sorted everything. and while greek sort have just finished to sort the last, the last data set. so greek sort is not, is not so fast on data sets that contains few unique elements and this is why. so just consider a dataset that consists of elements that are all equal to each other. so all elements are equal to each. this means that the selection, the partition procedure always partitions the array with respect to the element x, right? and then in this case, one of the parts, namely the part of the elements that are greater than x, is just empty. it has size zero. and the other part has size n minus one. so the records and equalities, the records equalities on the running time of how a algorithm on such a data set always satisfies the following relation, t of n is equal to t of n minus 1 plus a linear term plus t of 0. and we know already, so this is an unbalanced partition. we know the responds to the quadratic right in time so, which means that the running time of the quick sort algorithm a very simple array. so it contains all the elements of this array are equal. which means that actually this array is already sorted. in this array our quick sort algorithm spends a quadratic time to sort it to overcome this difficulty we'll do the following. instead of partitioning our rate into two regions. namely these regions contain all elements that contain all x and all elements that are greater than x. we are going to partition into three parts. the corresponding partition procedure is usually called three-way partition. formally, it returns two indices. m1 and m2, such that, all the elements inside the region from m1 to m2 are equal to x. all the elements to the left of this region are smaller than x. all the elements that are to the right of this region are greater than x. so this is how it looks pictorially. we have three regions. so, from l to m1 minus 1, we have all elements that are smaller than x. in the region from m1 to m2, we have all elements that are equal to x. in the region from m2 plus 1 to r, we have all elements that are greater than x. this procedure actually can be implemented in a similar way to their regional partition procedure. it can be implemented ties with a single kind of area with maintaing their regions or it can be implemented with two counts. so we first split our rate into regions which contain elements of most x or greater than x and then we split the region into two parts. well, this is how the modified randomized quick sort algorithm is going to apply. so we just replace it, the cold partition procedure by a cold two partition suite procedure. now we have three regions, and, actually the middle region is in its final place, so we do not touch it after the partition procedure. we make two recursive calls to the first region and to the last region. so, let's see whether the resulting algorithm is indeed greek. and for this, let's use the same visualization. the resulting algorithm is shown here in the last column. let's, once again, run all the algorithms and see what happens in the last column. well we see that now, the results in greek algorithm, is indeed greek. 
in this last video of the quicksort lesson, i would like to address two implementation issues. so the first issue is about space complexity of the quicksort algorithm. so on one hand, when sorting an array by a quicksort algorithm, we do not use any additional space. we just partition the array and with small elements inside the array. on the other hand, the quicksort algorithm is a recursive algorithm. and when we make a recursive call we store some information on this tech. right? so on one hand it is possible to show that the average recurrent depths is logarithmic. meaning that we need only a logarithmic additional space. on the other hand, there is a very nice and elegant trick that allows to re-implement the quicksort algorithm, such that it's worst case space complexity is at most logarithmic. so for this, let's recall that the quicksort algorithm contains of the call to the partition procedure and then of two recursive calls. so the situation when we have a recursive call is and, if the procedure is called tail recursion. and there is a known way to eliminate such a recursive call. namely, instead of making this recursive call, let's just update. well, in the second recursive call, we sort the right part of our array. i mean, the part from index n+1 to index r. instead of making this recursive call, let's replace the with a while loop, inside this while loop we call the partition procedure as shown on the slide. then we make a recursive call to the left part, but instead of making the recursive call for the right part, we'll just update the value of l to be equal to m+1. and then we go to the beginning of this while loop, and this essentially mimics our recursive call. so far so good. we've just realized that we can eliminate the last recursive call. at the same time let's also realize the following thing. in our quicksort algorithm we first call the partition precision, then we make two recursive calls. and these two recursive calls are in a sense independent. well it doesn't matter which comes first, right? so they do not depend on each other. this means that we can as well eliminate a recursive call through the first part. well, and this in turn means that we can always select which one to eliminate. and for us, it is better to remove a recursive call to a longer part. and this is why, if we always make a recursive call during the rate which is shorter, then we make a recursive call during the rate which is at least twice shorter than the initial already, right? and this in turn means that the depths of our recursion will be at most logarithmic. because well, the first recursive call is made for an array of size of at most n over 2, then at most n over 4 and so on. so the depth is logarithmic, which is good. and this can be implemented as follows. so we first call the partition procedure. it gives us a value of m. at this point, we know the length of two parts. and we just compare them. if, for example, the lengths of the first part is shorter, then we make a recursive call to this part. and instead of making the recursive call for the second part, we just update the value of l. in the other case when the right part is shorter, we make the recursive call for this part, and instead of making the recursive call for this part, we'll just update the value of r. right? so overall this gives us an implementation of the quicksort algorithm which uses in the worst case an additional logarithmic space. so the next implementation issue concerns the random bits used by our algorithm. so i assume that we would like to have a deterministic version of our randomized quicksort. and this is a reasonable thing to want because in practice we would like to have such a thing as reproducibility, which is for example essential for debugging. so we would like our program to always output the same, on the same dataset. and this is why we would probably not like to use random numbers, okay? then we can do the following. the following algorithm is known as intro sort and is used in many practical implementation of quicksort. so instead of selecting the pivot element randomly, let's select it as follows using, for example, the following simple heuristic. each time when we're given a summary, and we need to partition it with respect to some pivot. so for this we need to select pivot, and let's select it as follows. we take the first element of the summary, the last element and the middle element, for example. then we have three elements, and we sort them. we just compare them and we select the medium value of these. and we use this element as our pivot element. so this is a very simple heuristic, it can be implemented very efficiently. we just need three comparisons to select this median. and in many cases this is enough for the quicksort algorithm to work effectively. however, this is not what we want, right. we are not happy with the statement that this algorithm works. works well in many cases. we would like our algorithm to works well just on every possible input. unfortunately there are pathological cases in which these heuristics works badly. but we can overcome this in the following way. while running our quicksort algorithm, well let's count what is the current depths of our recursion three. and at some point when it exceeds some values here again, for some constant c, then we just stop the current algorithm and switch to some other algorithm. for example for the heap sort algorithm. this is another efficient algorithm, which is, asymptotically as good as mergesort i mean, it has asymptotic again. however greek sort is usually faster in practice. so, at this point, we switch to the quick sort algorithm. which means that for these pathological bad instances, for the quicksort with this simple heuristic of selecting the pivot element, we still work in the worst case in time n log m. because before we exceeded the depth c log n, we spend time n log m. and after this, we'll start this algorithm. immediately and we run the heap sort algorithm. so overall, we've spent time n log n. so this gives us an algorithm which in many cases performs like the quicksort algorithm and in any case, just in the worst case, its running time is bounded above by n log n. so to conclude, the quicksort algorithm is a comparison based algorithm whose running time is big o of n log n in the average case, and big o of n squared in the worst case. what is important in this algorithm is that it is very efficient in practice. it is more efficient than the north shore algorithim for example. for this reason it is commonly used in practice and for this reason it is called quicksort 
before recording this lecture, i stopped by the coffee shop. this cappuccino is good. and as soon as i gave $5 to the cashier, she faced an algorithmic problem of which coins to select to give me the change. and cashiers all over the world use an algorithmic approach called greedy algorithm to solve this problem. today we will learn how cashiers and computer scientists use greedy algorithm for solving many practical problems. so the change problem is finding the minimum number of coins needed to make change. more formally, input to the problem is integer money and positive integers, coin1, coin2, coind, that represents coin denominations. for example in the us, coin1 will be 1 cents, coin2 will be 5 cents, 10 cents, 25 cents, and 50 cents. and the output is the minimum number of coins, with denominations coin1, coin2, coind that changes money exactly. so today in the morning, when cashier had to return me 40 cents, she most likely used the following algorithm. first, finding the largest coin denomination that is smaller than 40 cents. it will be 25 cents. so she gave me 25, 15 cents left and then the next challenge is how to change 15 cents. the next step is she probably found the largest coin smaller than 15 cents, it is 10 cents. she gave me 10 cents, and finally, she returned 5 cents. as a result, she changed 40 cents as 25 plus 10 plus 5. do you think it's the minimum number of coins she could possibly return? it is the minimal number of coins in the united states. but if you travel to tanzania, it won't be the minimum number of coins because there is a 20 cent coin in tanzania. and therefore this greedy approach to solving the change problem will fail in tanzania because there is a better way to change 40 cents, simply as 20 cents plus 20 cents, using tanzanian 20 cents coin. since the greedy approach to solving the change problem failed, let's try something different. let's try the recursive algorithm for solving the same problem. suppose we want to change 9 cents, and our denominations are 1 cent, 5 cents, and 6 cents. what would be the optimal way to change 9 cents? well, if we only knew what is the optimal ways to change 9 minus 6 cents, 9 minus 5 cents and 9 minus 1 cents, then we would know, what is the optimal way to change 9 cents? in other words, to change 9 cents, we need to know how to change small number of cents, in our case, 3 cents, 4 cents, and 8 cents. and therefore, an approach to solving this problem would be to use this recurrence to write the recursive program. this idea is implemented in the program recursivechange. to change money, cents using coins, coin1, coin2, coind, we do the following. we first recursively call recursivechange with the amount of money, money minus coin1, money minus coin2, and money minus coind. and find the minimum amount of money for these d choices. we have plus 1 because there is one more coin to add and returns this way. this looks like the right approach to solve the problem, but let's check how fast the resulting program is. so, when we're changing 76 coins, there are actually three choices. we need to recursively call recursivechange for 70 cents, 71 cents, and 75 cents. but for each of these values, we need once again to call three choices. and we will continue growing this tree and very quickly it will turn into a gigantic tree. let's check how many times we have already tried to change 70 cents. three times, and we only started expanding this tree. in fact, if we continue further, we will see that there were six times when we needed to compute recursivechange for 70. how many times do you think we will need to run recursive calls when we compute the minimal number of coins for 30 cents? it turn out that we will need to call it trillions of times, which means that our seemingly very elegant recursivechange program will not finish before the end of your lifetime. so as simple as the change problem looks like, neither a greedy approach nor a recursive approach solve it in a reasonable time. 60 years ago, a brilliant mathematician, richard bellman, had a different idea. wouldn't it be nice to know all the answers for changing money minus coin i by the time we need to compute an optimal way of changing money? and instead of the time consuming calls to recursivechange, money minus coin i, that may require to be repeated trillions of times, they would simply look up these values. this idea resulted in dynamic programming approach that is applied in thousands of diverse, practical applications in a myriad of different fields. and the key idea of dynamic programming is to start filling this matrix, not from the right to the left, as we did before in the recursive change, but instead, from the left to the right. so, we will first ask the trivial question, what is the minimum number of coins needed to change 0 cents? and, of course, it is 0. what is the minimum number of coins to change 1 cents? obviously it is one, but we can compute this number by finding what is the minimum number of coins to change 0 cents and adding one coin. we will proceed in a similar fashion to compute the minimum number of coins to change 2 cents, 3 cents, and 4 cents. there is only one possibility to derive this number from the previous number. and for 5 cents, actually there are two possibilities, green and blue. for green one, you can derive it from 0 cents by adding 5 coins, and for blue possibility, we can derive it from 4 cents by adding one penny. well, which possibility would you select? of course the one that gives you the minimum change for 5 coins. and continue further and apply the code to 6 cents and there are three possibilities and once again we select the optimal choice that correspond to minimum number of coins. let's say it may be 0 coins plus 6 cents. we continue for 7 cents, continue for 8 cents, and finally, very quickly, we actually found the correct answer for 9 cents. we need four coins to change 9 cents. and this results in dpchange algorithm that simply fills up the table that i just showed you from left to right. dp change is the first dynamic programming algorithm that you saw in this course, and there will be thousands more. you may be wondering why is this algorithm called dynamic programming and what does it have to do with programming? well, in fact programming and dynamic programming has nothing to do with programming. amazingly enough, dynamic programming is one of the most practical algorithms computer scientists use. but when richard bellman was developing this idea for air force project he was working on, it looked completely impractical. and he wanted to hide that he's really doing mathematics from the secretary of defense, rather than working on air force project. therefore he invented a name that basically has nothing to do with what dynamic programming algorithms do. in his own word, he said, what name could i choose? i was interested in planning but planning is not a good word for various reasons. i decided therefore to use the word programming, and i wanted to get across the idea that this was dynamic. it was something not even a congressman could object. 
cystic fibrosis is one of the most common genetic diseases in humans. approximately one in 25 people carries a cystic fibrosis gene. and when both parents carry a faulty gene, there is a 25% chance that their child will have cystic fibrosis. in the early 1980s biologists started the hunt for cystic fibrosis genes, one of the first gene hunting projects in the framework of the human genome project. 30 years ago biologists narrowed the search for the cystic fibrosis gene to a million nucleotide-long region on chromosome 7. however, this region contained many genes, and it was not clear which of them is responsible for cystic fibrosis. how would you find which of these genes is the cause of cystic fibrosis? i'll give you a hint. cystic fibrosis in involves sweat secretion with abnormally high sodium levels. well, this is a biological hint that does not help us to solve the challenge of finding something in this one million nucleotide area that is responsible for cystic fibrosis. let me give you hint number two. by that time when cystic fibrosis hunt was on, biologists already knew the sequences of some genes responsible for secretions. for example, atp binding proteins act as transport channels responsible for secretion. you still may be wondering how these two hints may help you to find the cystic fibrosis gene in the found one million nucleotide-long region on chromosome 7. but here's my third hint. should we search for genes in this region that are similar to known genes responsible for secretion? biologists used this third hint and bingo, they found that one of genes in this region was similar to the atp binding proteins that act as transport channels responsible for secretion. to learn how biologists find similarities between chance, we will first learn how to play a simple game called the alignment game. the alignment game is a single person game. i give you two strings, and your goal is to remove symbol from the strings in such a way that the number of points is maximized. i have to explain to you how you can get points for playing the alignment game. you can either remove the first symbol from both strings. and in this case, you get one point if they're the same symbol, you don't get any points if they are different symbol. or you can remove first symbol from one of the strings and in this case you also don't get any points. so let's try to play this game. in the beginning it makes sense to remove the first symbol from both strings, we'll get plus one. then another pair of identical symbols, another plus one. and now symbols are different so it doesn't make sense to remove them both because we'll get zero point. maybe we should only remove c from the second string and after we've done this there is a possibility to remove two gs from both string. we get another point we continue, continue, continue, and finally after playing this game we get score of plus four. do you think you can get score of plus five playing this game? we also after playing this game have constructed something that is called alignment of two strings. alignment of two strings is a two row matrix such that, that first row consist of symbols of the first string in order, possibly interspaced with the space symbol. and the second row consists of the symbols of the second string once again, possibly interspersed with the space symbol. after we constructed the alignment, we can classify different columns in the alignment matrix as matches or mismatches or insertions. insertions corresponds to the case when we selected the symbol from the second string and deletions that correspond to the case when we selected the symbol from the first string. and more over we can score this alignment by giving premium for every match, we'll give premium plus one and penalty for every mismatch and every insertion and deletion that we denote as indel. in our case we will use penalty minus mu for mismatches and penalty minus sigma for insertions and deletions or indels. for example in our case if mu equals zero and sigma equal to one, then we get alignment score equal to one. so we define the alignment score as number of matches minus mu number of mismatches and minus sigma number of indels. and the optimal alignment problem is given two strings mismatch penalty mu, and indel penalty sigma find an alignment of two strings maximizing the score. we will be particularly interested in one particular score of alignment. we will define common subsequence as simply matches in an alignment of two strands. in this case, common subsequence is represented by atgt, and the longest common subsequence problems that we will be interested in is the following. given two strings we want to find the longest common subsequence of these strings. and of course, you have already recognized that to find longest common subsequence we simply need to find maximum score alignment with the parameters mu equals zero and sigma equals zero. another classical problem in computer science is the edit distance problem. given two strings, find the minimum number of elementary operations, insertions, deletions, or substitutions of symbols. that transform one string into another. and of course the minimum number of insertions, deletions, and mismatches in an alignment of two strings, represents the edit distance. for example, if you want to find the editing distance between the strings, editing and distance, they can construct optimal alignment of the string with appropriate scores. here i show matches, mismatches,insertions, deletions. and to see that the edit distance problem is equivalent to the alignment problem let's consider this alignment between editing and distance. and let's compute the total number of symbols in the two strings. obviously the total number of symbol in two strings is equal to twice number of matches, plus twice number of mismatches plus number of insertions plus number of deletions. i will take the liberty to derive this expression and after i rewrote it you will see that the first three terms corresponds to the alignment score, and the last three terms corresponds to the edit distance. therefore, minimizing edit distance is the same as maximizing the alignment score. which means the edit distance problem is just one particular version of the alignment problem. 
let's now see how dynamic programming algorithm solves the edit distance problem. we start by considering two strings, a of length n and b of length m and we will ask the question question what is an optimal alignment of an i-prefix of a, which is the first i symbols of a, and the j-prefix of b which are the first j symbols only. the last column of an optimal alignment is either an insertion or a deletion or a mismatch, or a match. and please notice that if we remove the last column from the optimal alignment of the strings, what is left is an optimal alignment of the corresponding two prefixes. and we can adjust the score of the optimal alignment for i prefix and j prefix by adding plus 1 in the case of insertion, plus 1 in the case of deletion, plus 1 in the case of mismatch and adding nothing in the case of match. let's denote d (i, j) to be the edit distance between an i-prefix and a j-prefix. and in this case, this figure at the top of the slide illustrates the following recurrency. d(i,j) equal to the minimum of the following four values: d(i,j-1)+1, d(i-1,j)+1, d(i-1,j-1)+1, in the case the last two symbols in the i prefix of a and j prefix of b are different. and d(i- 1, j- 1), if the last symbols in i and j prefix are the same. our goal now is to compute the edit distance d, i, j between all i prefixes of string a and all j prefixes of string b. in the case of string editing and distance we will construct eight by nine grid and our goal is to compute all edit distances d(i, j) corresponding to all nodes in this grid. for example, for i and j equal to four and four. how will we compute the corresponding distance d(i, j)? let's start by filling distances d(i, 0) in the first column of this matrix. it is easy because indeed we are comparing an i-prefix of string a against a 0-prefix of string d and therefore this edit distance for i-prefix will be equal to i. that's what's shown here, similarly we can easily fill the first row in this matrix. and now let's try to compute what will be the distance d(1,1) corresponding to comparison of string consisting of single symbol e, with a string consisting of single symbol d, there are three possible ways to arrive to the node (1, 1): from the nodes (0, 0), (0, 1), and (1, 0). which one should be the way we will select to find the optimal edit distance. according to the previous recurrency, we should select the one of three directions that gives minimal value for d(i, j), which is minimum of 2, 2, and 1 and therefore we arrive to node (1, 1) by diagonal edge. let's keep this in memory that the right direction to arrive at node (1, 1) was the diagonal direction. we will now try to compute the edit distance for the next node in the matrix. and in this case d(2,1) is equal to minimum d(2,0) + 1, d(1,1) + 1 and d(1,0) of each tells us that's the optimal way to arrive to this node would be again, by diagonal edge. you continue further, once again compare three values and it turn out that the best way to arrive to this node will be by vertical edge. we'll continue further and we'll fill the whole second column in the matrix. now let's continue with the circle, what about this node? for this node d(1,2) = minimum {d(1,1) + 1, d (0,2) + 1, and d (0,1) + 1}. and it is minimum of 2, 3, and 2. in fact, there are two optimal ways to arrive to this node and in this case we show both of them by diagonal edge into this vertex and by horizontal edge of this vertex. you'll continue further and slowly but surely we will fill the whole matrix. the edit distance pseudocode implements the algorithm we just discussed. it first fills in the first column and the first row of the dynamic programming matrix and then it continues filling it up by computing the cost of moving to vertex (i, j) using insertion, deletion, or mismatch or match or in other words, exploring all possibility. moving to the vertex i, j using vertical edge, horizontal edge, and diagonal edge. and then it finds out which of these possibilities results in the minimum edit distance. 
we now know how to compute the edit distance or to compute the optimal alignment by filling in the entries in the dynamic programming matrix. but it doesn't tell us yet how to construct the alignment two rows with the first row representing the first sequence and the second row representing the second sequence. here's an idea. let's use the backtracking pointers that we constructed while filling in the dynamic programming matrix to reconstruct optimal alignment between strings. we can start by noting that any path from (0, 0) to (i, j) in the dynamic programming matrix spell an alignment of an i prefix of a with a j prefix of b. for example let's start the line in the sequences, which means let's start traveling from the point 0, 0 to the point n, m in our dynamic programming matrix. as soon as we move along diagonal left it will correspond to either mismatch or match, then we'll continue using horizontal or vertical edges and it will correspond to insertions or deletions. then we will use once again diagonal edge. in this case it is a match, and you'll continue by constructing the n-alignment of two strings. please note that the constructed path corresponds to distance a and is not an optimal alignment because we know that an optimal alignment distance is 5. to construct an optimal alignment we will use the backtracking pointers by starting from the last vertex in this matrix particularly from this vertex where the added distance is recorded as 5. using backtracking pointers we see that there are two possible ways to arrive to this last vertex. let's arbitrarily choose one of them. one of them corresponds to a mismatch and another corresponds to insertion. so let's arbitrarily choose a mismatch edge that will correspond to mismatch between j and i, then from the previous point there is only one way to move into this point and it will correspond to an indel that will continue further, match, further, further, further, further, and we will finally arrive to the initial point at the same time constructing the optimal alignment between two strings. the output alignment pseudoode implement's this idea. we simply look at the backtracking pointers that enters in the node (i, j). if they arrive to node (i, j) by using a vertical edge that we will simply output one column of the alignment with a of i in the first row. if on the other hand it corresponds to horizontal edge we output column with b of j in the second row, and if it corresponds to a diagonal edge we output a column of alignment with a of i in the first row and v of j in the second row. it appears that we actually need to store all backtracking pointers to output alignment, but this slightly modified pseudocode tells you that you can compute backtracking pointers by analyzing entries in the dynamic programming matrix and thus saving a little space. edit distance is just one many applications of string comparisons in various disciplines that range from analyzing internet pages to finding similar genes. we started this lecture from the example of gene hunt for cystic fibrosis: one of the first successes of the human genome project. if you want to learn more about comparing genes, protein, and genomes you may enroll in the coursera specialization called bioinformatics or you can read the book bioinformatics algorithms: the active learning approach. 
hi, today we are going to revisit the knapsack problem, the problem that we already discussed in the greedy algorithms module. in this very first segment of this lesson, we will recall the definition of this problem, as well as motivate its study by providing a few examples of applying this problem in real life. our first example is the following. assume that you are given a time slot, say two or three minutes, and together with this time slot, you are given a set of tv commercials. for each commercial, you know its revenue and you know its duration, that is length in minutes, and your goal is to maximize the revenue. that is, you would like to select some subset of your available tv commercials, so that the total revenue is as large as possible while the total length does not exceed the length of your available time slot. in our second example, you are given a fixed budget and your goal is to purchase a number of computers so that to maximize the total performance. again, we assume that the part of your input in this case is a set of available computers or machine and for each machine you know its price and its performance. both the considerated problems can be easily seen to be special cases of the following general problem known as the knapsack problem. in this problem, you are given a set of items together with the total capacity of the knapsack. for each item you know its value and its weight. for example, the value of the green item here is four, while its weight is 12. and your goal is to select the subset of items such that the total value is as large as possible while the total weight is at most, the capacity of the knapsack. in our case, the total capacity of the knapsack is equal to 15. there are two versions of the knapsack problem. fractional knapsack and discrete knapsack.so, for the fractional version, which you are already familiar with, you can take any fraction off of any item, while in the discrete version, for each item, you either take the whole item in your knapsack or you do not take it at all. so, in turn, the discrete version has two variants also. so, the first variant is knapsack with repetitions. so in this case, you are given an unlimited quantity of each item. while in the knapsack without repetitions, you are given just a single copy of each item. so we know already that the fractional knapsack problem can be solved by a simple greedy algorithm. such an algorithm at each iteration just picks an element, an item with the currently maximal value per unit of weight. this strategy, however, doesn't work for the discrete version of the knapsack problem. so instead of using greedy strategy, we will design a dynamic programming solution to find an optimal value. now let me give you a toy example. assume that our input consists of a knapsack of total capacity of ten and four items shown on the slide. then the optimal value for the knapsack without repetitions problem is equal to 46 and it can be obtained by taking the first item and the third item into your knapsack. at the same time for the knapsack with repetitions problem. the optimal value in this case is equal to 48 and it can be obtained by taking one copy of the first item and two copies of the last item. finally, for the fractional knapsack problem, the optimal value is equal to 48 and a half and can be obtained by taking the first item, the second item, and half of the last item. let's also use this example to show that greedy algorithm fails for the discrete version of the knapsack problem. recall that the greedy strategy for this problem is to first compute the value per unit of weight for each item. in our case, the value per unit of weight for the first item is equal to five, for the second item it is equal to four and two thirds, for the third item it is equal to four, and for the last item it is equal to four and one half. so the first item has maximal value per unit of weight so we take it into our solution. the next available item with the maximal value per unit of weight is the second one, so we take it also into the solution. now the remaining capacity is too small to add any other element. so this is our constructed solution, and it has weight, it has value 44 which is not optimal, we know it already. for example here, by replacing the second item by the third item, we will increase the total value. this actually means that taking an element with a maximal value per unit of weight is not a safe step. just by doing this we can lose a possibility to construct an optimal solution. right, so this means actually that we need some other algorithm to solve this problem optimally. and we will design such an algorithm based on the dynamic programming technique in the next video. 
in this video, we will design a dynamic programming solution for the knapsack with repetitions problem. recall that in this problem, we are given an unlimited quantity of each item. this is a formal statement of the problem. we're given n items with weights w1, w2 and so on, wn. and its values are v1, v2 and so on, vn. by capital w we denote the total capacity or the total weight of the knapsack. and our goal is to select the subset of items where each item can be taken any number of times such that the total weight is at most capital w while the total value is as large as possible. to come up with a dynamic programing algorithm, let's analyze the structure of an optimal solution. for this consider some subset of items, of total weight, at most capital w, whose total value is maximal. and let's consider some element i in it, let's see what happens if we take this element out of this solution. so what remains is some subset of items whose total weight is at most capital w minus wi. right? so this is easy. what is crucial for us is that the total value of this remaining subset of items must be optimal. i mean it must be maximal amount all subset of items whose total weight is at most capital w minus w i. why is that? well, assume that there is some other subset of items whose total weight is at most, capital w- wi, but whose total value is higher? let's then take the highest item and put it back to this subset of items. what we get, actually, is the solution to our initial problem of higher value. i mean, its total weight is at most capital w, and its value is higher than the value of our initial solution. but these contradicts to the fact that we started with an optimal solution. so, such trick is known as cut and paste trick. and it is frequently used in designing dynamic programming algorithms. so, let me repeat what we just proved. if we take an optimal solution for a knapsack of total weight w and take some item i out of it, then what remains must be an optimal solution for a knapsack of smaller weight. so this suggests that we have a separate subproblem for each possible total weight from zero to capital w. namely, let's define value of w as a optimal total value of items whose total weight is, at most w. this allows us to express value of w using the values for a smaller weight knapsack. namely to get an optimal solution for a knapsack of total weight w we first take some smaller knapsack and an optimal solution for it and add an item i to it. so first of all to be able to add an item i to it and get a knapsack of total weight w we need this smaller knapsack to be of total weight at most w minus wi, also when adding i'th item to it we increase its value by vi, and the final thing is we do not know which element to add exactly. for this reason, we just go through all possible elements, n items, and select the maximal value. the maximal value of the following thing: value of w minus wi, plus vi. having a recurrent formula for value of w as we just discussed, it is not so difficult to implement an algorithm solving the knapsack problem with repetitions. recall that we expressed the solution for a knapsack, through solutions from knapsacks of smaller weight. this means that it makes sense to solve our subproblems in the order of increasing weight. so we do this in the pseudocode. initially we set value of 0 to 0 just to reflect that fact that the maximal possible total value of a knapsack of weight 0, clearly equals 0. then we go in a loop from w=1 to w. and for each such w we just compute the corresponding maximum as follows. we go through all items i such that wi is at most w. and for each such item i, we see what happens if we take an optimal solution of for a knapsack of size w minus wi, and add an item i into it. clearly in this case, the total value is value(w minus wi) plus vi, and the total weight is at most w. so this is a feasible solution for a knapsack of total weight w. so we check whether the result in value is larger and what we currently have and if it is we update value of w. in the end, we just return value of capital w. so this algorithm is clearly correct because it just implements our recurrent formula, right? so in particular this loop just computes the maximum from the previous slide. now let's estimate the running time of this algorithm. it is not difficult to see that the running time is of n multiplied by capital w. why is that? well just because we have two nested loops here. so this is the first loop, and this is the second loop. the first one has capital w on it, capital w iterations. and the second one has n iterations. n iterations. what happens inside in the loop here it takes just constant time. we conclude this video by applying our algorithm to the example considered a few minutes before. so in this case we are given four items and a knapsack of total capacity 10. we are going to compute the optimal value for all knapsacks of total weight from zero to ten. so, which means that it makes sense to store all these values just in an array. so, shown here on the slide. initially this array is filled by zero's and we're going to fill it in with values from left to right. so the first non-obvious cell is two. so this is the first weight for which we can add any item. so in this case we can actually say that to get a solution for knapsack of total weight two we can get a solution for knapsack of total weight 0 and add the last element to it. this will also give us plus nine to the value. so this is the only possible solution for this cell, so we do not even need to compute the maximum. so in this case, the value is equal to nine. so what about value of three? so in this case, we already have a choice. we can either get an optimal solution for total weight one, and add the fourth element to it, or we can get an optimal solution for a knapsack of total weight zero and add the second element to it, whose value is 14. so among these two values, the second choice is better. it gives us a solution of value 14, so we'll write it in this cell. now, for value of 4, there are already three choices. let's consider them. so also we can take an optimal solution for a knapsack of total weight two and add the last to it. so this is plus 9 or we can take an optimal solution for a knapsack of total weight one and add the second item to it so plus 14 or we can take an optimal solution for a knapsack of total weight 0 and add the third item. this is plus 16. right? so in this case, we need to select the maximum amount 16, 14 and 9 plus 9 which is 18. in this case, 18 is the maximum value. so we'll write it in this cell. so by continuing in the same manner, we can fill in the whole array and see that the last element is equal to 48, we just devise that the optimal value for this knapsack with repetitions problem is equal to 48. and also, let me remind you that this optimal value can be updated by taking one copy of this item, and 2 copies of the last item. in the next video, we will learn how to solve this problem when repetitions are not allowed. 
in this video we will be designing a dynamic formatting solution for the knapsack without repetitions problem. recall that in this problem we're give a single copy of each item. so this is also to remind you the formal statement of the problem, so we emphasize once again that we are not allowed to take more than a single copy of each item. well, we already know that our previous same reason cannot produce the right answer for our new very namely for the knapsack without repetitions problems. well this is simply because in our toy example is that optimal value for the knapsack with repetitions was 48 while the optimal value for the knapsack without repetitions was 46. so this means that if we just run our previous algorithm, it will produce an incorrect result. still it is important to understand where our algorithms, where our reasoning more generally fails for this problem. so once again, let's consider an optimal subset of items for a knapsack of total weight capital w. and assume for the moment that we know that it contains the nth element. that is the last item. so we argue, well similarly to the previous case that if we take this item out of the current knapsack, then what we get must be an optimal solution for a knapsack of smaller weight, namely of total weight w- wn. so if we take we the smaller solution and we add the nth item to it, we get an optimal solution for the initial knapsack of total weight, w. i assume however, that the optimal solution for the smaller knapsack, already contains the nth item. this means that we cannot add another copy of the nth element to it, right, because then the resulting solution will contain two copies of the nth element which is now forbidden by the problem formulation. so this is why we need to come up with a different notion of a subproblem. so still, let's take a closer look at our optimal solution. it is not difficult to see that there are only two cases, either it contains the lost item, or it doesn't contain it. i assume that it contains, and let's again take this nth item out of our current solution. so what is left? first of all, it is some solution for a knapsack of total weight, capital w- wn, and it also uses only items from 1 to n- 1, because, well, we just took out the nth item, right? if, on the other hand, the initial optimal solution for the knapsack of total weight w does not contain the nth item, well, then it contains only items from 1 to n minus 1. right? well this simple observation will help us to get the right definition of a subproblem for this version of the knapsack problem. well on the previous slide we argued as follows. consider an optimal solution for a knapsack of total weight capital w. and there are two cases. either it can contain the last item or it doesn't contain. if it contains we can take it out, and reduce the problem for small knapsack using only items from one to n minus one. on the other hand, if it doesn't contain the nth item, then we'll reduce it to another case when the knapsack only uses items from 1 to n-1. in any case, we reduce the number of items and in the first case, we also reduce the size of the knapsack, the total weight of the knapsack. we might continue this process, and express the solution for all sub-problems through solutions to force up subproblems. if we continue in the same fashion what we get somewhere in the middle is a solution for a knapsack of some weight that uses some first i items. well let's just use this as a definition of our subproblem. namely, for any w, from 0 to w, and for any i, from 0 to n, let's denote by value of w and i the maximum value that can be achieved by using only items from 1 to i, and whose total weight is at most w. right, then it is easy to express it through solutions for smaller such problems. once again, value of w and i, is a subset, is an optimal value of a subset, of the first items who stole the weight is utmost w. so we know that in this optimal subset, either there is the i-th item or the i-th item is not contained in it. so there are two cases. so we need to select the maximum out of two cases. and the first case if we take the i-th item out what is left is an optimal solution for the following problem. we are allowed only to use the first i-1 items and the total weight should be no more than w-wi, so this is the first term under the maximum. in the second case, if the i-th item is not used in an optimal solution, then we just know that the optimal solution is the same as for the knapsack of total weight, w, using only the first i- 1 items. so we managed to express the solution for our problems through solutions for smaller sub-problems. and this is probably the most important thing in designing dynamic problem in algorithms. we now done our recurrent formula into a dynamic problem in algorithm. as usual, we start from initialization namely with your set all the values of 0, j to 0 for all j and all the values of w, 0 to 0. well, this just expresses the fact that if we have no items, well, then the value is zero. if we have the knapsack of total weight zero, then the total value's also zero, of course. then recall, now, we need to somehow compute, all other values of w, i. recall that we expressed value wi of wi through values of w, smaller w and i- 1 and w and i- 1. this means that we always reduce the problem from wi to something with smaller number of items, to i- 1. this actually helps us to understand that it makes sense to gradually increase the number of allowable items. and this is why we have in this pseudocode an outer loop where i goes from 1 to n. when i is fixed, we will compute all the values of w, i. so for this, we also go from w equal to 1 to capital w and do the following. so now, i and w are fixed, we need to compute value of w, i. first, we just check what is the value of, what is the solution for the subproblem when we use the knapsack of the same weight w but we only use the first i-1 items. this is implemented as follows. we first just assign value of w, i to value of w, i-1. then we need to check whether we can improve this value by using the i-th item. first of all we can only do this if the weight of the ice item does not exceed the weight of the current knapsack which is just w. so, if it doesn't exceed we see what happens if we take an optimal value for the knapsack of the total weight w minus wi. that is filled only by elements from 1 to i minus 1, and add the i-th element to it. if it gives a larger value than we currently have, we will update the value of wi, so in the end we just return the value of capital w and n. because this is the solution to our initial problem. so this a solution for a knapsack of size capital w that uses just all the n items, right? now so it is clear that this algorithm is correct just because it directly implements the recurrent formula that we already discussed. so let's analyze its running time. it is not difficult to show, again, that its running time is actually the same. it is again n multiplied by w. well, this is again just because we have two loops here. so this is the first loop with n iterations, and this is the inner loop with w iterations. and what is going on inside only takes some constant time. now let's apply the algorithm that we've just designed to our toy example. recall that we need to store the values of all subproblems for wi, for all w from zero to ten, and all i from zero to four, in our case. for these purposes, it is natural to use a two-dimensional table, or two-dimensional array. you can see such a two-dimensional array on the slide already filled in. so here we have i, so all the rows of our columns are by all possible way of i, and all the columns in this set by all possible values of w. right, we start by initializing the first row, and the first column of this table by zero. that is, we fill this row by zeroes and we fill this column by zeroes also. then we start filling in this table row by row. that is, we first fill in this cell, then this cell, then this cell, then this cell, and so on. so we go like this. so we first fill in this row, then fill in this row, then fill in this row and then fill in this row. so the results in value 46 is actually the answer to our initial problem. now, let me show you how some particular value, just through this trait, let me show you how some particular value in this table was computed. for example, consider this cell. so formally, this is value, value(10, 2). which means that this is an optimal value of a knapsack of total weight 10 that only uses the first two items. so assume that we don't know what to put here. so we just need to compute it right now. so let's argue as we did before. so this is a knapsack of total weight 10 that uses only the first two items. well, we then say that the second item is either used or not. so if it is not used, then this is the same as filling in the knapsack of total weight ten just using the first item. and we already know this value because it is in the previous row. so this is value 10, 1, right? so the value in this case is 30. on the other hand, if the second item is used, then if we take it out, what is left is an optimal solution for a knapsack of total weight 10 minus 3. because 3 is the weight of the second item, which means that it is an optimal solution for a knapsack of size 7. of total weight 7 that only uses the first, that is only allowed to use the first item. also, if we add this item to, if we add the second item to the solution, we get 30 plus 14. which is much better than without using the second item, right? so that's why we have 44 here. and also for this reason we fill this matrix row by row. so now that when we need to compute the value of this cell, we already have computed the value of these two cells. so that's why we fill our metrics exactly row by row. now let me use the same example to illustrate an important technique in dynamic programming. namely reconstructing an optimal solution. reconstructing an optimal solution in this particular problem i mean finding not only the optimal value for the knapsack of size of total weight. but the subset of items that lead to this optimal value itself. for this we first create a boolean array of size four. in this array, we will mark, for each item, whether it is used in an optimal solution or not. now what we're going to do is to back trace the path that led us to the optimal value, 46. in particular, let's try to understand how this value of 46 was computed. well, first of all, 46 is formally value of 10, 4, that is is an optimal value for a knapsack of total weight ten using the first four items. we argued that the fourth item is either used or not. if it is not used, then this value is the same as the value 10, 3, which is shown here. that is the value of the knapsack of the same weight, using the first three items. if on the other hand it is used, then what is left must be an optimal solution for a knapsack of size 10 minus 2 which is 8, that uses also the first three items. well this value is already computed, it is 30, so we need to compute the maximum among 30 plus 9, because, well the value of the last item is 9 and 46. in this particular case there, the maximum is equal to 46 which means that we decided at this point not to use the last item, right? so we put 0 into our boolean array to indicate this, and we move to this cell. again, let's try to understand how this value was computed. it was computed as the maximum value of two numbers which depend on the following values. so either we do not use the third item, then it is the same, has the value of this cell or we use the third item. in this case, what remains is an knapsack of size, of total weight 6, and using the first two items and its value is 30. plus the weight of the third item, which is 16. in this particular case, 30 plus 16 is larger than 44, which means that this value of 46 was computed using this value. this, in turn, means that we decided to use the third item. let's mark it by putting 1 into our boolean array. now we stay in this cell and we try to understand how it was computed. it was computed as a maximum over this 30 and this 0, plus fourteen. right, in this case, the first value is larger so we move to this cell and we mark that we decided not to use the second item. okay and finally, we realize that we arrived at this value 30 from the right, from the left upper corner. right? so, this way we reconstructed the wall optimal solution. once again, we backtraced the path that led us to the optimal value. here, what is shown here, is that we decided to use the first item and the third item. so let's check that it indeed gives us the optimal value of 46. so indeed if we compute the sum of the weight of the first and the third item, it is 10. and while the total value is 30 plus 16 which is 46 indeed. and as i said before this technique is usually used in dynamic programming algorithms to reconstruct the optimal solution. 
we conclude this lesson with a few important remarks. the first remark is about a trick called memoization. usually when designing a dynamic program and algorithm, you start with analyzing the structure of an optimal solution for your computational problem. you do this to come up with the right definition of a sub-problem that will allow you to express the solution for a sub-problem through solutions for smaller sub-sub-problems. so, when you write down this recurrence relation you can actually transform it to an isa alternative algorithm or a recursive algorithm. the corresponding i 20 algorithm just solves all sub-problems, going from smaller ones to larger ones. and for this reason it is also sometimes called a bottom up algorithm. on the other hand, the recursive algorithm to solve a sub-problem makes recursive calls to smaller sub-sub-problems. and for this reason it is sometimes called the top down approach. well if you implement a recursive algorithms straightforwardly it might turn out to be very slow because it will recompute some radius many, many, many times. like with three-dimensional numbers for example. however, there is a simple trick, and it is called memorization, that allows you to avoid re-computing many times the same thing. namely, you can do the following, when solving sub-problems, right after solving it you store its solution into a table, for example. and when you make a recursive call to solve some sub-problem, before trying to solve it, you check in a table whether its solution is already stored. and if its solution is already in the table which means that it was already computed then you just return it immediately. so this recursive call, turns out to be just a table look up. so this is how a recursive algorithm with memoization works. let's see how a recursive algorithm with memoization for the knapsack problem looks like. for simplicity let's assume that we're talking about the knapsack we use repetitions. in this case, we need to compute our sub-problem for a knapsack of size w, is just the optimal rate of a knapsack of total weight w. so we computed as follows, we computed by recursive procedure. first of all, we check whether its solution is already in a hash table. we use hash table to store pairs of objects. so, for weight w, we store value of w if it is already computed. if it is already in the table, we return it immediately, otherwise we just compute it and we make recursive calls to compute the values for the sub-problem on w minus wi, okay? and when the value is computed, we just store it in our hash table. so this way, we use memoization by storing this in the hash table to avoid recomputing the same thing once again. so once again, an iterative algorithm solves all sub-problems going from smaller ones to larger ones, right? and eventually solves the initial problem. on the other hand the recursive algorithm goes as follows. so it stars from the initial problem and it makes recursive calls to smaller sub-sub-problems, right? so in some sense an iterative algorithm and the recursive algorithm are doing the same job, especially if we need to solve just old range of sub-problems. however, a recursive algorithm might turn to be slightly slower because it solves the same sub-problems on one hand. on the other hand, when making a recursive call you also need to put the return address on stamp, for example. so, the recursive algorithm has some overhead. there are however cases when you do not need to solve all the sub-problems and the knapsack problem is nice illustration of this situation. so, imagine that we are given an input to the knapsack problem where all the weight of n items together with total weight of the knapsack are divisible by 100, for example. this means that we are actually not interested in sub-problems where the weight of the knapsack is not divisible by 100, why is that? well just because for any subset of items since all the weight of items is divisible by 100 their total weight is also divisible by 100. so in this case an iterative algorithm still will solve just whole range of sub-problems. while a recursive algorithm will make only those recursive calls that i actually needed to compute the final solution. so, it will make only recursive course through sub-problems whose weight are divisible by 100. the final remark of this lesson is about the running time. so if you remember the running time of words that we recently designed in this lesson was the log of n multiplied by w. and this running time looks like polynomial, however it is not. and this is why, so consider for example, the following input. i mean, i assume that the total weight of the knapsack is as shown on this slide. this is a very huge number, roughly ten to the 20, i mean 20 digits of decimal representation. at the same time, the input size is really tiny, just 20 digits, right? so this is not gigabytes of data, just 20 digits but on this input already our algorithm will need to perform roughly ten to the 20 operations. this is really huge, for example we can't do this on our laptops, and this is because to represent the value of w, we only need log w digits. so, in case of the knapsack problem, our input is proportional not to n plus w, but to n plus log w. okay, and if you represent the running time in terms of n and log w, then you get the following expression, n multiplied by 2 to the log w, which means that our algorithm is in fact exponential time algorithm. put it otherwise, it can only process inputs where w is not large enough, it's roughly less than 1 billion, for example. okay, and in fact, we believe that it is very difficult to construct an algorithm that will solve this problem in polynomial time, in truly polynomial time. in particular, we will learn later in this presentation that this problem is considered to be so difficult that for solving the knapsack problem for example, in polynomial time, one gets $1 million. 
hello, and welcome to the next lesson in the dynamic programming module. in this lesson, we will be applying the dynamic programming technique for solving a wide range of problems where your goal is to find an optimal order of something. we will illustrate this technique by solving the so-called placing parentheses problem. in this problem, your input is an arithmetic expression consisting of numbers or digits and arithmetic operations, and your goal is to find an order of applying these arithmetic operations that maximizes the radian. you specify this order by placing parentheses, and that's why the problem is called placing parentheses. as usual we start with problem overview. consider the following toy arithmetic expression. 1 + 2- 3 x 4- 5. in this case we have five digits and four arithmetic operations. and we would like to find an order of applying these four arithmetic operations to maximize the value of this expression. so when the order of operation is fixed, you do the following. you take the first operation. you take two adjusting digits, and you apply these operations. for example, if the operation is multiplication, in this case, so then two digits are 3 and 4. so you multiply 3 and 4, you get 12, and you just replace 3 times 4 by 12. you then take the next operation, apply it also, and replace two numbers and the arithmetic sign by this result, until you proceed in a similar fashion. in the end here, you get a single number. and your goal is to find an order that guarantees that this number is as large as possible. you can specify an order just by placing a set of parentheses in your expression. for example, if you would like to apply all your four operations just from left to right, you place the parentheses as follows. in this particular case, we compute the results as follows. so we first compute 1 + 2, this is 3. we then subtract 3 from the results. this gives us 0. we then multiply the result by 4. this is still 0. and finally, we subtract 5. so this gives us -5. and this is actually non-optimal, because for example, there is a better order. in this case, we first multiply 3 and 4, this gives us 12. we then subtract 5, this gives us 7. then we go to compute the sum of 1 and 2, this gives us 3. so when the final operation is subtraction, we subtract 7 from 3. this gives us -4. so in this case the order of applying operations was the following. so we first compute the product of 3 and 4, so this is the first operation. we then subtract 5. this is the second operation. we then compute the result of 1 + 2. so this plus is the third operation, and this minus is the fourth operation, the last one. it is not difficult to see that the optimal value in this case is equal to 6. and it can be obtained as follows. you first subtract 5 from 4. this gives you -1. you then multiply it by 3, and you get -3. you then compute the sum of the first two digits. this is 1 + 2, and that is equal to 3. finally you subtract -3 from 3. this is the same as 3 + 3, it is equal to 6. well, you might find the result as follows, you just go through all possible orders. let's see how many different orders are there. well, there are four arithmetic operations in this case, so you can choose any of the four possible arithmetic operations to be the first one. you can choose any of these three remaining operations to be the second one, and you can select any of the two remaining operations to be the third one. and the last one is unique, it is the only remaining operations. so, in total, there are 4 by 3 by 2 by 1 different orders. this is equal to 24, and you can just enumerate all of them, write them down, compute an answer for each of these orderings and select the maximal value. however, our method of going through all possible orderings does not scale well. and this is why. consider the toy example shown on the slide. in this case we have six digits and five arithmetic operations. this example will require us to go through all possible 120 orderings. so just because there are five iterations, so any of five of them can be the first one, any of the remaining four of them can be the second one, and so on. so this is 5 by 4 by 3 by 2 by 1, which is equal to 120. this is already not so easy to do this by hand. i mean, to go through all possible such orderings. well, this is not easy, but we can teach a computer to do this, right? so we can implement an algorithm that goes through all possible orderings. however, in general, this algorithm will perform roughly n factorial steps, where n is the number of arithmetic operations, for exactly the same reason. if you have n arithmetic operations, then any of them can be the first one. any of the remaining n minus 1 operations can be the second one, and so on. so this is n times n minus 1, times n minus 2, and so on. this is equal n factorial, and n factorial is an extremely fastly growing function. for example, 20 factorial already equals roughly 2 times 10 to the 18. this means that if you implement such an algorithm, it will not be able to compute the maximum value of an expression consisting of just 20 digits in a reasonable time, even in one year, not to say about one second. which means, as usual, that we need another algorithm, as you might well have guessed. we will use dynamic programming to find, to design a more efficient algorithm. in the meantime, you might want to check your intuition by trying a few possible orderings to perform in this small expression, and by using our in video quiz. 
as usual, we start designing our dynamic program in algorithm by defining a subproblem in a way that allows us to solve a subproblem by solving smaller sub subproblem. as we said already, this is probably the most important step in designing dynamic programming solutions. so before doing this, we define our problem formally. so the input consists of n digits. d1, d2, and so on, dn. and then -1 operations between them, which we call op1, op2, and so on, opn. each operation is either summation, subtraction, or multiplication. and our goal is to find an order of applying these operations so that the value of the resulting expression is maximized. as we discussed already, we can specify this order just by placing parentheses into our expression. we start building our intuition by reconsidering our toy example. so assume that the multiplication is the last operation in some optimal ordering in an ordering leading to an optimal value in this toy example. well this means that in this expression we already have to pairs of parentheses. and our goal is to parenthesize the initial sub-expression and the second sub-expression, so as to maximize the value. this means that it would be good for us to know what is an optimal value for the first subexpression and the second subexpression, right? and in general if you have an expression and if you select a realistic operation, which is the last one, then it splits your initial expression into two subexpressions, right? and for both of them it would be good to know an optimal value. and, in turn, each of these two subexpressions are split into two sub subexpressions by the last arithmetic operations, and so on. so this suggests very good problem in our case would be find an optimal value for any subexpression or former initial expression. so we've just realized that it would be good to know the optimal values for all subexpressions of our initial expression. what do we mean however, by saying optimal values for all subexpressions? assume for example that we need to compute the optimal, the maximal value for the sum of two subexpressions, subexpression one and subexpression two. well this obviously means that we would like this subexpression to be as large as possible and this subexpression to be as large as possible. if on the other hand we would like to compute the maximum value of subexpression one minus subexpression two. well this means that we would like subexpression one to be as large as possible while we would like the value of subexpression two to be as small as possible, right? just because we compute subexpression one minus subexpression two. this suggests that knowing just the maximal value for each subexpression would not be enough. and this usually happens when designing a dynamic programming solution. this also suggests that, instead of computing just maximal, we will maintain what is the maximum value and the minimum possible value for each subexpression. let's illustrate this reasoning once again with our previous toy example. so in this case we are maximizing the product of two small subexpressions. in this case these two subexpressions, so small, that it is not difficult to compute their minimal and maximal values. for example, for subexpression 5- 8 + 7, the minimum value is- 10 and the maximal value is 4, right? at the same time, for the second subexpression, (4-(8+9)), the minimum value is- 13, while the maximum value is 5, right? now we would like to parenthesis both subexpressions, so that their product is maximal. well it is not difficult to see, that in this case the optimal way to do this is to take the minimal values of both sub expressions, right? so this will give us- 10 multiplied by -13, which is equal to 130. right? which is much larger than the product of the maximum values of these two sub expressions which is 4 by 5, which is 20 in turn. okay, we are now ready to write down the recurrent relation for our subproblems. before this, let's formally define e of ij to be the subexpression of our initial expression resulting by taking digits from i to j and all operations between them. then our goal is to compute the maximum value of the subexpression which we denote by capital m(i,j) and the minimum value of the sub expression denoted by m(i,j). okay, can you see that our initial subexpression from i to j and assumes that we would like to compute one of the extreme values of the subexpression and implies there is a minimum or the maximum. well we know that in many ordering for this subexpression there is some last operation, say okay so this separation splits our initial subexpression into two sub subexpression namely, subexpression i, k and subexpression k plus 1j. right? to compute the maximum value, we just go through all possible such case, from i to j- 1, and through all possible extreme values for two subexpressions. i mean, either we apply operation k to the maximum values of these two subexpressions or we apply operation k to minimum value, the minimum values of these two subexpressions. or we apply it to the maximum value of one subexpression and the minimum value of another or vice versa. to compute the maximum value of sub expression i j, we just select the maximum among all these possibilities. while to compute it's minimum value, we simply select the minimum among all such possibilities. 
we now convert our recurrence relation into a dynamic programming algorithm. we start by implementing a procedure that computes the minimum and maximum value of the subexpression (i,j) through optimal values for smaller sub subexpressions. so the procedure is called minandmax(i,j). so we first declared two intervals, max and min. initially min is equal to plus infinity, max is equal to minus infinity, or to a very large number, or to very small number. then we go through all possible values of k between i and j- 1. i mean between, we just go through all possibilities of splitting our subexpression (i, j) into two sub subexpressions from i to k and from k plus 1 to j. when such a splitting is fixed, we compute four possible values, applying opk to either two maximum values of this subexpression or two minimum values or two maximum and minimum value or two minimum and maximum value. when such two values are computed, we just check whether one of them can improve our minimum or maximum values. if it improves we update the min or max variable. finally we return the minimum value and the maximum value for our subexpression. our current relation expresses the solution for an expression (i,j) for a solution for smaller sub subexpressions. what do we mean by saying smaller? well, we mean just that they are shorter, right? so once again when we compute the value for a subexpression (i,j) we rely on the fact that those are values for shorter subexpressions are already computed. this means that our algorithm needs to compute the solutions for all subproblems in order of increasing length. namely, in order of increasing value of j minus i, right? so for this problem we have roughly quadratic number of subproblems. namely our subproblem, i, i, j, is parameterized by the value of i and j which in turn range from 1 to n. right, so it makes sense in this case to store the values for all subproblems in a two dimensional table of size n by n. recall also that we need to recall our subproblems in the order of increasing value of j- 1. we can do this just by going through all subproblems in an order shown on the slide. so, why this order? well this is simply because it goes through all possible values of i, j in order of increasing j minus y as required. so lets take a look. on this diagonal we have all the cells where i, where j- i is equal to 0, right? so the first cell here is 1, 1. the second cell is 2, 2. the third cell is 3, 3 and so on. we then proceed to this cell here i is equal to 1, j is equal to 2, so the difference is 1. we then proceed to this cell. this is the cell 2, 3 with the difference 1 again. we then proceed to this cell which is 3, 4 and so on. so on this cell we have on this diagonal we have all the cells i, j where i- j = 0. on this diagonal we have all cells i, j where j- i = 1. for this diagonal, this difference is equal to two. for this diagonal, this difference is equal to three and so on. the resulting value for our initial subproblem will be computed as the value of the last cell. right, because of this cell responds to the initial subexpression from one to n. now everything is ready to write down an algorithm. in the algorithm we will maintain two tables, m and capital m. the first one for storing the minimum values for all subexpressions, and the second one for storing the maximum values for all subexpressions. we start by initializing these tables as follows. so when subexpression contains just one digit, which means that when j = i, then there is nothing, actually to minimize or maximize because there are no operations. so there is no order on operations. so, because of that we just initialize the main diagonals of this table with the most current point in digits. this is with the following loop. so m(i,i) and m(i,i) = di. then we go through all possible subproblems in order of increasing size. and this is done as follows. we gradually increase the parameter s from 1 to n- 1. this is done in the following loop. when s is fixed, i goes from 1 to n- s. and j is computed as i + s. this is done to go through all possible payers (i,j) such that j- i = s. right when i and j are fixed we call the procedure min and max to compute the minimum and maximum value of the subexpression (i,j). all right. so finally we return the value of capital m of 1,n as the result for our initial problem because this subexpression, 1 n corresponds to our initial problem. containing all digits from 1 to n. okay so the running time of this algorithm is cubic. namely, big o of nq. and these can be seen by noting that we have two nested loops. the first one with n-1 iterations, the inner one is with n-s iterations, which is at most n. also, inside these two loops we have a call to min and max procedure. the running time of min and max procedure is proportional to j-i which is also at most n. so the right end time however algorithm is it must o and n times n time n, which is n cubed. this slide shows an example on how a table's m and capital m look like if we ran a well reason on our toy example, namely expression 5- 8 + 7 x 4- 8 + 9. let's just go through this example step by step. so we start by filling in the values on the main diagonal in both matrices. so this is 5, this is 8, this is 7, this is 4, 8, 9. so this response to subexpression consisted of just one digit. so there is nothing to maximize or minimize. so we do the same for capital m matrix. we then proceed to the second diagonal. well with -3 here and this corresponds to this subexpression again in this case there is just one operation. so there is nothing to minimize or maximize here, because there will just be one other when we have just one sign. so we put -3 here this corresponds to the problem, to the subproblem one, two. let me put all the indices here by the way. then we proceed through the cell to 3, which corresponds to this subproblem. again, there is nothing to maximize or minimize so we continue in the same way. in this case it is not so interesting and then we proceed to the third day namely to this cell. so this can respond to the subexpression 1,3 which consists of three digits and two operations, minus and plus. so we know that one of them is the last operation in the optimal order when computing minimal value for example. so as soon as this is minus. this will split the subexpression into two sub subexpression, 5 and 8 + 7. so for both the subexpressions we already know their maximum and minimum values. so once again, this subexpression corresponds to (1, 1), this subexpression corresponds to (2, 3). sort of from second to third digits, and third digit from first to first digit. so we know that for the first subexpression we know already it's minimum value it is here, and it's maximum value, it is here. so for the second subexpression, we already know it's minimum value, it is here. it is 15, and then its maximum value. it is also 15. so by going through all possible pairs of obviously maximum and minimum values, in this case, they're all the same. we compute the minimum value, which is just 5- 15. it is minus ten. however, this was only the first case of splitting this sub expression into two sub expressions. and as a possibility would be the following so we can split it into the following two subexpressions. so this corresponds to 1, 2 and this corresponds to 3,3. right? so, for one two we know its minimum value, it is minus three, and its maximum value, it is also minus three. for 3, 3 we know its maximum value. it is here, seven. its minimum value and its maximum value. so then we can compute- 3 + 7, which gives us just 4. so for the maximum value of the subexpression (1,3) we select 4. for the minimum value we select -10. so we proceed filling in this table in a similar fashion. so we then put 36 here in this cell, then -20 in this cell, and then parallel we put 60 here, 20 here, and so on. so, in the end we see the value 200 here. and this is the maximum value of our initial expression. this still doesn't give us the optimal load rate itself, but we will be able to reconstruct it from these two tables. now we are sure that the maximum value of our initial expression is 200, and we will find out the optimal ordering, or the optimal sizing in a minute. 
in this last video of this lesson, we show a method of reconstructing an actual solution from two tables computed by our dynamic programming algorithm. okay, here on this slide we see two tables, m and capital m. computed by our dynamic program and algorithm which contain minimal and maximal values respectively for all possible subexpressions of our initial expression. let me first put in this for all the rows and columns of these two matrices, as well as numbers for our initial digits. well, in particular, we see by reading the contents of this cell capital m of (1,6) that the maximal value of our initial expression is equal to 200, and our goal is to unwind the whole solution, i mean, parenthesizing of the initial expression, from these two tables. so our first goal on this way is to understand from which two subexpressions of the initial expression the value 200 was computed. well, let's see, when computing the value for the maximal value for subexpression (1,6), we tried all possible splittings of the expression (1,6) into two subexpressions. well, let's just go through all of them. the first possibility is to split it into two subexpressions (1,1), which corresponds just to the first digit which is just 5, and subexpression (2,6), with a minus sign between them, right. so for both these two subexpressions we already know minimal values and maximal values. well, let me mark them. so this is the minimal value for the subexpression (1,1). this is the maximal value for subexpression (1,1). for (2,6), this is the minimal value, -195, and this is a maximal value, 75. so we would like to maximize this subexpression one minus subexpression two, which means that we would like the first subexpression to be as large as possible and the second subexpression to be as small as possible. well, this means that we need to try to take the maximal value of the first subexpression which is five and the minimal value of the second subexpression which is -195. well, we see that in this case, 5 minus -195 is the same as 5 plus 195, which equals exactly 200, right, which allows us to conclude, actually, that the value 200 can be obtained as follows. so, we subtract the minimum value which is -195 over the second subexpression from 5, right. so we restored the last operation in an optimal parenthesizing of the initial expression. however, we still need to find out how to obtain -195 out of the second subexpression. well, let's do this. okay, so we need to find how the minimum value of the subexpression (2,6) was obtained. well, there are several possible splittings, once again, of the subexpression (2,6) into two smaller sub-subexpressions. the first of them is to split (2,6) into (2,2), which just corresponds to the digit 8 plus (3,6). well, in this case, we would like the value to be as small as possible and our sign is plus in this case, which means that we would like the value of subexpression (2,2) to be as small as possible and the value of subexpression (3,6) also to be as small as possible. and you already know these values, they are in our tables, so the minimal value of subexpression (2,2) is 8, while the minimum value of subexpression (3, 6) is minus 91, right. so we see that the sum of these two values is not equal to -195, right, which means that plus is not the last operation in the optimal parenthesizing that gives the minimum value of subexpression (2, 6), right. so let's check the next one. another possibility to split the subexpression (2, 6) is the following. we split it into subexpression (2, 3) times subexpression (4, 6), right. so once again, we would like to find the minimum value of subexpression (2, 6). well, let's see just all possibilities. the minimum value of subexpression (2, 3) is 15. it's maximal value is also 15. as to subexpression (4,6), its minimum value is -13. it's maximal value is 5. and we would like the product of these two values to be as small as possible. well, it is not difficult to see that if we take just 15 and multiply it, which is a minimum value of subexpression (2,3), and multiply it by the minimum value of the subexpression (4,6), which is -13, then we get exactly -195. and this, in turn, allows us to get -195 from the subexpression (2,6). we can do as follows. we can first compute the sum of 8 and 7. this gives us 15. and then to multiply it by the result of the second subexpression. well, now it remains to find out how to get -13 out of this subexpression for 6, but in this small example, it is already easy to get -13. well, we just first compute the sum of 8 and 9 and then subtract it from 4, right. so this way we reconstructed the whole solution, i mean, an optimal parenthesizing, or an optimal ordering, of our arithmetic operations, leading to this value, to this maximal value, 200. let's just check it once again that our parenthesizing leads to the value 200, indeed. so we first compute the sum of 8 and 9. this gives us 17. we then subtract 17 from 4. and this gives us -13. we then compute the sum of 8 and 7. this gives us 15. we multiply 15 by -13. it gives us -195, and, finally, we subtract this number from 5, and we get 200, indeed. so we reconstructed the whole solution. in general, i mean for an expression consisting of n digits and n minus 1 operations they can respond, an algorithm makes roughly quadratic number of steps, because it needs to reconstruct n minus one operations, i mean, an order of n minus one operations, going from last one to the first one. and for each operation, it potentially needs to go through all possible splittings into two subexpressions, and this number is at most m. so the running time is bigger of ten times m, which is bigger of m squared. and this technique is quite general. it applies in many cases in the dynamic problem in algorithms. 
so in this lecture we're talking about arrays and linked lists. in this video, we're going to talk about arrays. so here's some examples of declarations of arrays in a couple of different languages. along with, we can see the one dimensional array laid out with five elements in it, and then a two dimensional array with one row, sorry two rows and five columns. so what's the definition of an array? well we got basically a contiguous array of memory. that is one chunk of memory. that can either be on a stack or it can be in the heap, it doesn't really matter where it is. it is broken down into equal sized elements, and each of those elements are indexed by contiguous integers. all three of these things are important for defining an array. here, in this particular example, we have an array whose indices are from 1 to 7. in many languages, the same indices for this particular array would be from zero to six. so it would be zero based indexing, but one based indexing is also possible in some languages. and other languages allow you to actually specify what the initial index is. what's so special about arrays? well, the key point about an array is we have random access. that is, we have constant time access to any particular element in an array. constant time access to read, constant time access to write. how does that actually work? well basically what that means is we can just do arithmetic to figure out the address of a particular array element. so the first thing we need to do is start with the address of the array. so we take the address of the array and then we multiply that by first the element size. so this where the key part that every element was the same size matters, so that allows us to do a simple multiplication. rather than if each of the array elements were of different sizes, we'd have to sum them together, and if we had to sum together n items, that would be order n time. so we take our array address, we add to it the element size times i which is the index that's of interest minus the first_index. if we're doing zero based indexing, that first index isn't really necessary. i like this example because it really shows a more general case where we do have a first index. let's say for instance we're looking at the address for index four. we would take four minus the first index, which is one, which would give us three. multiply that by whatever our element size is, and then add that to our array address. now of course, we don't have to do this work, the compiler or interpreter does this work for us, but we can see how it is that it works in constant-time. many languages also support multi-dimensional arrays, if not you can actually kind of roll your own through an example i'll show you here, where you do your own arithmetic. so here, let's look. let's say that the top left element is at index (1, 1), and here's the index (3,4). so this means we're in row 3, column 4. how do we find the address of that element? well, first off what we need to do is skip the rows that, the full rows, that we're not using. so that is, we need to skip two rows, or skip 3, which is the row index minus 1, which is the initial row index. so that gives us 2 times 6 or 12 elements we're skipping for those rows in order to get to row 3. then we've got to skip the elements before (3,4) in the same row. so there are three of them. how do we get that? we take the column index, which is 4 and subtract it from the initial column index which is 1. so this basically gives us 15. six for the first row, six for the second row and then three for the third row before this particular element. we take that 15 and multiply it by our element size and then add it to our array address. and that will give us the address of our element (3,4). now we made kind of a supposition here. and that was that the way this was laid out is we laid out all the elements of the first row, followed by all of the elements of the second row, and so on. that's called row-major ordering or row-major indexing. and what we do is basically, we lay out, (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6). and then right after that in memory (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6). so the column index is changing most rapidly as we're looking at successive elements. and that's an indication of it's row-major indexing. we could lay out arrays differently, and some languages or compilers actually do that, where they would lay out each column in order, so you'd have the first column, then the second column, and then the third column. and so that, then, the successive elements would be (1, 1), (2, 1), (3, 1), followed by (1, 2), (2, 2), (3, 2), and so on. so there we see that the row index is changing most rapidly, and this is called column-major ordering. how long does it take to perform operations? we already said to read any element is o(1), and to write any element is o(1). that is a standard feature of arrays. what happens if we want to add an element at the end of an array? so let's say we have allocated seven elements for an array. we're only using four of them, okay? so we have kept track that we're using four and we want to add a fifth element. and again there's room for seven. then all we know it was just add it, then update the number of elements that are in use. that's an o(1) operation. if we want to remove the last element as well, that's an o(1) operation because we just update the number of elements that are in use, and so that's an o(1) operation. where it gets to be expensive, is if we want to, for instance, remove the first element. so we remove the five here, and what we've got to do then, we don't want to have holes left in it. so we need to move the 8 down, move the 3 down, move the 12 down. that's an o(n) operation. same thing would happen if he wanted to insert at the beginning. so we would need to move the 12, move the 3, and move the 8 to make space for our new element. so that also would be o(n). and if we want to add or remove somewhere in the middle, again that's an o(n) operation. if we want to add exactly in the middle, we have to move n/2 items, which is o(n). same thing for removal. so arrays are great if you want or remove at the end. but it's expensive if you want to add or remove in the middle or at the beginning. however, remember, a huge advantage for arrays is that we have this constant time access to elements, either read or write. in summary then, an array consists of a contiguous area of memory. because if it were non-contiguous then we couldn't just do this simple arithmetic to get where we're going. we have to have equal-size elements again so our arithmetic works. and indexed by contiguous integers again so our arithmetic works. we have constant time access to any element, constant time to add or remove at the end and linear time to add and remove at any arbitrary location. in our next video we're going to talk about linked lists. 
now let's talk about linked lists. so linked lists, it's named kind of like links in a chain, right, so we've got a head pointer that points to a node that then has some data and points to another node, points to another node and eventually points to one that doesn't point any farther. so here in our top diagram we show head points to the node containing 7, points to the node containing 10, points to the node containing 4, points to the node containing 13 doesn't point anywhere. how this actually works is that a node contains a key which in this case is these integers, and a next pointer. the diagram below shows more detail of what's going on. so head is a pointer that points to a node, and that node contains two elements, the value 7. and then a pointer that points off to the next node that contains a key 10, and a pointer that points off to the next node 4, points off to the next node 13, 13's next pointer is just nill. what are the operations that can be done on a linked list? there's several of them, and the names of these sometimes are different, in different environments and different libraries. but normally the operations provided are roughly these. so we can add an element to the front of the list, and that we're calling pushfront. so that takes a key, adds it to the front of the list. we can return the front element of the list. we're calling that topfront. or we can remove the front element of the list, called popfront. the same things that we can do at the front of the list, we can also do at the end of the list. with pushback, later on in a later module, we'll actually use the word append for that, or topback, or popback. these seem uniform in there, but there is a difference in that the runtimes are going to be different between those, and we're going to talk about that. you can find whether an element is in the list and it's as simple as just running yourself down the the list looking to find a matching key. you can erase an element and then again run yourself down the list til you find the matching key and then remove that element. so these latter ones are both o(n) time. is the list empty or not? that's as simple as checking is the head equal to nil. we can add a particular key--if we want to splice in a key into a list we can actually add in a key either before a given node or after a given node. so lets look at the times for some common operations. we've got here our list with four elements in it: 7, 10, 4, and 13. now we go ahead and push an element to the front. so we push 26 to the front of the list. so the first thing we do, create a node that contains the 26 as its key. and then we update our next pointer of that node to point to the head, which is the 7 element, and then update the head pointer to point to our new node, and that's it we're done. so it's o(1). allocate, update one pointer, update another pointer, constant time. if we want to pop the front element, clearly finding the front element is very cheap here, right? you can just look at the first element and return it. so topfront is o(1). popfront turns out is going to be o(1). first thing we're going to do, update the head pointer. then, remove the node. that's an o(1) operation. if we want to push at the back, and we don't have a tail pointer, we're going to talk about a tail pointer in a moment, then it's going to be a fairly expensive operation. we're going to have to start at the head and walk our way down the list until we get to the end, and add a node there, so that's going to be o(n) time. similarly if we want to topback or popback, we're going to also have to start at the head, walk our way down to the last element. those are all going to be o(n) time. if we had a tail pointer, some of these will become simpler. okay, so, we're going to have both a head pointer that points to the head element and a tail pointer that points to the tail element. so, that way, getting the first element is cheap. getting the last element is cheap. let's look at what happens when we try an insert when we have a tail. we allocate a node, put in our new key, and we then update the next pointer of the current tail, to point to this new tail. and then update the tail pointer itself. o(1) operation. retrieving the last element, so a popback, sorry a topback, is also an o(1) operation. we just go to the tail, find the element, return the key. if we want to pop the back however that's a little bit of an expensive operation. okay. we are going to need to update the tail to point from 8 to 13 so we're at 8 right now we want to go to 13, the problem is how do we get to 13? okay. we don't have a pointer from 8 to 13 we have a pointer from 13 to 8. and that pointer doesn't help us going back. so what we've got to do is, again, start at the head, walk our way down until we find the 13 node that then points to the current tail, and then update our tail pointer to point to that, and then update the next pointer to be nil. and then we can remove that old one. so that's going to be an o(n) operation. because we've got to walk all the way down there. okay, because even though we have a tail pointer we don't have the next to the tail pointer, we don't have the next to last element. the head is different because our pointers point this way, if we had the head its also cheap to get the second element, right, and one more to get the third element but the tail pointer doesn't help us get to the next to the last element. let's look at some of the code for this, so for pushfront we have a singly linked list: we're going to allocate a new node, set its key, set its next to point to the old head and then we'll update the current head pointer. if the tail is equal to nil, that meant that before the insertion, the head and the tail were nil, it was an empty list. so we've got to update the tail to point to the same thing the head points to. popping the front, well, if we're asked to pop the front on an empty list, that's an error. so that's the first check we do here and then we just update the head to point now to the head's next. and just in case that there was only one element in the list and now there are no elements, we check if our new head is nil and if so update our tail to also be nil. pushing in the back: allocate a new node, set its key, set its next pointer, and then check the current tail. if the current tail is nil again, it's an empty list. update the head and the tail to point to that new node. otherwise update the old tail's next to point to our new node, and then update the tail to point to that new node. popping the back. more difficult, right. if it's an empty list and we're trying to pop, that's an error. if the head is equal to tail, that means we have one element. so we need to just update the head and the tail to nil. otherwise we've got to start at the head, and start working our way down, trying to find the next to the last element. when we exit the while loop, p will be the next to last element, and we then update its next pointer to nil. and set our tail equal to that element. adding after a node? fairly simple in a singly linked list. allocate a new node, set its next pointer to whatever node we're adding after, to its next. so we sort of splice in, and then we need to update the node pointer. the one we're adding after, so that it points now to our new node. and just in case that node we're adding after was the tail we've got to now update the tail to that new node. adding before, we have the same problem we had in terms of popback in that we don't have a link back to the previous element. so we have no way of updating its next pointer other than going back to the beginning of the head and moving our way down until we find it. so addbefore would be an o(n) operation. so let's summarize what the cost of things are. pushfront, o(1). topfront, popfront, all o(1). pushing the back o(n) unless we have a tail pointer in which case its o(1). topback o(n), again unless we have a tail pointer in which it's o(1). popping the back: o(n) operation, with or without a tail. finding a key is o(n) we just walk our way through the list trying to find a particular element. erasing, also o(n). checking whether it's empty or not is as simple as checking whether the head is nil. adding before: o(n) because finding the previous element takes o(n) because we're going to walk all the way from the head to find it. addafter: constant time. 
there is a way to make popping the back and adding before cheap. our problem was that although we had a way to get from a previous element to the next element, we had no way to get back. and what a doubly-linked list says is, well, let's go ahead and add a way to get back. so we'll have two pointers, forward and back pointers. that's the bidirectional arrow we're showing here conceptually. and the way we would actually implement this is, with a node that adds an extra pointer. so we have not only a next pointer, we have a previous pointer. so this shows for example that the 10 element has a next pointer that points to 4 but a previous pointer that points to 7. so at any node we can either go forward or we can go backwards. so that means if we're trying to pop the back, that's going to work pretty well. what we're going to do is update the tail pointer to point to the previous element because again we ca get there in an o(1) operation. and then update its next pointer to be nil and then finally remove the node. so that's o(1). so if we have a doubly linked list it's slightly more complicated (our code) because we've got to make sure to manage both prev pointers as well as next pointers. so if we're pushing something in the back, we'll allocate a new node. if the tail is nil, which means it's empty, then we just have a single node whose prev and next pointers are both nil and then head and tail both point to it. otherwise, we need to update the tail's next pointer for this new node, because we're pushing at the end and then go update the prev pointer of this new node to point to the old tail and then finally update the tail pointer itself. popping the back, also pretty straightforward. we're going to again check to see whether this is first an empty list, in which case it's an error. a list with only one element, in which case it's simple. otherwise we're going to go ahead and update our tail to be the prev tail, and the next of that node to be nil. adding after, fairly simple again we just need to maintain the prev pointer but adding before also now works in the sense that we can allocate our node, our new node and its prev pointer will be the prev pointer of the existing node we're adding before. we splice it in that way and then we'll update the next pointer of that previous node to point to our new node. and finally, just in case we're adding before the head, we need to update the head. so in a singly-linked list, we saw the cost of things. working with the front of the list was cheap, working with the back of the list with no tail, was all linear time. if we added a tail, it was easy to push something at the end, easy to retrieve something at the end, but hard to remove something at the end. by switching to a doubly linked list, removing from the end (a popback) becomes now an o(1) operation, as does adding before which used to be a linear time operation. one thing to point out as we contrast arrays versus linked lists. so in arrays, we have random access, in a sense that it's constant time to access any element. that makes things like a binary search very simple, where we start searching in the middle, and then tell (if we have a sorted array), and then can decide which side of the array we're on. and then, go to one side or the other. for a linked list, that doesn't work. finding the middle element is an expensive operation. because you've got to start either at the head or the tail and work your way into the middle. so that's an o(n) operation to get to any particular element. big difference in between that and an array. however, linked lists are constant time to insert at or remove from the front, unlike arrays. what we saw in arrays, if you want to insert from the front, or remove from the front, it's going to take you o(n) time because you're going to have to move a bunch of elements. if you have a tail and doubly-linked, it is also constant time to work at the end of the list. so you can get at or remove from there. it's linear time to find an arbitrary element. the list element are not contiguous as they are in an array. you have separately allocated locations of memory and then there are pointers between them. and then, with a doubly-linked list it's also constant time to insert between nodes or to remove a node. 
so now we're going to start talking about two very important data structures, stacks and queues. in this video we're going to talk about stacks. so, what is a stack? it's an abstract data type, and here are the operations we have. we can push a key, so we've got a collection of values and we can push it. we can find the most recently added key with top. and we can pop, which returns and removes the most recently added. so, the way to think of it is as if you have a stack of books. you can put a book on top of the stack, or you can take a book from the top of the stack. but you can't take the element at the bottom of the stack of books without taking off all of the previous elements. so it's really pretty simple. push items on, you can find what the top one is. you can pop off the top one and you can intermingle these operations. last one: you can find is is it empty? so are, do we have an empty stack? this turns out to be really useful for lots and lots of things, where you need to be keep track of what has happened in this particular order. let's look at an example. so let's say we've got a balanced brackets problem. so, here we have a string. and it has left parens, right parens, left square brackets, and right square brackets. and we want to determine whether or not that string of parentheses and square brackets, whether they're balanced. and balance meaning there's a matching left paren for every right paren. and they're in the right order. and they don't cross over. let's look at some examples of unbalanced and balanced. so, the first string here left paren, square bracket, matching square bracket, matching right paren, square bracket, matching square bracket, left paren, matching paren, balanced. the second one also balanced. the unbalanced one for example here a left paren with no matching right paren. assuming that the two parens on the right side are matching themselves. the square brackets match, but then we have got an unmatched right bracket. in the last case we've got a square left bracket and a square right bracket, but the problem is that they are in the wrong order. it is the square right bracket followed by the square left bracket. how do we keep track of that? and the problem is that in some cases we have to kind of keep track of a lot of information. for instance, in the second example, here, we've got our opening left paren. doesn't get matched with the right paren for quite a while. there's a lot of intervening stuff, and we have to sort of keep track that we've got a left paren whose right paren we need to match, even as all this other stuff happens. and it turns out a stack is a good way to keep track of it, so here's what we'll do. we'll create a stack and then we'll go through every character in the string. if we have an opening paren or an opening square bracket, we'll go ahead and push it on the stack. so the stack is going to represent the parens that are still open, the parens and brackets which have yet to be matched and the order in which they need to be matched, so the outermost ones will be at the bottom of the stack and the last one we saw (the innermost one) would be at the top of the stack. then if it's not one of these opening ones. then if our stack is empty then that's a problem, because basically we've got a closing paren or bracket and there's no matching element. so if the stack is empty, no, we're not balanced. otherwise we'll pop the top element off and then we'll check and see. does it match, an element from the stack, does it match the character we've got? so, if the top was a left paren, did we just read a right paren. if so, great. they match. and now those two match, the next one we need to match is still the newly top one on the stack or similarly if we have a square bracket on the stack and a square bracket we read, those match as well. if they don't match, then we've got a problem, right? if we've got a right paren on the stack and a, sorry a left paren on the stack and right bracket that we just read, those don't match for example. so then we return false. once we've run through all of the strings, were matched, right? no. not necessarily. imagine we have the string, left paren, left paren, left paren.. we go through, we push left paren, we push left paren, we push left paren and then we'd be done. it won't ever return false, but, it's no good because we didn't actually match what's on the stack. so, once we go through all of the characters in the string, then we're going to have to check and make sure, is our stack empty? did we successfully match everything? this is only one example of the use of stacks. stacks are used in lots of other places. they're used for compilers. they're used in a lot of algorithms that you'll be seeing throughout this course. so how do you actually implement a stack? well, let's see. you can implement a stack with an array fairly easily, so allocate an array of some maximum stack size. so, in this case, we decided it's five, just for the sake of example, and we're going to keep a variable, which is the number of elements that are actually in the stack. when we push, so in this case we going to push a, we're going to go ahead and put it at the end of the array, that we've got so far. so, for whatever elements we have, we'll append it to those. so in this case, we will put it at the beginning of the array because we haven't used any elements. and we will kept track of the number of elements, as well. we push b, put in the next spot and now our number of elements is two, fairly straight forward, right, we're just appending to the array and these are clearly o(1) operations. what's the top element? well that's really simple. if the number of elements is two, that means we need the second element from the array. which in this case, is b. again, a constant time operation. we push c, our number of elements is three, and now let's say we pop. well, what do we do? we need to go get the third element, which is c, and erase it, and then adjust numelements so it's now 2. now we can push an element, push another element, push a fifth element, and now if we try to push again, that's an error, right? because we don't have any more space. so, that wouldn't be allowed. is it empty? no, how do we know? because the number of elements is greater than zero. again, an o(1) operation. and now we can start popping, which will be returning the appropriate element of the array based on the numelements value, and keep popping until we get down to no elements. if we're at no elements, and we ask it's empty? yes. that's true. we can also implement a stack with a linked list. so, one disadvantage--one limitation--of the array is that we have a maximum size, based on the array we initially allocated. but all of the operations are o(1), which is good. the other potential problem is that we have potentially wasted space. so if we allocated a very large array, to allow a possibly large stack, we didn't actually use much of it, all the rest of it is wasted. if we have a linked list, what we do then is every element in the list of course will represent a particular element in the stack. and so we'll push a, and then if we push b, we're going to go ahead and push b at the front. so basically, pushes will turn into pushfront. if we want to get the top element, just get the head element, so top will really just be topfront, we can keep pushing. pushing at the front or popping, all of those are o(1) operations. we can keep pushing, and the nice thing about this is there's no a priori limit as to the number of elements you can add. as long as you have available memory, you can keep adding. there's an overhead though, like in the array, we have each element size, is just big enough to store our key. here we've got the overhead of storing a pointer as well. on the other hand there's no wasted space in terms of allocated space that isn't actually being used. so we keep pushing, is it empty? no, because the head is not nil. and then we can go ahead and pop, so, if we had a linked list it's very simple to implement the stack operations in terms of the linked list operations. push becomes pushfront. top becomes topfront and pop which is supposed to both return and pop the top element then become a combination of a topfront followed by a popfront. empty is just empty. we keep popping and then eventually we pop the last element and now if we ask whether it's empty, the answer will be true. okay, so that's our stack implementation. stacks can be implemented with either arrays or linked lists, i talked a little bit about the pros and cons of each of those, the linked list has fixed amount of overhead, that is for every element you are pushing, you have an additional pointer. for arrays you have, potentially, space that you've over-allocated, basically, to allow for a stack to grow to maximum size. for arrays, stacks do have a maximum size. for linked lists, they don't. each stack operation is constant time for either one of these implementations. sometimes we know stacks as lifo queues. lifo meaning last in first out. the last one that was inserted is the first line that comes out. this reminds me sometimes of also what's known as gigo, garbage in garbage out. that if you input garbage into a system, you get garbage out. but of course this is different. so that is stacks. in the next video we're going to go ahead and look at queues. thanks. 
now let's talk about queues. so, a queue has some similarities with a stack. but in a fundamental way is different. so it's an abstract data type and these are the operations that it has. you can enqueue a key, it adds the key to the collection. and then when you dequeue, that gives you back a key and removes it from the queue. it removes and returns the least recently added key, rather than in the case of a stack, the most recently added key. so that's the fundamental difference. if you think about queues as like queuing up in line or waiting in line, this is a first come first serve situation. so the longer you've been waiting in line, so the longest person waiting in line is the next person to be served. makes sense. so you can imagine if you had a grocery store that had a stack that it used for serving people, people would be pretty annoyed, right? because the person who'd just arrived, you've been waiting in line ten minutes, a person just arrives, they get served before you do, that would not make you happy. so, queues are very useful for instance, for things like servers. where you've got a bunch of operations coming in and you want to service the one that's been waiting the longest. the other operation is you can find out whether the queue is empty or not. so these are often called fifo, first in, first out, and this distinguishes them from stacks which are lifo. last in, first out. first in first out, or first come first serve, same thing. how can you implement a queue? well, one way is with a linked list, where you have a head and a tail pointer. so let's say we start out with an empty linked list. we can go ahead and enqueue, and what we're going to do basically in an enqueue, is we are going to push to the back of the linked list, so that's how we'll implement enqueue. so here, we enqueue (a), it's now at the back of the linked list. if we enqueue (b), it's going to be then added, again, at the end of the linked list. is it empty? no. how do we know it's not empty? well the simplest thing is we would just call to the underlying list implementation and say hey, list are you empty? it would say no. and so empty for the queue is no. we know it's really happening just by checking whether the head is nil or not. if we enqueue(c) then, again it goes to the tail of the list. and if i now dequeue, which one is going to be removed? again this is not a stack, in a stack c would be removed. in our case, (a) is going to be removed because it's been there longest. that's just an implementation of popping from the front. so that would return (a). we can now do some more enqueueing, enqueue(d), enqueue(e), enqueue(f), and now if we start dequeueing, we dequeue from the front. so dequeuing (b), dequeuing (c), dequeuing (d), dequeuing (e), and finally dequeuing (f). if we ask whether the queue is empty now, the answer is yes. again, because the head is nil. so enqueue uses list's pushback call and dequeue uses both the list's topfront to get the front element as well as popfront to remove that front element. and empty just uses the list's empty method. what about with an array? we could think of doing something similar. that is, we could add at the end and then pop from the front. but you can imagine, so, we said the front of the array is the beginning of the queue. then enqeueing is easy, but dequeuing would be an expensive o(n) operation. and we want enqeueing to be o(1). we can do that, in a fashion i'll show you right now which is basically keeping track of sort of the array as a circular array. so we're going to go ahead and enqeue (a), and we have a write index. and the write index tells us where the next enqueue operation should happen. and the read tells us where the next dequeue operation should happen. so we enqueue a, we enqueue b, and now update our write index. if we ask whether we're empty? no, we're not empty because read is not equal to write. that is we have something to dequeue that has been enqueued. so empty would be false. we enqueue (c), we dequeue, again we're going to dequeue (a), so we dequeue from the read index. so we basically read what's at the read index and then increment the read index. if we now dequeue again, we read what's at the read index which is (b) and we increment the read index. now we will do some more enqueueing. notice at this point that when we enqueue(d), the write index is 4, that's the next place we're going to enqueue(e), which will have us write to the index 4 and then the write index wraps back around to the initial element. and, here it's important to note we're using zero based indexing with this array because of the fact that the first element is zero. we enqueue again, enqueue (f), and now if we try enqueue (g), it's not going to allow us to do that. so that will be an error. the reason it would be an error, is if we did enqueue(g), the read and the write index would be both be 2. and it would be hard to distinguish read and write index 2 because the queue is full, or read and write index both 2 because the queue is empty. so therefore, we have a buffer of at least one element that can't be written to, to make sure read and write are separate and distinct if the queue's not empty. now we'll dequeue, so we'll dequeue (c), basically reading from the read index and updating it. dequeue (d), read from the read index and update it. dequeue (e) and here again, the read index wraps around back to 0. and now finally, we do our final dequeue and now the read and write index are both 1. which means if we ask whether dequeue is empty, the answer is yes, it is empty. so what we see here is that the cost for doing a dequeue and an enqueue, as well of course as empty, are all o(1) operations this way. so we can use either a linked list, although we have to have a tail pointer, so that pushback is cheap, or an array. each of the operations is o(1). one distinction between the array and the linked list implementation, is that in the array implementation, we have a maximum size that the queue can grow to. so it's bounded. maybe you want that in which case it's fine, but if you don't know a priori how long the queue you need is going to be an array is a bad choice. and any amount that is unused is wasted space. in a queue that's implemented with a linked list, it can get arbitrarily large as long as there's available memory. the downside is, every element you have to pay for another pointer. 
in this lecture, we're going to talk about trees. let's look at some example trees. so here we have a sentence, "i ate the cake". now, we're going to look at a syntax tree for that, which shows the structure of the sentence. so it's similar to sentence diagramming that you may have done in grade school. so we have at the top of the tree, the s for sentence and then children: a noun phrase and a verb phrase. the child of the noun phrase is the word i from the sentence. and the child of the verb phrase is a verb and noun phrase, where the verb is ate, and the noun phrase is a determiner and a noun, the and cake. so along the bottom of the tree, we have the words from the sentence, "i ate the cake", and the rest of the tree reflects the structure of that sentence. we can look here at a syntax tree for an expression  2sin(3z-7), we can break that up into the structure. so at the top level, we have a multiplication, that's really the last thing that's done, multiplying the 2 and the sine. within the sine, what we're applying the sine to is 3z-7, so we have the minus that's happening last with a 7 and then this 3z, 3 times z. so this shows again the structure of the expression and the order in which you might evaluate it. so from the bottom, you would first do 3 times z, and then you would subtract 7 from that, you'd apply the sine to that, and then you multiply that by 2. trees are also used to reflect hierarchy. so this reflects hierarchy of geography where we have at the left hand side the top level of the hierarchy, the world. and then below that, entities in the world, united states, all sorts of other things, united kingdom. and then below that, various subcomponents of the geography. so we've got, for the case of the united states, states, and then within those states, cities. another example of a hierarchy is the animal kingdom. this is part of it where we've got animals, and then below that, different types of animals, so invertebrates, reptiles, mammals, and so on. and then within each of these, we have various subcategorizations. so this shows this entire hierarchy. we also use trees in computer science for code. so in order to represent code, we will do that with an abstract syntax tree. so our code here is a while loop. while x is less than 0, x is x+2, f of x. so we reflect that at the top, we have while, which is our while loop. and the children of the while loop are the condition that needs to be met for the while loop to continue and then the statement to execute. so the condition is x less than 0, so comparison operation, the variable x and the constant 0. and then the statement to execute, well, it's actually multiple statements so we have a block. and in those blocks, we have two different statements, an assignment statement and a procedure call. the assignment statement, the left child is the variable we're assigning to, which is x, and the right child is an expression, in this case, x+2. the procedure call, the left child is the name of the procedure, and subsequent children are the arguments to that procedure. in our case, we just have one argument x. binary search tree is a very common type of a tree used in computer science. the binary search tree is defined by the fact that it's binary, so that means it has at most two children at each node. and we have the property that at the root node, the value of that root node is greater than or equal to all of the nodes in the left child, and it's less than the nodes in the right child. so here less than or greater than, we're talking about alphabetically. so les is greater than alex, cathy, and frank, but is less than nancy, sam, violet, tony, and wendy. and then that same thing is true for every node in the tree has the same thing. for instance, violet is greater than or equal to tony and strictly less than wendy. the binary search tree allows you to search quickly. for instance, if we wanted to search in this tree for tony, we could start at les. notice that we are greater than les, so therefore, we're going to go right. we're greater than sam so we'll go right. we're less than violet so we'll go left and then we find tony. and we do that in just four comparisons. it's a lot like a binary search in a sorted array. so with all these examples of trees, what's the actual definition of a tree? well a tree is, this is a recursive definition. a tree is either empty or it's a node that has a key and it has a list of child trees. so if we go back to our example here, les is a node that has the key les and two child trees, the cathy child tree and the sam child tree. the cathy child tree is a node with a key cathy and two child trees, the alex child tree and the frank child tree. let's look at the frank child tree. it's a node with a key frank and two, well, does it have any child trees? no, it has no child trees. so let's look at some other examples. an empty tree, well, we don't really have a good representation for that, it's just empty. a tree with one node is the fred tree, and it has no children. a tree with two nodes is a fred with a single child sally, that in itself has no children. in computer science commonly, trees grow down, so parents are above their children. so that's why we have fred above sally. so let's look at some other terminology for trees. so here, we have a tree, fred is the root of the tree. so it's the top node in the tree. and here, the children of fred are kate, sally, and jim. we are actually showing that with arrows, commonly, when you show trees, you don't actually show the arrows. we just assume that if a node is above another node, that it's a parent of that node. a child has a line down directly from a parent, so kate is a parent of sam, and sam is a child of kate. an ancestor is a parent or parent's parents and so on. so sam's ancestors are kate and fred. hugh's ancestors are also kate and fred. sally's ancestors are just fred. the descendant is an inverse of the ancestor, so it's the child or child of child and so on. so the descendants of fred are all of the other nodes since it's the root, sam, hugh, kate, sally and jim. the descendants of kate would just be sam and hugh. sibling, two parents, sorry, two nodes sharing the same parent, so kate, sally and jim are all siblings. sam and hugh are also siblings. a leaf is a node that has no children. so that's sam, hugh, sally, and jim. an interior node are all nodes that aren't leaves. so this is kate and fred. another way to describe it is all nodes that do have children. a level: 1 plus the number of edges between the root and a node, let's think about that. fred, how many edges are there between the root and the fred node? well, since the fred node is the root, there are no edges. so its level would be 1. kate has one edge between fred and kate, so its level would be 2, along with its siblings, sally and jim. and sam and hugh are level 3. the height: the maximum depth of the subtree node in the farthest leaf, so here we want to look, for instance, if we want to look at the height of fred, we want to look at what is its farthest down descendant. and so its farthest down descendant would either be sam or hugh. its height would be 3. so the leaf heights are 1. kate has height 2. fred has height 3. we also have the idea of a forest. extending this tree metaphor, so it's a collection of trees. so we have here two trees with a root kate and a root sally, and those form a forest. so a node has a key, children, which is a list of children nodes, and then it may or may not have a parent. the most common representation probably of trees, is really without the parent. but it's possible to also have parent pointers, and that can be useful as a way to traverse from anywhere in a tree to anywhere else by going up and then down, following parent nodes and then child nodes. on rare occasions, you could have a tree that's represented just with parent pointers. okay, but that's unusual because a lot of times, kind of the way you get access to a tree is via its root and you want to go down from there. there are other less commonly used representations of trees as well, we're not going to get into here. binary trees are very commonly used. so a binary tree has, at most, two children. rather than having in this general list of children, for a binary tree, we normally have an explicit left and right child, either of which can be nil. as with the normal tree, the general form of a tree, you may or may not have a parent pointer. let's look at a couple of procedures operating on trees. since trees are recursively defined, it's very common to write routines that operate on trees that are themselves recursive. so for instance, if we want to calculate the height of a tree, that is the height of a root node, we can go ahead and recursively do that, going through the tree. so we can say, for instance, if we have a nil tree, then its height is a 0. otherwise, we're 1 plus the maximum of the left child tree and the right child tree. so if we look at a leaf for example, that height would be 1 because the height of the left child is nil, is 0, and the height of the nil right child is also 0. so the max of that is 0, 1 plus 0. we could also look at calculating the size of a tree that is the number of nodes. again, if we have a nil tree, we have zero nodes. otherwise, we have the number of nodes in the left child plus 1 for ourselves plus the number of nodes in the right child. so 1 plus the size of the left tree plus the size of the right tree. in the next video, we're going to look at different ways to traverse a tree. 
in this video, we're going to continue talking about trees. and in particular, look at walking a tree, or visiting the elements of a tree, or traversing the elements of a tree. so often we want to go through the nodes of a tree in a particular order. we talked earlier, when we were looking at the syntax tree of an expression, how we could evaluate the expression by working our way up from the leaves. so that would be one way of walking through a tree in a particular order so we could evaluate. another example might be printing the nodes of a tree. if we had a binary search tree, we might want to get the elements of a tree in sorted order. there are two main ways to traverse a tree. one, is depth-first. so there, we completely traverse one sub-tree before we go on to a sibling sub-tree. alternatively, in breadth-first search we traverse all the nodes at one level before we go to the next level. so in that case, we would traverse all of our siblings before we visited any of the children of any of the siblings. we'll see some code examples of these. in depth-first search, so we're going to look here at an in-order traversal. and that's really defined best for a binary tree. this is inordertraversal is what we might use to print all the nodes of a binary search tree in alphabetical order. so, we're going to have a recursive implementation, where if we have a nil tree, we do nothing, otherwise, we traverse the left sub-tree, and then do whatever we're going to do with the key, visit it, in this case, we're going to print it. but often there's just some operation you want to carry out, and then traverse the right sub-tree. so let's look at an example of this. we've got our binary search tree. and we're going to look at how these nodes get printed out if we do an in-order traversal. so to begin with, we go to the les node. and from there, since it's not nil, we're going to do an in-order traversal of its left child, which is cathy. similarly now we're going to do an in-order traversal of its left child, which is alex. we do an in-order traversal of its left child which is nil, so it does nothing. so we come back to alex, and then print out alex, and then traverse its right sub-tree which is nil and does nothing. we come back to alex. and then we're finished with alex and we go back to cathy. so, we have successfully completed cathy's left sub-tree. so we did an in-order traversal of that, so now we're going to print cathy, and then do an in-order traversal of its right sub-tree, which is frank. so we go to frank, similarly now we're going to print out frank. we've finished with frank and go back to cathy, and now we've completed cathy totally, so we go back to les. we completed les' left sub-tree, so we're now going to print les and then traverse les' right sub-tree. so that is sam, traverse its left sub-tree which is nancy. print it out, go back to sam, we've completed sam's left sub-tree, so we print sam, and then go ahead and do sam's right sub-tree which is violet, which will end up printing tony, violet, and then wendy. we're completed with wendy. we go back to violet. we completed her right sub-tree, so we go back to sam, completed his right sub-tree, go back to les, completed his right sub-tree, and we're done. so we see we get the elements out in sorted order. and again, we do the left child. and then the node and then the right child. and by our definition of a binary search tree, that then gives them to us in order because we know all the elements in the left child are in fact less than or equal to the node itself. the next depth-first traversal is a pre-order traversal. now the in-order traversal really is only defined for a binary tree because we talk about doing the left child and then the node and then the right child. and so it's not clear if you had let's say three children, where it is you'd actually put the node itself. so you might do the first child and then print the node, and then second and third child. or first child and then second child and print the node, and then third child. it's kind of undefined then, so not well-defined. however, these next two, the pre-order and post-order traversal are well defined. not just for binary trees, but for general, arbitrary number of children trees. so here the pre-order traversal says, we're going to go ahead first if it's nil we return. we print the key first, that is, we visit the node itself and then its children. so we're going to, in this case, go ahead and go to the les tree and then print out its key and then go to its children. so we're going to first go to its left child which is cathy, and for cathy, we then print cathy, and then go to its left child which is alex, print alex, we go back to cathy. and we finished its left child, so then we go do its right child, which is frank. we finished frank. we finished cathy. we go back up to les. we've already printed les. we've already visited or traversed les' left child. now we can traverse les' right child, so it'll be sam, which we'll print out. and then we'll go to nancy, which we'll print out, we'll go back up to sam and then to violet, and we will print violet, and then print violet's children, which will be tony and wendy and then return back. a post-order traversal is like a pre-order traversal expect instead of printing the node itself first, which is a pre, we print it last, which is the post. so all we've really done is move where this print statement is. and here then, what's the last of these notes that's going to be printed? well it's actually going to be les, because we're not going to be able to print les until we've finished completely dealing with les' left sub-tree and right sub-tree. so we'll visit les, and then visit cathy, and then alex, and then we'll actually print out alex. once we're done with alex, we'll go back up to cathy and down to frank, and then print out frank, and then once we're done with both alex and frank we can then print cathy. we go back up to les, and we now need to go deal with les' right child which is sam. in order to deal with sam we go to nancy, print nancy, go back up to sam and down to violet, and deal with the violet tree, which will print out tony, and then wendy, and then violet. and on our way back up, then, when we get up to sam, we have finished its children, so we can print out sam. when we get up to les, we've finished its children, so we can print out les. one thing to note about the recursive traversal is we do have sort of under the covers, a stack that's being used. because in a recursive call, every time we make a call back to a procedure, we are invoking another frame on the stack. so we are saving implicitly our information of where we are on the stack. breadth-first, we're going to actually use a queue instead of a stack. so in the breadth-first, we are going to call it level traversal here, we're going to go ahead and instantiate a queue, and on the queue first put the root of the tree. so we put that in the queue and then while the queue is not empty, we're going to dequeue, so pull a node off, deal with that by printing it and then if it's got a left child, enqueue the left child, if it's got a right child, enqueue the right child. and so this will have the effect of going through and processing the elements in level order. we see the example here, and we're going to show the queue. so here let's say we're just before the while loop, the queue contains les. and we're going to now dequeue les from the queue, output it by printing it, and then enqueue les' children which are cathy and sam. now, we visit those in order, so first we're going to dequeue cathy, print it out and then enqueue its children. remember when we're enqueuing we go at the end of the line, so alex and frank go after sam. so now we're going to dequeue sam, print it, and then enqueue its children nancy and violet. so we can see what we've done then is, we first printed les, that's level one and then we printed the elements of level two, which are cathy and sam, and now we're going to go on to the elements at level three. so notice, all the elements in level three, alex, frank, nancy, and violet are in the queue already. and they're all going to be processed before any of the level four nodes are processed. so even though they'll be pushed in the queue, since the level three nodes got there first that they're all going to be processed before we process the level four ones. so here, we dequeue alex, print it out, and we're done. dequeue frank, print it out, we're done with frank. dequeue nancy, print it out, we're done with nancy. and violet, we print it out, but then also enqueue tony and wendy, and then dequeue those and print them out. so this is a breadth-first search, with an explicit queue, you can do depth-first searches rather than recursively, iteratively, but you will need an additional data structure which is a stack to keep track of the work still to be done. so in summary, trees are used for lots of different things in computer science. we've seen that trees have a key and normally have children, although there are alternative representations of trees. the tree walks that are normally done are traversals, are dfs: depth-first search, and bfs: breadth-first search. there are different types of depth-first search traversals, pre-order, in-order, and post-order. when you work with a tree, it's common to use  recursive algorithms, although note that we didn't for the breadth-first search where we needed to go through the elements of the tree in kind of a non-recursive order. and finally, in computer science, trees grow down. 
so in this lecture, we're going to talk about dynamic arrays and amortized analysis. in this video we're going to talk about dynamic arrays. so the problem with static arrays is, well, they're static. once you declare them, they don't change size, and you have to determine that size at compile time. so one solution is what are called dynamically-allocated arrays. there you can actually allocate the array, determining the size of that array at runtime. so that gets allocated from dynamic memory. so that's an advantage. the problem is, what if you don't know the maximum size at the time you're allocating the array? a simple example, you're reading a bunch of numbers. you need to put them in an array. but you don't know how many numbers there'll be. you just know there'll be some mark at the end that says we're done with the numbers. so, how big do you make it? do you make it 1,000 big? but then what if there are 2,000 elements? make it 10,000 big? but what if there are 20,000 elements? so, a solution to this. there's a saying that says all problems in computer science can be solved by another level of indirection. and that's the idea here. we use a level of indirection. rather than directly storing a reference to the either static or dynamically allocated array, we're going to store a pointer to our dynamically allocated array. and that allows us then to update that pointer. so if we start adding more and more elements, when we add too many, we can go ahead and allocate a new array, copy over the old elements, get rid of the old array, and then update our pointer to that new array. so these are called dynamic arrays or sometimes they're called resizable arrays. and this is distinct from dynamically allocated arrays. where we allocate an array, but once it's allocated it doesn't change size. so a dynamic array is an abstract data type, and basically you want it to look kind of like an array. so it has the following operations, at a minimum. it has a get operation, that takes an index and returns you the element at that index, and a set operation, that sets an element at a particular index to a particular value. both of those operations have to be constant time. because that kind of what it means to be an array, is that we have random access with constant time to the elements. we can pushback so that adds a new element to the array at the end of the array. we can remove an element at a particular index. and that'll shuffle down all the succeeding ones. and finally, we can find out how many elements are in the array. how do we implement this? well, we're going to store arr, which is our dynamically-allocated array. we're going to store capacity, which is the size of that dynamically-allocated array, how large it is. and then size is the number of elements that we're currently using in the array. let's look at an example. so let's say our dynamically allocated array has a capacity of 2. but we're not using any elements in it yet, so it's of size 0. and arr then points to that dynamically allocated array. if we do a pushback of a, that's going to go ahead and put a into the array and update the size. we now push b, it's going to put b into the array and update the size. notice now the size is equal to the capacity which means this dynamically allocated array is full. so if we get asked to do another pushback, we've got to go allocate a new dynamically-allocated array. we're going to make that larger, in this case it's of size 4. and then we copy over each of the elements from the old array to the new array. once we've copied them over, we can go ahead and update our array pointer to point to this new dynamically allocated array, and then dispose of the old array. at this point now we finally have our new dynamically allocated array, that has room to push another element, so we push in c. we push in d, if there is room we put it in, update the size. and now if we try and push another element, again we have a problem, we're too big. we can allocate a new array. in this case, we're going to make it of size 8. we'll talk about how you determine that size somewhat later. and then copy over a, b, c, and d, update the array pointer, de-allocate the old array, and now we have room we can push in e. so that's how dynamic arrays work. let's look at some of the implementations of the particular api methods. get is fairly simple. so we just check and see, we're going to assume for the sake of argument, that we are doing 0-based indexing here. so if we want to get(i), we first check and make sure, is i in a range? that is, is it non-negative, and is it within the range from 0 to size i minus 1? because if it's less than 0 or it's greater or equal to size, it's out of range, that will be an error. if we're in range then we just return index i from the dynamically allocated array. set is very similar. check to make sure out index is in bounds, and then if it is, update index i of the array to val. pushback is a little more complicated. so, let's actually skip the if statement for now and just say, let's say that there is empty space in our dynamic array. in that case, we just set array at size to val and then increment size. if, however, we're full, we're not going to do that yet, if size is equal to capacity, then we go ahead and allocate a new array. we're going to make it twice the capacity, and then we go through a for loop, copying over every one of the elements from the existing array to the new array. we free up the old array and then set array to the new one. at that point then, we've got space and we go ahead and set the size element and then increment size. remove's fairly simple. check that our index is in bounds and then go ahead through a loop, basically copying over successive elements and then decrementing the size. size is simple, will just return size. there are common implementations for these dynamic arrays and c++'s vector class is an example of a dynamic array. and there, notice it uses c++ operator overloading, so you can use the standard array syntax of left brackets, to either read from or write to an element. java has an arraylist. python has the list. and there is no static arrays in python. all of them are dynamic. what's the runtime? we saw get and set are o(1), as they should be. pushback is o(n). although we're going to see that's only the worst case. and most of the time actually, when you call pushback, it's not having to do the expensive operation, that is, the size is not equal to capacity. for now, though, we're just going to say that it's o(n). we'll look at a more detailed analysis when we get into aggregate analysis in our next video. removing is o(n), because we've gotta move all those elements. size is o(1). so in summary, unlike static arrays, dynamic arrays are dynamic. that is, they can be resized. appending a new element to a dynamic array is often constant time, but it can take o(n). we're going to look at a more nuanced analysis in the next video. and some space is wasted. in our case, if we're resizing by a factor of two, at most half the space is wasted. if we were making our new array three times as big, then we can waste two-thirds of our space. if we're only making it 1.5 as big, then we would waste less space. it's worth noting dynamic array can also be resized smaller, that's possible too. it's worth thinking about what if we resized our array to a smaller dynamic array as soon as we got under one-half utilization? and it turns out we can come up with a sequence of operations that gets to be quite expensive. in the next video, we're going to talk about amortized analysis. and in particular, we're going to look at one method called the aggregate method. 
so we'll discuss now what amortized analysis is and look at a particular method for doing such analysis. sometimes, we're looking at an individual worst case and that may be too severe. in particular we may want to know the total worst case for a sequence of operations and it may be some of those operations are cheap, while only certain of them are expensive. so if we look at the worst case operation for any one and multiply that by the total, it may be overstating the total cost. as an example, for a dynamic array, we only resize every so often. most of the time, we're doing a constant time operation, just adding an element. it's only when we fully reach the capacity, that we have to resize. so the question is, what's the total cost if you have to insert a bunch of items? so here's the definition of amortized cost. you have a sequence of n operations, the amortized cost is the cost of those n operations divided by n. this is similar in spirit to let's say you buy a car for, i don't know, $6,000. and you figure it's going to last you five years. now, you have two possibilities. one, you pay the $6,000 and then five years later you have to pony up another $6,000. another option would be to put aside money every month. so five years is 60 months. so if you put away $100 a month, once the five years is over, then when it's time to buy a new car for $6000, you'll have $6000 in your bank account. and so there that amortized cost (monthly cost) is $100 a month, whereas the worst case monthly cost is actually 6,000, it's 0 for 59 months and then it's 6,000 after one month, so you can see that, that amortized cost gives you a more balanced understanding. if you really want to know what's the most i spend in every month, the answer yes is $6,000. but if you want to know sort of an average what am i spending, $100 is a more reasonable number. so that's why we do this amortized analysis, to get a more nuanced picture of what it looks like for a succession of operations. so let's look at the aggregate method of doing amortized analysis. and the aggregate method really says, let's look at the definition of what an amortized cost is, and use that to directly calculate. so we're going to look at an example of dynamic array and we're going to do n calls to pushback. so we're going to start with an empty array and n times call pushback. and then we'll find out what the amortized cost is of a single call to pushback. we know the worst case time is o(n). let's define c sub i as the cost of the i'th insertion. so we're interested in c1 to cn. so ci is clearly 1. because we have got to actual, and what we're going to count for a second here is writing into the array. so the cost is 1 because we have to write in this i'th element that we're adding. regardless of whether or not we need to resize. if we need to resize, the first question is when do we need to resize? we need to resize if our capacity is used up. that is if the size is equal to capacity. well when does that happen? that happens if the previous insertion filled it up. that is made it a full power of 2, because in our case we're always doubling the size. so that says on the i'th insertion we're going to have to resize if the i'th- 1 filled it up. that is the i- 1 is a power of 2. and if we don't have to resize, there's no additional cost, it's just zero. so the total amortized cost is really the sum of the n actual costs divided by n. so that's a summation from i = 1 to n of c sub i. and again c sub i is the cost of that i'th insertion. while that's equal to n, because every c sub i has a cost of 1, so we sum that n times, that's n plus then the summation from what's this, this looks a little complicated so j = 1 to the floor of log base 2 of n- 1 of 2 to the j. that just really says the power of twos. all the way up to n- 1. so to give an example, if n is 100, the power of 2s are going to be 1, 2, 4, 8, 16, 32, and 64. and it's the summation of all of those. well that summation is just order n. right. we basically take powers of 2 up to but not including n. and that is going to be no more than 2n. so we've got n plus something no more than 2n, that's clearly o(n) divided by n, and that's just o(1). so what we've determined then is that we have a amortized cost for each insertion of order 1. our worst case cost is still order n, so if we want to know how long it's going to take in the worst case for any particular insertion is o(n), but the amortized cost is o(1). in the next video, we're going to look at an alternative way to do this amortized analysis. 
in this video, we're going to talk about a second way to do amortized analysis, what we call the banker's method. the idea here is that we're going to charge extra for each cheap operation. so it's sort of like we're taking the example where we looked at saving money for a car. we're going to actually take that $100 and put it in the bank. and then we save those charges somewhere, in the case of the bank we put it in the bank. in our case we're going to conceptually save it in our data structure. we're not actually changing our code, this is strictly an analysis. but we're conceptually thinking about putting our saved extra cost as sort of tokens in our data structure that later on we'll be able to use to pay for the expensive operations. to make more sense as we see an example. so it's kind of like an amortizing loan or this case i talked about where we're saving $100 a month towards a $6000 car, because we know our current car is going to run out. let's look at this same example where we have a dynamic array and n calls to pushback starting with an empty array. the idea is we're going to charge 3 for every insertion. so every pushback, we're going to charge 3. one is the raw cost for actually moving in this new item into the array, and the other two are going to be saved. so if we need to do a resize in order to pay for moving the elements, we're going to use tokens we've already saved in order to pay for the moving. and then, we're going to place 1 token, once we've actually added our item. 1 token on the item we added and then 1 token on an item prior to this in the array. it'll be easier when we look at a particular example. let's look at an example we have an empty array. and we're going to start with size 0, capacity 0. we pushback(a), what happens? well we have to allocate our array of size one, point to it, and then we put a into the array. and now we're going to put a little token on a and this token is what we use to pay later on to moving a. in this particular example for the very first element there's no other element to put a token on. so we're just going to waste that other, that third token. we push in b. there's no space for b so we've got to allocate a larger array and then move a. how are we going to pay for that moving a? well with the token the token that's already on it. so we prepaid this moving a. when we actually initially put a into the array, we put a token on it that would pay for moving it into a new array. so that's how we pay for moving a and then we update the array, delete the old one, and now we actually put b in. so we put b in at the cost of one, we still have two more tokens to pay. so we're going to put one on b and we're going to put one capacity over two that is one element earlier, so we're going to put one on a. so we've spent three now. one for real and two as deferred payment that we're going to use later in the form of these tokens. remember these tokens are not actually stored in the data structure. there's nothing actually in the array. this is just something we're using for mental accounting in order to do our analysis. when we push in c, we're going to allocate a new array. we copy over a and we pay for that with our pre-paid token. we copy over b, paying for that with our pre-paid token. and now we push in c. that's one, the second payment we have to make is, we put a token on c and we then we put token on a. four divided by two, that is the capacity divided by two, or two elements prior. we push in d, we don't have to do any resizing, finally. okay, so we just put in d and that's the cost of one. second, put a token on d. third, put a token capacity over two or two elements prior to that. so notice what we've got now is a full array and everything has tokens on it which means when we need to resize, we have prepaid for all of that movement. so we push in e, allocate a new array. and now we use those prepaid tokens to pay for moving a, b, c, and d. get rid of the old array, and now push in e. and again, put a token on e, and a token on a. so, what we've got here then is o(1) amortized cost for each pushback. and in particular, we have a cost of three, right? so we have clearly seen. so lets look back at how we did this. for this dynamic array we decided we had to charge three, and other data structures with other operations we not did have to charge a different amount. we have to figure out what will be sufficient, in our case three was sufficient, and we decided that we would go ahead and store these tokens on the elements that needed to be moved. so it's a very physical way to keep track of the saved work that we have done, or the prepaid work that we have done. so we charge 3, 1 is the raw cost of insertion. if we need to resize, we've arranged things such that whenever the array gets full, we've actually, in order for the array to have been full, we had to have done enough pushbacks such that every element got a token on it. all the new ones that we added since the previous resize, plus every time we added one of those new ones, we prepaid for a prior element as well. so, we pay our one insertion, we pay one for the element we're adding now and we pay one for sort of a buddy element earlier. in the next video we're going to look at a third way of doing amortized analysis, which is the physicist's method. 
now, let's talk about the final way to do amortized analysis, which is the physicist's method. the idea of the physicist's method is to define a potential function, which is a function that takes a state of a data structure and maps it to an integer which is its potential. this is similar in spirit to what you may have learned in high school physics, the idea of potential energy. for instance, if you have a ball and you take it up to the top of a hill, you've increased its potential energy. if you then let the ball roll down the hill, its potential energy decreases and gets converted into kinetic energy which increases. we do the same sort of thing for our data structure, storing in it the potential to do future work. couple of rules about the potential function. first, phi of h sub 0. so, phi is the potential function. h sub 0 is time 0 of the data structure h, so that means the initial state of the data structure, and that has to have a potential of 0. second rule is that potential is never negative. so, at any point in time, phi of h sub t is greater than or equal to 0. so, once we've defined the potential function, we can then say what amortized cost is. the amortized cost of an operation t is c sub t, the true cost, plus the change in potential, between, before doing the operation and after doing the operation. so, before doing the operation, we have phi(h sub t-1) after we have phi(h sub t), so it's c sub t plus phi(h sub t)- phi(h sub t-1). what we need to do is choose a function phi, such that, if the actual cost is small, then we want the potential to increase. so that we're saving up some potential for doing later work. and if c sub t is large, then we want the potential to decrease. in a way to sort of pay for that work. so, the cost of in operations is the sum of the true costs which is a summation from i goes from one to n of c sub i. and, what we want to do is relate the sum of the true costs to the sum of the amortized costs. so, the sum of the amortized costs is the summation from i equals 1 to n of the definition of the amortized cost. which is (c sub i + phi(hsub i) - phi(h sub i-1)). or, we could just rewrite that. so, removing the summation is c sub 1 + phi of (h sub 1)- phi of (h sub 0), + c sub 2 + phi of (h sub 2)- phi of (h sub 1) and so on. what's important to note is that we have a phi of h sub 1 in the first line and then a minus phi of h sub 1 in the second line, so those two cancel out. similarly, we have a phi of h sub 2 in the second line, and we have a phi of h sub 3 when we look at the amortized cost at time three. and, that goes on and on until at time n-1, we would have a phi of h sub n-1 positive and a negative phi of h sub n-1 negative. so, if all these cancellations and all we're left with is the very first term phi of h sub 0, negative phi of h sub 0, and the very last term in the last line which is phi of h sub n. so, this really just equals phi of h sub n minus phi of h sub 0 because all the other phis cancel, plus the summation from i equals 1 to n of c sub i, that is the true costs. since phi of h sub n is non negative and phi of h sub 0 is 0, this value is greater than or equal to just the summation of the true costs. what that means then is we've come up with a lower bound on the sum of the amortized costs which is the sum of the true costs. so therefore, if we want to look at a cost of a entire sequence of operations, we know it's at least the sum of the true costs. so, let's look at applying this physicist's method to the dynamic array. so, we're going to look at n calls to pushback. phi of h, so, at any given time the data structure's going to be two times the size minus the capacity. so, as the size increases, the potential's going to be increasing for a given fixed capacity. phi of x sub here, so we want to make sure that our phi function satisfies our requirements. so, first phi of 0 is 2 x 0- 0, assuming we have an initial array of size 0, and that's just 0. also, phi of h sub i is 2 x size - capacity. we know that size is at least capacity over 2, so therefore, 2 x size - capacity is greater than 0. now, let's look at our amortized cost. so, we're going to assume we don't have to do a resize and let's look at the amortized cost. so, we add a particular element i and the amortized cost is the cost of insertion plus phi(h sub i) - phi(h sub i-1). so, the cost of insertion is just going to be 1 because we're adding an element and we don't have to do any moving of elements. phi of h sub i is 2 x size of i - the capacity of i, and phi of h sub i- 1 is 2 x size i- 1 - capacity i- 1. well, what do we know? since we're not resizing and the capacities don't change. so, the capacities cancel themselves out. and so, we are left with 2 times the difference in sizes. what's the difference in size? difference in size is just 1, because we added one element, so this is 1 + 2 x 1 or 3. it's no accident that this 3 is the same value that we saw when we used the banker's method. and then, let's look at the cost when we have to do a resize. so, we're going to define here k is size sub i-1, which is the same thing as capacity sub i-1. why is it the same? because we're about to do a resize. so, that means that after the previous operation, we must have made the dynamic array full. and then, phi(h sub i-1) is just 2 times the old size minus the old capacity, and that's just 2 x k - k, or k. phi(h sub i) is 2 times the size of i - capacity of i, and that's 2(k + 1), because the size sub i is one more than the size of i-1, minus 2k. why 2k? because we double the capacity each time. so, that's just equal to 2. so, the amortized cost of adding the element is c sub i + phi(h sub i) - phi(h sub i - 1), which is just size of i, because that's the number of elements we have to, we have to move size of i-1 elements and then add the one new element, so, that's size of i. so, we have (sizei)+2-k, which is just (k+1)+2-k, which is 3. so, what we have seen now is that the amortized cost using the physicist's method of adding elements is 3. 
let's go back to the dynamic array. so are there alternatives to doubling the array size? right, we doubled each time. what happens if we didn't double? well we could use some different growth factor. so for instance, we could use 2.5. so grow the array by more than two, or grow the array by less than two. as long as we used some constant multiplicative factor, we'd be fine. the question is can we use a constant amount? can we add by a particular amount, like, let's say, 10 each time? and the answer is really, no. and the reason is, as the array got bigger and bigger, and we have to resize every ten times, we just don't have enough time to accumulate work in order to actually do the movement. let's look at another way. let's look at an aggregate method. let's say c sub i is the cost of the i'th insertion. we're going to define that as one, for putting in the i'th element, plus either i-1 if the i-1'th insertion makes the dynamic array full. so that is if i-1 is a multiple of 10 and it's 0 otherwise. by the definition of aggregate method which is just the sum of the total costs divided by n and that's n plus again that's the one summed n times is just n plus the summation from one to (n-1)/10 of 10j. that is just the multiples of 10. all the way up to but not including n. so 10, 20, 30, 40 and so on. all that divided by n. well, we can pull the 10 out of that summation so it's just 10 x the summation j = 1 to (n- 1)/10 of j. so that's just numbers 1, 2, 3, 4, and so on, all the way up to (n- 1)/10. that is o(n squared). that summation. so we've got n+10 times o(n^2)/n=o(n^2)/n=o(n). so this shows that if we use a constant amount to grow the dynamic array each time that we end up with an amortized cost for push back of o(n) rather than o(1). so it's extremely important to use a constant factor. so in summary we can calculate the amortized cost in the context of a sequence of operations. rather than looking at a single operation in its worst case we look at a totality of a sequence of operations. we have three ways to do the analysis. the aggregate method, where we just do the brute-force sum based on the definition of the amortized cost. we can use the banker's method where we actually use tokens and we're saving them conceptually in the data structure. or the physicist's method where we define a potential function, and look at the change in that potential. nothing changes in the code. we're only doing runtime analysis, so the code doesn't actually store any tokens at all. that's an important thing to remember. that is dynamic arrays and amortized analysis. 
hello everybody. welcome back. today, i'm going to be talking about priority queues. this popular data structure has built-in implementations in many programming languages. for example in c++, java, and python. and in this lesson, we will learn what is going on inside these implementations. we will see beautiful combinatorial ideas that allow to store the contents of a priority queue in a complete binary tree, which is in turn stored just as an array. this give an implementation which is both time and space efficient. also, it can be implemented just in a few lines of code. a priority queue data structure is a generalization of the standard queue data structure. recall that the queue data structure supports the following two main operations. so we have a queue and when a new element arrives, we put it to the end of this queue by calling the method pushback(e). and when we need to process the next element we extract it from the beginning of the queue by calling the method popfront(). in the priority queue data structure, there is no such thing as the beginning or the end of a queue. instead we have just a bag of elements, but each element is assigned a priority. when a new element arrives, we just put it inside this bag by calling the method insert. however, when we need to process the next element from this bag, we call the method extractmax which is supposed to find an element inside this bag whose priority is currently maximum. a typical use case for priority queues is the following. assume that we have a machine and we would like to use this machine for processing jobs. it takes time to process a job and when we are processing the current job, a new job may arrive. so we would like to be able to quickly perform the following operations. first of all, when a new job arrives we would like to insert it to the pool of our other weekly jobs quickly, right? and when we are done with the current job, we would like to be able to quickly find the next job. that is, the job with the maximum priority. okay, and now we are ready to state the definition of priority queue formally. formally a priority queue is an abstract data type which supports the two main operations, insert and extractmax. consider a toy example. we have a priority queue which is initially empty. we then insert element 5 in it, we then insert 7, then insert 1, and then insert 4. so we put these elements in random places inside this box on the left, just to emphasize, once again, that there is no such thing as the beginning or the end of a priority queue. so it is not important how the elements are stored inside the priority queue. what is important for us now is that if we call exractmax() for this priority queue, then an element with currently highest priority should be extracted. in our toy example it is 7. so if we call extractmax for this priority queue, then 7 is taken out of the priority queue. then, well let's insert 3 into our priority queue and now let's call extractmax(). the currently highest priority is 5, so we extract 5. then we extractmax() once again, and now it is 4, okay? some additional operations that we might expect from a particular implementation of a priority queue data structure are the following. so first of all, we might want to remove an element. i mean, not to extract an element with a maximum priority, but to remove a particular element given by an iterator, for example. also, we might want to find the maximum priority without extracting an element with a maximum priority. so getmax is an operation which is responsible for this. and also, we might want to change the priority of a given element. i mean, to increase or to decrease its priority. so changepriority(it,p) is the operation responsible for this. let us conclude this introductory video by mentioning a few examples of famous algorithms that use priority queues essentially. dijsktra's algorithm uses priority queues to find efficiently the shortest path from point a to point b on a map or in a graph. prim's algorithm uses priority queues to find an optimum spanning tree in a graph, this might be useful for example in the following case. assume that you have a set of computers and you would like to connect them in a network by putting wires between some pairs of them. and you would like to minimize the total price or the total length of all the wires. huffman's algorithm computes an optimum prefix-free encoding of a string or a file. it is used, for example, in mp3 audio format encoding algorithms. finally, heap sort algorithm uses priority queues to efficiently sort the n given objects. so it is comparison based algorithm. it's running time is n log n, as in particularly in the worst case. and another advantage of this algorithm is that it is in place, it uses no extra memory for sorting the input data. so we will go through all these algorithms in this specialization and the heap sort algorithm will be even covered in this lesson, in the forthcoming videos. 
as usual before going into the details of efficient implementation let's check what is wrong with naive implementations? for example, what if we store the contents of a priority queue just in an unsorted array or in an unsorted list? in this example on the slide, we use a doubly linked list. well, in this case inserting a new element is very easy. we just append the new element to the end of our array or list. for example, as follows, if our new element is seven we can just put it to the next available cell in our array where we can just append it to the end of the list. so we put 7 to the end. we say that the previous element of 7 is 2 and that there is no next element, right? so it is easy and it takes constant time, okay? now, what about extracting the maximum element in this case? well, unfortunately we need to scan the whole array to find the maximum element. and we need to scan the whole list to find the maximum element which gives us a linear running time. that is we o(n), right? in our previous naive implementation using an unsorted array or list, the running time of the extract max operation is linear. well, a reasonable approach to try to improve this, is to keep the contents of our array, for example array, sorted. well, what are the advantages of this approach? well, of course, in this case, extract max is very easy. so, the maximum element, is just the last element of our array. right? which means that the running time of extractmax in this case is just constant. however, the disadvantage is that now the insertion operation takes linear time, and this is why. well, to find the right position for the new element we can use the binary search. this is actually good, well it can be done in logarithmic time. for example, if we need to insert 7 in our priority queue, then in logarithmic time we will find out that it should be inserted between 3 and 9 in this for example. however unfortunately after finding this right position, we need to shift everything to the right of this position by one. right just to create a vacant position for 7. for this we need to first shift 16 to this cell. then we move 10 then to this cell, then we move 9 to this cell, and finally we put 7 in to this cell, and we get it sorted already. so in the worst case, we need to shift a linear number of cells, a linear number of elements, which gives us a linear running time for the insertion operation. as we've just seen, inserting an element into a sorted array is expensive because to insert an element into the middle we need to shift all elements to the right of this position by one. so, this makes the running time of the insertion procedure linear. however if we use a doubly linked list, then inserting into the middle of this list is actually constant time operation. so let's try to use a sorted list. well, the first advantage is that the extract max operation still takes constant time. well this is just because, well, the maximum element in our list is just the last element, right? so for this reason, we have a constant time for extract max. also, another advantage is that inserting in the middle of this list actually takes a constant amount of work, not linear, and this is why. again let's try to insert 7 into our list. well, this can be done as follows. we know that inserting 7 should be done between 3 and 9. so we do just the following. we will remove this. we remove this pointer and this pointer. we'll say that now the next element after 3 is 7 and the previous element before 7 is 3. and also the next element after 3, after 7 is 9, and previous element before 9 is 7. so inserting an element just involves changing four pointers. right? so it is a constant time operation. however everything is not so easy, unfortunately. and this is because just finding the right position for inserting this new element takes a linear amount of work. and this in particular because we cannot use binary search for lists. given the first element of this list and the last element of this list, we cannot find the position of the middle element of this list because this is not an arry. we cannot just compute the middle index in this array. so for this reason, just finding the right position for the new element i mean to keep the list sorted takes already a linear amount of work. and for this reason, inserting into a sorted list still takes a linear amount of work. well to conclude if you implement a priority queue using a list or an array sorted or not then one of the operations insert and extract max takes a linear amount of work. in the next video we will show a data structure called binary heap which allows to implement a priority queue so that both of these operations can be performed in logarithmic amount of work. 
hello. in this lesson we will consider binary heaps in full detail. a binary heap is one of the most common ways of implementing a priority queue. so just by definition a max binary heap is a binary tree where each node has zero, one, or two children where the following property is satisfied for each node. the value of the node is at least the value of all its children. or, put it otherwise, if you take any edge in this tree, then the value of its top end is at least the value of its bottom end. so this is an example of a binary max heap, and it can be easily checked that this property is satisfied on all the edges of this tree. on the other hand, this is an example of a binary tree, which is not a binary max heap. and this is why. here, we have five edges where the property is violated. for example, we see that on this top edge while it's the value of its top end is 10 while the value of its bottom end is 25. and there are four other edges where the property is violated. 
let's see how basic operations work with binary max heaps. what is particularly easy for binary max heaps is finding the maximum value without extracting it. i mean, it is easy to implement getmax operation. well, recall that the main property of that binary max heap tree is the following. for each edge its top value is greater or equals than its bottom value. this means that if we go from bottom to top now in our trees, the values can only increase. this in particular means that the maximum value is stored at the root of our tree. so just to implement getmax, we just return the value at the root of our tree. and this takes us just a constant time of course. now let see how inserting a new element into to the max binary heap works. so first of all a new element should be attached somewhere to our tree. we cannot attach it to the root in this case for example, because the root already has two children. therefore, we just attach it to some leaf. let's select for example, the leaf seven and attach a new node to it. the new node in this case has value 32. well, it is still a binary tree. right? because seven, before attaching seven, had zero children, now it has just one child. so it is still a binary tree. however, the heap property might potentially be violated. and it is violated actually in this case, right? which is shown by this red edge. so for this red edge the value of it parent which is seven, is less than the value of its child which is 32. so we need to fix it somehow. so to fix it we just allow that the new element to sift up. so this new element has value 32, which is relatively large with respect to all other elements in this tree, so we need to move it somewhere closer to the root. so the process of moving it closer to the roof is called sifting up. so the first thing to do is we need to fix this problematic edge. to fix it, we perform the following simple operation. we just swap the corresponding two elements. in this case, we'll swap seven and 32. after they swap, there is no problem on this edge. however, it might be the case that the new element 32 is still smaller. is still greater than its parent and this is the case, in our toy example. so the parent of 32 is now 29, which is smaller than 32, so we still need to fix this red problem. and we just repeat this process, we again swap the new element with its parent, right? so we swap it and now we see that the property is satisfied for all edges in this binary tree. so what we've just done is that we let the new element to sift up. and what is important to note here is that we maintained the following invariant, that the heap property at any point of time of sifting the new element up, the heap property is violated on at most one edge of our binary tree. so and if we see that there is a problematic edge, we just swap its two elements, right? and each time during this process the problematic node gets closer to the root. this in particular implies that the number of swaps required is at most the height of this tree. which in turn means that the running time of insertion procedure, as well as the running time of the sifting up procedure, in this case is big o of the tree height. now let's see how the extract max procedure works for binary max heaps. first of all, recall that we already know that the maximum value is stored at the root of the tree. however, we cannot just take and detach the root node because it will leave two sub trees, right? so we need to somehow preserve the structure of the tree. what is easy to detach from a binary tree is any leaf. so let's do the following, let's select any leaf of our tree and let's replace the root with this leaf. so in this case this produces the following tree. this potentially might violate the heap property. and in this case, this does violate the property. so the new root 12, is less than both its children. so the property is violated on two edges. so 12 is a relatively small number in this case. so we need to move it down to the leaves. great, so for this we will implement a new procedure, which is called siftdown, okay? so, similarly to siftup, we are going to replace, to replace the new element with one of its children. in this case we have a choice actually, we can replace it either with its left child or with its right child. by thinking a little bit we realize that it will make more sense to replace it with the left child in this case. because the left child is larger than the right child, because after this, after we replace 12 with 29, the right problematic edge will be fixed automatically, right? so this is how we are going to perform the siftdown procedure. once again, we select the largest of two child and we replace. the problematic node with this larger child. as you can see, the right problematic edge is fixed automatically. the left edge is also fixed, just because we swapped two elements. however, the new problematic node might introduce new problems, right closer to the bottom of the tree. now we see that there is still a problematic edge, so in this case, we have just one edge so 12 is smaller than 14, but it is greater than seven, so we are safe in the right tree. in this case we swap 14 with 12 and after that we just get a tree where the property is satisfied on all edges. so once again we maintain the following invariant. at each point of time we have just one problematic node, and we always solve the problematic node. with the larger one of its children, so that to fix both problematic edges. right? and the problematic node always gets closer to the leaf, which means that the total running time of the extract max as well as the sift down procedures is proportional to the tree height. now, when we have implemented both procedures, sifting up and sifting down, it's not so difficult to implement also the changepriority procedure. so assume that we have an element for which we would like to change its priority. this means that we are going either to decrease its priority or increase its priority. well, to fix the potential problems that might be introduced by changing its priority, we are going to call either sifting up or sifting down. well, let me illustrate this again on the toy example. assume that we are going to change the priority of this leaf 12. so we've just changed it. we just increased the priority of this element to 35. in this case, we potentially introduced some problems and we need to fix some. well we see that 35 is a relatively large number which means that we need to sift it up. so we need to move it closer to the root. so to do this we just call siftup procedure. which repeatedly swaps the problematic node with its parent, so in this case this will produce the following sequence of swaps. first will swap 35 with 18 this gives us the following picture, we see there is still a problem 35 is still larger than its parent so we swap it again. now we see that 35 is smaller than its parent. and actually, the heap property is satisfied for all edges. once again, what is important in this case is that at each point of time, the heap property is violated on at most one edge of our tree. so since our problematic node always gets closer to the root at each step, i mean, after each swap. we conclude that the running time of change priority procedure is also at most big o of the tree height. there is an elegant way of removing an element from the binary max heap. namely it can be done just by calling two procedures that we already have. so i assume that we have a particular element that we're going to remove. so the first step to do is we just change its priority to plus infinity, that is, to a number which is definitely larger than all the elements in our binary maxheap. when we call it, the change priority procedure will sift this element to the top of our tree, namely to the root of our tree. then to remove this element it is enough to call the extract max procedure. so in this particular example it will work as follows. so assume that we're going to remove the element 18, which is highlighted here on this slide. so we first change it's priority to infinity. then the changepriority procedure calls the siftup procedure. this procedure realizes that there is, that the property is violated on this edge. and swaps these two elements. then it swaps the next two elements and each at this point well this, this node that we're going to remove is at the root. well, to remove this node, we just call the extractmax procedure. so recall that the first step of extractmax is to replace the root node with any leaf. so let's select, for example, 11. so we replace, we replace the root with 11. then we need to call sift down, just to let this new root go closer to the leaves. well, in this case, 11 will be replaced first by 42, then there is still a problem on the edge from 11 to, to 18. so we swap 11 with 18 and finally we swap 11 with 12. well, once again since everything boils down just to two procedures. first is change priority. and the second one is extracting the max. and they all, they both work in time proportional to the tree height. so we conclude that the running time of the remove procedure is also, at most, big o of the tree height. so to summarize, we were able to implement all max binary heap operations in time proportional to the tree height, and the getmax procedure even works in constant time in our current implementation. so we definitely would like to keep our trees shallow. and this will be the subject of our next video. 
our goal in this video is to design a way of keeping the height of our binary max heap shallow. well, what is a natural approach to create a tree out of n given nodes, whose height is as small as possible. well, it is natural to require that all the levels are fully packed, right. this leads us to a notion of a complete binary tree. by definition a binary tree is called complete if all its levels are filled completely. except possibly the last one where we require additionally that all the nodes at this last level are in left most positions. let me illustrate this with a few small examples. so this is a complete binary tree. this is also a complete binary tree, and this is also a complete binary tree. so this is a binary complete tree too. and this is our first example of a binary tree which is not complete. well it is not complete because on the last level the two nodes shown here are not in the left most positions. this is also not a complete binary tree. this binary tree is also not complete because well this child is missing here, right. and this is also an example of a binary tree which is not complete. the first advantage of complete binary trees is straightforward, and it is exactly what we need actually. namely, the height of any complete binary tree with n nodes is o(log n). intuitively, this is clear. a complete binary tree with n nodes has the minimum possible height over all binary trees with n nodes. well, just because all the levels of this tree, except possibly the last one, are fully packed. still let me give you a formal proof. well, for this consider our complete binary tree and let me show a small example. so i assume that this is our complete binary tree. so in this case, n = 10 and the number of levels, l = 4. well, let's first do the following thing, let's complete the last level. and let's denote the result in number of nodes by n prime. in this case in particular the number of nodes in the new tree is equal to 15. well the first thing to note is that n prime is at most 2n. well this is just because in such a tree where all levels including the last one are fully packed, the number of nodes on each level is equal to the number of nodes on all the previous levels minus one. okay, so for example here the number of nodes on the last level is 8, and the number of nodes on all previous levels is 7. so we added at most seven vertices. now, when we have such a tree where all the levels are packed completely, it is easy to relate the number of levels with the number of vertices. namely with the number of nodes. namely n prime = to 2 to the l- 1. this allows us to conclude that l = binary logarithm of n prime + 1. now, recall that n prime is at most 2n, which allows us to write that l is at most binary logarithm of 2n + 1 which is of course, o(log n). the second advantage of complete binary trees is not so straightforward, but fortunately it is still easy to describe. to explain it, let's consider again a toy example, i mean a complete binary tree shown here on this slide. let's enumerate all its nodes going from top to down, and on each level from left to right. so this way the root receives number 1, to its children receive numbers 2 and 3 and so on. so it turns out that such a numbering allows for each vertex, number i for example, to be specific, to compute the number of its parent and the numbers of each children using the following simple formulas. once again, if we have a node number i, then its parent has number i divided by 2 and rounded down while its two children have numbers 2i and 2i + 1. to give a specific example, i assume that i = 4, which means that we are speaking about about this node. then to find out the number of its parent, we need to divide i by 2, this gives us 2. and indeed, vertex number 2 is a parent of vertex number 4. while to find out the numbers of two children of this node, we need to multiply i by 2, this gives us this node and multiply i by 2 + 1 and this gives us this node. and these two nodes are indeed children of vertex number 4, right? and this is very convenient. this allows us to store the whole complete binary tree just in an array. so we do not need to store any links for each vertex to its parent and to its two children. so these links can be computed just on the fly. again to give a concrete example, assume that we are talking about vertex number 3. so in this case i = 3. to find out the number of its parent, we just divide i by 2 and round down. so this gives us vertex number 1, and indeed vertex number 1 is a parent of the vertex number 3. and to find out the numbers of its two children, we just multiply i by 2 and also multiply i by 2 and add 1. this gives us, in this case, vertices number 6 and vertex number 7. so, and we know its indices in theory. okay, we have just discussed two advantages of complete binary trees, and it would be too optimistic to expect that these advantages come to us at no cost. so we need to pay something, and our cost is that we need to keep the tree complete. right, we need to ensure that at each point of time, our binary tree is complete. well, to ensure this, let's just ask ourselves what operations change the shape of our tree. essentially, these are only two operations, namely insert and extract max. so, two operations, sift up and sift down, they actually do not change the shape of the tree. they just swap some two elements inside the tree. another operation which actually change the shape is remove an element, however, it does so by calling the extractmax procedure. so on the next slide we will explain how to modify our insert and extractmax operations so that they preserve completeness of our tree. to keep a tree complete when we insert something into a complete binary tree, we just insert the new element, as a leaf, to the leftmost vacant position on the last level. well, an example is given here. so, we insert element 30 just to the right of the last element on the last level. okay, then we need to let this new element sift up so we perform a number of swaps. so 30 is swapped with 14, then still there is a problem, so 30 is greater than 29, so we swap it again. okay, now the property of the heap is satisfied for all the edges. well, when we need to extract the maximum value, recall that we first replace the root by some leaf. well in this case, to keep the tree complete, let's just select the last leaf at the last level. in this case it is 14. so we'll replace 42 with 14, and then, again, perform a number of swaps required to satisfy the property of the heap. okay, so in this case, 14 is swapped with 30, and then 14 is swapped with 29. this gives us a correct heap whose underlying tree is a complete binary tree. well so far, so good, we now know how to maintain the tree complete and how to store it in an array. in the next video we will show the full pseudocode of the binary max heap data structure. 
in this video, we provide the full pseudocode of the binary max heap data structure. here we will maintain the following three variables. h is an array where our heap will stay. maxsize is the size of this array, and at the same time, it is the maximum number of nodes in our heap. and size is the actual size of our heap. so size is always at most maxsize. so let me give you an example. in this case, we're given a heap of size 9. and it is stored in the first nine cells of our array h whose size is 13. in particular, you may notice that there are some values here, and it is actually some garbage. we just don't care about any values that stay to the right of the position numbered 9. so our heap occupies the first nine positions in the array. also let me emphasis once again that we store just the array h and also variables size and maxsize. so this tree is given to us implicitly. namely, for any node, we can compute the number of its parent and the number of its two children. and we can compute it and access the corresponding value in this array. for example, if we have no number three, then we can compute the index of its left child, which is 2 multiplied by 3. so the value of its right child is 18. these implementations showing how to find given a node i, the index of the parent of i and two children of i. so they just implement our formulas in a straightforward way. to sift element i up, we do the following. while this element is not the root, namely, i is greater than 1, and while the value of this node is greater than the value of its parent, we do the following. we swap this element with his parent. so this is done on this line. and then we proceed with this new element. i mean, we assign i to be equal to parent of i and go back to this while loop, and we do this until the property is satisfied. to sift an element number i down, we first need to select the direction of sifting. namely, if element number i is smaller than one or two of its children, we first need to select the largest one of its two children, right? so this is done here. so initially, we assign to the variable maxindex the value of i. then we compute the index of the left child of the node number i. then in the next if loop we first check whether i indeed has a left child. this is done in the following check. we check whether l is at most size. namely, whether l is an index which is in our heap, okay? then if h of l is greater than h of maxindex, we assign maxindex to be equal to l, okay? then we do the same with the right child. we first compute its index, then we check whether this index is indeed in our heap. then we check whether the value in this node is greater than the value of our current maximum index. and if it is, then we update the value of maxindex. and finally, if the node i is not the largest one among itself and the two of its children, namely, if i is not equal to maxindex, then we do the following. we swap element number i with element number maxindex. it is done here. and then we continue sifting down the just-swapped element. okay, so, this is done recursively. however, it is not so difficult to avoid using recursion here, just by introducing a new while loop. to insert a new element with priority p in our binary max heap, we do the following. we first check whether we still have room for a new element, namely whether size is equal to maxsize. if it is equal, then we just return an error. otherwise, we do the following. we increase size by 1, then we assign h of size to be equal to p. at this point we add a new leaf in our implicit tree to the last level, to the leftmost position on the last level. and finally, we call siftup to sift this element up if needed. to extract the maximum value from our binary max heap, we first store the value of the root of our tree in the variable result. so result is assigned to be equal h of 1. then we replace the root by the last leaf, by the rightmost leaf on the last level, so this is done by assigning h of 1 to be equal to h of size. okay? then we decrease the value of size by 1, just to show that the last leaf is not in our tree anymore. and finally, we call siftdown for the root, because it was replaced by the last leaf, which is potentially quite small and needs to be sifted down. and the last instruction in our pseudocode is, we return the result. that means the value which was initially in the root of our tree. removing an element. so as we've discussed already, this actually boils down to calling two procedures that we already have. once again, to remove element number i, we do the following. first, we change its priority to be equal to infinity, so we assign h of i plus infinity. then we siftup this node, so this will move this node to the root of our tree. and then we just call extractmax() procedure, which will remove the root from this tree and make necessary changes in the tree to restore its shape. finally, to change the priority of a given node i to the given value p, we do the following. we first assign h of i to p, okay? then we check whether the new priority increased is greater than the old priority or is smaller. if it is greater, then potentially we need to sift up the new node. so we just call sift up. if the new priority is smaller, then we call siftdown for this node. time to summarize. in this sequence of videos, we considered the binary max heap data structure, which is a popular way of implementing the priority queue data type. the considered implementation is quite fast, all operations work in logarithmic time and getmax procedure works even in constant time. it is also space efficient. in this data structure, we store a tree, but this tree is stored implicitly. namely, for each node, we do not store a connection or a link to its parent and its two children. instead, we compute the index of the corresponding nodes on the fly. well, in this case, we store just n given cells in an array, nothing more. okay? another advantage of this data structure, of this implementation, is that it is really easy to code. as you've seen, the pseudocode of each operation is just a few lines of code. well, in the next video, we will show how to use binary heap to sort data efficiently. 
>> in this video we will use binary heaps to design the heap sort algorithm, which is a fast and space efficient sorting algorithm. in fact, using priority queues, or using binary heaps, it is not so difficult to come up with an algorithm that sorts the given array in time of analog n. indeed, given a rate a of size m, we do the following. first, just create an empty priority queue. then, insert all the elements from our array into our priority queue. then, extract the maximum one by one from the given array. namely, we first extract the maximum. this is the maximum of our array, so put it to the last position. then, extract the next maximum and put it to the left of the last position, and so on. this clearly gives us a sorted array, right? so, we know that if we use binary heaps as an implementation for priority queue, then all operations work in logarithmic time. so, this gives us an algorithm with a running time big o of n log n. and recall that this is asymptotically optimal for algorithms that are comparison-based. and this algorithm is clearly comparison-based, all right? also, know that this is a natural generalization of selection sort algorithm. recall that in selection sort algorithms, we proceed as follows. given an array, we first scan the whole array to find the maximum value. so, then we get this maximum value and swap it with the last element. then, we forget about this last element and we can see that only n-1 first elements. again, by scanning this array, we find the maximum value, and we swap it with the last element in this region, and so on. so, here in the heap sort algorithm, instead of finding the maximum value at each iteration, namely, we use a smart data structure, namely a binary heap, right? so, the only disadvantage of our current algorithm is that it uses additional space. so, it uses additional space to store the priority queue. okay? so, in this lesson we will show how to avoid this disadvantage. namely, given an array a, we will first permute its elements somehow, so that the result in array is actually a binary heap. so, it satisfies binary heap property. and then, we will sort this array, again, just by calling extract marks and minus one times. building a heap out of a given array turns out to be surprisingly simple. namely, given array a of size n where there is a following. we first assign the value of n to is variable size just to indicate that we have a heap of size n. then, we do the following. for each i, going down from n over two, rounded down to one, we just sift down the i's element. let me explain it on a small picture, why, how we doing this. so, consider the following. the following heap, that we actually, let me remind you, we do not store it explicitly. we have just an array, in this case, of size 15. so, this is the first node, the second one, the third one, four, five, six, seven. so, if we just can see the corresponding array of size 15, and then imagine this complete binary tree. then, the heap property might potentially be related on many edges. however, in this tree we have 15 nodes, so we have 15 sub trees. and for these sub trees, i mean the root that the leaves of these of this tree, the heap property is satisfied for an obvious reason. there are no edges in these subtrees. so, the first node where the property might be related is node number seven. so, potentially, there might be a problem in this subtree. to fix this problem, we just call siftdown for node number seven. okay? after this call, this small sub tree must be here, right? then, we do the same for node number six. after this call, there are no problems in the sub tree rooted at node number six. then, we do the same for four i equal to 5 and 4i equal to 4. then, we proceed to node number three. note that, at this point, everything is fine in this subtree, and in this subtree, right? we already fixed everything in these two subtrees. so, to make, to satisfy the heap property in the sub tree rooted at the node three, it is enough to call siftdown from node number three. okay? then, we proceed to node number two. and, again, i have to call and siftdown to node number two. we fix the heap property in this sub tree, and so on. so, in this example, actually the last thing that needs to be done is to call siftdown for node number one. when we are done with this, we are sure that what we have in array a is actually a heap. so, it corresponds to a complete binary tree where the heap property is satisfied on every edge. let me repeat slowly what just happened. so, we, to turn a given array into a heap, we start repairing the heap properties in larger and larger subtrees. so, we start from the leaves, and go to the root. initially, so, our induction base is that the heap property is satisfied at all the leaves. i mean, in all subtrees rooted at the leaves for an obvious reason. any subtree rooted at the leaf has just one node, so the property cannot be violated, right? then, we gradually go up and we fix the heap property by shifting down the current vertex. and, finally, when we reach a root, the property is satisfied in the whole sub-tree, right? so, this is just a link for an online visualization. you can download the slides and play with this visualization if something is unclear to you in this process. let me now mention that the running time of this procedure is n log n. again, for an obvious reason. if, so, we use a binary max heap to implement this siftdown down procedure. so, we call siftdown roughly n over two times and each call is just log n, right? so, we have n log n running time. we already have everything to present, to present the in-place heap sort algorithm. given an array a of size m, we first build heap out of it. namely, we permute its elements so that the resulting array corresponds to a complete binary tree, which satisfies the heap property on every edge. so, we do this just by calling buildheap procedure. in particular, this buildheap procedure assigns the value n to the variable size. then, we repeat the following process n minus 1 times. so, we first, we call that just after calling to build heap, the first element of our array is a maximum element. right? so, we would like to put it to the last position of our array. so, we just swap a1 with a of n. and currently, a of n is equal to n of size, okay? so, we swap it. and then, we forget about the last element. so, we decrease size by one. so, we say that now our heap occupies the first n-1 element. and since we swapped the last element with the first element, we potentially need to sift down the first element. so, we just call siftdown for the element number one. and we proceed in a similar fashion. i mean, now the heap occupies n-1, the first n-1 position. so, the largest element among the first n-1 element is the first element. so, we swap it with element n-1. we forget about the element n-1 by reducing the size by 1. and then, see if bounds a first element. okay? so, we repeat this procedure n-1 times, each time finding the currently largest element. so, once again, this is an improvement of a selection sort algorithm. and this is an in-place algorithm. so, once again, let me state some properties of the resulting algorithm which is called heap sort. it is in place. it doesn't need any additional memory. everything is happening inside the given array a. so this is in advantage of this algorithm. another advantage is that its learning times is n log n. it is as simple, as it is optimal. so, this makes it a good alternative to the quick sort algorithm. so, in practice presort is usually faster, it is still faster. however, the heap sort algorithm has worst case running time n log n. while the quick sort algorithm has average case running time n log n. for this reason, a popular approach and practice is the following. it is called intrasort algorithm. you first run quick sort algorithm. if it turns out the be slow, i mean, if the recursion dips, exceeds c log n for some constant, c, then you stop the current call to quick sort algorithm and switch to heap sort algorithm, which is guaranteed to have running time n log n. so, in this case, in this implementation, your algorithm usually, in most cases it works like quick sort algorithm. and even in these unfortunate cases where it works in larger, where quick sort has running time larger than n log n, you stop it in the right point of time and switch to heapsort. so, this gives an algorithm which in many cases behaves like quick sort algorithm, and it has worst case running time. that must be [inaudible] n log n. 
in this video, we are going to refine our analysis of the buildheap procedure. recall that we estimated the running time of the buildheap procedure as n log n, because it consists actually of roughly n over 2 calls to siftdown procedure, whose running time is log n. so we get n and over 2 multiplied by log n, which is of course o(n log n). note, however, the following thing. if we call siftdown for a node which is already quite close to the leaves, then the running time of sifting it down is much less than log n. right? because it is already close to the root. so the number of swaps until it goes to the leaves cannot be larger than the height of the corresponding subtree, okay? note also the following thing. we actually, in our tree, we actually have many nodes that are close to the root. so we have just one node, which is exactly the root, whose height is log n. we have two nodes whose height is log n minus 1, we have four nodes whose height is log n minus 2, and so on. and we have roughly n over 4 nodes whose height is just 1. okay? so it raises the question whether our estimate of the running time of buildheap procedure was too pessimistic. we will see on the next slide. let's just estimate the running time of the buildheap procedure a little bit more accurately. okay, so this is our heap, shown schematically. so this is the last level, which is probably not completely filled. but all the leaves on the last level are in leftmost position. so, on the very top level, we have just one node, and sifting down this node costs logarithmic time. at the same time, on the last level, we have at most n over 2 nodes, and sifting them down makes at most one swap. actually, we do not need even one swap, just zero swaps, but let's be just generous enough and let's allow one swap. on the next level, we have at most n over 4 nodes, and sifting down for them costs at most two swaps, and so on. so if we just compute the sum of everything, so we have n over 2 nodes, for which the cost of the siftdown procedure is 1. we have n over 4 nodes on the next level, for which sifting them down makes at most two swaps. on the next level, we have n over 8. now it's, for each, sifting them down costs at most three swaps, and so on. so now let's do the following. let's just upper bound this sum by the following sum. first of all, let's take the multiplier n out of this sum. so what is left here is the following, 1 over 2 + 2 over 4 + 3 over 8 + 4 over 16 + 5 over 32, and so on, right? so this can be upper-bounded by the following sum. so this is just the sum from i equal to 1 to infinity of the following fraction, i divided by 2 to the i. once again, in our case, in the running time of buildheap, this sum is finite. so the maximum value of i is log n. we do not have any nodes on height larger than log n. but we just upper-bound it by an infinite sum, where we can see they're just all possible values of i. and even for this sum, we will show that the value of this sum is equal to 2. which gives us that the running time of the buildheap procedure is actually at most 2n. to estimate the required sum, we start with a simpler and more well-known sum. so this is given by the following picture, and the sum is given here. so 1 over 2 + 1 over 4 + 1 over 8 + 1 over 16 and so on. it is equal to 1. and this can be proved geometrically as follows. consider a segment of length 1. now, above the segment, let's draw a segment of size 1 over 2, of length 1 over 2. okay? this is half of our initial segment. what remains is also a segment of size one-half. so when we add a segment of size 1 over 4 here, we actually get 2 times closer to this vertical line, right? when we add the next one, 1 over 8, we get, again, 2 times closer than we were before adding this segment to this vertical line. when we add 1 over 16, again, our current distance to the vertical line is 1 over 16, and so on. so if we go to infinity, we get infinitely closer to this vertical line, which means that this sum is equal to 1. now, what about the sum that we need to estimate? well, to estimate it, let's first do the following strange thing. let's consider all the segments shown above, and let's adjust them by their right end. so consider the segment 1 shown here. now consider the segment of length 1 over 2, the segment of length 1 over 4, the segment of length 1 over 8, and so on. so we continue this process to infinity. and we know that the sum of the lengths of all these segments is equal to 2, of course. now, why we're doing this? well, for the following reason. first, consider the following vertical lines. what we need to estimate is the following sum, 1 over 2 + 2 over 4 + 3 over 8 + 4 over 16 and so on. let's see, so this is a segment of size 1 over 2, okay. this is two segments of size 1 over 4, okay. so this is three segments of length 1 over 8, and so on. so if we put another vertical line, we will get four segments of size 1 over 16, and so on. so if we do this to infinity, we will cover all our segments that are shown here, which proves that this sum is equal to 2. which in turn proves that the running time of our buildheap procedure is actually just linear, it is bounded above by 2n. our new estimate for the running time of the buildheap procedure does not actually improve the running time of the heapsort algorithm. because the heapsort algorithm first builds a heap, and now we know that it can be done in linear time, but then we need to extract max n minus 1 times. so we still have n log n time, and actually we cannot do better than n log n asymptotically. we already know this, because it is a comparison-based algorithm. however, this helps to solve a different problem faster than naively. so assume that we're given array and we're given a parameter k, which is an integer between 1 and n. and what we would like to do is not to sort the given array, but to find the k largest elements in this array. or put it otherwise, we need to output the last k elements from the sorted version of our array. so using the new estimate for the buildheap procedure, we can actually solve this problem in linear time, when k is not too large. namely, when k is at most big o(n) divided by log n. for example, if you have an array of size n, and you would like just to find square root of n largest elements, then you can solve this just in linear time. so you do not need to sort the whole array in time n log n to solve this problem. so linear time is enough to solve it. and this is how it can be done. given an array a of size n and parameter k, you just build a heap out of a given array, and then you extract the maximum value k times. right? so easy. the running time of this procedure is the following. first, you need to build a heap, so you spend a linear amount of work for this, then you need to extract max k times. for this, you spend k multiplied by log n. so if k is indeed smaller than n divided by log n, so let me write it. so if k is smaller than n divided by log n, then this is at most n. so the whole running time is at most linear. so to conclude, what we've presented here is a heap sort algorithm which actually sorts a given array in place without using any additional memory and in optimal time n log n. we also discussed that to build a heap out of a given array, it's actually enough to spend a linear amount of work. 
we'll conclude the lesson by a few remarks. first of all, for implementing a binary heap in an array, we can as well use zero based arrays. in this case, the formulas for computing the parent and the index of the parent of two children of a given node, i changed as follows. so, the parent of i, is given by the number (i-1)/2 and rounded down. the leftchild is given by the number 2i + 1 and the rightchild is given by 2i + 2. the next remark is that you can implement the binary min-heap in exactly the same way. and binary min-heap is a heap where each edge is value of their parent, is at most the value of the child. a case like this is useful for the case when a iteration just in your priority queue, you need to extract an element not with maximum priority, but with minimum priority. the final remark is that binary heaps can be easily generalized through d-ary heap. in such a heap, each node has, at most, d children. and we require to call a d-ary heap complete, we again require that all the levels are completely filled, except for possibly the last one where all the nodes are in leftmost position, okay? so the height of this tree is in this case, log base d of n, not the binary log of n. this in particular means that the running time of the siftup procedure is, at most, o of log base d of n, right? just because the height of the tree is, at most, log base d of n. and the element just goes down, just goes up. if needed, we swap an element with its pairing. however, the running time of siftdown procedure increases through d when supplied by a log base u of n. this is because when we go down, we always need to find a direction where to go in reach of this to go. and this is because, when we need to replace, to swap a node with one of its children we first need to select which on of these children is the largest one. right, so for this reason the running time of siftdown procedure in this case is o of d multiplied by log base d of n. okay, time to conclude in this segment of lessons we started by introducing the abstract data type called priority cues. this abstract data types supports the following two main operations insert an element and extract an element with the highest priority. this priority queues find a lot of applications. we will see many other reasons that you used efficiently this data type. then we explain that if implemented naively using an array, or at least sort it or not, one of these two operations will take a lenient amount of work, in the worst case. then we presented binary heaps. so this is a way of implement priority queues that gives the worst case running time, o of log n for all operations. and also, finally, we explained that this also can be made space efficient. namely, a binary heap is a tree, however to store this tree we do not need you to store connections to a parent and to children. it is enough to store everything in an array. again, this makes binary heaps both time and space efficient. 
hello and welcome to the next lesson of the data structures class. it is devoted to disjoint sets. as a first motivating example, consider the following maze shown on the slide. it is basically just a grid of cells with walls between some pairs of adjacent cells. a natural question for such a maze is given two points, given two cells in this maze whether there is a path between them. for example for these two points for these two cells shown on these slides there is a path and it is not too difficult to construct it. let's do this together. so this is, we can go as follows. and there is actually another path here, we can also go this way. great. on the other hand, there is no path between these two points shown on the slide and to show this we might want to construct just a set of all points that are reachable from b. let's again do this, so let's just mark all the points that are reachable from b. so it is not difficult to see that we marked just every single point which is reachable from b. and we now see that a does not belong to this set. which actually justifies that a is not reachable from b in this case. the maze problem can be easily solved with the help of the disjoint set data structure which supports the following three operations. the first operation is called makeset. it takes one argument x and it creates just a set of size one containing this element x. the second operation is called find. it takes an argument x and returns an id of the set that contains this element x. well, we expect this id to satisfy the following two natural properties. if x and y, if two elements, x and y lie in the same set, then we expect the operation find to return the same id was for x and y. well just because x and y lie in the same set, and find returns some identifier of this set, right? if, on the other hand, x and y lie in different sets, then find(x) should not be equal to find(y), right? the last operation is called union and it takes two arguments, x and y. and then it considers two sets containing x and y, and it merges them. in particular, if we just called union(x,y) and right after this we called find(x) and find(y). then these two call to find operation should return exactly the same identifier. just because x and y after the call to union, lie in the same merged set. recall that our maze example shows a particular point b is not reachable from a particular point a, we did the following. we first constructed the region of all cells, reachable from this point b in our maze. we then just checked that a, that point a does not belong to this region. so this was a justification of the fact that a is not reachable from b in our maze. and in fact, any maze can be partitioned into disjoint regions, where in each region, between any two cells there is a path, right? and using the disjoint sets data structure it is easy to partition any maze into such disjoint regions. we can do this by preprocess the maze as follows. we first call makeset for all cells c in our maze. this creates a separate region for each cell. so initially we have as many regions as there are cells in our maze. then we do the following. we go through all possible cells in our maze. so when a cell c is fixed, we also go through all possible neighbors of this cell in our maze. we say that n is a neighbor of c if n is an adjacent cell of c and there is no wall between them. so at this point c belongs to some region and n belongs to some region and we just discovered the fact that there is a path between c and n. which means that, actually, any cell from this region is reachable from any cell from this region, right? to reflect this fact, we just call union(c,n). this creates a separate set for these two regions. this merges these two regions, right? so after the call to this preprocess and procedure each region in this maze, receives a unique id, right? so then to check whether a particular cell is reachable from other cell we just need to check whether the find operation returns the same for them or not. to give another example of using the disjoint sets data structure, assume that we're building a network. in each area we have four machines we call makeset(1) for the first machine. makeset(2) for the second machine, and so on, so, to reflect the fact that initially, each machine lies in a separate set. in particular, if we now check whether find(1) is equal to find(2), then it is false just because 1 and 2 lie in different sets. now, let's add a wire between the third machine and the fourth machine. to notify our data structures that now 3 and 4 belong to the same set, we call union(3,4), okay? now let's introduce the fifth machine so we do this by calling makeset(5) then let's add another wire between the second machine and the third machine. to notify our data structure about this event we call union(3,2). if we now call find(1) and find(2), they should return, these two calls should return different values because 2 and 1, machines 2 and 1 still belong to different sets. okay, now we have the wire between the machines 1 and 4. and now yes, to notify our data structure about this event we call union(1,4), and now if we check whether find(1) is equal to find(2) it should return true. just because now 1 and 2 lie in the same set that contains machine 1, 2, 3, and 4. later in this specialization, we will learn the kruskal algorithm, which builds a network of a given set of machines in an optimal way, and uses the disjoint set data structure essentially. 
now when we've seen a few examples of using the disjointed set data structure and when we formally defined it, let's start to think about possible ways of implementing it. as usual we will start with a few naive implementations that will turn out to be slow on one hand, but on the other hand they will help us to come up with an efficient implementation. first of all, let us simplify our life by assuming that our n objects are just integers from 1 to n. this in particular will allow us to use the subjects as indices in a race. okay, so our sets are just sets of integers. and we need to come up with a notion of an id, of a unique id for each set. so, let's use the smallest element in each set as its id. in particular, since our objects are integers, for each element we can store the id of the set, this element belongs to in the array called smallest. for example, if we have the following three sets, then in the first set the id, namely the smallest element is two. in the second set the smallest element and the only element is five. in the third set the smallest element is one. then this information is stored in the array called smallest of size nine. separations makeset and find can be implemented in just one line of code in this case. namely to create a singleton set consisting of just element i, we set smallest of i to be equal to i, right. to find the id of the set containing the element i, we just return the value of smallest[i]. the running time of both of these operation is constant. everything is not so easy with the union operation unfortunately. so to merge two sets containing elements i and j, we do the following. first we find out the id of the set containing the element i. we do this by calling find(i) and restores the result in the variable i_id. then we do the same for the element j. we call find(j) and we store the resulting id in the variable j_id. then we check whether i_id is equal to j_id. and if they are equal, this means that i and j already lie in the same set. which means that nothing needs to be done. which means, in turn, that we can just return. if i_id and j_id are different, we do the following. well, we need to merge two sets. the smallest element in this set is i_id. the smallest element in this set is j_id. which means that the smallest element in the merged set should be just the minimum among i_id and j_id. restores as minimum in the variable m. then we need to scan all the objects, all our n objects and update the id of each. and update the value of the smallest array for reach objects for which their id is i_id or j_id. so this is done in the loop here where k ranges from 1 to n. so we check where the smallest of k is i_id or j_id and if it is equal then we update it to be equal to m. the running time of this operation is linear of course, just because essentially what we have here is a single loop that goes over all n objects. the bottleneck of our current implementation is the union operation, whose running time is linear as opposed to the finite make-set operations. whose running time is just constant. so we definitely need another data structure for storing sets, which allows for more efficient merging. and one such data structure is a linked list. so let's try to use the following idea. let's represent each set just as a linked list and let's use the tail of a list as the id of the corresponding set. let me illustrate this with two examples. in this case we have two sets, each one of them is represented, is organized as a linked list, and we treat the tail of a list as the id of the corresponding set. for example in this case, 7 is the id of the first set and 8 is the id of the second set. now to find the id of the set that contains the element three for example, we just follow the pointers until we reach the tail of the corresponding list. so in this case id is well defined. what is also nice, is that in this case. we can merge two sets very efficiently. actually since they are organized as lists, we just need to append to the other list and this requires changing just one pointer. what is very nice in this case is just the id of the merge itself is updated automatically. so after the merging, the hat of the resulting list is 8, so the id is updated for all the elements of two sets automatically. as we've just discussed, there are at least two advantages of the current implementation where we store each set as a linked list. first of all the running time of the union operation is, in this case, just constant. this is because to merge, to linked lists, we'd just append one of them to the other one. and for this we need just to change one pointer. another advantage here is that we have a well-defined id in this case. namely if two elements lie in the same set then find will return the same tale element from the corresponding list, right? and also if two elements lie in different sets then the tales of the corresponding two lists are different. and this is exactly what we want. there are however also two disadvantages. the first disadvantage is that now the running time of the find operation is linear in the worst case. this is because to find the tail of the corresponding list, i mean, given an element, we would like to find the corresponding tail of a list. for this, we need to follow the pointers til we reach the tail of this list. for this, we might need potentially to traverse a linear number of elements. because the list might contain a linear number of elements. so in the worst case, the running time of find declaration is linear, which is not so good. the second disadvantage is that actually implementing union operation is not so, in constant time, is not so easy as it was shown on our previous two examples. namely, we assumed implicitly in this example, then when given two elements x and y, we can find the beginning of the list containing x, and the end of the list containing y in constant time. so to be able to do this, we might need to store some additional information. and this in turn will require us to update this information when we merge two elements. so once again, this means that to implement union procedure in constant time we need to store some additional information, but not just pointers from a particular element to the next element. in search of an inspiration for improving our current implementation, let's review our previous two examples. so we've discussed that merging these two sets shown on this slide is follows as good because it requires just constant time and it updates the id of the resulting set automatically. on the other hand it is bad because it creates a long list. this in particular requires find(9) to traverse the whole list, and this makes the find operation a linear time operation. right, so let's try to think about a different way of merging these two lists. for example, what about the following one? in this case, first of all we, the resulting in structures is not least, it's just strange right? however, it is still constant time, right? and also 7 can still be considered as a id of the resulting set. because 7 is reachable from any other element, right? however, so what about this structure? it is not a list, but it is a tree. right, so it is a tree whose root is seven, and that has two branches, right. in the next video we will develop this idea to get a very efficient implementation of the disjoint sets data structure. namely, we will represent each set as a tree. and we will show that in this case the running time, the amortized running time of each operation is nearly constant. 
hi. in the previous video we considered a few naive implementations of the disjoint sets data structure. in one of them, we represented each set as a linked list. let me give you a small example. so these four elements are organised into a linked list. and we treat the tail of this element of this list, so this last element has the idea of the correspondence set. and this is well defined idea because it is unique for any list, and it can be easily reached from any other element in the correspondence set. so if we need to find the id of the set that contains this element, we just follow the next pointers shown here until we reach the tail of this list. another advantage is that merging two sets is very easy in this case. so assume that this is our first set and the second set looks as follows. then, to merge these two sets, we just append one of the least to the other one. like this. the first advantage of this merging is that it is clearly constant time. we just change one pointer. another advantage is that it updates the id of the result in that list automatically. now, with three these elements as id of the result in list. it still can be reached for many as an element of this list, just by following these pointers. the main disadvantage of this approach is that over time, lists get longer and longer. which in turn, implies that the find declaration gets slower and slower. well, we then discussed another possibility to merge two lists, namely we can do the following. again, consider the same two lists. and now i assume that instead of just appending one list to the other one, we do the following strange thing. we'll just change this pointer as follows. well, as you see what we get is not actually a list, however it is a tree whose root is this element and it has two branches, so we do not get a long trees, but instead we get a tree. and in this tree we can still treat this last element, this root element is the idea of the correspondent set. because it is unique for this for this tree, and also it can be reached from many as an element. so in this lesson, we'll going to further develop this idea. by doing this we will eventually get a very efficient implementation of the disjointed data structure. the general setting is the following. each set is going to be represented as a rooted tree. we will tree the root of each tree as the idea of the corresponding cell. for each element, we will need to know its parent and this will be stored in the array parent of size m. namely parent of i will be equal to j if element j is a parent of i or in case i is a root. then, parent of i will be equal to i. so this is a toy example. here, we have three trees and there are three roots, five, six, and four. and these three trees are stored in their right parent as follows, for example, to indicate that four is the root, we store four in the fourth cell of this array. to indicate that 9 is the parent of 7, we'll put 9 into the 7th cell. recall that makeset of i creates a single [inaudible] set consistent of just a single element i. to do this, we just assign parent of i to be equal to i. this creates the three, whose only element is right and this is the root of these three. so for this reason, we assign parent to five be equal to i. the running time of this operation is, of course, constant. to find the root of the three that contains a given element, i, we just follow the parent links from the node i until we reach the root. this can be done as follows. while i is not equal to parent[i], namely while i is not the root of the corresponding tree, we replace i by its parent. so each time, we go close it, there's a route. and eventually, we will reach a route. and then this point, we return the results in element. the running time of this operation is, of course, at most, the height of the correspondent tree. now, we need to design a way of nurturing to a trees and there is a very natural idea for doing this. we have two trees, let's just take one of them and camp under the root of the other one. let me illustrate it with a small example. assume that this is our first tree. it contains just three nodes and this is the root of this tree so it points to itself, and this is our second tree. this is the root, so it points at itself again. to merge these two trees we just change one pointer. namely, we say that now. this node is not the root anymore but its parent is this node. so we hang the left tree on this root of the right tree. once again this node is not the root anymore, while this node is the root of the resulting tree. and at this point, there is a natural question. we can hang the left tree on the root of the right tree. but also, vice versa, we can hang the right tree under the root of the left tree. so which one to choose? and after thinking a little bit, we realize that it makes sense to hang a tree whose height is smaller under the root of the true whose height is larger. and the reason for this is that we would like to keep our trees shallow. and in turn the reason for this is that the height of the trees in our forest influences the running time of the find operation. namely, the worst case running time of the find operation is actually at most the maximal height of a tree in our forest. to give a specific example, let's consider the following through trees shown on the slide. in this case, we have a tree of height one and tree of height two. assume that we call union of 3 and 8, in this case we need to merge these 2 trees and these will discuss there are 2 possibilities for doing this. either we hang the left tree under the root of the right tree or vice versa, we hang the right tree under the root of the left tree. the results of these two cases are shown here on the slide, and you see that in the last case the height of the tree increased. and this is not something that we want, because as we've discussed the height of this tree influences the worst case running time of the find operation. so this illustrates that to keep our trees shallow, when merged into a trees we would like to hang a tree who's height is smaller. and there's a root of the tree whose height is larger. 
okay, when merging two trees we're going to hang the shorter one under the root of a taller one. this means that when merging two trees we need a way to quickly find the height of both trees. instead of just computing them we're going to keep the height of each possible subtree in our forest in a separate array called rank. name the rank of i is equal to the height of the subtree rooted at i. the reason we call it rank will become clear a little bit later. let me also mention that this way of merging two trees, based on the height is called the union by rank heuristic. to keep the rank, we need a small addition to our makeset implementation, namely when creating a single set, we also set rank of i to be equal to zero. this reflects the fact that it is currently just a root containing one node, that just a tree containing one node, that is a tree of height zero. we do not need to change find. so the find operation doesn't need to change rank, and it also doesn't use rank in any way. to merge two trees containing the given two elements i and j, we do the following. we first find the roots of the point in two trees by calling the find operation two times. we store this root in variables i_id and j_id. we then check whether i_id is equal to j_id. if they are equal, this means that elements i and j already lie in the same set. so we just return in this case. so this is done in the following if loop. we then check whether the height of the tree containing element i is larger than the height of the tree containing element j. if it is larger, then we hang the tree with the root j_id, and this root of element i_id. this is done as follows. parent of j_id is set to i_id. otherwise, we do the opposite thing. we just assigned parent of i_id to be equal to j_id. so the last thing is that we need to check whether the height of the corresponding two threes are just equal. let me illustrate this again with a small example. assume that we are merging the following two trees. in this case the height of these two elements and this element is zero and this height of this element is 1. so in this case roots are equal. the ranks of the corresponding roots are equal. to merge these two trees we do the following. we just hang the left tree under the root of the right tree. if you can see, in this case, the height of the resulting tree actually increases and this is the only case when the union operation increases the height of this tree. so in this case, initially, the longest path contained just one edge. in this case we go the path that can contain two edges. so we need to update this rank and this is done in the last check. so if initially the ranks of our two trees that are going to be merged are equal we hang one of them under the root of the other one and increase the rank of the resulting tree by one. let's consider a small example, in this case we have six elements. let's call makeset for each of these elements. these fields have a data structure as follows. so currently, each element is its own parent, right? so its current set is just a single one set. also, the height of each sub-tree in our data structure is currently equal to 0. now let's call union(2,4). in this case, the rank of the subtree rooted at 2 is equal to 0. the height of the subtree rooted at 4 is equal to 0. so it doesn't mean which one to hang under the root of the other one, so let's hang 2 under 4. this changes the data structure as follows. now it's a parent of 2 is 4 and the rank of the subtree rooted at 4 is equal to 1. okay, now let's call union(5,2). in this case the height of the tree that contains the element number 2 is equal to 1, right? while the height of the tree that contains element number 5 is equal to 0. so, in this case we're going to hang 5 under 4. we do this as follows. so, this change the data structure, only this changes only this cell. so now 4 is the parent of 5, and it doesn't change any rank in our sub tree, in our forest. okay, now lets call union(3,1). this is done as follows, now 1 is rank 1, and now the parent of 3 is equal to 1, okay? now, let's call union(2,3), and again, in this case, 2 lies in a set in the tree whose root is 4. and currently, the rank of 4 is equal to 1. also, 3 lies in a set whose root is 1. and currently, rank of 1 is equal to 1. which means that after merging these two trees we will get a tree of height 2. so we do this as follows, now 1 is the root of the resulting tree and its rank is equal to 2. finally we call union(2,6) and this will just attach 6 to 1, as follows. in our current implementation, we maintain the following important invariant. at any point of time and for any node i, rank of i is equal to the height of the subtree rooted at this node, i, right? we will use this invariant to prove the following lemma. the height of any tree in our forest is at most binary logarithm of n. this will immediately imply that the running time of all operations with our data structure is at most logarithmic, right? to prove this lemma we will prove another lemma shown here on this slide. we're going to prove that if we have a tree in our forest whose height is k then this tree contains at least two to the k nodes. this will imply the first lemma as follows. i assume that some tree has height with more, strictly greater than binany logarithm of n. using the second lemma it will be possible to show then that this tree contains more than n nodes, right? which would lead to a contradiction with the fact that we only have n objects in our data structure. here we are going to prove the second lemma by induction on k. recall that we proved that any tree of height k in our forest contains at least 2 to the k nodes. we're going to prove this by induction on k. when k is equal to zero this means that we have a tree just of height 0, which means that it contains just one node. so, in this case, the statement clearly holds. now, to prove the induction step, let's recall that the only way to get a height, to get a tree of height k, is to merge two trees, whose height is equal to k- 1. i mean to merge both trees such that the height of the first tree is equal to k- 1 and the height of the second tree is equal to k-1. by the induction hypothesis the both of these two trees contain at least 2 to the k-1 node. which means that our resulting tree contains at least 2 to the k- 1 + 2 to the k- 1 nodes, which is exactly equal to 2 to the k, right? which means that the lemma is proved. to conclude, the running time of both union and find operations in our current implementation is at most logarithmic. why is so? well, just because we keep our trees shallow, so that the height of any tree in our forest is at most logarithmic. this immediately implies the time of any find operation is also big o of logarithm of n. recall also that the union operation consists of two calls to the find operation and also a few constant time operations, which means also that the running time of union is also big o of log n. in the next video, we will see another beautiful heuristic which will just decrease the running time of both these operations to nearly a constant. 
to build our y intuition involves a second characteristic for the disjoint at the destruction. let's again consider the example shown here on the slide. assume that we call find of six. this will traverse the following path from six to the root of this tree. so let's know that in this case we find the root of three that contains element six, but we also find the root of the tree that contains element 12 and contains element three. in general, by reversing this path we find the root for all the elements on this path. so, why lose this information? let's just store it somehow. and one way to do this, for example, is to re-attach all these notes directly to the root, we can do this as follows. now as you can see the parent of element 12 for example is five, and also the parent of element six is also five. we've just attached them directly to the root. and this can not only save us space in the future. save us time, i'm sorry, in the future. for future calls of find operation. so this heuristic is called path compression. implementing this heuristic i mean path compression heuristic, don't sound to be surprisingly simple. it is actually only three lines of code. here we do the following. we first check whether i is equal to parent of i. if it is equal, if it is the root, then we will just return the result. if i is not the root, if i is not the root, we do the following. we call find recursively for the parent of the node i. this is done here. so we call find for the parent of the node i. it will return the root of the correspondent three. and then we set parent of i to be equal to the returned root. that is, we attach the node number i directly to the root. and we do this recursively for all the elements on this pass. finally we return the new parent of i, which is now just the root of the corresponding tree. before stating an upper bound on the running time of operations of our current implementations, we need to introduce the so-called iterated logarithm function, which is also denoted by log star of n. so by definition, iterated logarithm of n is the number of times the binary logarithm should be applied to n before we get a number which is at most one. let me illustrate this with an example so n equal to one is a binary logarithm of n is equal to zero. we do not need to apply binary logarithm to get a number which is at most one, because n is already almost one in this case. for n equal to two, we need to apply binary logarithm once to get the number which is at most one. mainly, if we apply binary algorithm to two we just get the number one. okay for n equals to three and four, the binary algorithm is equal to two and so on. and for example, for the numbers shown here, two to the sixth, 5536 if we apply binary logarithm once, then just by definition of binary logarithm we get this number, just 65536, which is two to 16, okay? so if we apply the binary logarithm once again we get 16, 16 is two to the four. if we apply the binary logarithm once again we get four. if we apply, again, we get two, and if we apply it finally once again, we get one. and at this point, we stop. so we applied the binary logarithm five times to get a number which is at most one, which ensures that for this number, two to the 65536 the log star is equal to five, okay? so, and this shows that for any practical value of n, the binary log, the log star function is, at most, five. because this number is extremely huge. we will never see any value of m which is greater than this number in practical value. we will never get an input, a sequence that consists of so many elements. so theoretically speaking, the lock star function is not bound. it is not constant. so there are extremely huge numbers for which lock star is equal to ten or twenty or 100 and so on. however, for all practical values of n, log star of n is at most five. we're now ready to state an upper bound. assume that we used both union by rank heuristic and past compression heuristic, and assume that we make m operations with our data structure, of which m are calls to the makeset does a makeset operation. namely, we have n object, and we make m operations with them. then the total running time of all these calls is o(mlog*n). put it otherwise, the amortized running time of a single operation is o(log*n). and recalls it for all practical values of n, log star of n is at most five. which means that we have a constant average time for a single operation for all practical values of n. so once again, log star theoretically speaking, log star function is not bound, however it is at most five for all practical values of n which makes our data structure very efficient in practice. we will prove this upper bound in the next video. 
our goal in this video is to show that if we use both path compression and union by rank heuristics then the average running time of a single operation is upper bounded by big o of log star of n. before going into the details of the proof, let's realize a few important facts. first of all, know that since we are now using path compression, it is no longer the case that the rank of node i is equal to height of the tree rooted at vertex i. however, it is still true that the rank is an upper bound. on the corresponding height. let me illustrate this with a toy example. so, i assume that we have a tree like this. so, this is root say vertex 5. here we have vertex 2 and we have node 3, node say 6. assume that currently rank of this node is 2 say 0, this is 1 and this also 0. we now recall find of 6. this will re-attach 6 to the current root of this tree. so what we get is the following, 5, 3, 1. and also 6 now stays here, all right? so we see that the height of these three is equal to 1. however the rank of the root is still equal to 2. recall that find doesn't use and doesn't change any rank values. also for this node 3, it's height. the height of the tree rooted at element 3 is equal to 0, however the rank is equal to 1. well, intuitively it is clear path compression can only decrease the height. so for this reason, rank is no longer equal to the height of the corresponding tree however the height is at most the length. okay. another important thing is the following. it is still true that for any root node i of rank k. the corresponding sub 3 contains at least 2 to the k elements. and this can be seen by realizing that the past compression does not affect any root nodes. i mean, if we have a root node whose height is k, then no matter how many times and for which nodes in these sub tree we call find with path compression, all this nodes are still in this subtree rooted at this nodes exactly because this node is a root. so any node from this subtree cannot be attached to some other vertex and some other subtree. this node is still, so once again if we have a node who is still a root and whose rank is k, then the corresponding subtree contains at least 2 to the k elements, 2 to the k nodes. on this slide we discuss a few more important properties. the first one of them says that we have, in our forest, at most n divided by 2 to the k nodes of rank k. why is that? well, recall that if we create a new node of rank k then it was created by merging two nodes of rank k-1. okay. so we know that currently this node is a root. at the same time, we know that the correspondence subtree contains at least 2 to the k nodes. if we have another another node of rank k. then it also contains at least 2 to the k nodes. which means that if we have too many such nodes. i mean, too many nodes of rank k. by saying too many, i mean that its number is greater than n divided by 2 to the k. then overall, we have more than n element. which is a contradiction, right? the second property is that, when we go up, the rank strictly increases. well, this was clear if we do not use past compression. i mean, if rank is equal to the height of the corresponding subtree, then this is completely clear. let me recall that if we have for example a tree of height two, then the height of this tree is two. the height of this subtree is one. the height of this subtree is zero. let's say this is element 5. this is 4. this is 8. now we have passed compression, so we need to check what happens when we compress some paths. if we call find(5), for example, then we'll be reattached to the current root. but it will still be 2. that the rank of the parent. so let me fix this. this is node 8. the rank of the parent is strictly greater than the rank of a child. the last property is, says that when a node becomes an internal node it will be an internal node forever, right. it will not have a chance to become a root and this is just because the find operation doesn't change any roots in our forest. while union operation takes two roots and makes one of them a child of the other one. so it takes two roots and leaves only one root. okay. so once again when a vertex becomes an internal vertex, a non root vertex, it will be a non root vertex forever. we now start to estimate the running time of m operations. first of all note that the union operation boils down to t(all calls to find) operation. and also to some constant operations. namely, when we have two roots that were found by two calls to. to find that operation, we need to hank one of them below other one which is a constant operation. we just need to change one parent. and also possibly we need to change the rank value. okay. so for this reason when estimating the total running time we will just assume that we have m calls to find operation. paths node that each find operation traverses some pass from a note to find the root of the corresponding tree. so we traverse some number of edges. so the total run in time of all the defind operations, of all the calls to defind operation is just the total number of edges traversed. so this is what is written here, we just need to count the number of edges from parent a node i through it's paring j, at traverse you know these codes. for technical reasons we will split this number into three terms. in the first term we will account all the edges that lead from a vertex to the node to the root of the corresponding tree. so the first term includes all the edges that lead from a node to another node, which is the root in this case. the second term include all the remaining edges where we go from i to j. such as there log* of rank of a is strictly smaller than log* of rank of j, okay? and their remaining term accounts for everything else, namely for all the edges where we go from i to j such that j is not the root. and that log*(rank[i]) = log*(rank[j])). we're going to show separately for each of these terms that it is upper bounded by big 0 of m multiplied by log star of m. let me show this on a small example as usual. assumes that we have such a path that we're going to traverse. a path from the node at the bottom to the root of the corresponding tree. so the numbers shown here indicate the ranks of the corresponding nodes. then these two nodes, these two edges will be accounted in the first term. just because these two nodes lead from a node to the root this thread just lead from a node to the root of the corresponding tree. well, this edge for example will be accounted in the last term because the rank of this node is 17 and the rank of this node is 21. and log star of this numbers are equal. at the same time, here we have 14 the rank 14, and here we have rank 17. and the log star of these two numbers are different. for this reason this hatch will be accounted in the second term, okay? so on the next sequence of slides, we are going to estimate separately each of these three terms. and for each of them, we are going to show that it is at most big o of m multiplied by log* of m. the first term is easy to estimate. recall that in this term we account for all the edges traversed by find operation. where we go from node i to its parent j such that j is the root. clearly for each call to find the operation there are at most two side edges, right? which means that we have an upper bound big o of m. in the second term, we need to estimate that a long number of edges traversed it during all m calls to the find operation. such that we go from node i to its parent j such that j is not the root and also log star of rank of i is strictly less than log star of rank of j. we're going to prove here that it is upper bounded by big o of m multiplied by log star of n. and this is just because, when we go up from some node to the root of the corresponding tree, the rank always increases. however, the rank of the root is at most log n, which means that during one call to find procedure the lock star of rank can only increase log star of n times. okay. this is just because we've had an upper bound for the rank of the root. it is upper bounded by log m which means that there are only log star of m different possibilities for log star of rank of folds and nodes on this. which means, that these can only increase, at most, log star of m times. and we have at most, m calls to find the operations. which gives us, an upper bound m, to get, m multiplied by log star of m. now it remains to estimate the last term. where we account for all the address traversed during m calls to the find operations. where we go from node i to node j through its parent j such that j is not the root, first of all. and then the rank, the log star of rank of i is equal to log star of n of j. what we're going to show is that the total number of side edges is upper bounded by big o of m multiplied by log star of m. note that this is even better than what we need. what we need is a recap upper bound which is m log star of n. recall that we know that m is greater than m just because m is the total number of operations, while n is the number of calls to make said operations. to estimate the required term, consider a particular node i and assume for completeness that it's rank lies in an interval from k plus one to two to the k. recall that this was the form of interval through which the lock star function is fixed. okay? now let's compute the total number of nodes whose rank lies in section interval. so we know that the total number of nodes whose rank is equal to k plus one is at most n divided by two, to the k plus one. so total number of nodes was ranked equal to k plus two is at most n divided by two, to k plus two. and so on, so the total number of nodes whose rank lies in this interval is at most n divided by two to the k. okay. the next stop equation is that each time when we call find of i, it is adopted by a new parent and since it is new. so, at this point we know that if we have a node i and its parent j is not the root. yes, this is essential. which means that when we go up we find another root when we cofind a find of i. and at this point we will reattach node i to this new root. and this new root has strictly larger rank. and this in turn means that after most 2 to the k calls to find of i. find(i) will be adopted by a new parent whose rank, for sure, does not lie in this interval. just because the rank of this interval is at most 2 to the k. so if we increase the rank of the parent of i, at least 2 to the k times, it will be greater than 2 to the k for sure. right. we now have everything to conclude this proof. to conclude estimating the required term. recall that we are estimating the total number of address traversed during n calls to client. where we go from a node i to its parent j, such that log star of rank of a is equal to log star of rank of j. so we have just proved that there are at most n divided by 2 to the k nodes whose rank lies in the interval from k plus 1 to 2 to the k. we then explained why any such node whose rank lies in such an interval, contributes, at most, 2 to the k to the term that we are currently estimating. this means that just all the nodes from, whose rank lies in such an interval. contributes, at most big o of n to this term, right? now it remains to note that the total number of different intervals is at most log star of n. which means that the total contribution of all such nodes i to the estimated term is at most big o of n multiplied by log star of n. we now conclude the whole lesson. so in this lesson we considered it an implementation of the joins of data structure where each set is represented by a tree. and for this reason we have a forest of trees. the id of each set is just the root of the corresponding tree. we then consider it to heuristics. the first one is called union by rank. it says that when merging two trees, it makes sense to attach a shorter one to the root of a taller one, okay? this helps to keep trees in our forest shallow. and for this we keep the rank of each subtree rooted at a particular node in a separate array called rank. we then discussed another heuristic which is called path compression. the idea is the following. when we traversed a path from a node to the root of the corresponding tree, we may want to reattach all the nodes found on this path directly to the root. because this potentially will save us time for further find operations, right. we then proved that the resulting implementation turns out to be extremely efficient, namely the amortized running time of each operation is big o of log star of n. and log star of n is an extremely slowly growing function. in particular, log* n is at most five for all practical values of n. 
hi. in this module, we'll study hashing, and hash tables. hashing is a powerful technique with a wide range of applications. in this video, we will learn about some examples of those applications, just to have a taste of it. the first example that comes to mind is, of course, programming languages. in most of the programming languages, there are built-in data types or data structures in the standard library that are based on hash tables. for example, dict or dictionary in python, or hashmap in java. another case is keywords of the language itself. when you need to highlight them in the text editor or when the compiler needs to separate keywords from other identifiers in the problem to compile it. it needs to store all the keywords in the set. and that set is usually intuitive using the hashtag. another example is file system. when you interact with a file system as a user, you see the file name, maybe the path to the file. but to actually store the correspondence between the file name and path, and the physical location of that file on the disk. system uses a map, and that map is usually implemented as a hash table. another example is password verification. when you use some web service and you log into that and you type your password, actually if it is a good service, it won't send your password in clear text through the network to the server to check if that's the correct password or not, because that message could be intercepted and then someone will know your password. instead, a hash value of your password is computed. on your client side and then sent to the server and the server compares that hash value with the hash value of the stored password. and if those coincide, you get authenticated. special cryptographic hash functions are used for that. it means that it is very hard to try and find another string which has the same hash value as your password. so you are secure. nobody can actually construct a different string which has the same hash value as your password and then log in as you in the system, even if he intercepted the message with the hash value of your password going to the server. another example, storage optimization for online cloud storages, such as dropbox, google drive or yandex.disk. those use a huge amount of space to store all the user files and that can actually be optimized using hashing. we will discuss this example further in the lectures of this module. 
hi, in this video, we will introduce a problem about a web service, and ip addresses of it's clients. we will use this problem, to illustrate different approaches throughout the whole lesson. suppose you have a web service with many, many clients, who access your service through the internet from different computers. in the internet, there is a system which assigns a unique address to each computer in the network. just like every house in the city has its own address. those addresses of computers are called ip addresses or just ips. every ip address looks like this, four integers, separated by dots. every of the four integers is from 0 to 255. so that it can be stored in eight bits of memory. and the whole ip address, can be stored in 32 bits of memory as the standard integer type in c++ or java. so there are 2 to the power of 32 different ip addresses, which is roughly 4 billion. recently, the internet became so big that 4 billion is no longer enough for all of the commuters in the network. that's why people designed the new address system called ipv6. and the number of addresses there is 2 to the power of 128, which is a number with 39 digits. and it will be sufficient for a long time. in this problem, we will start talking about old system called ipv4, which is still in use. and which contains only 2 to the power of 32 different ip addresses. when somebody accesses your web service, you know from which ip address did he or she access it. and you store this information in a special file called access log. you want to analyze all the activity, for example, to defend yourself from attacks. an adversary can try to kill your service by sending lots and lots of requests from his computer to your service, so that it doesn't survive the lot and fails. this is called denial of service attack. and you want to be able to quickly notice the pattern. that there is a unusual high number of requests from the same ip address during some period of time for example, the last hour. and to do that, you want to analyze your access log. you can think of your access log as of a simple text file with many, many lines. and in each line, you have date and time of the access, and the ip address from which the client accessed your servers. and you want to be able to quickly answer the queries like, did anybody access my service from this particular ip address during the last hour? and how many times did he access my service? and how many different ips were used to access the service during the last hour? to answer those questions, we'll need to do some log processing. but of course, we don't want to process whole one hour of logs each time we want to answer such a simple question because one hour of logs can easily contain dozens of thousands or hundred of thousands or even millions of lines depending on the load of your web service. want to do that much faster. so to do that we'll keep count. for each ip address, we'll keep a counter that says how many times exactly that ip address appears in the last one hour of the access log, or how many times during the last hour clients accessed your service from that particular ip address. and we'll store it in some data structure c, which is basically some data structure to store the mapping from ip addresses to counters. we don't know yet how to implement that data structure c. we will discuss that further. we will update the counter corresponding to ip addresses every second. for example, if now is 1 hour 45 minutes and 13 seconds from the start of the date and we'll ignore the date field in the access log for the sake of simplicity. then we need to increment the counters corresponding to the ip addresses in the last two lines of the log, because those are new lines. we also need to remember to decrement the counters corresponding to the ip addresses in the old lines of the log. for that we'll look at the lines exactly 1 hour ago in the log. because the lines which are older than that, for them we've already decremented the counters in the previous seconds. and the lines which are more recent than that, we still don't need to decrement the counters because the ips in those lines are still in the 1 hour window ending in the current second. so we'll decrement the counters corresponding to the lines which are 1 hour ago from the current moment. now let's look at the to pseudo code. in the main loop we have the following variables. log represents the access log. we will think of it as an array of log lines. each log line has two fields. time and ip address. c is some mapping from ips to counters. we still don't know how to implement that but we suppose that we have some data structure for that. i is an index in the log which points to the first unprocessed log line. so when a new second starts, we'll need to start incrementing counters corresponding to lines starting from i and further in log. j is the first or the oldest line in the current 1 hour window. so that when the next second starts we'll need to decrement counters for some of the lines starting from line number j. we initialize i and j with 0 and c with an empty mapping, because there is nothing to store in the start. and then each second, we call procedure updateaccesslist, and we pass there the access log to read data from. we also pass i and j, which we will use inside and also update. and we pass data structure c, which is our goal to updated. so now let's look at the pseudo code for update access list. it consists of two parts. the first part deals with the new lines and the second part deals with the old lines. new lines start from line number i which is the first unprocessed line. look at this line and we increase the counter corresponding to the ip in this line using our data structure c. and then we go on to the next line. we'll proceed with this while the time written in the log line i is still less than or equal to the time when updateaccesslist was launched and then we stop processing new lines. and we want to all blinds. how do we determine that the line is old enough, to decrement the counter? we compute the time now, which we assume is computed in seconds. so then we need to subtract, exactly one hour from that and that is 3600 seconds. and if the time written in line j is less than or equal to that, we need to decrement the corresponding counter. so we'll start with line number j, which is the first line in our 1 hour window. we check that it is old enough to decrement the calendar. we decrement the calendar if that's the case and then we move on to the next line. in the and when we stop in this while loop, j will point again to the first or oldest line in the current 1 hour window. so we've implemented the updating procedure correctly. now how to answer the question whether this particular ip was or was not used to access our service during the last hour. that is really easy. if the counter corresponding to that ip is more than 0, then this ip was used during the last hour. otherwise the counter will be 0. so,we've implemented all the procedures necessary to answer the questions, but for one small detail. we don't know how to implement data structure c. and we will discuss that in the next lectures. 
hi. in this video we will talk about direct addressing, which is the first step on the way to hashing. remember this computer code from the last video. we implemented procedure updateaccesslist using a data structure c, which stores a counter for any ip address. now the question is, how to implement the data structure c itself. the idea here, is that there are 2 to the power of 32 different ip addresses. according to ip(v4) format. and we can actually convert each ip to a 32-bit integer. and it will be a one to one correspondence between old possible ips. and all numbers between zero and two to the power 32 minus one. thus, we can create an array a, of size exactly two to the power of 32, with indexes zero to two to the power of 32 minus one. and then for each ip, there will be exactly one position in this array, correspondent to this ip. and then, we will be able to use the corresponding answer in array a. instead of the counter for this ip. now, how do we actually convert ip addresses to integers? if you look at this picture, you will see that any ip address actually consists of 4 integer numbers. which are all, at most, 255. and each of them corresponds to 8 bits, or 1 byte in the total 4-byte or 32-bit integer number. basically, if you just coordinate all the 8 bytes corresponding to first number with 8 bytes. corresponding to the second number and to the third number and to the fourth number. you will get 32 bytes. and if you then convert this string of 32 bytes into the decimal form. you will get an integer number in the form which we are used to. for example, if you take a very simple ip address, 0.0.0.1. it will convert to integer 1, because all the higher bits are zeroes and in the lowest byte. the only bit set is the lowest bit and that corresponds to number 1. if we convert the number in the picture, to the decimal form, we will get 2886794753. now, what do you think will be the integer number corresponding to this ip? and the correct answer is 1168893508. now, here is the formula and the code to convert an ip address to an integer number. why is that? well, the lowest eight bits are in the fourth number of the ip address. so we use them without changing. the next eight bits, are in the third number of ip. but to use them, we need to move them to the left by eight positions in the binary form. and to do that, we need to multiply the corresponding integer number by two to the power of eight. the next eight bits are in the second number of the ip. and to use them we need to move them to the left by 16 positions in the binary form. to do that, we multiply the corresponding integer number by two to the power of 16, and so on. this gives us a one to one correspondence between ip address and integer number. now, we can rewrite the code for updateaccesslist using array a, instead of mysterious data structure c. and the only thing that changes is the incrementing and decrementing the counters. so when we need to increment a counter corresponding to the ip in the ith line. we first convert this ip to integer number from 0 to 2 to the power of 32 minus 1. and then we increase the entry of the integer ra a, add this index. note, that each ip is converted to its own integer number. so, there will be no collisions between different ip numbers. when we try to increment a counter for one ip number and by chance increment the current correspondent to another ip address. all ip addresses are uniquely mapped into integers from zero to two to the power of 32 minus one. we do the same thing when we need to decrement the counter. so basically, in the position in array a corresponding to any ip address, we will store the counter. which measures how many times this particular ip was accessed during the last hour. now, how to answer the question, whether this ip was or was not used during the last hour, to access your services. this is very easy. we first convert the ip to the corresponding position in the area a, and then we look at the counter this position. if the ip was used, then the counter will be more than zero. otherwise it will be exactly zero. so, now lets look at the asyptotics of this implementation. updateaccesslist is as fast as we can do. it is constant time per log line. because for each log line, we only look at some position in the array and increment it. and also increment some counter, or decrement some counter. accessedlasthour is also constant time. because the only thing we do is, we look at some position in their rate. which is a constant time impression and compare it with a zero, but there is a drawback. even if during the last hour, for example, in the night, there are only five, or 10, or 100 ips. from which your clients use the service. you will still need 2 to the power of 32 memory cells, to store that information. and in general, if you have for example, new ip protocol. ipv6, it already contains 2 to the power of 128 different ip addresses. and if you create an array of that size, it won't fit in memory in your computer. in the general case, we need o(n) memory, where n is the size of our universe. universe is the set of all objects, that we might possibly want to store in our data structure. it doesn't mean that every one of them will be stored in our data structure. but if we at least at some point might want to store it, we have to count it. so for example, if some of the ip addresses never access your service. you will still have to have a cell in your array for this particular ip, in the direct addressing method. so, this method only works when the universe is somewhat small. and we need to invent something else to work with the universes which are bigger than that or even infinite. such as, for example, the universe of all possible words, all possible strings, or all possible files on your computer. and we will talk in the next videos about that. 
hi, in this video we will study another approach to the ip addresses problem. in the last video we understood that the direct addressing scheme sometimes requires too much memory. and why is that? because it tries to store something for each possible ip address while we're only interested in the active ip addresses. those from which at least some user has accessed our service during the last hour. so the first idea for improvement of the memory consumption is let's just store the active ip's and nothing else. another idea is that if our error based approach from the last video has failed, then lets try to use list instead of an error. so let's store all the ip addresses which are active in a list. sorted by the time of access. so that the first element in the list corresponds to the oldest access time during the last hour, and the last element in the list corresponds to the latest, newest access from some ip address to our service. let's jump from here right into the pseudo code, because it's pretty simple. we're going to have our procedure update access list which takes in the log file log. it also takes in i which is the index of the first log line which hasn't been processed yet. and also it has input l which is the list and instead of some abstract data structure see from the first videos and instead of the area a from the direct addressing scheme. we put parameter l which is a list into this procedure and this is the list with active ip addresses. so our code have to pass first deals with new lines and second deals with old lines. we just go searching from the first unprocessed line. and if we need to added to our list because it was processed during the last hour, we just append it to the end of the list. and now again, the last element of the list corresponds to the latest, newest access from some ip address. and note that in our list we will start not just the ip address but, both ip address and the time of the axis. and then we will go to the next element in the log file and go and go while we still have some log lines which we need to add to the end of our list. and then the second part we just look at the oldest event during the last hour, which is corresponding to the first element of the list. and if that is actually before the start of the last hour, then we need to remove it from the list. and so we just do l.pop. and we do that while the head of the list is still too old. and when we stop, it means that all the elements in the list are actually with time during the last hour. why is that? because the list is always kept in the order by increasing time of access. when we add new log lines to the list. we add only those which have time even more than last element of the list currently, and we remove something from the list. we remove the oldest entries. so, all the entries are always sorted, and as soon as we removed everything from the start which is too old, all the entries in the list are not too old. they are made during the last hour. so this is pretty simple and now we need to answer questions like, whether my ip address was used during the last hour to access the service and how many times. to answer the first one we just need to find out whether there is an element in our list with the given ip address. and that is done by find by id, which is different from the standards find procedure of the least by the fact that we search not by the whole object, which is a log line, which contains both ip address and time. but we search just by the first field, by the ip address. so our list contains tuples of ip addresses and times of access, and we only look by ip address. but the implementation will be the same. we'll just go from the head of the list to the end of the list, and compare the ip field of the log lines with the ip address given as the input. and if it coincides we will return this element, otherwise we'll return that there is nothing with this ip address in the list. and the reason we return some special [inaudible]. so then, in the accessedlasthour, just compare the results with null. if it's not null then this ip address is in the list, otherwise it's not. and to count the number of times our service was accessed from a particular ip address, we just need to count the number of log lines in the list which have the same ip address. and that can be done by procedure countip of the list which again differs from the standard count procedure in the list by the fact that it counts by the first field, not by the whole object which is a log line. but it just goes from to the end of the list. compares the ip field with the given ip and if they coincide, it increases the counter by 1. and returns the counter in the end. so this is all the implementation. now let's analyze it. let n be the number of currently active ips, then the memory consumption is bigger of n. because we only store the active ip addresses and the corresponding times of x's, but the times of x's on the add constant memory per active ips. so it's all null linear in the number of active ips which is much better than the direct addressing scheme because it require an amount of memory proportionally to the number of all possible ip addresses. and here will only require amount memory proportional to the number of currently active ip addresses. what about running time? we know the standards list procedures such as append, top and pop all working constant time and that's why the updateaccesslist works in constant time per log line. of course, any particular call to updateaccesslist could take more than constant number of operations if we need to add more new lines to the end of the list or remove many many old lines from the start of the list. but for each log line we will only append it at most once and we will only removed from the beginning at most once. so it's constant time per log line plus constant time per each call of updateaccesslist just to check whether we need to append something and whether we need to remove something from the beginning. but this amount of operations can be controlled by how often do we actually call update access list. what about answering the questions? we know that find by ip and count ip have to go through the whole list in the worst case and actually count. ip has to go through the whole list all the time to find out how many log lines have the same ip as the given one and so accesslasthour and accesscountlasthour are both linear in the number of active ips. and that is actually now good because even without introducing any additional data structures, we could just take the log file, take the last line in it before the current time, and go back from it. and just look through each log line and compare its ip address with the ip address in the question. and count how many times it occurs during the last hour and just stop as soon as we go through the border of the last hour. and that will take the same time without any additional data structure. so this solution is not more clever than the trio approach. so, we failed somewhat with direct addressing scheme and we failed with this list based approach. it is overall a failure? well no, in the next videos we'll combine the ideas from direct addressing scheme with the list based approach. and we'll come up with solution which is both good in terms of memory consumption and is much faster than the trivial approach in terms of the running time. 
hi, in this video, you will learn what a hash function is, how could we apply it to solve our problem with ip addresses, and why it is not straightforward to make it to work. remember the direct addressing approach worked particularly fast, but it used a lot of memory that's because it encoded ip with numbers and those numbers were sometimes huge. so we had to create an array of size 2 to the power of 32 just to store all those numbers. what if we could encode our ip addresses with smaller numbers, for example, numbers from 0 to 999? we'll still need the code for different ip addresses, which are active currently to be different because we want a separate counter for each ip in our solution. let's define a hash function. so if you have universal object s for example. a set of all ip addresses or a set of all files stored on your computer or a set of all words or cures in the programming language, so that is our universe. and we will call it a set s. and now we want to encode each object from that universe with a small number. a number from 0 to m- 1 where m is a positive integer number. while any function, which encodes some object from s as a number from 0 to m- 1, is called a hash function. and m is called the cardinality of hash function h. so what are the desirable properties of the hash function in our problem? first, h should be fast to compute because we need to encode some object for each query. second, we want different values for different objects because we want a separate counter for each ip address in our problem from them. and also, we want to use direct addressing scheme because it was very fast, but we want to use a direct addressing scheme with a small amount of memory. and it's only logical to use in this case direct addressing scheme with o(m) memories. just create an area a of size m, and then encode each id with some value from 0 to m- 1, and store the corresponding counter in the cell of this array. the problem is that we want small cardinality m and it won't work if m is smaller than the number of different objects in the universe. because if we have for example 25 object in the universe and m is only 10, then at least two objects will have the same code from 0 to 9 because there are only 10 different codes and there are 25 different objects. so that won't work for all possible universes and for small m. in this situation, when the values of the hash function are the same, but the objects which are being encoded are different, is called a collision. so collisions cause us problems. because of collisions, we cannot just directly apply the scheme called direct addressing with o(m) memory. and in the next lecture, we will see how to overcome this problem. 
in this video, we will study chaining, which is one of the most frequently used techniques for using hashing to store mappings from one type of object to another type of object. so, let us define a map. we often want to store mapping from some objects to some other object. for example, i'm mapping from ip addresses to integer numbers. or from filenames to the physical location of those files on the disk. from student id to the name of the student. or from contact name in your phone book to the contact phone number. the general definition of a map from set of objects s to the set of values v is a data structure which has three methods. haskey, which tells us whether there is an entry in the map corresponding to object o from set s. method get, which returns to us the value corresponding to the object o, if there is one. if there is no such value, it returns a special value telling us that there is no entry corresponding to this object o in the map. and the last method is set, the most important method, which sets the value corresponding to object o to v. here, objects o are all from the set s and values v are from the set big v. we want to implement a map, using hash function, and some combination of ideas from direct addressing, and least based solution from one of the previous videos. so what we'll do is called chaining. we will create an array of size m, where m is the cardinality of the hash function, and in this case, let m be eight. this won't be an array of integers, though. this will be an array of lists. so in each cell of this array, we will store a list. and this will be a list of pairs. and each pair will consist of an object, o. and a value v, corresponding to this object. let's look at an example. for example, our objects are ip addresses, and the values are the corresponding counters. as in our initial problem about web service, and ip addresses of its class. now we're processing the log, and we see an ip address, starting with 173. and it so happens that the value of hash function on this ip address is four. then, we look at the cell four, the list there is now empty. but we append, in the pair of our ip address. and the corresponding counter one, to this list. the value is one because this is the first time that we encounter this ab. now we'll look at the next ip in the log. it starts with 69, and the hash value for this ip is one. so we'll look at the cell number one, and we append the pair of this ip address and the corresponding counter one to the list. again the counter is one because this is the first time we see this ip address. now it looks at the next ip address in the log and we see that it again starts with 173 and actually it coincides with the first ip that we've already seen. and the hash value is again four, because hash function is deterministic, it always returns the same number for same object. so we'll look at the cell number four, we'll look through the whole list and we find out that there is already a pair containing this ip address as the key. so instead of appending this ip address again to the list, we will increase the value of the counter by one because this is the second time we've seen our ip address. of course in the interface of a general map, there is no method for incrementing a counter, there is a method to set so we will need to first use method get to get the value corresponding to this ip address, we will get one. we will then increase it by one ourselves, get two. and then we will call set for this ip address and value two. and it will just rewrite the value from one to two in this list element. then, we'll look at the next line in our log, and we see that this is ip starting from 91. and it so happens that the hash value for this ip address, again, is four, although this is a different ip address. and that has to happen at some point, because there are many, many different ip addresses, and only eight entries in our array. so what do we do? if we look at the cell number four, there is a non-empty list there. we go through the whole list, but we see that our new ip address starting from 91 is not in the list. so we add our new ip address to the end of this list along with the corresponding counter of one. and these two ip addresses in the list for cell number four already make a chain together. and if we go further and further through the log, and we add some ip addresses to this map, some of the chains will become longer. if where some point we'll need to remove some ip address from the list we can do that and the chain can become shorter. but anyway, you see the general structure that a chain maybe empty, maybe non-empty, starts in any cell of the array. the array size is m, which is equal to the cardinality of the hash function. and for each such cell we store a list with all the ip addresses which occurred before and which have hash value the same as the number of the cell. 
how to implement this in code? well, let's just assume that we have a hash function h from the set of all possible objects s to the set of numbers from 0 to m-1. and let us denote by o and o prime objects from set s and by v and v prime values from set big v. and let us have an array a, which consists of m lists, where m is the cardinality of the hash function. and those lists we'll call also chains, and those chains consist of pairs of objects o and values v. now let us implement the first method, haskey, which would return whether there is an entry in our table or in our map for the object o. first, we compute the value of hash function on the object o. we'll look at the corresponding cell in the array a, and we'll take the list out from there. then we go through this list and when we go through it, we'll look at pairs o prime, v prime that are elements of this list. if for some pair o prime is the same as the object o for which we are looking, we return true because it means that there is an entry in our map corresponding to the object o. if we don't find any corresponding pair in the list, return false because that means there is no such object and there is no key corresponding to this object in our map. because it only could be in the list corresponding to the cell with number h of o and we didn't find it there. next let's implement method get, which should return the value corresponding to object o if there is one. otherwise, return some special value telling us that there is no entry corresponding to object o. again, we start with computing value of hash function on object o and looking at the cell number h of o in the array a and take the list, which is stored in that cell. then we again go through all the pairs in that list l, pairs o prime, v prime, and if for some of the pairs, o prime is the same as the object o for which we are looking, then we'll return the corresponding value v prime as the value corresponding to that object o. if we go through the whole list and we don't find corresponding player, we'll return special value n/a, which means that there is no value corresponding to object o in our map. why is that? because if there was some value, it has to be in the list corresponding to the cell number h of o because that's the way we store our chains, and if we didn't find it there, then there is no entry corresponding to the object o. now the last, most interesting method, set, which accepts two arguments, object o and the value v, which we need to set corresponding to this object. we need to either rewrite this value if there was already an entry corresponding to the object o with different value. or we need to create a new value in the map corresponding to the object o if it didn't happen to be in the map before. we again start with computing the hash function on the object o and looking at the corresponding cell in the array a and we'll take the list, we just start there. now we go through all the pairs p in that list l, and each pair p contains two fields, first field is p.o, which is the object of that pair, and p.v, which is the value of that pair. if for some pair, we see that the object of that pair is the same as object o for which we need to set the value v, then we just assign the value to the p.v, the new value. we will write the old value with the new value for that object o and then we return, we exit from the function. because we've already done everything we need. if we go through the whole list and we don't find any pair corresponding to our object o, it means that there was no entry in our map corresponding to the object o previously. and it means that we need to add a new pair to our list, and we just append a new pair containing object o and value v to the list l, corresponding to the cell number h of o. now let's look at the sm project of the chaining scheme. the first lemma says that if c is the length of the longest chain in a, then the running time of all three methods is theta of c+1. first, if we look at the list corresponding to some object o, the list in the cell number h of o, then the length of this list can be c, this can be the longest list itself. and if object o is not in this list and would call some of the methods for this object, we will need to scan the full list so we'll need to scan all c items in this list. also, if c is 0 so that our map is empty, our array a is comprised of m after this. we still need constant time to check that. so that's why c+1 and not just c. another lemma is talking about memory consumption. so let n be the number of different keys that we're storing in the map and m is the cardinality of the hash function. then the memory consumption is theta of n+m. that is very easy to prove. first, we need to store n pairs of objects and corresponding values in the map. that's where we get theta of n. and we get additional theta of m to store the array of m lists. although those lists can be empty, we'll still need to use some memory to store the pointers to the heads of those lists, and that's why memory consumption is theta of n+m. 
hi, in this video, we will finally start talking about hash tables. we will define what a hash table is and what we can do with it. in the last video, we've introduced the notion of map, and now we'll introduce a very similar and natural notion of a set. by definition, a set is a data structure which has at least three methods, to add an object to the set, to remove an object from the set, and to find out whether a given object is already in the set or not. one of the examples we already know very well, set of all ips through which clients access to your service during the last hour. this is an example with which we've worked for the last few videos. another example would be to store the set of all students currently on campus. and another one is to store all the key words of a given programming language so that we can quickly highlight them in the text editor, which you used to code. there are two ways to implement a set. one of them is when you already have an implementation of a map, you can base your implementation of set on the map. basically, you can set a map from all the objects s that you need to store in the set to the set of values, v, which only contains two values, true and false. if the object is in the set, then the corresponding value to this object will be true. if the object is not in the set, it is either not in the map or the corresponding value to it in the map is false. but that is not a very efficient way because we will have to store twice as much objects and values as we need. and also, when we remove objects from the set, it will be hard to remove them from the map. we will probably have to store them with value false, so there's a better way. we can again use chaining. but instead of storing pairs of objects and corresponding values in the chains, we'll just store objects themselves. let's see how can we implement that into the code. again, we'll have a hash function from all the objects s to the set of integer numbers from 0 to m-1. we denote it by o and o' objects from the set s, and we initialize array a with an array of size m which consists of lists or chains. and each chain consists of object o. initially all the chains are empty. when we need to find an object inside a set, we first compute the hash value of our object, we look at the corresponding cell in the array a. we take the list of objects from there, and then we go through the whole list and try to find object o there. if we find it, return true. otherwise, return false because our object o can be only in the list corresponding to the cell in the array a, number h(o). to implement add, we again compute value of hash function on object o, we take the list corresponding to this cell. and we go through this list, if we find our object o on this list, then we don't need to do anything because our object o is already in the set. otherwise, we append our object to the list corresponding to cell number h(o). to remove object from the set, we first try to find it in the set. if it's not in the set, initially we don't need to do anything. otherwise, we again compute the hash value of our object, take the corresponding list, and erase our object from that list. so, now we are ready to say what is a hash table? a hash table is any implementation of a set or a map which is using hashing, hash functions. it can even not use chaining. there are different ways to use hash functions to store a set or a map in memory. but chaining is one of the most frequently used methods to implement a hash table. we have a few examples of hash tables already implemented and built in our standard library types and programming languages, for example. set is implemented as unordered_set in c++, as hashset in java, as set in python. and map is implemented as unordered_map in c++, as hashmap in java, and as dict, or dictionary in python. why those types are called unordered in c++? you will learn in one of the next modules about data structures. for now, you just know that hash tables were already implemented in the main languages we used for the specialization. in conclusion, we've learned what is chaining. we've learned what is a hash table. and now we know that chaining is a technique that can be used to implement a hash table. we know that the memory consumptions for the chaining technique is big o(n + m) where n is the number of objects currently stored in the hash table. and m is the cardinality of the hash function. we also know that the operations with such a hash table implemented using chaining work in time c+1, where c is the length of the longest chain. now the question is, how to make both m and c small? why do we need that? because we want both small memory consumption and fast operations. for example, if m is very big, then we can use direct addressing, or something like that. but for some universes, some sets of objects, we will use too much memory, or we will have just too much overhead on top of our o of n memory which is needed to store n objects, anyway. if n is small, but c is big, well that's one different match from the list based approach where we used only o of n memory to store the list, to store only the active ips. but then we have to spend o of n time to actually look through all the list every time we want to make a query. so we want both m being relatively small and c. how can we do that? well, we can do that based on a clever selection of a hash function, and we will discuss this topic in the next lessons. 
hi, in the previous lesson you've learned what is a hash function, what is a hash table, and how to use those to implement data structures for storing sets of objects and mappings from one type of object to another one. however, the speed of this data structure depends a lot on the choice of hash function and in this lesson you will learn how to choose a good hash function. you will learn how to implement an efficient context book. and you will also learn how is hashing of strength objects in java implemented. we will start with the phone book problem. when you use your phone you want to be able to quickly look up a phone number of a person by name to be able to call him. and to determine who is calling you and to see not their phone number but their name if it's in your contact book. so, you need a data structure that is able to efficiently add and delete contacts from your phone book. to look up phone number by name and to do the reverse, look up the name given the phone number. to do that we will need two mappings, one from phone numbers to names, and another one from names to phone numbers. we will implement both of those maps as hash tables and we will start from the mapping from phone numbers to names. one approach that we know from the previous lesson is direct addressing. first, we'll need to convert phone numbers to integers and that is very easy to do. we'll implement simple function called int, as in integer that just deletes all characters of the phone number other then digits. and then you are left with an integer number like in this example. then we'll create an array called name, which will contain 10 to the power l cells, where l is the maximum allowed length of the phone number. that way it will be able to store a cell for each integer number from 0 to 999,999 and that 9 is going l times, where l is the maximum length of a phone number. so it will be basically enough to store each phone number of allowed length. and in this array, we'll store the names corresponding to the phone number. so to store a name corresponding to some phone number p, we will first convert p to an integer, and the store the name in the cell with this number. and if there is no contact with some particular phone number p, we'll just store a default value n/a in the corresponding cell. this is how it will look like. on the right is our array name, and on the left, we have two contacts. natalie with number 123-45-67 which is converted to 1,234,567 and is stored in the cell with this number in the array. it is somewhere in the middle of the array, there are a lot of cells before that. a few cells next to it are probably filled with default value n/a. because of course we have much less phone numbers in your phone book than 10 to the power of 7, which is 10 million. and then there is another contact of steve which is stored at position 2232323. and of course, there are more n/as in this array. so as we know operations in the direct addressing scheme work in constant time. however the memory consumption is exponential in this case. is big o of 10 to the power of l, where l is the maximum allowed phone number length. and that is problematic, because with international phone numbers, which can contain 12 digits or more for european countries, for example, we will need one terabyte, just to store one phone book, of one person. no smart phone is able to store a phone book of size one terabyte. and in the next video, we will suggest a scheme that avoids this problem with memory consumption. 
another scheme that we know from the previous lesson is chaining. to use that we first select the hash function with some cardinality m, then we create an array name again of size m. but instead of storing the names themselves in the array, we store chains or lists. and in these lists, we'll store both names and the phone numbers. and to determine where to put name and phone number, we first convert the phone number to integer, then we'll apply hash function to it and get a hash value. and put both name and phone number in the chain corresponding to the cell with such index. here is how it looks like. for example, we have a contact of steve and his phone number is 223-23-23. we first convert it to the number 2 million and a few thousand, and then we compute the hash value of this integer number and it turns out to be 1. then we put both steve's name and his phone number in the chain corresponding to the cell number 1 in our array name. then we do the same for natalie and her phone number. and it turns out that the hash value of the integer corresponding to her phone number is 6, so we put her contact in the 6th cell. and then we do the same for sasha, and the hash value of his phone number turns out again to be 1. so sasha gets in the same cell as steve. there are the following parameters of the chaining scheme. first, n is the total number of phone numbers stored in our phone book. m is the cardinality of the selected hash function, which is the same as the size of our area name, which works as a hash table. c is the length of the longest chain in our hash table. we use big o(n + m) memory to store the phone book. and also alpha, which is n/m, the number of phone numbers stored divided by the size of the hash table, which measures how filled up is our hash table, is called the load factor. and we will need it later. so we know that the operations with a hash table run in time big o(c + 1). and so we want both small m to use fewer memory and small c so that everything works faster. and here's a good example. we see a hash table of size 8 with a few chains and we see that the length of the chains are relatively the same, with the longest chain being of length just 2. so everything will work fast. and here's a bad example. when we again have hash table of size 8, but all the keys fell in the same cell 1. and they make up a very long chain of size m, in this case, it is 8. so this is what we want to avoid. let's try a few hash functions that come to our mind. first, let's select cardinality of 1000. and choose the first three digits as the hash value for the phone number. for example for this phone number it will be 800, because the first three digits are 800. however there is a problem with this hash function because the area code, which is the first three digits will be the same for many, many people in your phone book. probably because they live in the same city with you, and so they will have the same area code. and the hash values for their phone numbers will be the same, and they will make up a very long chain. another idea is to take the last digits, again the cardinality is 1,000, and we take the last three digits as the hash value. so for this number, it will be 567. but still, there can be a problem if there are many phone numbers in your phone book, which, for example, end in three zeros, or in some other combinations of three digits. so, another approach is to just select a random value as the hash function, a random number between 0 and 999. and then the distribution of hash values will be very good, probably the longest chain will be short. however, we cannot use such hash function actually because when we'll call the hash function again to look up the phone number we stored in the phone book we won't find it because we are looking in the wrong place. because the value of the hash function changed because it's not deterministic. so we learned that the hash function must be deterministic, that is return the same value if given the same phone number as the input each time. so good hash functions are deterministic. fast to compute because we do that every time we need to store something or modify something or find something in our hash table. and they should distribute the keys well in different cells and have few collisions. unfortunately, there is no universal hash function. most specifically, the lemma says that the number of all possible keys, the sizes of the universe with keys is large enough. much larger than the cardinality of the hash function that we want to use to save memory. then for any specific deterministic hash function there is a bad input, which results in many, many collisions. why is that? well, let's look at the universe u and select some cardinality for example, 3. then, our universe will be divided into three groups. all the keys that have hash value 0, all the keys that have hash value 1, and all the keys that have hash value of 2. now, let's select the biggest of those groups. in this case, it's the group with hash value of 1. this group will definitely be of size at least one-third of the whole universe and it can be even bigger. in this case for example, around 42%. and then, if we take all these keys or a significant part of these keys as an input, they will have the same hash value. and so, all of them will make collisions between themselves and they will form a very long chain in the hash table and everything will work very slowly. of course, if we change the hash function for this particular input, it will distribute the keys more uniformly among hash values. but for this particular hash function, this will be a bad input. and for any specific hash function with any cardinality, we'll be able to select a bad input this way. and in the next video, you will learn how to solve this problem. 
hi, in the previous video you learned that for any deterministic hash function, there is a bad input on which it will have a lot of collisions. and in this video, you will learn to solve that problem. and the idea starts from, remember when you started quicksort algorithm? at first, you learned that it can work as slow as m squared time. but then you learned that adding a random pivot to the partition procedure helps, because now you know that quicksort works on average in n log n time. and in practice, it works usually faster than the other sorting algorithms. so we want to use the same randomization idea here for hash functions. but we already know that we cannot just use a random hash function because it must be deterministic. so instead, we will first create a whole set of hash functions called a family of hash functions. and we'll choose a random function from this family to use in our algorithm. not all families of hash functions are good, however, and so we will need a concept of universal family of hash functions. so let u be the universe, the set of all possible keys that we want to hash. and then a set of hash functions denoted by calligraphic letter h, set of functions from u to numbers between 0 and m- 1. so hash functions with the same cardinality. such set is called a universal family if for any two keys in the universe the probability of collision is small. so, what does that mean? our hash function is a deterministic function, so for any two keys it either has a collision for those two keys or not. so, what does it mean that the probability of collision for two different keys is small? it means that if we look at our family calligraphic h, then at most 1/m part of all hash functions in this family, at most 1/m of them have a collision for these two different keys. and if we select a random hash function from the family with probability at least one minus one over m, which is very close to one, there will be no collision for this hash function and these two keys. and of course it is essential that the keys are different. because if keys are equal then any deterministic hash function will have the same value on these two keys. so, this collision property with small probability is only for two different keys in the universe, but for any two different keys in the universe this property should be satisfied. it might seem that it is impossible but later you will learn how to build a universal family of hash functions and practice. so how are randomization idea works in practice. one approach would be to just make one hash function which returns a random value between 0 and m-1, each value with the same probability. then the probability of collision for any two keys is exactly 1/m. but that is not a universal family. actually we cannot use this family at all because the hash function is not deterministic and we can only use deterministic hash functions. so instead, we need to have some set of hash functions such that all the hash functions in the set are deterministic. and then, we will select a random function h from this set of hash functions, and we will use the same fixed function h throughout the whole algorithm. so that we can correctly find all the objects that we store in the hash table, for example. so, there is a lemma about running time of operations with hash table if we use universal family. if hash function h is chosen at random from a universal family then on average the length of the longest chain in our hash table will be bounded by o(1 + alpha), where alpha is the load factor. load factor is the ratio of number of keys that we store in our hash table to the size of the hash table allocated. which is the same as the chronology of the hash functions in the universal family that we use. so, it makes sense. if the load factor is small it means that we only store a few keys in a large hash table, and so longest chain will be short. but as our table gets filled up, the chains grow. this lemma says, however, that if we chose a random function from a universal family they won't grow to much. on average, the longest chain will still be of length just (1 + alpha). and probably that is just a small number because alpha is usually below one, you don't want to store more keys in the hash table than the size of the hash table allocated. so alpha will be below 1 most of the time and then (1+ alpha) is just two, so this is a constant actually. so, the corollary is that if h is chosen at random from the universal family, then operations with hash table will run on average in a constant time. now the question is, how to choose the size of your hash table? of course, it control the amount of memory used with m which is your chronology of the hash functions and which is equal to the size of the hash table. but you also control the speed of the operations. so ideally, in practice, you want your load factor alpha to be between 0.5 and 1. you want it to be below 1 because otherwise you store too much keys in the same hash table and then everything could becomes slow. but also you don't want alpha to be too small because that way you will waste a lot of memory. if alpha is at least one-half, then you basically use linear memory to store your n keys and your memory overhead is small. and operations still run in time, o(1 + alpha) which is a constant time, on average if alpha is between 0.5 and 1. the question is what to do if you don't know in advance how many keys you want to store in your hash table. of course, there is a solution to start with a very big hash table, so that definitely all the keys will fit. but this way you will waste a lot of memory. so, what we can do is copy the idea you learned in the lesson about dynamic arrays. you start with a small hash table and then you grow it organically as you put in more and more keys. basically, you resize the hash table and make it twice bigger as soon as alpha becomes too large. and then, you need to do what is called a rehash. you need to copy all the keys from the current hash table to the new bigger hash table. and of course, you will need a new hash function with twice the chronology to do that. so here is the code which tries to keep loadffactor below 0.9. and 0.9 is just a number i selected, you could put 1 here or 0.8, that doesn't really matter. so first we compute the current loadfactor, which is the ratio of the number of keys stored in the table to the size of the hash table. and if that loadfactor just became bigger than 0.9, we create a new hash table of twice the size of our current hash table. we also choose a new random hash function from the universal family with twice the cardinality coresponding to the new hash table size. and then we take each object from our current hash table, and we insert it in the new hash table using the new hash function. so we basically copy all the keys to the new hash table. and then we substitute our current hash table with the bigger one and the current hash function with the hash function corresponding to the new hash table. that way, the loadfactor decreases roughly twice. because we added, probably just added one new element, the loadfactor became just a little more than 0.9. and then we increase the size of the hash table twice while the number of keys stayed the same, so the loadfactor became roughly 0.45, which is below 0.9, which is what we wanted. so to achieve that, you need to call this procedure rehash after each operation which inserts something in your hash table. and it could work slowly when this happens because the rehash procedure needs to copy all the keys from your current hash table to the new big hash table, and that works in linear time. but similarly to dynamic arrays, the amortized running time will still be constant on average because their hash will happen only rarely. so you reach a certain level of load factor and you increase the size of our table twice. and then it will take twice longer to again reach too high value of load factor. and then you'll again increase your hash table twice. so the more keys you put in, the longer it takes until the next rehash. so their hashes will be really rare, and that's why it won't influence your running time with operations, significantly. 
hi, in the previous video, you've learned the concept of universal family of hash functions and you learned how to use it to make operations with your hash table really fast. however, now we need to actually build a universal family and you will start with a universal family for the most important object which is integer number. because any object on your computer is represented as a series of bits or bytes, and so you can think of it as a sequence of integer numbers. and so first, we need to learn to hash integers efficiently. so we will build a universal family for hashing integers. but we will look at our example with phone numbers because we need to store contacts in our phone. so first, we will consider only phone numbers up to length seven and for example we will consider phone number 148-2567. and again, we'll convert all of those phone numbers, we want to start from integers from zero to the number consisting of seven nines. and for example, our selected phone number will convert to 1,482,567. and then we will hash those integers to which we convert our phone numbers. so to hash them, we will need to also choose a big prime number, bigger than 10 to the power of 7, for example, 10,000,019 is a suitable prime number. and we will also need to choose the hash table size which is the same as the chronology of the hash function that we need. so now that we selected p and m, we are ready to define universal family for integers between 0 and 10 to the power of 7 minus 1. so the lemma says that the following family of hash functions is a universal family. what is this family? it is indexed by p, p is the prime number, 10,000,019, in this case that we choose. and it also has parameters a and b, so those parameters are different for different hash functions in these family. basically, if you fix a and b, you fix a hash function from this hash functions family, calligraphic h with index p. and x is the key, it is the integer number that we want to hash, and it is required that x is less than p. it is from 0 to p minus 1, or less than p minus 1, but definitely, it is less than p. so, to create a value of this integer x with some hash function, we first make a linear transform of this x. we multiply it by a, corresponding to this hash function, and add b, corresponding to this hash function. then we take the result, modulo our big prime number p. and after that, we again take the result modulo the size of our hash table or the chronology of the hash functions that we need. so all these hash functions indexed by a and b will have the same chronology m. and the size of this hash family, what do you think it is? well, it is equal to b multiply by p minus 1, why is that? because there are p minus 1 variance for a, and independently from that, there are p variance for b. so the total number of pairs, a and b, is p multiplied by p minus 1, that is the size of our universal family. and the lemma states that it really will be a universal family for integers between 0 and p minus 1. we will prove this lemma in a separate, optional video. and here, we'll look at an example of how this universal family works. so, for example, we selected hash function corresponding to a = 34 and b = 2, so this hash function h is h index by p, 34, and 2. and we will compute the value of this hash function on number 1,482,567 because this integer number corresponds to the phone number who we're interested in which is 148-2567. well, remember that p that we chose is a prime number 10,000,019. so first, we multiply our number x by 34 and add 2, and after that, we take the result modulo b, modulo 10,000,019, and the result is 407,185. then we take this result and take it again modulo 1,000, and the result is 185. and so the value for our selected hash function on number x is 185. and for any other number x, you would do the same, you would multiply x by 34, add 2, take the result modulo b, then take the result modulo 1,000. and so any value of our hash function is a number between 0 and 999 as we want. and if we do different a and b, instead of 34 and 2, we'll just multiply x by different a, add different b. take a modulo b, take the result modulo m, and get the value for our hash function. so in the general case, when the phone numbers can be longer than seven, we first define the maximum allowed length, l, of the phone number. and again, convert all the phone numbers to integers which will derive from 0 to 10 to the power of l- 1, and then we'll hash those integers. to hash those integers, we'll choose a sufficiently large number p, p must be more than 10 to the power of l for the family to be universal. because otherwise, if we take some p less than 10 to the power of l, there will exist two different integer numbers between 0 and 10 to the power of l- 1, which differ by exactly p. and then, when we compute the value of some hash function on both those numbers and we take linear transformation of those keys, modulo b, the value of those transformations will be the same. and then when we take, again, module m, the value again will be the same. and that means that for any hash function from our family, the value of its function on these two keys will be the same. so there will be a collision for any hash function from the family, but that contradicts the definition of universal family. because for a universal family and for two fixed different keys, no more than 1 over m part of all hash functions can have collision for these two keys. and in our case, all hash functions have a collision for these two keys, so this is definitely not a universal family. so we must take p more than 10 to the power of l, and in fact, that is sufficient. then, we choose hash table of size m, and then we use our universal family, calligraphic h with index p. we choose a random hash function from this universal family, and to choose a random hash function from this family, we need to actually choose two numbers, a and b. and a should be a random number between 1 and p-1, and b should be an independent random number from 0 to p-1. if we selected those two numbers, we define our hash function completed. so now we know how to solve the problem of phone book in the direction from phone numbers to names. so we first define the longest allowed length of the phone number. we convert all the phone numbers to integers from 0 to 10 to the power of l -1. we choose a big prime number, bigger than 10 to the power of l. we choose the size of the hash table that we want based on the techniques you learned in the previous video and then you add the context to your phone book as a hash table of size m. hashing them by a hash function randomly selected from the universal family, calligraphic h with index p. and that is the solution in the direction from phone numbers to names. this solution will take bit of m memory, and you can control for m, and it will work on average in constant time if you select m wisely using the techniques from the previous video. and now we also need to solve our phone book problem in the different direction, from names to phone numbers. and that we will do in the next video. 
hi, in this video, you will learn how to prove the upper bound on the expected chain length in the hash table using chaining and a hash function from the universal hash family. this proof is needed to be confident that our hash tables will work fast in practice. the proof will use some advanced math, which we won't cover, but this video is optional. we will use probabilities, mathematical expectation, and the linearity of expectations. first, let us recall what is a universal family. so if u is the universe, the set of all possible keys that we want to store and calligraphic h is the set of function from this universe to the set of numbers from 0 to m- 1, where m is the selected cardinality of the hash function. then this set, calligraphic h, is called universal family, if for any two different keys from the universe, the probability of collision for a random hash function on these two keys is at most 1 over m. what do we mean by probability? it is taken over the random choice of a hash function from the set calligraphic h. or in other words, the reformulation of this definition is that for any two different keys, x and y from the universe, at most 1 over m part of all hash functions in calligraphic h produce a collision on these two keys. let's also recall what is a load factor. you have a hash table of size m, storing n keys. then n/m is denoted by alpha and called load factor. that measures how filled up is your table. if you store just one key in a hash table of size 10, then load factor is 0.1. and if you store nine keys in a hash table of size 10, then the load factor is 0.9. we'll also use the property called linearity of expectation. it says that for any finite set of random variables, x1 through xk, the random variable y, which is equal to sum of these random variables, has expectation which is equal to the sum of expectations of these random variables. we won't prove it, we will use it as a fact from probability theorem. now the main theorem is that if you select, at random, a hash function from a universal family, calligraphic h and it is used to hash n keys into hash table t of size m giving load factor alpha. then for any key k, the expected length of the chain in table t which contains key k if it stored in it, is at most 1 + alpha. what it means is that on average the length of the chain containing any key will be just 1 + alpha, which is not too long. of course, in the worst case, the chain containing some key can be very, very long such as n keys, and all of them are in the same chain. but, on average, if it's like the random hash function from the universal family, the chain length will be at most 1 + alpha, where alpha is the lowest factor. so to prove that, we fix some arbitrary key k and consider all other keys l and define a random variable xkl which takes value of 1 if the hash value of k is equal to hash value of l, if there's a collision between keys k and l. and it takes value zero otherwise. let's estimate the expectation of this random variable. it just takes two different value, so we need first to multiply, the first one is zero by the probability of zero, but we will anyway get zero. and we need to add 1 multiplied by the probability of 1, which is the same as the probability of collision and we know that for a universal family that probability is at most 1 over m. so expectation of xkl for k and any l different from k is at most 1 over m. now let's estimate the number of collisions, and we denote by yk the number of collisions that k has with different keys l in the table. and to do that, we can just sum variables xkl for all l which are different from key, and are in the table t. why is that? because when a collision happens for some key l different from k, variable xkl takes value 1. otherwise, it takes value 0. so the sum of all these variables is just the number of collisions with key k. then the length of the chain containing key k, potentially, in the table t, is 1. because this key itself could be in the table. plus the number of collisions with different keys, which is yk. so the expectation of the number of collisions is by the property of linearity of expectation, equal to the sum of expectations of variables xkl. and we have an upper bound on the expectation of each of those variables, which is 1 over m. so this sum is at most sum for all keys l different from k which are in the table t of value 1 over m. and there are at most n summands in this sum. because the size, the number of keys in the hash table t, is n. so at most n of them are both on the table and different from k. so this sum is at most n over m. and that is the same as alpha, the load factor. and so the expectation of the chain length is the expectation of 1 + yk, and by linearity, is the same as the expectation of 1 plus expectation of yk. but expectation of constant 1 is just 1, and expectation of yk we've just estimated from above, and so we have an upper bound on the expected chain length, which is 1 + alpha. so we proved our theorem. now here is a corollary from this theorem, which basically says that if you work with your hash table right, then your operations will on average run in constant time. more formally, if you do some n operations with your hash table which include insertions, deletions and modifications, but the total number of insertions is big o of m, where m is the size of the hash table. then, the total time to make all those operations will be big theta of n. well, of course it will be at least m to make n operations, but it will be at most proportional to m. and thus the amortized time for each operation will be big o of 1 on average. let's prove this corollary. we have big o of m insertions, so the total number of keys stored at any moment is, at most, big o of m. n is big o of m. and then alpha, which is m over m, must be big o of 1, must be constant. and then 1 +alpha, our upper bound on the expected chain length, is also big o of one. and it means that on average, the expected training time of the operation, is big o of one. and then when we sum the expectations for running time of each operation we get the expected running time of all n operations, again by linearative expectations. and so the expected running time of n operations is big theta of n, because it's big o of n, and also it is at least n. in conclusion, we've just proved upper bound 1 + alpha on the expected chain length. in the case when we use a hash table with chaining scheme and select a random hash function from a universal family. and we've also proven constant amortized expected running time for operations with a hash table, if you use universal family and chaining and you don't put too many keys on your hash table. basically if you keep your loss factor below 1, as we discussed in one of the previous videos, then your operations on average run in constant time. and in the next video, we'll also prove that the set of hash functions that was suggested as a universal family for integers is really a universal family. we will prove that formally. 
hi, in this video we will prove that the set of functions we suggested to you with integers is really a universal family of hash functions. this video will be heavy on math but it is optional. we will use properties of prime numbers of modular arithmetics. we will use the concept of one to one correspondence. properties of upper integral parts of real number and probabilities of course. so the theorem is that the set of functions was suggested polygraphic h with index b is the set of all hash functions indexed by b, a, and b where b is a prime number and a and b are parameters. and those parameters generate hash functions. so the formula for the hash function is on the side, and parameters a and b are from one to p-1 and from zero to p-1, respectively. so this is a family of hash functions which contains p multiplied by p-1, different hash functions for different bayers ab. so these functions is the universal family for the universe of keys, which are integers from 0 to p- 1. and of course, it will be also universal family for any sub-set of this universe. now, let's prove this theorem. to do that, we first need a lemma. so, select some hash function with parameters a and b from our set, a b. and fix some keys, x and y which are different, then, the intermediate values, which would compute while computing the hash value of keys x and y. so ax + b modular p and ay + b modular p, those are the intermediate values before we take the value modular m and get the actual value of the hash function from our family. so those values are always different for different keys in the universe. we will prove this lemma by contradiction. so suppose is equal to s or (ax + b) is equal to (ay +b) mod p. we subtract the right part from the left part and we see that a(x-y) is equal to 0 mod p. and that means that b divides product of a and x minus y. now recall that b is a prime number and so it means that either b divides a or that b divides x minus y. but a is between 1 and p minus 1 so a is a positive integer less than p and p is prime so p cannot divide a and thus p must divide x minus y. but x and y are between 0 and p minus 1. so their difference is, by absolute value, less than p. and p divides x minus y. so, the only value for x minus y, for which it is possible is 0. x minus y is 0. and so x is equal to y. but this contradicts our initial assumption that x and y are different keys. and so we prove by contradiction that r is not equal to s. there is a corollary from that that for hash function h which is without taking modular m, there are no collisions. and an important corollary is that for a hash function, which is before taking modulo m, there are no collisions for the universe of integer keys from 0 to p minus 1. now we need another lemma. that for any pair of different keys x and y, there is a 1 to 1 correspondence between pairs of numbers a and b which generally different hash functions in our family. and pairs of numbers r and s, which are the intermediate values in the computation of hash function of keys x and y. let's prove that. first know that different pairs a, b generate different pairs r, s. and that is because if we have some pair r, s we can uniquely solve for a and b module b with these formulas. so if there is any other pair, a prime b prime which leads to the same r and s, then actually a prime and b prime have to be equal to a and b respectively because of these equations. so now we know that different pairs a and b generate different pairs r, s. and we know that the total number of pairs a,b is p multiplied by p-1 because there are p-1 values for a and independently p values for b. so total number of pairs is p multiplied by p-1. and also, it turns out that the total number of pairs r,s is also p multiplied by p-1 because the total number of pairs r, s with values from 0 to p- 1 is p squared. but r must be different from s so we must subtract p from that, and that will be p squared- p which is the same as p multiplied by p- 1. so the number of pairs a, b is the same as the number of pairs r, s. and different pairs a,b led to different pairs (r,s) and that means that there is a one to one correspondence between these pairs and these pairs. and the corollary from that is that if we select some pair of different keys, x and y and we select any hash function at random. for all our family with equal probability, which is one of p multiplied by p minus one, because they're all, p multiplied by p minus 1, different hash functions in our family, then each pair of values are s. which are intermediate values, happen with equal probability, again, one over p multiplied by p- 1 so why is that? well we know that there's a one to one correspondence between pairs a, b and pairs r, s and selecting the hash function from our family is the same selecting random pair a, b we know that the probability of any pair a, b is one over p by p -1. and so the probability of any pair r,s is the same as the probability of the corresponding pair a,b. so it is always 1/p(p- 1). so we proved our corrollary. now let's prove the initial theorem. the probability of collision for a fixed pair of keys, x and y, which are different is the same as the probability that r module m is the same as s module m. we know that r is different from s, but they still can be equal module m because m can be less than pm. in most cases it is actually less than b. so we, again, know that each pair r, s has probability 1 over p by p minus 1. and we'll estimate the probability of collision using this fact. know that for each fixed r from 0 to p minus 1, there are at most. upper integral part of p over m, minus 1 such value of s that s is different from r and r modular m is the same as s modular m. so why is that? well, let's assume, without loss of generality, that r modular m is 0. and look at the roll of numbers from 0-p-1 than the numbers which have the same remainder module m as r are 0 m, 2m, 3m, 4m and so on up to a paranted real part of p over m-1 multiplied by m. we cannot multiply the next integer by m because that is already bigger than p minus 1, so in total there are part of pure m, different integers between 0 and p minus 1 which have the same remainder modular m as r. but, s also needs to be different from r. so, the number of such s', less by 1. and now, the probability of collision is equal to probability r module m, equal to s module m, and that is, at most, some of probabilities of all such pairs are s for which this equality holds. so we sum over all different values of r from 0 to p-1 and then we sum over all pairs which contain this r. and for which s module m is the same as r module m. we estimated that their at most integral part of p over m-1 such as for any fixed r. and the probability of any final pair of this r and any s with the same remainder module m is 1 over p by p-1 so the total probability is at most this sum. and the sum has exactly p and it has p in the denominator so we can just remove the sum and the denominator and we get upper integral part of p over m-1 divided by p-1. then we use the property of that upper integral part of p over m at most p plus m minus one over m. and then, when we make the common denominator, and remove all the things that cancel out, we see that it is equal to 1 over m. so we just proved that the probability of r modular m being the same as s modular m is, at most, 1 over m. and that proves that the probability of collision for some fixed pair of different keys, x and y is, at most, 1 over m. and that is the property of universal family that we needed to prove. so we finally proved our theorem. in conclusion, we've just proved that a suggested family is really a universal family for integers, but now the important thing that integers are not unbounded on the integers from 0 to p- 1. so if you need to hash really, really big integers, you will need to select a prime number that is bigger, even bigger than the largest of those numbers and then you will get universal family for that prime number and it will work for your integers. we also proved in the previous video the upper bound for expected chain length using universal family and constant amortized expected running time of operations with hash table, also provided in the last lecture. so now we have the whole picture. we have a universal family for integers. and we have proofs that if you use a universal family, that your operations work fast. so now we are confident that if you use the suggested family for integers, then the operations with the hash tables storing those integers will run fast in practice, at least on average. 
hi, in the previous videos, you've learned how to quickly look up name in your phonebook given the phone number. and we want to learn to solve the reverse problem given a name, look up a phone number of the corresponding person. to do that, we need to implement the map from names to phone numbers. and we can again use hash tables and we can again use chaining as in the previous sections. but we need to design a hash function that is defined names. and more generally, we want to learn to hash arbitrary strings of characters. and by the way in this video, you will also learn how hashing of strings and implemented in the java programming language. but first, let's introduce a new notation. denote by lsl enclosed in vertical lines the length of string s. for example, the length of string l"a"l is 1, length of string l"ab"l is 2, and length of string l"abcde"l is 5. so now how do hash strings? well when we're given a string, we're actually given a sequence of characters from s[0] to s length of s- 1. we number the characters of the strings from 0 in this lecture. and s[i] is an individual character that is in the i-th position in the string. i say that we should use all the characters when we compute our hash function of a string. indeed, if we don't use the first character, there will be many collisions. for example, if the first symbol of the string is not used, then the hash value of strings ("aa"), ("ba") and so on, up to ("za") will be the same. because however we compute the value of the hash function, it doesn't use the value of the first character. and if everything else in the strings stays the same, and we only change the first character that doesn't influence the value of the hash function then the value of the hash function must be the same. and so there will be a lot of collisions and we want to avoid collisions. so we need to use value of each of the characters. now, we could do a lot of things with that. for example, sum the values of all the characters or multiply them, but we'll do something different. well first, to even compute something on a string, we need to convert each character of the string to an integer code. for example, that can be ascii code or unicode, corresponding to that symbol on your computer, that doesn't really matter. and also we'll again need to choose a big prime number p, the same as we used in the integer hashing. so suppose we've chosen some big prime number p, now we introduce a new family of hash functions called polynomial family of hash functions. so calligraphy p is the family of hash functions which is index by small p, which is our big prime number. and also index by x, and x is a parameter which changes from 1 to p- 1. so the value of a hash function index by p and x on a string s, is the following sum. it is a polynomial sum where we multiply the integer quote corresponding to the ith character of s, which is noted by s of i, the same as the character itself. we multiply it by x to the power of i. we sum all these things up, and we take the value modular p. so this is a family of hash functions, and the chronology of all those hash functions is p. so any such hash function returns value from 0 to p- 1. and how many hash functions are there in this family? well of course, there are exactly p- 1 different hash functions, because to choose to define a hash function from this family you would just need to choose the value of x. and x changes from 1 to p- 1, and it's an integer number of course. so how can we implement a hash function from this family? s, the procedure polyhash which takes it's input string s, prime number p and parameter x, implements the hash function from our peril. it starts with the signing values of 0 to the result to the hash value will return to end. and then it will go from right to left in our string and compute new value based on the value of the corresponding character. and there is a formula in the code that does exactly that. and i will show you by example that what we get in the end by applying this formula is exactly what we want. so basically, we start with a hash value of 0, and then we start with i equal to 2 if the length of our string s is 3. we start with length of s- 1 which is 2. we have current value of hash = 0. so we multiply the 0 by x and get 0, then we add the value of s[i] which is s[2], and take it mod p. and so after first iteration of the for loop, we get s[2] mod p. what happens is the next iteration, that i is decreased and i is now 1. and we multiply the current value s[2] by x. and we add s[1], and take everything modular p. and what we get is the same as of s[1] + s[2] multiply by x modular p. and then the last iteration, i is decreased to 0. we multiply the current value by x. what we get is s[1] multiply by x + s[2] multiply by x squared. and then we also add s[0] to the sum and take everything modular p. and the result is s[0] + s[1] multiply by x + s[2] multiply by x2, exactly as we wanted. a polynomial hash function, with prime p and prime parameter x. and by the way, the implementation of the built in hash code methods in the class stream in java, is very similar to our procedure polyhash. the only difference is that, it always uses x = 31. and for some technical reasons, it avoids the modular p operator it just computes the polynomial sum without any modular division. so now you know how a function that is used probably trillions of times a day by thousands and many thousands of different programs, how this function is implemented. so now about the efficiency of our polynomial family. first, lemma says that for any two different strings s1 and s2 of length at most l + 1. if you choose a random hash function from the polynomial family by selecting a random value of x, parameter x from 1 to p- 1. you can select a random hash function from the family. so if you select a random hash from the polynomial family, then the probability of collision on these two different strings is at most l divided by p. so that doesn't seem like a good estimate because l can be big, but actually it is your power to choose p. if you choose very, very big prime number p then l over p will be very small. and know that it won't influence the running time of the polyhash procedure, because the running time of this procedure is big length of s. it only depends on the length of the string. it doesn't depend on the length of number p more or less. so if you select a really big number p, then the probability of collision will be very small and the hash function will still be computed very fast. the idea of proof of this lemma is that the equation polynomial equation of power l, modular prime number p has at most l different solutions x. basically, when we consider two strings s1 and s2. the fact that the hash value or some hash function from the polynomial family is the same for these two strings means that x corresponding to our hash function is a solution of this kind of equation. and the fact that strings are different makes sure that at least one of the coefficients of this equation is different from 0, and that is essential. if the strings were the same of course, the value of any hash function on them will be the same. but if they're different then the probability is at most l over p. because there are only l or less different x for which the hash function can give the same value on these two strings. 
>> now we know of polynomial hash family or hashing strings. but there's a problem with that family. all the hash functions in that family have a cardinality of p, where p is a very big prime number. and what we want is the cardinality of hash functions to be the same as the size of our hash table. so, once a small cardinality. so, we won't be able to use this binomial hashing family directly in our hash tables. we want to somehow fix the cardinality of the functions in the polynomial family. and a good way to do that is the following. we design a new complex transformation from strings to numbers, from zero, to m minus one. so, we select the cardinality m, and we want to design a function from strings to numbers, between zero, and m minus one. and, to that, we first apply our random hash function from the polynomial family to the string. and we get some integer number module p, and then we can apply a random hash function from the universal family for integers less than p, and get a number between 0 and m -1, if we select it from universal family from cardinality m. so, we now have a complex transformation which is two stage. first, take a stream and apply a random function from the polynomial family and then apply a random function from the universal family for integers to the result. and you get a number from zero to m-1 from the string. note that it is very important that we first select both random function from the polynomial family and the random function from the universal family of our integers. and we fix them, and we use the same pair of functions for the whole algorithm. and then, the whole function from string to integer number from between zero and minus one is a deterministic hash function. and it can be shown that the family of functions define this way is a very good family. it is not a universal family, but it is a very good family with [inaudible]. more specifically, if you take any two different strings s1 and s2 of length at most l + 1, and you choose a cardinality m, and you apply the process described to build a hash family from strings of length at most l + 1 to integers numbers between zero and m minus 1. then the probability of collision for random function from that family is at most 1 over m + l over p. so, that is not an universal family because for a universal family there shouldn't be any summon l over p the probability of collision should be at most 1 over m. but we can be very, very close to universal family because we can control p. we can make p very big. and then l over p will be very small. and so, the probability of collision will be at most will 1 over m plus some very small number. and so, it will be either even less than 1 over m or very close to it. so 1 ml hash, and then universal hash for integers is a good construction of a family of hash functions. a corollary from the previous lemma is that, if we specifically select the prime number p to be bigger than m multiplied by l, then the probability of collision will be, at most of 1 over m, so it won't be less than 1 over m itself, but it will be at most 1 over m multiplied by some constant. why is that? well, because if we rewrite 1 over m plus l over p by 1 over m + l over ml. then the second expression will be bigger because p is bigger than ml. and then it is equal is 2 over m which is big o(1 over m). so that way, we proved that combination of polynomial hashing with universal hashing for integers, is a really good family of hash functions. now what if we take this new family of hash functions and apply it to build a hash table? well, i say that for big enough prime number p, we'll again have running time on average c=o(1 + a). the length of the longest chain will be o(1 + a). where alpha is the lowest factor of our hash table. and so, by wisely controlling this size of the hash table on the lowest factor as we learned in the previous videos. we can control the running time and the memory consumption. of course, computing the hash function itself on sum string s is not a constant time operation, because the string can be very long. and we need to look through the whole string to compute our hash function. but in the case when the lengths of the strings in question are bounded, like for example with names definitely there are no names longer than a few hundred characters i think, so all they are bounded by some constant l. and so computing hash function on the names, tags, of course go off the length of the stream time, but it is also we go off constant time because l is a constant itself, and so we can implement a map from names to phone numbers using chaining, using the newly created family of hash functions, which is complex. it first applies polynomial hashing to the stream, to the name, and then applies universal family or integers to the result. so we can choose a random hash function from this two staged family. and store our names, and phone numbers in the hash table, using this hash function. in conclusion, you learned how to hash integers, and strings, really good, so that probability of collision is small. you learned that a phone book can be implemented as two maps, as two hash tables, one from phone numbers to names, and another one back, from names to phone numbers. and if you manage to do that in such a way you don't waste too much memory where all factors of your hash table is between one five and one, and search and modification, on average, work in constant time, which is great. and then the next lesson. we'll learn to apply hash functions to different problems such as searching for patterns in text. 
hi. in this lesson, you will learn about applications of hashing to problems regarding strings and texts. we will consider the problem of finding patterns in text. the problem is, given a long text t, for example a book or a website or a facebook profile, and some pattern p which can be a word, a phrase, a sentence. find all occurrences of pattern in the text. some examples of that can be that you want to find all occurrences were name on the website or you want to find all the twitter messages about your company to analyze the reviews of your new product. or, you could potentially want to detect all the files in your computer which are infected by specific computer virus and in that case you won't find letters in text, you will find code patterns in the binary code of the program. anyway the algorithm will be the same. first we introduce some new notations, substring notation, we denote by s from i to j the substring of string s, starting in position i and ending in position j. both i and j are included in the substring. for example, if s is the string abcde, then s from zero to four is the same string abcde because we index our characters from zero and a is the character number zero and e is the character number four. s from one to three is bcd because b is the character with index one and d is the character with index three. and s from two to two is also allowed. it's a sub-string of length one, string c. and i shouldn't be more than j of course because otherwise there is no sub-string from i to j. so, the formal version of our problem to find pattern in text is that you're given strings t and p as input and you need to find all such positions i in the text t that pattern p occurs in text t starting from position i. that is the same that to say that a substring of t from i to i plus length of t minus one, the substring of t starting from i with length equal to the length of the pattern is equal to the pattern. so we want to find all such positions i and, of course, i can be from zero to length of text minus length of pattern. it cannot be bigger because otherwise the pattern just won't fit in the text, it will be ending to the right from the end of the text. so we've start with a naive algorithm to solve this problem. physically we go through all possible positions, i from zero to difference of the length of the text and pattern. and then for each such position i would just check character by character, whether the corresponding sub string of t starting in position number i is equal to the pattern or not. if it is equal to the pattern we advance position i to the result. first we need to implement a function to compare two strings and we start with checking whether their lengths are the same or not of cvourse if the lengths of strings is different then the strings are definitely difference. if that's not the case, then the length of the strings are equal. and then we go through all the positions in both strings with i going from zero to length of the first string minus one. and if the corresponding symbols on the ith position differ, then the strings are different. otherwise they are the same. now we will use this function to find our occurrence of pattern in the text. the procedure find pattern naive implements our naive algorithm. so let's start with an empty list in the variable result and then we'd would go through all the possible positions where pattern could start with x for i from zero to lines of text minus length of the pattern and we check whether the substring starting in i with length equal to length of the pattern is equal to the pattern itself. if it is, then we append position i to the result because this is a position where pattern occurs in text and then, we just return the list that we collected by going through all possible positions of pattern in the text. i'd say that the running time of this naive algorithm is big o of length of the text multiply by length of the pattern. why is that? well, each call to the function areequal, runs in time big o, of length of the pattern, because both strings we pass there, are of lengths, the same as the length of the pattern. and, the running time of areequal, is linear. and then we have exactly length of t minus length of p plus one calls of this function, which total to big o of length of t multiplied by length of p, because we always consider that length of the text is bigger than the length of the pattern, and so this is the upper bound for our running time. actually, this is not just the upper bound, it's also lower bound. for example, consider, text t, which consists of many, many letters, a, and pattern p, which consists of many, many letters a, and then letter b in the end, and also we choose such text that it is much longer than the pattern which is basically almost always true in the practical problems. for each position i and t which we try to observe the goal to our equal to make has to make all of the maximum possible number of comparisons which is equal to the length of the pattern b. why is that? because one would call our equal for substring of t starting in position i and for the pattern b. we see that they differ only in the last characters so our equal has to check all of the previous characters until it comes to the last character of p and determines that actual pattern is different from the corresponding substring of d. last in this case the naive algorithm will do at least proportional to length of t multiplied by length of t operations. that's our estimate is not just big o, it is big letter which means that it is not only in upper bound but also a lower bound on the writing time on the naive algorithm. in the next video we will introduce an algorithm based on hashing which has better running time 
hi, in this video, we'll introduce rabin-karp's algorithm for finding all occurrences of a pattern in the text. at first it will have the same running time as the naive algorithm from the previous video. but then we'll be able to improve it significantly for the practical purposes. so we need to compare our pattern to all substrings s of text t, with length the same as the length of the pattern. and in the naive algorithm, we just did that by checking character by character whether pattern is equal to the corresponding substring. and the idea is we could use hashing to quickly compare p with substrings of t. so, how to do that? well, let's introduce some hash function h and of course if it is a deterministic hash function. and we see that the value of hash function on the pattern p is different from the value of this hash function on some string s. then definitely p is not equal to s, because h is deterministic. however if the value of hash function on p is equal to the value of hash function on s, p can be equal to s or it can be different from s if there is a collision. so to exactly check whether p is equal to s or not we will need to call our function areequal(p,s). and so this doesn't yet save us any time. but we hope that we could call this function areequal less frequently because there will be only few collisions. so we'll use polynomial hashing family. polygraphic p with index p small with some big prime number p. and if p pattern is not equal to s substring of text, then the probability that the value of the hash function on the pattern is the same as the value of hash function on the sub string is at most length of the pattern divided by our big prime number p. and we'll choose, a prime number p big enough, so that this probability will be very small. so here is the code, of our algorithm rabinkarp. it takes its input, text t, and pattern p. and it starts by initializing the hash function from polynomial family. we first choose a very big prime number p. we'll talk later about how to choose it, how big it should be. and we also choose a random number x between 1 and p- 1. choose the specific hash function from the polynomial family. initialize all our list of positions where pattern occurs in text with an empty list. we also precompute the hash value of our pattern, and we call the polyhash function to do that. and then we again need to go through all possible starting positions of pattern and text. so we go from i from zero to difference of the length of text and pattern. and for each i, we take the substring starting in this position i and of length equal to the lengths of the pattern, which is t from i to i plus length of the pattern minus 1. and you compute the hash value of this substring. and then we'll look at the hash of the pattern and the hash of the substring. if they are different, then it means that definitely, p is not equal to this substring. and so, p doesn't occur in position i and so we don't need to do anything in this iteration so we just continue to the next iteration of the loop without calling areequal. however, if has values phash and thash aren't equal, then we need to check if it's true that p is really equal to the substring of t starting in position i or it is just a collision of our hash function. and to do that we make a call to areequal and pass there the substring and the pattern. if areequal returns true, it means that pattern is really equal to the correspondence substring of texts, and then we advance position i to resolve. because pattern p occurs in position i in the text t. otherwise we just continue to the next situation of our for loop. so this more or less the same as naive algorithm, but we have an additional checking of hash value, and so we're not always calling areequal. we are calling areequal either if p is equal to the corresponding sub string of t or if there is a collision. let's estimate the running time of this algorithm. so first we need to talk about false alarms. we'll call false alarm the event when p is compared with a substring of t from i to i plus length of p minus 1. compared inside the areequal procedure, but pattern p is actually not equal to this substring. so there's a false alarm in the sense that p doesn't occur in the text t starting from position i, but we still called the areequal function. and we need to go character by character through p and the substring to test that they're actually not equal. so the probability of false alarm as we know from the previous lesson, is at most length of the pattern over prime number p, which we choose. so on average, the total number of false alarms will be the number of iterations of our for loop, multiplied by this probability. and so this total number of false alarms can be made very small if we choose prime number p, bigger than the product of length of the text, and length of the pattern. much bigger. so now let's estimate the running time of everything in our code except for calls to the areequal function. so the hash value of the pattern is computed in time big o of length of the pattern. hash of the substring corresponding to the pattern is computed in the same big o of length of the pattern time. and this is done length of text minus length of the pattern plus 1 times because that is the number of iterations of the for loop. so the total time to compute all those hash values is big o of length of text multiplied by the length of the pattern. now what about the running time of all calls to areequal? each call to areequal is computed in big o of length of the pattern because we pass there are two strings of length equal to length of the pattern. however, areequal is called only when the hash value of the pattern as the same as the hash value of the corresponding substring of t. and that means that either p occurs in position i in text t or there was a false alarm. and by selecting the prime number to be very big, much bigger than the product of the length of text, and the length of pattern, we can make the number of false alarms negligible, at least on average. so, if q is the number of times that pattern p is actually found, in different positions in the text t, then the total time spent in areequal, on average, is big o of q. which is number of times p is really found, plus the fraction t minus p plus 1 multiplied by p and divided by prime, p. which is the average number of times that a false alarm happens. so q plus number of false alarms is the number of times that we need to actually call function areequal. and then the time spent inside the function areequal is proportional to the length of the pattern. so, this is the same as the o of q multiplied by the length of the pattern, because the second summoned can be made pretty small, less than 1 if we choose big enough prime number p. and we'll only get the first summoned multiplied by the length of the pattern. and now the total running time of the rabin-karp's algorithm in this variant is big o length of text multiplied by length of pattern plus q multiplied by the length of pattern. but, of course we know that the number of times that pattern occurs in text is not bigger than the number of characters, in text. because there are only so many different positions where the pattern could start, in text. so, this sum is dominated by the sum of big o of length of text, multiplied by length of the pattern. so, this is basically the same running time as our estimate for the naive algorithm. so we haven't improved anything yet, but this time can be improved for this algorithm with a clever trick. and you will learn it in the next video. 
hi, in this video you will learn to significantly improve the running time of the rabin-karp's algorithm. and to do so we'll need to look closer into the polynomial hashing and its properties. recall that to compute a polynomial hash on the string s but first choose a big prime number for the polynomial family, then we choose a random integer x from 1 to p minus 1 to select a random hash function from the family. and then the value of this hash function is the polynomial of x with coefficients which are characters of the string s. and to compute this hash functional substring of text t starting in position i and having the same length as the pattern for which we are looking in the text. we need to also compute a similar polynomial sum. it goes from character number i to character number i plus length of the pattern minus 1. and we need to multiply each character by the corresponding power of x. for example t of i will be multiplied by x to the power of zero because this is the first character of the substring and the last character will be multiplied by x to the power length of the pattern minus 1, and here is a formula on the slide. and the idea for the improving of the running time is that the polynomial hash value for two consecutive substrings of text with length equal to the length of the pattern are very similar and one of them can be computed given another one in constant time. we introduce a new notation, we denote by h[i]. the hash value for the substring of the text starting in position i and having the same length as the pattern. now let's look at the example, our text is a, b, c, b, d. and we need to convert the characters to their integer codes. and let's assume for simplicity that the code for a is zero, for b is one, for c is two, and for d is three. then our text is actually 0, 1, 2, 1, 3. also, we will assume in this example, that the length of the pattern is three. we don't need to know the pattern itself, we just fix its length. so we will need to computer hash values for the substrings of the text of length three. there are three of them, abc, bcd, and cbd. we start with the last one, cbd. to compute its hash value, we first need to write down the powers of x under the corresponding characters of the text. then we need to multiply each power of the x by the corresponding integer code of the character and we get 2x and 3x squared. and then we need to sum them and we also need to take the value module of b, but on this slide we'll just ignore module of p, it will be assumed in each expression. now let's look at the hash value for the previous substring of lines three which is bcb. we again need to write down the powers of x under the corresponding integer codes of the character. and again need to multiply the powers of x by the corresponding integer codes and get one to x and x squared, we need to sum them. now note the similarity between the hash value for the last substring of line three and the previous substring of line three. to get the last two terms for bcb, we can multiply the first two terms for cdb by x. and we will use this similarity to compute the hash for bcb given the hash for cdb. so again h[2] is the same as hash value of cbd because it starts in the character with index two and it's equal to 2 + x + 3x squared. now let's compute the age of 1 based on that this is the hash value of bcb and we know it's equal to 1 + 2x + x squared module of p. now let's rewrite this using this property of multiplication by x the terms for the cbd. so it's equal to 1 + x multiplied by the first two terms for cbd which are 2+x. now we don't want to use just the first two terms for cbd, we. we want to use the whole cbd so we write this as following 1+ x multiplied by the whole expression for cbd but now we need to subtract something to make the equality true. and that something is the last term, x multiplied by 3x squared, which is the same as 3x cubed, so we subtract 3x cubed. now we regroup the summons, and we right as this is equal to x multiplied by the hash value for cbd which is big h[2], we add 1 to it and we subtract through 3x cubed. in the general case, there is a very similar formula. so, here is the expression for big h[i + 1], and notice that the powers of x are, in each case j- i- 1 because the substring starts in position i plus one. so, we subtract i + 1 from each j in the sum, and the expression for big h[i] is very similar. but, for each power of x, we subtract just i from j. because the substring starts in position i. now let's rewrite this expression so that it is more similar to the gauge of i + 1. and to do that, we start summation not from i, but from i + 1 and also end it one position later. so, the first sum is now very similar to the expression for h[i+1], which has the powers of x are always bigger by one. and also we need to add t[i] which is not accounted for in the sum, and we need to subtract its last term, because it's not in the expression for big h[i]. and that is t[i] plus length of the pattern, multiplied by x to the power of length of the pattern. now we notice that the first sum is the same as x multiplied by the value of hash function for the next substream, big h[i+1]. and the second and third terms are the same. so now we get this recurrent formula. to compute the gauge of i, if we know already the gauge of i + 1, we need to multiply it by x and then add t[i] and subtract another term. notice that t[i] and t of i plus length of the pattern we just know. and x to the length of the pattern is a multiplier that we can pre compute and use for each i. now let's use this in the pseudo code. here's the function to pre compute all the hash values of our polynomial hash function on the substrings of the text t with the length equal to the length of the pattern, and with prime number, p and selected integer x. we initialize our answer, big h, as an array of length, length of text minus length of pattern plus one. which is the number of substrings of the text with length equal to the length of the pattern. also initialize s by the last substring of the text with a length equal to the length of the pattern. and you compute the hash value for this last substring directly by calling our implementation of polynomial hash with the substring prime number p and integer x. then we also need to precompute the value of x to the power of length of the pattern and store it in the variable y. to do that we need initialize it with 1 and then multiply it length of p times by x and take this module of p. and then the main for loop, the second for loop goes from right to left and computes the hash values for all the substrings of the text, but for the last one for which we already know the answer. so to compute h[i] given h[i + 1], we multiply it by x. then we add t[i] and we subtract y, which is x to the power of length of p, by t[i + length of the pattern]. and we take the expression module of p. and then we just return the array with the precomputed values. so to analyze its training time, we know that initialization of array h of s and with the accommodations with the hash value of the last substring, i'll take time proportional to the length of the pattern. also pre-computation of the x to the power of length of p takes time proportional to the length of the pattern. and the second for loop takes time proportional to length of the text minus length of the pattern. and all and all it's big o of length of the text plus length of the pattern. now again, polynomial hash is computed in time proportional to the length of the pattern. first for loop, computing the power of x, also and the second for loop, which goes through all the substrings of the text with length equal to the length of the pattern, x length of text minus length of pattern time. and the total precomputation time is proportional to the sum of length of the text and the pattern. and in the next video we'll use these precomputed values to actually improve the running time of the rabin-karp's algorithm. 
hi, in this video we'll use the precomputed hashes from the previous video to improve the running time of the rabinkarp cell algorithm. and here is the pseudo code. actually it is very similar to the pseudo code of the initial rabinkarp algorithm and only a few lines changed. so again, choose a very big prime number p and we choose a random number x from 1 to p- 1 to choose a random hash function from the polynomial family. we initialize the result with an positions. and we compute the hash of the pattern in the variable phash directly using our implementation of polynomial hash. and then we call the precomputehashes function from the previous video to precompute big h, an array with hash values of all sub strings of the text with length equal to the pattern p. we need them to check whether it makes sense to compare pattern to a sub string if their hashes are the same. or maybe if their hashes are different then, there is no point comparing them character by characte,r because it means that pattern is definitely different from the substream. so, then our main for loop goes for all i, starting positions for the pattern, from 0 to length of text minus length of pattern as in the previous version of the rabinkarp's algorithm. and the main thing that changed is that. we compare the hash of the pattern, not with a vary of hash functions computed on the fly, but with the pre-computed value of the hash function for the substream starting in position i, h[i]. if they are different, it means that the pattern is definitely different from the substream starting in position i and we don't need to compare them character by character, so we just continue to the next iteration of the for loop. otherwise, if the hash value of the pattern is the same as the hash value of the substring, we need to actually compare them on the quality, character by character. and to do that, we call function areequal for the substring and the pattern. if they are actually equal, we append position i to the result to the list of all the occurrences of pattern indexed. otherwise, we proceed to the next iteration. and in the end, we return result, the list of all positions in which pattern occurs in the text. let's analyze the running time of this version of rabinkarp's algorithm. first we compute the hash value of the pattern in time proportional to its length. then we call the precomputehashes function, which we estimated in the previous video around in time proportional to the sum of length of text and the pattern. and then the only other thing that we do is we compare the hashes, and for some of the substrings we call function areequal. and we already know from the previous videos that the total time spent in areequal is on average, proportional to q multiplied by length of the pattern, where q is the number of occurrences of pattern and text. why is that? because we only compared pattern to a substring if they're equal or if there's a collision. you can compare such a big prime p, that the collisions have very low probability, and on average, they won't influence the running time. so, on average, total time spent in areequal is proportional to q multiplied by length of the pattern. and then the total average running time, is proportional to length of the text, plus q plus 1, multiplied by length of the pattern. and this is actually much better, than the time for the algorithm, because usually q is very small, q is the number of times you actually found pattern in text. if you are, for example, searching for your name on a website or for infected code pattern in the binary code of the program, there will be no or only a few places where you actually find it. and that their number is q and it is usually much, much less than the total number of positions in the test which is length of the test. so the second sum of [q plus 1 multiplied by 1 looks like by length of the pattern is much smaller than length of the text multiply it by length of the pattern. and if pattern is sufficiently long, then the first summoned is also much smaller than length of the text multiplied by length of the pattern. so we improved our running time for most practical purposes very significantly. of course it's only an average, but in practice, this will work really well. and to conclude. in this module, we cited hash tables and hash functions, and we learned that hash tables are useful for storing sets of objects and mappings from one type of object to another one. and we managed to do it in such a way that you can search and modify keys and values of the hash tables in constant time on average. and to do so, you must use good hash families, and you must select random hash functions from good hash families. and you also learned that hashes are not only useful for storing something, but they're also useful while working with strings and texts, for finding patterns in long texts. and actually, there are a lot more applications of hashing in distributed systems, for example, and in data science. and i'll tell you about some applications and distributive systems in the next few optional videos. 
hi, in this optional lesson we will learn a bit about more than distributed systems. and we will start with some interesting inner working online storage services which you probably use such as dropbox, google drive and the yandex disk. have you wondered how a very big file of tens or hundreds of megabytes can be uploaded almost instantly to your dropbox account? or maybe your interested, how dropbox, google drive and yandex disk save petabytes of storage space using the ideas from this module on hash tables and hash functions. or maybe you're interested in distrusted systems and distributed storage in general. then, this lecture is for you. so services like dropbox and google drive used extra bytes of storage to store data of millions and millions of users worldwide. and there's a very simple idea on how to actually save some of that space and save some of the cost so it sometimes happens that users upload the same files. the first user liked the video with the cats and uploaded it to his dropbox account just to save it and to show his friends. and then another user also loved this video file. he may have called it different way but still uploaded it to his dropbox account, the exactly same video. and then another user also uploaded this video, because this was a viral video and many, many people liked it and some of them decided to upload it to their user accounts in dropbox. and then what we can do on the level of the whole dropbox service is instead of storing all three copies of the same video, just save one copy and have links from the user's files to this actual, physical, stored file. and then we've just saved 66% of the storage space because we basically reduced three times. and if you have some large videos which are also very popular that you can save this way significant portion of this storage space which all the users collectively use in dropbox to store their files. so the question is how to actually implement that. so, when you do a new file log, you need to determine if there is already the same file in the system or now, and if there is, you just ignore the plot, and sent a link to the register's file in the user's account, instead of a real file. so, there are a few ways to do that, and we'll start with a really simple one. naive comparison. you take the new file that the user wants to upload. you actually upload it to a temporary storage, then you go through all the storage files, then you compare the new file with each of the storage files, bye, bye, bye. and if there is exactly the same file, you store a link to this file instead of the new file that user's wants to upload. so, there are a few drawbacks of this approach. first, you have to first, upload the file anyways. so you won't see this miraculous instant upload time of large files with hundreds of megabytes. and second is to compare a file of size s with n other files, it takes time proportional to product of n and s. and that can be huge because the number of files in dropbox or google drive is probably on the order of hundreds of billions or even trillions. and the files uploaded are often also very large like gigabytes. and also, if we use the strategy, then, as n grows, as service for online storage grows, the total running time of all uploads will grow as n squared because each new upload is big o(n) and it's longer and longer and longer as the number of files increases. so, this approach won't work long-term anyway. so what can we do? first idea is, instead of comparing the files themselves, try to compare hashes. as in the rabin karp's algorithm, compare hashes of the files first. if the hashes are different, then the files are definitely different. and if there is a file with the same hash, then upload this new file that the user wants to upload to his account, and compare the new file with the old file with the same hash directly byte by byte. still, there are problems with this approach. first, there can be collisions so we cannot just say that if two files have the same hash value then they're equal and we don't need to store the new file. sometimes, different files can have the same hash value and we'll still have to compare the two files, and also we still have to upload the file to compare directly even if the same file's already stored. and we still have to compare with all n already stored files. so what can we do? another idea is we can use several hash functions. if we have two equal files, even if we compute five different hash functions there values on these two files will be the same. so the idea is choose several different hash functions independently for example, take functions from polynomial family with different multiplier x or with different prime numbers p. and then compute all the hashes for each file and if there is a file which is already stored and has all the same hash values, then the new file is probably the same as the file already stored. in this case, we might want to not even upload the new file at all and save the time and make the upload seem immediate. and to do that we need to just compute hashes locally before upload and only send through the network, which can be slow, the variants of the hash functions, which are much, much less in terms of space than the initial huge file. so we can do the hash values locally. we send those three or five hash values over the network to the service. they're compared to the hash values of the files already stored. and if there is a file with same set of hash values, we don't upload our new file. and this is how the instant upload works sometimes. when you try to upload file which is already stored but by someone else. well of course, there is a problem with collisions. because collisions can happen even if you have several different hash functions. still, there can be two different files which have the same set of hash values even for several hash functions. and there are even algorithms which on purpose find two different files which have the same value of a give hash function. if you know for which hash function you are trying to find a collision. however, for hash functions used in practice, collisions are extremely rare and hard to find. and if you use more than one hash function, if you use three or even five then you probably won't see a collision in a life time. so this is actually done in practice. you compute several different hash functions which no body knows and then it is so hard to find two files for which all the hash functions have the same values. that a new file is considered to be equal to the old stored file if all the hash values coincide with the hash values of the file already stored. so we still have an unsolved problem that we need to do and comparisons with all the already stored files. so how can we solve this problem? well, we can first precompute hashes because when a file is submitted for upload, hash values for this file are computed anyway. so we can store the addresses of the files which already stored in the service in a hash table and along with the addresses of the files will store those hash values for each file. so, we recompute them and store them and when we need to search for a new file we actually only need to search in the hash table, and we need only the values of the hash functions on this file to search for it. we don't need to provide the file itself. so we search for the hash values in the hash table. and if we find some files stored in this hash table with the same set of hash values, then we know that there is already such files stored in the system. so the final solution is the following. we choose from three to five different good hash functions, for which it is hard to find solutions. so that we don't see collisions in practice. we store the addresses of the files and the hashes of those files in a hash table, and before we upload a new file, we compute the hashes locally, we send them over the network to the service. we check whether there is a file in a hash table with the same hash values. and if all the hashes for some stored file coincide with the hashes of the new file then the search is successful. and in this case we don't even upload the file, we just store a link in the user account to the existing, already stored file. so this how we can do instant upload to dropbox or google drive for [inaudible] this, and this is actually how they save a lot of space, probably petabytes of space in their services. however, there are more problems to this, because it turns out that billions of files are uploaded daily, for example, into dropbox. and that means that probably around trillions are already stored there. and that is just too big for a simple hash table on one computer. and also, millions of users upload simultaneously their files, and so this is also too many requests for a single hash table. and so you need some more sophisticated solution to cope with this those two things. and see our next lecture to understand how that problem is solved. 
hi. in this video we will learn how to store a whole lot of objects, how to store big data, using distributed hash tables. so big data is when you need to store trillions or more objects. for example, trillions of file addresses in dropbox, or user profiles, or emails and user accounts, for example, in gmail or services like that. and you need fast search and fast access to that data. so hash tables, in general, is a good solution for that problem because they give a constant time search access on average. but for number of keys in the order of ten to the power of 12, the amount of memory that a single hash table will store becomes too big to store it in one computer and so we need to do something else, we need to use more computers probably. and the solution to this is distributed hash tables. so the first idea for distributed hash table is the following, just get more computers. get 1,000 computers. if you are google or dropbox you can do that. and then you will store your data on many computers. and you will do the following. you create a hash table on each of those computers. and then you will separate the data between those computers. so each computer will store its own part of the data. and you need to determine quickly, and automatically, and deterministically which computer should store some object o. and there is a simple way, just compute some hash function of this object, modular 1000, so we get basically a value from 0 to 999 for each object and that will be the number of the computer which should store this object. and then you send a request to that computer and search or modify what you need to do with that object in the local hash table of that computer. and that seems to already solve our problem because if a new request comes you quickly compute the hash function on the object and you know where to send your request. and then that computer just looks up in its local hash table. each of the local hash tables can be 1,000 times less than the total amount of data stored, and so it is scalable. if you need more data, you just get more computers and everything works. still there are problems with this approach. and the main problem is that computers sometimes break. and especially if you have a lot of computers, then they break pretty often. for example if a computer breaks once in two years on average, then if you have 1,000 computers, on average, more than one computer breaks every day. because there are less than 1,000 days in two years, and you have 1,000 computers. so what do you do in that case? you don't want to lose your user's data. so you need to store several copies of the data. so basically you can do it in a way that every computer stores each part of data. each part of data should be stored on several computers. and what happens then when some computer breaks? well, luckily the data which is stored on this computer is also stored somewhere else. but if that's the only copy left after this computer broke, you also need to also copy that data to some other computer, so that it is again stored in several places. and you need to relocate the data from the broken computer and also sometimes your service grows and you want to buy more computers. you want to reply faster to your clients and new computers are added to the cluster. and then this formula take hash value of the object modular 1000. and this is the number of computer on which your object is stored. it no longer works. because the numbers of the computers always change. new computers come in. broken computers come out. and so you need something else. and one way to solve this is called consistent hashing. so first, we choose some hash function with sum cardinality m. and we choose a circle, a regular circle, and you put numbers from zero to m minus one on the circle in a clockwise order. and then each object, o, is mapped to some point on the circle corresponding to the number hash value of this object. which is from 0 to m- 1, so it always maps to some of the numbers on the circle. and also, each computer id is mapped to the same circle. we hash the id of the computer and we get the number of the points to which this computer is mapped. so let's look at the picture. here's our circle. and, for example m is 12. then we put 12 points around the circle. and we put numbers from 0 to 11 around the circle. and then, objects, such as for example, name steve, can be mapped to some of those 12 points. and if hash value of steve is 9, then steve is mapped to the point with number 9. and also computers can be mapped to points, and for example, this computer with id 253. if the hash value of 253 is 5, then this computer is mapped to the point 5. so what do we do then? we make a rule that each object is stored on the so-called closest computer, closest in terms of the distance along the circle. and in this case, each computer stores all objects falling on some arc, which consists of all objects which are closer to this computer than to any other computer. let's again look at the picture. this is the circle and there are six computers and these computers mapped to some points on this circle. and then the arcs of the same color as the computers near them, are the sets of points, which are closer to the corresponding computer than to any other computer. and so each computer is responsible for some arc of this circle. for all the keys that are mapped to this arc. and so what happens when computers come in because new computers are bought or when computers are broken. when a computer goes off when it is broken, it's neighbors take its data. so it has two neighbors, and it's arc is divided into parts, and one part goes to the right neighbor and the, another part goes to the left neighbor. and when a new computer is added it takes data from its neighbors. so it comes between some two already existing computers, and it takes a part of the arc of one of them, and a part of the arc of another one, and he gets its arc. so let's look at an example. for example, the yellow computer breaks and it goes away. and then the green and the blue computer will take its arc and divide it between themselves. so that's what happens. another problem which still needs to be solved is that when some computer breaks, we need to copy or relocate the data. and how will a node, a computer, know where to send the data that is stored? well, we need another rule for that. we cannot store the addresses of all the other computers on each of the computers because that is inconvenient. we will have to constantly update that information. but, each node, each computer will be so called acquainted with a few neighbors. so it will store the metric addresses of some of its neighbors. the rule is that for any key, each node will either store this key itself, or it will be acquainted. it will know some other computer which is closer to this key in terms of the distance on the circle. and, that way, if a request comes to some node, any node in the network, about some key, it either can find this key inside it's own storage, or, it will redirect the request to another node which is closer to this key. and that that node will either store the key, or direct the code to the next node, which is even closer to that key. and in finite number of iterations the request will come to the node that will actually stores the key. so that's the idea. and in practice, what we can do is we can put the computers, the nodes on the circle. and then each node will know its immediate neighbors, its neighbors of neighbors. and then its neighbors in distance of 4 and distance of 8, and distance of 16. and for all powers of 2 it will know neighbors to the right and to the left at distance of this part 2. of course less than n over half. and it's easier to see on the picture again. so suppose we have many, many nodes. and then the upper node will have links to its right and left neighbor. to its right and left neighbor on distance of two, and to its right and left neighbor, the distance of four, and so on. so each node will contain algorithmic number of links to other nodes, which is much better than storing all the other nodes. and, if we need to come to some key from some node that doesn't contain it we'll first jump in the direction where the distance to the key decreases. and we will jump as much as we can. if the computer at distance eight is closer than our computer to the key, we will jump at least by eight. if computer with distance 16 is closer, we'll jump at least 16. if computer with distance 32 is farther, then we'll jump just by 16. in this way, we will always jump by at least a half of the distance which divides us from computer that stores the key itself. and so in algorithmic number of steps, we will actually come from the current computer, to the computer that actually stores our key. and this network of nodes which know some neighbors and they know some of their neighbors is called, overlay network. so in conclusion, distributed hash tables is a way to store big data on many many computers and access it fast, as if it was on one computer. consistent hashing is one way to determine which computer actually owns the data, which computer stores this particular object. and to do that, consistent hashing uses mapping of keys and computer ids on a circle. and each computer stores a range of keys on an arc, which is closest to this computer in terms of distance along the circle. and also overlay network is used to route the data to and from the right computer. so when a computer is broken, first, its data needs to be copied to some other computer. and its neighbors take its data. so computer disappears, and its arc disappears, but this is actually divided between two neighbor computers. and each of those arcs increases a bit, and they cover the whole data and then we proceed. if a new computer appears, it takes some data from its right neighbor, some data from its left neighbor, and assembles an arc for itself. and i hope that after this lecture, you understand how important are data structures we study in this course, to the modern technological industry, distributed systems, and big data. 
hello everybody, welcome back. today, we're going to be starting with a new data structures topic. in particular, we are going to be talking about binary search trees. and today, we're going to be giving some of introductions to the topic and really going to try and do two things. one is to sort of motivate the types of the problems that we want to be able to solve this new data structure. and secondly, we'll talk a little bit about why the data structures we already know about are not up to this task and why we really do need something new. so to begin with let's talk about a few problems that you might want to solve. so one is you want to search a dictionary. you've got a dictionary and you want to find all the words that start with some given string of letters. or similarly you've got a bunch of emails and you'd like to find all the emails that were sent or received during a given period. or maybe you've got a bunch of friends or class or something and you'd like to find the other person in this class whose height is closest to yours. now all of these are examples of what we might call a local search problem. what you want for them is you have a data structure that stores a bunch of elements. each of them has some key that comes from a linearly ordered set. something like a word sorted by alphabetical order, or a date, or a height, or something like that. and we want this data structure to support some operations. things like range search, which should return all of the elements whose keys are between two numbers x and y. or nearest neighbors, where given another key z, you want to find the things closest to z on either side in this data structure. so, for example, if we have such data structure storing the following numbers, if we wanted to do a range search for 5 to 12, it should return 6, 7 and 10, the three numbers that are stored that are between 5 and 12. if we want the nearest neighbors of 3 we should return 1 and 4 since those are the closest things to we have to 3 on either side. now if we just wanted to do that, it turns out you can do it, but in practice, you really want these data structures to be dynamic. you want it to be possible to modify them. so, two more operations that we would like to be able to implement are insert and delete. insert(x) adds a new element with key x, and delete(x) removes an element with key x. fine. so, for example, we have this array. if we want to insert 3 into it, we do whatever we need to, 3 is now stored in this data structure in addition to everything else. and then we can delete 10, remove that and we've got slightly different elements that we're storing. so just to make sure we're on the same page, if you start with such a data structure and it's empty and you insert 3, and then insert 8, and then insert 5, then insert 10, then delete 8, then insert 12 and ask for the nearest neighbors of 7 what are you going to return? well, if you figure out what the data structures stores at the end of the day, you've inserted 3, 5, 8, 10 and 12, 8 got deleted, so, you've only have the other four left over. and you want the things closest to 7 of the remaining guys, which would be 5 and 10. so, that should be the answer. okay, so this is the data structure that we're trying to implement. what can we say about being able to do it? we've seen a bunch of data structures, maybe one of them will work. for example, we could try implementing this by a hash table. hash tables are good at storing and looking up elements very, very quickly. unfortunately, they're hard to search. you can't really search for all the elements in the hash table in the given range more or less at all. in some sense all the hash table lets you do is check whether or not a given element is stored there. you can't find about elements in a range. similarly nearest neighbor is not really a thing you can do with hash tables, but they are good at inserts insertion into a hash table is all of one as is deletion. but the searching aspect doesn't work here, so maybe we need something else. well, the next thing we can try is an array. and in an array you can do the searches, but they're a little bit slow. if you want to do a range search on an array, the best you can do is scan through the entire array, figure out which elements are in the range you want and return those. similarly you have a nearest neighbors search in o(n) time by scanning through the entire array, keeping track of the closest things on either sides of the query, and then returning the best ones at the end. on the other hand, arrays, at least if they're expandable arrays, are still fine with insert and delete. to insert a new element you just add it to the next square over at the end. to delete you can't just remove it from the array because then you'd leave a gap, but if you take the last element and move it over to fill the gap. once again, this delete operations is o(1) time. perhaps more interestingly than just any array though is a sorted array. here, we're storing all of our elements in an array, but we're going to store them in a sorted order. and the critical thing about this is it allows us to do binary search. if we want to do a range search, we can do a binary search to find the left end of the range in our array and that takes logarithmic time. and then scan through until we hit the right end of the range we want and return everything in the middle. so, the range search here is basically log n time at least assuming the number of things we actually want to return is small. similarly nearest neighbors is log arithmetic time. we do a binary search to find the thing that we're looking for and reach to return the elements on either side. unfortunately, updates to a sorted array are hard. you can't just insert a new element at the end of the array, because the array needs to remain sorted at the end and this will generally destroy the sorted order. if you want to insert 3, it really needs to go between 1 and 4. but, you can't really do that, you can't just sort of add a new cell in the middle of an array. the only way to actually do this, is you can put 3 in that plot and then everything 4 and onwards needs to shift over one cell to make room. and so, insertion here is o(n) time which is a lot longer than we want. similarly deletions are going to be hard. if you delete an element, you can't just leave a gap, you need to fill it somehow. you can't just bring an element over from one of the ends to fill the gap, because that would destroy your sorted structure. so the only way to fill the gap is to sort of take everything and shift it back over 1 in order to fill things up, and that again takes o(n) time. a final thing to look at are linked lists. now here you can do a rangesearch in o(n) time, you just scan through the list and find everything in the range. similarly nearest neighbors are going to be o(n). of course linked lists, insertion, and deletions are very fast, o(1) at least if you've got a doubly linked list. these things are very good. unfortunately our searches are slow. and even if you make this a sorted linked list, if you sort of guarantee that everything comes in sorted order, you still can't do better than linear time for your searches. because you can't binary search a linked list, even if it's sorted, because there's no way to sort of jump to the middle of the list and sort of do comparisons. and so the moral here really is that the data structure we've seen up til this point don't work for this sort of local search data structure. and so, we're going to need something new. and that's what we're going to start talking about in the next lecture. 
hello everybody, welcome back. today, we're going to start talking about binary search trees. in particular, we're going to talk about what the binary search tree data structure is, how it's constructed, and the basic properties that need to be maintained. so last time we came up with this idea of a local search problem, we wanted a data structure to be able to solve it. and we know that none of the data structures we had seen up till this point were sufficient to solve the problems that we wanted. but one maybe came closer than the others. sorted arrays were okay, in that you could actually do searches efficiently on them. but unfortunately, you couldn't do updates in any reasonable way. but the fact that these things allowed for efficient binary searches sort of maybe gives us a good starting point for what we're looking for. so, what we should look at is, we should really see this operation of binary search. what does it entail, and what exactly makes it work? and so we all know how a binary search works, right? so you've got your list of numbers, you pick the one in the middle. you ask, is the thing i am looking for bigger than this or less than this? if it's smaller, i sort of look at the middle of first half of the array, and say, is it bigger or less than that? if it's larger, i look to the second half of the array, and ask, is it bigger or less than that? and i sort of keep on asking these question and each time it sort of narrows down my search space until i get an answer. but as you'll note, sort of associated to this sort of binary search procedure is a search tree. if you sort of consider which questions you ask. first, i ask about, is it bigger or less than seven? if it's smaller, i ask about four. if it's bigger, i ask about 13. if i got four and said it was bigger than four, i'd then ask about six. and i have this sort of whole tree of possibilities. every time i ask a question it sort of splits into two different cases. and maybe the key idea here is that if you want to do a binary search, instead of doing it on the array, you could just have this search tree. you start at the top of the tree, at seven. and then you head down to 4 or 13, depending on where you go, and then you keep going down until you find your answer. and so in some sense, the search tree is as good as the array. but while a sorted array, as we saw, was hard to insert into, the tree is actually a lot easier to work with in that way. and it turns out this search tree going to be the thing that allows us to implement these operations in much better way. okay, so what do we need to be the case for the subtree? well, i mean, like all trees, it should have a root node, each node should have two children. it should have a left side, which is sort of where you're going to go when you find out that things are smaller than that. and then you have a right side, which is where you go when things are bigger than that. so, to be a little bit more formal, the tree is constructed out of a bunch of nodes. each node is sort of a data type that stores a bunch of things. importantly, it stores a key, it stores a value that you're comparing things to. it also should have a pointer to the parent node and a pointer to the left child and a pointer to the right child. and to be a search tree, it needs to satisfy one very critical property. if you look at the key of a node x, then, well, the stuff on the left should be where you're going if you do a comparison and find the thing you're looking for is smaller than x. and that means that, all the keys stored on all the nodes in the left subtree of x, all the descendants of its left child, need to have a smaller key than x does. and similarly, if you found that something was bigger than x and go to the right, it had better actually be on the right. and so, the things whose keys are larger than x need to be on the right subtree of x. so just review this. i mean, we have this following three trees, a, b, and c. which one of these trees satisfy the search tree property? well, it turns the only correct one is b, b it works out. a has this issue that up at the top you've got this node 4 and on the left side, it has everything bigger than 4 and on the right side, it has everything smaller than 4. and it's supposed to be the other way around, but if you switch 4's left and right sides, everything would work out there. now case c is a little bit more subtle. there's really only one problem here. and that's that you have this root node which is a 5. and there's another 4, but 4 is part of 5's right subtree. and remember, everything on the right subtree of any node has to be larger than it. and this 4 is smaller. and so other than that one mistake, things are okay there as well. okay, so this is the structure. next time we're going to talk about how to do basic operations on binary search trees and sort of give a little bit of pseudocode for how to do these things and then we'll sort of have a basic start for this project. 
hello everybody, welcome back. we're continuing to talk about binary search trees. and today, we're going to talk about how to implement the basic operations of a binary search tree. so we're going to talk about this and talk about a few of the difficulties that show up when you're trying it. okay, so let's start with searching. and this is sort of the key thing that you want to be able to do on the binary search tree. and the primary operation that we're going to look at for how to do this is what we're going to call find. now find is a function. what it takes is a key k and the root r of a tree. and what it's going to return is the node in the subtree with r as the root whose key is equal to k. okay, that's the goal and the idea is pretty easy. i mean the search tree is set up to do binary searches on. so what we're going to do is we're going to sort of start at the top of the tree. we're going to compare 6, the thing we're searching for, to 7. 6 is less than 7 and that means since everything less than 7 is in its left subtree, we should look in the left subtree. so we go left. we now compare 6 to 4, the root of this left subtree. 6 is bigger than 4. everything bigger than 4 in the place that we're looking is going to be in 4s right subtree, so we had down in that way. we now compare 6 to 6. they're equal, and so we are done with our search. and so this algorithm is actually very easy to implement recursively. if the key of r.key = k, then we're done. we just return the node r at the root and that's all there is to it. otherwise, if r.key > k, we need something less than r, so the thing we're looking for should be in the left subtree. so we recursively run find on k and r's left child. on the other hand, if r.key < k, we have to look in the right subtree, so we find k in the subtree of rs child. okay, this works fine as long as the thing we're looking is in the tree, but what happens if we're looking for a key that isn't there? so we're trying to find 5 in this tree. we checked it's less than 7. it's more than 4. it's less than 6. 6 doesn't have a left child. we have a null pointer, what do we do here? well, in some sense we could just return some errors saying the thing you were looking for wasn't there, but we did actually find something useful. we didn't find 5 in the tree, because its not there, but we sort of figured out where 5 should have been in the tree if it were there. and so, if you stop your search right before you hit the null pointer, you can actually something useful. you find the place where k would fit in the tree. so it makes a little bit of sense to modify this find procedure so that if say r.key > k, then instead of just checking the left points here, we first check to see if r actually has a left child. if rs left child isn't null, we can recursively try to find k in the left subtree. but otherwise, if it is null, we'll just stop early and return r and do something similar for the other case. and this sort of means that if we're searching for something that's not in the tree, we at least give something close to it. okay, so that's one thing we can do. another thing that we might want to do is sort of talk with adjacent elements. if we've got some element in the tree, we might want to find the next element. and so in particular another function we might want which we will call next. it takes a node n and outputs the node in the same tree with the next largest key. and maybe one way to think about this is instead of searching for every key and has we should search the tree for something just a tiny bit bigger than that. and, now if n has a right child this is kind of easy. the first bunch of steps lead you to the node n, and then you want to go right, because it is bigger than n. but after you do that you just keep going left, because it's not, it's smaller than all of these other things. they're a little bit bigger than n. and you just keep going until you hit a node where you can't go left any further. it's left pointer is null, and that's going to be the successor. now, this doesn't work if n has no right child, because you can't go right from n. you also go looking on the left side of n, doesn't work. everything is going to be smaller there. so instead what you have to do is you have to go up the tree. you check its parent, and if its parent is smaller than n as well, you have to check the grandparent. you just keep going up until you find the first ancestor that's bigger than n. and once you have that, that will actually be the successor. so, the algorithm for next involves a little bit of case analysis. if n does not have a right child, we're going to run this protocol we call rightancestor, which goes up until you take the first step right? otherwise, we are going to return what we're going to call the leftdescendant of ns right child which means you sort of go left, until you can't go left anymore. now both of these are easy to implement, recursively for leftdescendant if you don't have a left child, you're done, you return n. otherwise you take one step left and repeat. for rightancestor you check to see if your parent has a larger key than you if so you return your parent otherwise you go up a level and repeat, and just keep going until you find it. and so putting these together that computes next. now it turns out that this range search operation that we talked about before, this you're given two numbers x and y and the root of the tree and you'd like to return a list of all the nodes whose keys are between x and y you can implement this pretty easily using what we already have. so, the idea is, well, you want to find the rangesearch, say, everything between 5 and 12. first thing you do is you do a search for the first element in that range, in this case, it will be 6. then you find the next element, which is 7, and the next element, which is 10 and the next element is 13, it's too big so you stop. so the implementation is pretty easy. we create a list l that's going to store everything that we find, we let n be what we get when we try to find the left n point x within our tree. and then while the key of this note n that we're working at is less than y as long as the key is bigger than x, we're going to this node to our list and then we're going to replace n by next(n). we're just going to iterate through these nodes until they're too big, and then we return l. okay, so that's how you do range search. and nearest neighbors, you can figure it out, it's a similar idea. now, the interesting things are how do we do inserts and deletes? so, for insertion we want to be given the key k and the root r of the tree and we'd like to add a node with key equal to k to our tree. and the basic idea is that unlike with our certain array with the tree we can just have a new element and just have it hanging off one of our leaves. and this works perfectly well. there is a big of a technical problem here, though. we can't just have it hang off anywhere. i mean, three that we're inserting is smaller than seven, so it needs to be on the left side of seven. and furthermore, there are a whole bunch of other things that needs to satisfy to keep the search property working out. but, fortunately for us, this find operation. if we tried to find a node that wasn't in our tree actually did tell us where that node should belong was. so to insert we just find our key within r, and that gives us p, and the new node that we want should be a child of p, on the appropriate right or left side, depending on the comparison between things. and that's that. a little bit more difficult is delete. so, here we just want to node n, we should remove it from our tree. now there's a problem we can't just delete the node because then its parent doesn't have a child. it's children don't have parents, it breaks things apart. so we need to find some way to fill the gap. and there is a natural way to fill this gap. the point is you want to fill the gap with something nearby in the sorted order, so you try and find the next element, x, and maybe you just take x and fill the gap that you created by deleting this. unfortunately, there could be a problem. now, x turns out because it's the next element that often not going to have a left child, because the left child would be sort of even closer. but it might have a right child and if it does have this right child, then by moving x side of the way it's right child is now going to be orphaned. it's not going to have a proper parents. so in addition to moving x to fill that gap you have to move y up to fill the gap that you made by moving x out of the way. but once you do that it's actually perfectly good. you've done a reasonable rearrangement tree and removed nodes you want. so the implementation takes a little bit of work. first you check to see if n has a right child. if it's right child is null then it turns out we're not in this other case. but you can just remove n and you need to promote ns left child, if it has one. so ns left child should now become the child of ns parent instead of the other way around. otherwise we're going to let x be next of n and note that x does not have a left child. and then we're going to replace n by x and promote xs right child to sort of fill the gap that we made by moving x out of the way. but this all works. just to review it, so if we have the following tree and we're deleting the highlighting node. which of the following three trees do we end up with? well the answer here is c. the point is that we deleted 1 so we want to replace it with the next element which is 2. so we took 2 and put it to the place where 1 was. now 2s child, 4, needs to be promoted. so 4 now becomes the new child. six and everything works nicely and in this tree. okay so if i tell you implement some basic operations for binary search trees next time we'll going to talk about the run time of these operations, which are going to leads us to some interesting ideas about the balance of these trees. 
hello, everybody, welcome back. we're continuing to talk about binary search trees and today, we're going to talk about balance. in particular, we're actually going to look at sort of the basic runtime of these operations that we talked about in the last lecture and from that, we're going to notice that they'll sometimes be a little bit slow. and to combat this, we want to make sure that our trees are balanced and well, doing that's a bit tough and we're going to talk a little bit about how we're going to do this with rotations. okay, so first off we've got this great operation, it can do local searches, but how long do these operations take? and maybe a key example is the find operation. so we'd like to find 5 in the following tree. we compare it to 7, and it's less than 7, and it's bigger than 2, and it's bigger than 4, and it's less than 6, and it's equal to 5, and we found it. and you'll note that the amount of work that we did we sort of had to traverse all the way down the tree from the root to the node that we're searching for and we had to do a constant amount of work at every level. so the number of operations that we had to perform was o of the depth of the tree. now, just to sort of make sure we get this. so if we have the following tree, we could be searching for different nodes a, b, c or d, which ones are faster to search with, avoiding which others? well, a is the fastest. it's up at the root. then d has depth only three, b has depth four, and c has depth five, and so a is faster than d is faster than b is faster than c probably. so the runtime is the depth of the node that you're looking for. but it's unfortunate, the depth can actually be pretty bad. in this example, we have only ten nodes in the tree, but this 4 at the bottom has depth 6. in fact, if you think about it, things could be even worse. a tree on n nodes could have depth n if the nodes are just sort of strung out in some long chain. and so this is maybe sort of a problem. that maybe our searches only work in o of n time were not any better than any of these other data structures that didn't really work. on the other hand, even though depth can be very bad, it can also be much smaller. this example has the same ten nodes in it, but the depth maximum depth is only four. and so by rearranging your tree, maybe you can make the depth a lot smaller. and in particular, what you realize is well, in binary search the questions that we asked, in order for it to be efficient, we wanted to guess the thing in the middle. because then no matter which answer we got, we cut our search space in two, and so what this means for a binary search tree is at any node you're asking that one question. you want things on the left and the things on the right. those two subtrees should have approximately the same size. and this is what we mean by balance. and if you're balanced, suppose that you're perfectly balanced, everything is exactly the same size, then this is really good for us. because it means that each subtree has half the size of sort of the subtree of its parent. and that means after you go down, logarithmically, many levels the subtrees have size one and you're just done. and so, if your tree is well balanced, operations should run in o(log(n)) time, which is really what we want. but there's a problem with this, that if you make insertions they can destroy your balance properties. we start with this tree, it's perfectly well balanced and just has one node i guess but we insert two and then we insert three and then we insert five and then we insert four and you'll note that suddenly we've got a very, very unbalanced tree. but all we did were updates. so, somehow we need a way to get around this. we need a way to do updates without unbalancing the tree. and the basic idea for how we're going to do this, is we're going to want to have some mechanism by which we can rearrange the trees in order to maintain balance. and there's one problem with this, which is that however we rearrange the tree, we have to maintain the sorting property. we have to make sure that it's still sorting correctly or none of our other operations will work. and well, there's a key way to do this, and this is what's known as rotation. the idea is you got two nodes, x and y. we say x is y's parent. and there's a way to switch them. so, that instead y is x's parent. and the like sort of sub-trees a, b, and c that hang off of x and y. you need to rearrange them a little bit to keep everything still sorted. but there is this sort of very local rearrangement. you can go back and forth. and it keeps the sorting structure working. and it rearranges the tree in some hopefully useful way. so just to be clear about how this works, it takes a little bit of bookkeeping. but that's about it. you let p be the parent to x. y be its left child. b be its right child. and then what we're going to do is we just need to reassign some pointers. p is the new parent of y and y is its child, y is the new parent of x and x is its right child, x is the new parent of b and b is x's new left child. and once you've sort of rearranged all those pointers then everything actually works. this is a nice constant time operation and it does some useful rearrangements. so what we really need to do though is we need to have a way to sort of use these operations to actually keep our tree balanced and we're going to start talking about how to do that next time when we discuss avl trees. 
hello everybody, welcome back. we're still talking about binary search trees but today we're going to talk about avl trees. and avl trees are just sort of a specific way of maintaining balance in your binary search tree. and we're just going to talk about sort of the basic idea. next lecture we're going to talk about how to actually implement them. but okay, what's the idea? we learned last lecture that in order for our search operations to be fast, we need to maintain balance of the tree. but before we can do that we first need a way to measure the balance of the tree so that we can know if we're unbalanced and know how to fix it. and a natural way to do this is by what's called the height of a node. so if you have a node in the tree, its height is the maximum length of a path from that node to a leaf of your tree. fair enough. so for example if we have the following tree, what's the height of the highlighted node? well, this node has height six, the following path of length six leads down from this node and it turns out to be nothing longer. so, we can define height recursively in a very easy way. if you have leaf its height is one because you're just there and you can't go any further. otherwise, well the longest path downwards you can have, it's either through the longest path on your left side or the longest path on your right side. so you want to take the maximum of the height of your left child and the height of your right child, and then you need to add one to that because n sort of gets added to this path. okay, that's fine. now, in order to actually make use of this height we actually are going to want to add a new field to our nodes. so, the nodes that made up our tree previously stored a key and are pointed to the parents on the left child and the right child, and now they also need to store another piece of data, the height of node. and note that we are actually going to have to do some work, and we'll talk a little bit about how to do this later, to insure that this height field is actually kept up to date. we can't just store it as a number and leave it there forever. if we rearrange the tree, we might need to change its heights. in any case, back to balance. height is a very rough measure of the size of a sub-tree. for things to be balanced, we want the size of the two sub-trees of the left and right children of any given node to be roughly the same. and so there's an obvious way to do this. we'd like to force the heights of these children to be roughly the same. so the avl property is the following. for all nodes n in our tray, we would like it to be the case that the difference between the height of the left child and the height of the right child is at most one. and we claim that if you can maintain this property for all nodes in your tree, this actually ensures that your tree is reasonably well balanced. okay and so, really what we'd like to know is that if you have the avl property on all nodes, then the total height of the tree should be logarithmic. it should be o(log(n)). so basically what we want to say is that you have an avl tree and it doesn't have too many nodes. then the height is not too big. but it turns out that the easier way to get at this is to turn this on its head. we want to show instead that if you have an avl tree and the height isn't too big, then you can't have too many nodes. and this we can do. so we're going to prove the following theorem. suppose that you have an avl tree, a tree satisfying the avl property, and n is a node of this tree with height h. then the claim is that the sub-tree of n has to have size at least the fibonacci number f's og h. and so just to review, we talked about fibonacci numbers way back in the introductory unit for this, but for the previous course in this sequence. but this is just a sequence of numbers. the zeroth one is zero, the first is one, and from there after each fibonacci number is the sum of the previous two. now, these are a nice predictable sequence, they grow pretty fast, the end fibonacci number's at least two to the n over two for all n at least 6. okay so let's look at the proof, we're going to do this by induction on the height of our node. if the node we're looking at has height one, it's a leaf. and it's sub-tree has one node which is the first fibonacci number, great. next we need an inductive hypothesis if you've got some node of height h. by sort of definition of the height, at least one of your two children need to have height h minus one. then, by the avl property, your other child needs to have height at least h minus two. so by the inductive hypothesis, the total number of nodes in this tree is at least the sum of the h -1 fibonacci number plus the h- 2 fibonacci number, which equals the h fibonacci number. and so that completes the proof. so what does this mean? it means that if a node in our tree has height h, the sub-tree of that node has height at least two to the h over two. but if our tree only has n nodes, two to the h over two can't be more than n. so the height can't be more than two log base two of n. which is o of log n. and so the conclusion is if we can maintain the avl property, you can perform all of your find and operations in such in o(log(n)) time. and so next lecture we're going to talk about how to maintain this property but this is the key idea. if we can maintain this property, we have a balanced tree and things should be fast. so i'll see you next time as we discuss how to ensure that this happens 
hello, everybody, welcome back. today, we're going to continue talking about avl trees, and in particular, we're going to talk about the actual implementation and what goes into that. so, as you recall, the avl tree was this sort of property that we wanted our binary search tree to have, where we needed to ensure that for any given node, its two children have nearly the same height. so the following is an ideal tree everything's labelled by their height, it all works out. now, there's a problem that if we update this tree it can destroy this property. so if we try to add a new node where the blue node is, then what happens is, a bunch of nodes in the tree, their heights change because now they have a longer path which leads to this new node. and now there are a couple locations at which the avl property fails to hold. so, in other words, we need a way to correct this issue. and there is one thing that actually helps a little bit here, which is that when we do an insertion the only heights of nodes that change are along the insertion path. the only time when a height can get bigger is because the new path from it to a leaf ends up at the leaf you staggered. so we only need to worry about nodes on this path, but we do actually need to worry. okay, just sort of review what it is, we have this avl tree, we want to insert a new node either a b c or d. which one of these will require us to do some rebalancing? it turns out that d is the only place where we have a problem, but if you insert d it changes a bunch of these heights and that destroys avl program. the other inserts it turns out would be fine. okay, so let's actually talk about how to do this. so we need a new insertion algorithm that involves some rebalancing of the tree in order to maintain our avl property. and the basic idea of the algorithm is pretty simple. first you just insert your node as you would before. you then find the node that you just inserted and then you want to run some rebalance operation. and this operation should start down at n and should probably work its way all the way up to the root, sort of following parent pointers as you go. just to sort of make sure that everything that could have been made unbalanced has been fixed, and we're all good. so the question is how do we actually do this rebalancing? and, well, the idea is the following. at any given node, if the height of your left child and the height of your right child differ by at most 1, you're fine, you're already satisfied the avl property. on the other hand it could be the case that your children's heights differ by more than one. in that case you actually do need to do some rearranging. if your left child is two taller than your right, you need to fix things and probably what you need to do is move your left child higher up in the tree relative to your right to compensate for the fact that it's sort of bigger. fortunately for us, you can actually show that these inserts, the height difference is never going to be more than 2. and that simplifies things a little bit. okay, so the basic idea is the following. in order to rebalance n, first we need to store p, the parent of n just because we're going to, after we fix n, we're going to want to fix things at p, and so on recursively. now, if the height of n's left child is bigger than the height of its right child by more than one, we need to rebalance right-wards. if the height of the right child is bigger than the height of the left child by more than one we need to rebalance left-wards. then after that, no matter what happens, we maybe need to readjust the height of n, because the height field that was stored might be inaccurate if we inserted things below it. and then if the parent that we fixed wasn't the null point, if we weren't already at the root, we need to go back up and we need to rebalance the parent recursively. so quickly, this adjustheight function, this sort of just fixes the number that we're storing in the height field. all it does is we sort of set the height to be one plus the maximum of the height of the left child and the height of the right child. just given by this recursive formula we had for the height. okay! but the key thing we still haven't really touched. we need to figure out how to do the rebalancing. so you have a node, its left child is heavier than its right child. its left child has exactly two more height to it. and the basic idea is the left child is bigger, it needs to be higher up, so we should just rotate everything right. and it turns out that in a lot of cases this is actually enough to solve the problem. there is one case where it doesn't work. so b is the node we're trying to rebalance. a is its left child which is too heavy, and we're going to assume that a is too heavy because its right child has some large height, n+1. the problem is that if we just rotate b to the right, then this thing of height, n+1, switches sides of the tree when we perform this rotation. switches from being a's child to being b's child. and when we do this we've switched our tree from being unbalanced at b to being unbalanced at a now, in the other direction. and so, just performing this one rotation doesn't help here. in this case the problem is that a's right child, which we'll call x, was too heavy. so the first thing we need to do is make x higher up. so what you can do is, instead of just doing this rotation at b, first we rotate a to the left one, then we rotate b to the right one. and then you can do some case analysis and you figure out after you do this you've actually fixed all the problems that you have. and it's good. the operation for rebalancing right is you let m be the left child of n and then you check to see if we've to be in this other case. if m's right child has height more than m's left child, then you rotate m to the left, and then no matter what you did, you rotate n to the right. and then no matter what you did, all the nodes that you rearranged in this procedure, you need to adjust their heights to make sure that everything works out. once you do this, this rebalances things at that node properly, it sets all the heights to what they should be, and it's good. okay, so that's how insert works. next, we need to talk about delete. and the thing is deletions can also change the balance of the tree. remember generally what we do is the deletions we removed the node. but, we replaced it by its successor and then promoted its successor's child. and the thing to note is that when you do this, sort of the space in the tree where the successor was, the height of that denoting that location decreased by one. because instead of having successor and then its child and then some such, you just have the child and such. and this of course can cause your tree to become unbalanced even if it were balanced beforehand. so, we of course need a way to fix this, but there's a simple solution. you delete the node n as before. you then let m be this left child of the node that replaced n this thing that might have unbalanced the tree. and then you run the same rebalance operation that we did for our insertions starting on m and then filtering its way up the tree. and once you've done that, everything works. and so what we've done is we've shown that you can maintain this avl property and you can do it pretty efficiently, all of our rebalancing work was only sort of o of 1 work per level of the tree. and so if you can do all of this, we can actually perform all of our basic binary search tree operations in o of log n time per operation, using avl trees. and this is great. we really do have a good data structure now for these local search problems. so that's all for today, coming next lecture we are going to talk about a couple of other useful operations that you can perform on binary surgeries. 
hello everybody, welcome back. last time, we talked about avl trees and showed that they can perform your basic binary search tree operations. but now we're going to introduce a couple of new operations and talk about how to implement them. so, another useful feature of binary search trees is in addition to being able to search them, there's also a bunch of sort of interesting ways that you could recombine them. and so we're going to discuss here two of these operations. one of them is merge, which takes two binary search trees and combines them into a single one. and the other one is split, which takes one binary search tree and breaks it into two. so, let's start with merge. and the idea is, in general, if you have two sorted lists and want to combined them into sort of a single sorted list. this is actually pretty slow, it's going to take o(n) time, because you sort of need to figure out how the lists interweave with each other. this is the thing you do in merge sort. however, there is a case where you can merge them a lot faster. and that's the case where they're separated from each other, where they're sort of, one's on one side, one's on the other side. and so, this is the case that merge is going to work with. so you're giving two trees, r1 and r2, or the roots of these trees. and they're going to need to have the property that all the keys in r1's tree are smaller than all of the keys in r2's tree. and if we're given this, we should then return the root of a new tree that had all of the elements of both of the trees. so just to review this condition that we have, of the three trees below, which of them can be properly merged with the one above? the answer is that only a can because all of the elements of a are less than all of the elements of the guy above. b has this problem that it's got a 9, and 9 is between 8 and 10 so it can't really be separated from them. and c has the problem that it has both 2, which is smaller than everything above, and 12 which is bigger than everything above. so, a is the only guy that actually works here. okay, so how do you do this merge operation? well, there's actually a case where it's super easy to merge trees and that's if, instead of just being given the two trees, we also have an extra node that we can use as the root. because then you just have the node up top, you have the guys in r1, the small guys on the left of it, the guys in r2, the big guys on the right of it. that's your tree. so, the implementation, we'll call this function mergewithroot, is you let r1 be the left child of t, r2 be the right child of t. let t be the parents of these two. if you need to do things involving restoring heights, you adjust those as appropriate, and then you return t. this takes o(1) time, very simple. the problem is, well, what if we're not given this extra node? then there's actually a pretty simple solution. you find the largest element, of say, the left subtree. you remove that, and then turn it into the root of the new guy. and that works. so now if we want to merge r1 and r2, we're going to find the largest element of r1, call that t. you're going to delete t from its subtree, and then you're going to merge with root. and that works. the run time's a little bit worse. you have to spend o of the height time to find this node t, but other than that it's pretty efficient. so, just to review, we've got this tree, we find the biggest element to the left tree 8. we then delete it, and then we merge with root. that's all there is to it. now if we didn't care about balance, that would be all we'd have to say about this operation. unfortunately, this merge operation doesn't preserve balance properties. and the problem is that, well, the two trees, you didn't really touch them very much, so they stay balanced. but when you stick them both together under the same root, well, if one tree is much, much bigger than the other, suddenly the root is very, very unbalanced, and this is a problem. so we need a way to fix this. and there's actually a not very difficult way to do that. the idea is that we can't merge the, say, left subtree with the right subtree because the left one is way too big. so what we're going to do is not merge the whole guy with the whole guy here. we're going to have to find some node of approximately the same height as this guy on the right so that we can merge that. and so what we're going to do is, we're going to sort of climb down the sort of right edge of the bigger tree until we find a sub tree of the right height that we can merge with our guy on the other side. okay, so how do we implement this? avltreemergewithroot. what we're going to do is, if the heights of the two trees differ by at most 1, we can just merge with root as before. we then figure out what the height of t needs to be and return it. that simple. otherwise though, what happens if say r1's height is bigger than r2's height? well, what we want to do is we want to step down to instead of merging r1 with r2, we merge r1's right child with r2. so, we use merge with root to merge the right child with r2 at this t, and we get some new tree with root r prime. r prime we set back to be the right child of r1, and similarly set r1 to be the parent. and then we need to rebalance this at r1 because things might be off by a little bit, but it turns out not more than about one. we knew how to deal with that with our old rebalance operation. and than we return the root of the tree. if, on the other hand, r1's height is smaller than r2's height, we sort of do the same operation but on the opposite side. okay, so, let's analyze this. every step we take down the side of the tree decreases the height difference by either 1 or 2. so we're just going to keep decreasing the height difference until we're off by at most 1. then we merge, and we soon have to go back up the chain. but the amount of time this takes isn't the depth of the tree, it's sort of the number of steps we need to take down the side. and that's approximately the difference in the two heights. and this will actualy be important in a bit. okay, so that was the merge operation. next, we're going to talk about sort of the opposite operation, split. merge takes two trees and turns them into one. split breaks one tree into two. what you do is you pick an element, say 6, and then you've got the tree consisting of all the elements less than 6, and then another one from all the elements bigger than 6. so split should take the root of a tree and the key x, and the output should be two different trees, one with all the elements less than x in your tree, and one with all of the elements bigger than x. okay now, the idea is actually not so hard. what we're going to do is we're going to search for x, and then the search path, well it's got a few nodes on the path. and then it has a bunch of trees hanging off to the left of the path. these things are all going to be smaller than x. and it's going to have a bunch of trees hanging off to the right that are all going to be bigger than x. and so all we have to do is take these trees that are smaller than x, merge them all together, take these things bigger than x, merge them all together, and then we have two trees. so let's see how this works. so we're going to do this recursively. we're going to split at r, at this point x. if our root is null, if we just have the null tree, we just return a pair of null vectors, whatever. next if x is less than the key at the root, what that means is that everything on the right is bigger than x. that's sort of all on the right. but the left side of the root, we need to split that in two. so we're going to recursively run split on r.left, and that gives us two trees, r1 and r2. now r1 is actually everything in the whole tree that's smaller than x. that half is done, but r2 needs to be combined with the whole right subtree of our original tree. so we run mergewithroot on r2 and r.right with r as the root. fortunately we have this extra nodes, uses the root. and this gives us an r3. and we can return r1 and r3 as our split. if x is bigger than the key, we can do the same thing on the opposite side. hopefully you can figure that out. okay, so the first thing to note is that if we, instead of just doing a mergewithroot, we used avlmergewithroot. this insures that the two trees we produce are both balanced, which is good. also if you look at the run time of this algorithm, well, the run time of avlmergewithroot is o, the difference in the two heights. so we have to look at the difference between the biggest type and the next biggest and the next biggest and the one after that and so on. this sum actually telescopes, and the total run time is o of the maximum height of one of these trees we're trying to merge, which is just o(log(n)). and so we've got these two operation, merge combines trees, split breaks them in two, and both operations can be implemented in o(log(n)) time using avl trees. so that's these two operations. next time we're going to talk about a couple of applications. one of them's going to sort of talk about a way that we can make use of these split and merge in an interesting way. 
hello everybody, welcome back. today we're going to talk more about binary searches, in particular we're going to give a couple of applications to computing order statistics. and then a sort of additional use of binary search trees, to store assorted lists. okay, so, there's some questions that you might want to ask. you've got a bunch of elements that are stored in this binary search tree data structure. they're assorted by some ordering. things we might want to do. we might want to find the 7th largest element. or maybe we want the median element, or the 25th percentile element. now, these are all instances of an order statistic problem. we would like to be able to be given the root of the t tree and the number k, we should be able to return the kth smallest element stored in the t tree. so, the basic idea is that this is sort of a search problem. we sort of should treat it like one. but to do that we need to know which subtree to search in. so i mean, is the case smallest element in the left subtree? well, the left subtree does store a bunch of the smallest elements. but the real question is, does it store k of them? if it stores at least, k, k's smallest element should be in there, otherwise it won't be. so the thing that we need to know, is how many elements are in the left subtree? and so, we really need a tree where we can easily tell how many elements are in each subtree. well, there's an easy fix for that. you just add that as a new field. so n.size should return the number of elements in the subtree of n. it's a new field, it stores that, it should satisfy the property, that the size of n is the size of the left subtree, and the size of the right subtree, plus one, all added together, where if you have null pointers, these have size zero. okay? now, we have to be a little bit careful, just like with the height, we couldn't just define this field and hope that it always has the right value. you actually need to do some work to make sure this stays correct. for example, when you perform a rotation, the sizes of various subtrees will change. fortunately, only the sort of two nodes that you moved around, will actually have their subtree sizes changed, but you do need to deal with those. and so, we're going to have an operation called recomputesize, which just recomputes the size as the sum of the size of its left child, and the size of its right child, and one. and then to do a rotation, we should do this as we did before. but then we need to recompute the sizes of the two nodes that we rotated. and you need to make sure to do this in the right order, actually, because the size of the parent depends on the size of the child. so you need to recompute the child first. okay, but once you got all the sizes of nodes stored, it's actually not hard to compute order statistics. so to compute the kth smallest element in your tree, r, what you do is we let s be the size of the left sub tree, r.left.size. now, if k = s+1, they're exactly s things smaller than the root. the root is the s plus firsts smallest element, so we return r. otherwise, if k < s + 1, the kth smallest element is in the left subtree, so recursively return the kth smallest element of our .left subtree. on the other hand, if k > s+1, we need to look in the right subtree. now unfortunately, it's no longer the kth smallest element of the right subtree, because there were already s+1 smaller elements. so what we actually need to compute is the k-s-1st smallest element in the right subtree. but with this minor adjustment, this algorithm works. the runtime is all of the height of the node that you search for, it's basically just a binary search. but we need to use these sizes of trees in order to just figure out which side to look at, instead of comparing the actual keys, but this works perfectly well. now here's a puzzle that i'm going to give you. we're not going to do homework on this, but it's something you should think about. what we've shown you how to do is, given a rank, k, we can find the kth smallest node. how would you do the opposite? given a node, figure out how many nodes in the tree are smaller than it. you can do it using this kind of thing, but it takes a little bit of thinking. okay that was one problem. we're going to talk about one more application of binary search trees. and, this problem's a little bit weird, but it's going to introduce some very important ideas. so we have an array of squares, they're each colored black or white. and we want to be able to perform this flip operation, which what it does is it sort of points at some square, x, and every square after that index, it flips them from black to white, and white to black. so, in the array pictured, we're flipping the last four guys, and they go from being white, white, black, white, to being black, black, white, black. okay. so, formally, we sort of want a data structure that maintains this. you can create an array of size n, you can ask for the color of the mth square, or you can fun this flip operation, which flips the color of all the squares after index x. okay, those are the things we want to support. now, we could do this by just having an array and storing all the colors. the problem is that the flip would be pretty slow. because if you wanted to flip all of the things with index bigger than x, then there's no good way to do this. you just have to sort of go through every square of index bigger than x, and flip them, and that could take linear time. so it turns out that there's a nice way to use binary search trees to solve this. and this requires sort of a slightly different way of thinking about binary search trees. up until now we thought of a search tree as sort of having a bunch of elements there stored, and they allow you to do searches on them. and so somehow you're given the keys, and the search tree allows you to find them. but there's another way to do it. and maybe a good way to illustrate it, is by looking at the logo for our course. so this is a binary search tree. every node has a letter in it. but you'll note that this isn't sorted in terms of these letters. for example, o is the left child of i, but o comes after i in alphabetical order. these things aren't sorted alphabetically. on the other hand, you'll note that the binary search tree structure actually does tell you what order these letters are supposed to be going in. i mean, the smallest thing here should be a, because it's the left child, of the left child, of the left child, of the left child. then l is the next smallest, then g, then o, r, i, then t, h, m, s. and so, it tells us what order these layers are supposed to be, and they spell algorithms. and that's the basic idea, that you can use a tree to store some sort of sorted list of things, in a convenient way. so, for example we have the following tree. there are no actual keys stored on them, but of this a, b, c, d, and e, one of these is the 5th smallest element in the tree. which one is it? well, it's d. i mean you sort of count the smallest sum left, then b, then this other thing, and d is the 5th smallest. okay, so what's the point? how are we going to use this to do our flip arrays? what we're going to do is, instead of storing the sequence of black and white cells as an array, we're going to store it as a list. and in this list we're going to have a bunch of nodes, they're going to be colored black or white, fine. actually, there's a bit of a clever thing. we're actually going to want two trees, one with the normal colors, and one with the opposite colors. and the reason for this is that when we want to do flips, we want to be able to replace things with their opposite colors. so, it helps to have everything opposited already stored somewhere. but now comes the really clever bit. if we wanted to do this flip operation, say we wanted to take the last three elements of our tree and flip all of their colors, well this second tree, this dual tree, the last three elements of that tree, have the opposite colors. so all that we need to do is swap the last three elements of the tree on the left, the last three elements of the tree on the right, and we have effectively swapped those colors. and what's even better is that using these merge and split operations from last time, we can actually do this. so let's see how this is implemented. firstly, to create this thing, we just build two trees, where t1, all of the things are colored white, and t2, they're all colored black. great. to find the color of the mth node, you just find the mth node within t1 and return its color. great. the flip operation is the interesting bit. if we wanted to flip(x), what you do is, you split t1 at x into two halves, and you split t2 at x, and then you merge them back together. but you merge the left half of t1, with the right half of t2, and you merge the left half of t2, with the right half of t1. and that effectively did move around the sort of last n bits, and it works. and so, as the moral, trees can actually be used for more than just performing searches on things. we can use them to store these sorted lists, and merge and split then become very interesting operations, in that they can allow us to recombine these lists in useful ways. okay, so that's all for these applications. next time we're going to sort of talk, we're going to give an optional lecture, and it's going to talk about an alternative way to implement many of these useful binary search tree operations. so, please come to that, it'll be interesting, but it's not really required. it's another way to do some of this stuff. but, i hope i'll see you there. 
hello everybody. welcome back. today we are going to talk about something a little different. up until this point, we've talked about avl trees, we've talked about how to keep them bound, and how to use them, to implement all of our binary search tree operations, and all of log on time pre-operations. but it turns out that there are a wide number of different binary search tree structures that give you different ways to ensure that your trees are balanced, there are trepes, there are red block trees, and today we're going to talk about splay trees as sort of another example of the types of things that you can do. and to motivate this suppose that, well, if you're searching for random elements, one after the other, you can actually show that no matter what splay your use or data, searcher you use it will always take at least log n time per operation. that's actually the best you can do. however, if some items are searched for more frequently than others, you might be able to do better. if you take the frequently queried items and put them close to the root, those items will be faster to search for. and some of the other items might be a little bit slower, but you should still be okay. to compare for example, we've got the unbalanced tree and the balanced one with the same entires. but you'll note that 1, 5 and 7 are much higher up in the unbalanced tree. now if we search for random things, if we search for 11, 11 is much higher up in the balanced tree than the unbalanced one, so the unbalanced one is slower there. but when we search for 1, it takes a lot less time in the unbalanced case, we search for one again, we search for seven, it's again a lot cheaper in the unbalanced case. and if we do some sequence of searches well, it might turn out that's is actually cheaper to use the unbalanced tree than the balanced one if these elements that tend to be higher up in the unbalanced tree are searched for more frequently than other elements. so the idea here is that we'd like to have a search tree where we can put the commons searched common nodes near the root of the tree so that they are cheaper to look up. however, very often it will be the case that we won't know ahead of time which those commonly searched for nodes will be. and so if we, or in this situation we'll need an adaptive way to bring them close to the root. and one natural way to do it is every time you find a node in your tree you do something to rearrange the tree and bring that node up to the root. and that way at least heuristically, if there are elements that you search for frequently, then since you keep bringing them up to the root, they'll usually stay somewhat close to the root. and they'll be very cheap to access. so if we want to phrase this in a nice simple way, one thing that you could do is if you have your tree and you've got this node that you searched for, you could just bring it to the root by just rotating to the top. you rotate it up and again and again and again, and again until it ends up being the root. unfortunately, this simple idea is actually not very good. as you'll note, we started with an unbalanced tree, but after we did this operation, the tree is still unbalanced. and in fact, if you keep doing this you'll get a bad sequence of inputs. you can note that there's this loop here, these five rearrangements of the tree. where if you keep doing the appropriate search and then you rotate the searched for a note all the way to the top, they just go in this loop. and when this happens, if you count the total time it takes to perform the sequence of operations, it takes o of n squared time to perform n operations. and so the average time per operation is linear rather than logarithmic which is far to slow. so this rotate things up to the top doesn't actually work very well, we need something better. and for this we're going to make just a slight modification. the rotate to top algorithm basically says you look at the node and its parent and you rearrange them and then you, again, look at the node and its parent and rearrange them, and keep going until you get to the root. the modification, instead of just looking at the node and its parent, you're going to look at the node and its parent and its grandparent. and there are a few cases here. firstly, there's the case where the node and its parent and grandparent are on the same side of it. this is called the zig-zig case. then what we're going to do is we're going to elevate the node up so that it's now the parent of what was its old parent, and that's on top of what was the old grandparent. on the other hand, it could be that the parent and grandparent are on opposite sides of the nodes, what's known as the zig-zag case. then you rearrange the tree as follows, so that the node is now the new parent parent of its old parent and grandparent. and finally there's one more case where the node's parent is actually the root node, so it doesn't have a grandparent. and then you actually just rotate the node up and so if we combine these operations, together we get what's called the splay operation. if you want to splay a node n, and this is a way of bringing the node n to the root of the tree, you determine which case you are in, the zig-zig, the zig-zag, or the zig case. you apply the appropriate local operation to rearrange the tree. and then if the parent of the node is not null ie, if the node is not the root of the tree yet, you splay n again. and you just keep splaying until it gets to become the root. okay so to make sure that we're on the same page with this. if the take the tree up top and we splay the highlighted node number 4, which of these three trees, a, b, or c, do we end up with afterwards? well, the answer, here, is a. so, the point is, we start in this configuration, we note that we're, originally, in the zig, zig case, two, three, and then, four. so, we elevate four, such that three, and then, two come down from it, as children. and then we're in the zig zag case. one and five are on opposite sides of four so we elevate four, one and five are it's new children and three and two now hang off of one and that is exactly what we were supposed to end up with and so that is the answer to this question. okay, so that's what the splay operation is. next time we're going to talk about how to use the splay operation to rebalance your circuitry, and how to use it to perform all the basic binary circuitry operations efficiently. so i'll see you then. 
hello everybody. welcome back. today we're going to talk more about splay trees. in particular, we can tell how to implement your basic search tree operations using a splay tree. so remember, last time, we had this idea to design a binary search tree, where every time you queried a node, you brought it to the root. and we know that simple ways of doing this didn't quite work out so well, so we introduced the splay operation, which is a little bit better. now, there's this problem with the splay operation that the way that the splay trees are built, you don't actually guarantee the tree is always balanced. sometimes you'll end up with very unbalanced trees. and when that happens, your splay operation will actually be very slow because you have to sort of bring your node up to the root one or two steps at a time, and it will actually take a while. however, you'll note that if i have this long stretched out tree, and i splay this leaf all the way to the root, we have rearranged the tree, it's now a little bit more balanced than it was before. and so, when you use the splay operation rather than to sort of rotate to top operation, it's actually the case that you can't have a long sequence of expensive splay operations. because every time you have an expensive splay operation, it will rebalance the tree and make it more balanced. and so, if you keep picking really unbalanced nodes, pretty quickly, the tree will balance itself out, and then you'll have nice, short login time operations. but this does mean that we're no longer dealing with worst case time. well, we need to talk about amortized analysis, average case time. and the big theorem that we're not going to prove today is that the amortized cost of first doing o(d) work, and then splaying a node of depth d is actually o(log(n)), where n is the total number of nodes in the tree. and we'll prove this later, but using it, we can analyze our splay tree operations. and the basic idea is that, if you have to do a lot of work and then splay a very deep node, we're going to be able to pay for that work by the fact that the splay operation will rebalance our tree in some useful way. and that will pay for it and so amortized cost will only be o(log(n)). okay, using this, how do we implement our operations? so a splay tree find is actually very simple. first we find the node in the way we normally would. we then splay the node that we found and then return it. pretty simple. so how does the analysis work? now the node, remember, might not be at small depth. it could be at depth d, or d could be as big as n. we then do o(d) work to find the node because that's how long a find operation takes. we then run a splay, so we did o(d) work, and then we splayed a node of depth d. and so the amortized cost is o(log(n)) for this operation, which is what we want. now, the idea here is that you're paying for the work again of finding this n by splaying it back to the root to rebalance the tree. and so, if the node was really deep, you did do a lot of work. but you also did some useful rebalancing work, which means you're not going to have to keep doing a lot of work. now, there's a very important point to note here, that it could be that we were doing this search, you fail to find a node with exactly that key that you were looking for. but when this happens, you still have to splay the closest node that you found in this operation. because otherwise, what's happening is your operation did o(d) work, but since you're not doing a splay, there's nothing to amortize against. you actually just spent o(d) work. what you need to do is if you're doing this big, deep search, you have to pay for it by rebalancing the tree. and you have to, therefore, splay whatever node you found, even if it does not have the right key. okay, so that's fine. let's talk about insert. insert, it turns out, is also really easy. first, you insert a node in the same way that you would before. and that's o of depth much work. and then you run the splay tree find. you find the node again, and you splay it to the top. it all works. now to get deletes to work, there's actually a pretty clever way to do it. if you splay your node and successor both to the top of the tree, you end up with this sort of third diagram in this picture. and you'll note that if we want to get rid of the red node, all we have to do is sort of promote the blue node, its successor, into its place. because of the way this works out, the blue node will never have a left child, and things will just work. so the code for delete is you splay the successor of n to the root, you then splay n to the root, and then we just need to remove n and promote its successor. so we let l and r be the left and right children of n, and basically what we have to do is we need to make r to become l's new parent and l r's new child, and then set r to be the root of the tree. and once we've rearranged a few pointers, everything works. now, there is one special case here, which is what if n is the largest key in the entire tree, there is no successor, you need to do something slightly different. i'll leave that to you to figure out. finally, let's talk about the split operation. now, the split is actually also very nice with splay trees. the point is there's one case where split entry is really easy. it's if the key at which you're supposed to split is right at the root. because then all you need to do is you need to split things into two subtrees by just breaking them off the root. and so, but with the splay tree, it's really easy to make any node that we want be the root. so what we're going to do is we're going to do a search to find the place at which we are supposed to do our split. take whatever node we found, splay it to the root, and then we're just going to break our tree into two pieces right at the root. so to see pseudocode for this, we're going to let n be what we find when we search for the x that we're trying to split at. we then splay n to the root. now if n's key is bigger than x, we have to cut off the left subtree. if the key is less than x, we cut off the right subtree. and if the key is actually equal to x, well, the x that we're trying to split is actually in the tree. so, i mean, you might do one, or the other, depending on if you actually want to keep the node in the tree, or maybe you want to throw it away, and we just want to return the left subtree and the right subtree. now just to be clear, if we want to say, cut off the left subtree, all we have to do is we let l be the left child, and we just have to sort of break the pointer between our node and in its left child, so that they're now separate trees. and we just return l and n as the two roots. so that's how we do a split. to do a merge, we basically have to do the opposite of this. and the idea is that it's very easy to merge two trees together when you sort of have this element that's in between them right up there at the root. and once again, there's an easy way to do this with splay trees. you just find the largest element of the left subtree, you splay it to the top, and then just attach the right subtree as a child of that node. and then you're done. so, in summary, splay trees. using this, we can perform actually all the operations that we wanted very simply in an o(log(n)) amortized time per operation. and so this provides a very clean way to do this. we left out some things in the analysis though. so if you'd like to see really what the math behind how we can show all of these things work, please come back for the next lecture. 
hello everybody. welcome back. today we're still talking about splay trees and we're actually going to go into a little bit of the math behind analyzing their run times. so remember last time we analyzed splay trees and in order to do so we needed the following important result, that the amortized cost of doing o(d) work and then splaying a node of depth d is actually o(log(n)) where n is the total number of nodes in the tree. and today we're going to prove that. so to do this of course we need to amortize, we need to pay for this extra work by doing something to make the tree simpler. and the way we talk about this being simple is we're going to pick a potential function, and so that if we do a lot of work it's going to pay for itself by decreasing this potential. and it takes some cleverness to find the right one and it turns out more or less the right potential function is the following. we define the rank of a node n to be the log of the size of it's subtree, where the size of it's subtree is just the number of nodes that are descendants of n in that tree. then the potential function for the tree is p is just the sum over all nodes n in the tree of the rank of n. now to get a feel for what this means if your tree is balanced, or even approximately balanced, potential function should be approximately linear in the total number of the nodes. but if on the other hand, it's incredibly unbalanced, say just one big chain of nodes, then the potential could be as biggest n(log(n)). and so, a very large potential function means that your tree is very unbalanced. and so, if you are decreasing the potential, it means that you're rebalancing the tree. so what we need to do is we need to see what happens when you perform a splay operation, what does it do to the potential function. now, to do that, the splay operation is composed of a bunch of these little operations, zig, zig zigs and zig zags and we want to know for each operation what does that do to the potential. so for example when you perform a zig operation how does the potential function change? well you'll note that other than n and p, these two nodes that were directly affected, none of the nodes have their subtrees change at all. and therefore the ranks of all the other nodes stay exactly the same. so the change in potential function is just the new rank of n plus the new rank of p, minus the old rank of n and the old rank of p. now, the new rank of n and the old rank of p are actually the same, because each of those had subtrees that were just the entire tree. and so this is just the new rank of p minus the old rank of n and it's easy to see that's at most, the newer rank of n minus the old rank of n. that's not so bad. now let's look at the zig-zig analysis which is a little bit trickier. so here the change in the potential is the new rank of n plus the new rank of p plus the new rank of q minus the old rank of n, and the old rank of p and the old rank of q. so the new ranks minus all the old ranks. now the claim here is that this is at most 3 times the new rank of n minus the old rank of n minus 2. and to prove this we need a few observations. the first thing is that the rank of n is equal to the old rank of q and that this term is actually bigger than any other term in our expression. and that's simply because, i mean, both of these nodes what are their subtrees? well, it's n, p and q. and then the red, green, blue and black subtrees. they're the same subtrees, the same size. they've got the same rate. but the next thing to note is that the old size of n's subtree and the new size of q's subtree, when you add them together, it's going to be the red subtree, the green subtree, the blue subtree, and the black subtree plus two more nodes. and that's actually one less than the size of either of these two big terms. and what that says, when you take logarithms, you can actually get that the size, the rank of n, the old rank of n plus the new rank of q is at most twice the new rank of n minus 2. because they're sort of half the size each. and therefore, if you combine these inequalities together, you can actually get the one that we wanted on the previous slide. now, the zig-zag analysis is pretty similar to this. here, you can show the change in potential is at most twice the new rank of n minus the old rank of n minus 2. okay, great. so now we perform an entire splay operation. so we splay once, and then again, and then again, and then again, all the way up until we finally have the final version of n that's the root. and we want to know what the total change in the potential function is over all of these little teeny steps. well what is it? well it's at most the sum of the changes in potentials from each steps. the last one you have three times the final rank of n minus the rank of n one step before that, minus two. you add to that the rank of n one step before the n minus the rank of n two steps before the n minus 2. then you add three times the rank of n two steps before the end minus the rank three steps before the n minus 2 and so on and so forth. and this sum actually telescopes. the rank of n one step before the end there are two versions of it and they cancel. the rank two steps before the end, there are two terms that cancel and so on and so forth. and the only terms that are left is well you've got three times the rank of n at the very end of your splay operation, minus the rank of n at the very beginning of your splay operation, and then for each of these steps, for each two levels the n went up the tree, you have this copy of -2. so that's minus the depth of the node app. and so the change in potential is just o of log(n) minus the depth, which is minus the work that you did. and note it's o of log(n) because the rank of n is at most log of the total number of nodes in the tree. and so if you add the change in potential to the amount of work that you did, you get out o of log(n). and so the amortized cost of your o of d work plus your splay operation is just o of log(n). now, that shows there our splay trees runs an o of log(n) amortized time per operation. and if that was all you could say there is nothing really to be too excited about. i mean it gets about the same run time, maybe it's a little bit easier to implement. it's a little bit more annoying because it's only amortized rather than worst case. some operations will be much more expensive than log(n) even if on average the operations are pretty cheap. but another great thing is that splay tree has also have many other wonderful properties. for example, if you assign weights to the nodes in any way such that the sum of all nodes of their weight is equal to one, the amortized cost of accessing a node is actually just o(log(1/wt (n))). and that means that if you spend a lot of time accessing high weight nodes it might actually be much quicker that log(n) time per operation. and so, and note that this run time bound holds no matter what weights you assign. you don't need to change the algorithm based on the weights. this bound happens automatically. and so if there are certain nodes that get accessed much more frequently than others, you could just sort of artificially assign them very high weights and then that actually means that your display tree automatically runs faster than log(n) per operation. another bound is the dynamic finger bound. the amortized cost of accessing a node is o(log(d + 1)) where here, d is the distance in terms of the ordering between nodes between the last access and the current access. so if say you want to list all the nodes in order or search for all the nodes in order, it's actually pretty fast to do a display tree because d is 1. it's a constant per operation rather than o of log(n). another bound is the working set bound. the amortized cost of accessing a node n is o(log(t+1)) where t is the amount of time that has elapsed since that node n was last accessed. and what that means, for example, is that if you tend to access nodes that you've accessed recently a lot. so you access one node pretty frequently and then they move to accessing a different node pretty frequently, then this actually does a lot better again than o of log(n) per operation. finally we've got what's known as the dynamic optimality conjecture. and this says if you give me any list of splay tree operations, inserts, finds, deletes whatever. and then you build the best possible dynamic search tree for that particular sequence of operations. you can have it completely optimized to perform those operations as best possible. the conjecture says that if you run a splay tree on those operations it does worse by at most a constant factor. and that's pretty amazing. it would say that if there is any binary search three that does particularly well on a sequence of operations than at least conjecturally a splay tree does. so the conclusion here is that splay trees, they're pretty fast, they require only o of log(n) amortized time per operation which, remember, it can be a problem if you're worried that the occasional operation might take a long time. but in addition to this, splay trees can actually be much better if your input queries have extra structure, if you call some nodes more frequently than others, or you tend to call nearby nodes to the ones that you most recently accessed, and things like that. but that's actually it for today. that's a splay tree, that's why they're considered to be useful. and that's this course, i really hope that you've enjoyed it, i hope you'll come back for our next course and best of luck. 
hello everybody, welcome to the course on graph algorithms. the first unit in this course, we're going to be talking about various algorithms to compute various sorts of decompositions of graphs. but to begin with, we're going to start with the very basics about what is a graph, what are they good for, and how do we sort of interpret them and draw them. so to start with, a graph is sort of an abstract idea, and it's an abstraction that sort of represents connections between objects. graphs are useful both in computer science and elsewhere in the world, because they can be used to describe many important phenomena. for example, the internet often can be thought of as a graph, where you've got various web pages and theyre connected to each other by links. and this is sort of a very, very high level view of the internet that sort of pays no attention to the content of a given page and other things like that. on the other hand, if you want to do something like google's page rank algorithm, make sort of heavy use of this very high level view of the internet and connectivity within it. another example is maps, you can think of maps as a graph, sort of where you have intersections that are connected by roads. and this is a view that would be very useful if you, say, wanted to plot a course between two locations. you need to find some connections, some path along this map that gets you from one place to the other. social networks can also be interpreted as graphs. you have people connected by friendships or following relationship or whatever, and this provides you another example of graph structure. you can also sort of use more complicated things, if you have some sort of robot arm that can be in many possible configurations. what you could do is, you could think of the configurations, some of them are connected to each other by various motions. and then if you want to solve problems like, how do i best reconfigure this robot arm from this position to this other position, you again, want to sort of consider this graph relating these things to each other in order to figure out how best to do that. okay, so these are the sorts of problems that we would like to be able to deal with using graph theory. so what is a graph? well, the formal definition of an undirected graph we'll talk about directed ones in a few lectures is that it's a collection v of vertices. and then a collection e of edges, each of which connects a pair of vertices to each other. so when you draw them on the paper what you do is your verses are usually drawn as points or circles or something. and the edges are lines or curves or something that connect pairs of these point. so for example the graph drawn on the screen has four vertices labelled a, b, c, and d and it also has four edges one between a and b, one between a and c, one between a and d and one between c and d. and so to make sure that were all in the same page in we draw the following graph below, how many edges does it have? well, the graph here has 12 edges. you can sort of count them all and each of these segments between two of these points gives us an edge. and so, there are 12 of them. now, there's one thing that sort of fits into this definition of graphs that i gave that is a little bit weird perhaps, or unusual. one thing that you might have is loops, this would be an edge that connects a vertex to itself. so here we just have the one vertex a, and then a loop that connects a to itself. and there are some instances in which you might want to use this. sometimes you actually do have traffic circle that just goes in a loop or something like that. but only sometimes this is the sort of a thing we we're talking about. also sometimes you can have multiple edges between the same pair of vertices. so in this picture we have two vertices a and b and two edges between them. and somehow if you're just talking about, can i get from a to b? it doesn't matter if there's one edge or two between them. but sometimes having both of them is relevant. on the other hand a lot of the time, these loops and multiple edges are just a sort of a that we don't really need to deal with. so often times you work with what are known as simple graphs which don't have loops and don't have multiple edges. and so, this is sort of the basics of what a graph is that we've just covered. next time we're going to talk about a couple of things. first thing we're going to talk about how to represent graphs on a computer and then we're going to talk a little bit about how we talk about runtimes for graph algorithms. so i hope this has been a useful introduction and i'll see you at the next lecture when we talk about these other things. 
hello everybody, welcome back to our graph algorithms. of course, today we're going to be talking about how to represent graphs on a computer. and then after that we are going to talk a little about how to talk about graph runtime instead of how it depends on one graph you are using. so the first thing is graph representations, so from last time recall, the graph is a thing that consists of a bunch of vertices sometimes called nodes and also a bunch of edges that connect pairs of these vertices. and we're going to want to do is we're going to want to do a lot of computations on graphs to determine various properties of them, but before we can get to any of that, we first need to talk about how to actually represent a graph inside a computer. and there are in fact several ways to do this, and exactly how we do it will affect the runtimes of some of these algorithms. so we're going to spend a little bit of time talking about that. so, the first thing to do is well you want your vertices and you want your edges. a natural way to do this is just store a giant list of edges. each edge is a pair of vertices so this is a big list of pairs of vertices. but now what we have is we've got these four vertices, a, b, c, and d. and then we just store a list. one is an edge between a and b, and then there's an edge between a and c, and there's an edge between a and d, and there's an edge between c and d. and this is a perfectly good way to store this graph on a computer, but there are other ways of thinking about it. one of them is the following. so at least, that you have a simple graph. we're only didn't really matter is which pairs of vertices have edges between. and so, if i gave you a pair of vertices. so, it's the only thing that you need to know is you need to know, is there an edge between the more is there not. so we can do is we can just rebuild a look-up table for this, we build a matrix where you have an entry that's 1 if there is an edge between that pair of vertices and a 0 if there's not. so for example, in this graph there is an edge between a and d and therefore the ad entry of this matrix is 1. there isn't an edge, however, between b and c, so the bc entry is 0. and so you just fill this in, you get this nice 4 by 4 matrix of 0s and 1s. it is another way of representing this graph. on the other hand there is sort of a third maybe a hybrid way of looking at things. and the idea here is that for each vertex in the graph it's going to have a bunch of neighbors, it's going to have other vertices that have edges from the one that we're considering to them. and another way of representing the graph is for each vertex we can just keep a list of its neighbors. so vertex a in this graph is adjacent to b, c, and d. b is adjacent to a. c is adjacent to a and d. d is adjacent to a and c. so for each of these vertices we store a list of just all of it's neighbors and that's another way to represent the graph. now for example, just to be sure we're on the same page. if we have the following graph. what are the neighbors of vertex c? well, we just draw all the edges that leave c and it connects to a, and b, and d, and f, and h, and i. and, those are its neighbors. okay, but so we have these three ways of representing a graph. and it turns out the different basic operations you want to perform could be faster or slower depending on which representation you have. for example, if you want to determine if there's an edge between this specific pair of vertices. this is very fast in the adjacency matrix representation. all you have to do is check the appropriate entry of the matrix. see if it's 0 or 1. and that's your answer, constant time. however, when you have an edge list, the only thing that you can really do is scan the entire list of edges to see if that particular edge shows up. that takes time proportional to the number of edges. for the adjacent lists, it's a little bit faster than that. what you do is you pick one end of this edge that you're looking for and then look it for all it's neighbors who wants to know if a is adjacent to b, you look at the list of a's neighbors and see if b is on the list. and so the time there is proportional to the degree of the vertex. the number of neighbors that this one guy has. now another thing you might want to do is list all of the edges in the graph. and the adjacency matrix is terrible here because the only thing you do there is you scan through every entry of the matrix. and each one gives you an edge, but that takes time proportionate to the number of vertices squared. however, the edge list does this very easily. it just lists all the edges in order which is what it does. the adjacency list is about as good though, because you just need to, for each vertex, you find all of its neighbors. those are a bunch of edges. and you just do this for every vertex, and this actually counts each edge twice, because if there's an edge between a and b you count both. that a is neighbor of b and that b is a neighbor of a. but the only counts them twice so the time is still proportional to the number of edges. finally, if you want to list all the neighbors of a given vertex, the adjacency matrix is pretty slow here again. because the only thing you can really do is scan through that row of the matrix, and find all the ones. now, in the edge list representation, you have scan through all the edges in your graph to see which ones include the vertex you want. whereas the adjacency list, you just scan through the list of neighbors of the vertex, and it's very fast. now, it turns out that for many problems that we'll be considering this on, and throughout most of the rest of this unit on what we really want is the adjacency risk representation of our graph, just because a lot of the operations that we need, we really want to be able to find neighbors of a given vertex. okay, so that's how we represent graphs. let's spend a little bit of time to talk about runtimes of graph algorithms. so, when we talk about algorithm runtimes, it's generally a function of the size of the input. and for the most of the algorithms we've seen so far, the input sort of has one size parameter, n. and so you runtimes like o of n squared or o of n cubed, or something to that effect. however, graph algorithms, well, the graph sort of has two measures of its size. the number of vertices and the number of edges. and so graph algorithms runtimes depend in some way on both of these quantities. you could have runtimes like o size of e plus size of e, which is generally considered to be linear time. but you can also have o of size of e, times size of e or size of v to the three-halves or v log v plus e. and there are many different possibilities for types of runtimes that we have. and an interesting question now is, again, if you sort of only have one parameter, it's easy to compare. n log n runtime versus n squared versus n cubed. but if you want to say which one's faster, say one algorithm runs in time size of v to the three-halves and the other one runs in time size at e. it's not actually clear which one's faster. and in fact which algorithm is faster actually depends on which graph you're using. in particular depends on the density how many edges you have in terms of the number of vertices. if you have many, many, many edges o of e is going to be worse. however, if you have few edges its going to be better. and in terms of density theres sort of two extremes to consider. on the one hand, you have dense graphs. here, the number of edges is roughly proportional to the square of the number of vertices, meaning that almost every pair of vertices or some large fraction of the pairs of vertices actually have edges between them. if you're banned, you're going on tour and want to like plot a route between a bunch of cities, well there is actually some transportation option that would get you between basically any pair of cities on the map. now, it might be indirect or take a while but you should still probably plot all of these out in order to plan your tour, just what will matter is not whether or not it's possible to get between two cities, but how hard it is to get between these cities. but for this time, you actually want sort of a dense graph, that talks of all pairs relations between these vertices. on the other end of the spectrum, we have sparse graphs. here the number of edges is small relative to the number of vertices, often as small as roughly equal to the number of vertices or some small constant times the number of vertices. and in a sparse graph, what you have instead is that each vertex has only a few edges coming out of it. and this is actually very reasonable for a lot of models you want to think about. if you think of the internet as a graph, well, there are billions of web pages on the internet, but any given web page is only going to have links to a few dozen others. and so the number or the degree of any given vertex, the number of neighbors that any given vertex has, is much smaller than the total number of vertices, so you end up with a very sparse graph. now a lot of things like this, like the internet, like social networks, like actual maps that you've drawn on a piece of paper, these tend to be sparse graphs. whether or not your graph is sparse or dense will affect the runtime of your algorithms and may even help determine which of your algorithms you want to be run. okay, so that's it for today. now that sort of the basics are out of the way we're going to actually talk about some of the algorithms and in particular we're going to talk about how to explore graphs and sort of figure out which vertices can be reached from which others so come back next time and we'll talk about that. 
hello everybody, welcome back to the graph algorithms course. today we're going to talk about algorithms for exploring graphs, in particular, ways to tell us which vertices can be reached from which others. so for an example of the type of problem we're trying to solve here, suppose that you're playing a video game. and you found the exit to some level, but you don't want to go there just yet. you want to first sort of explore this whole level and make sure you found all the secrets inside it, make sure you get all the loot and xp that there is to be found. and you can certainly wander around between the various rooms and find a bunch passageways and so on. and you've been wandering around for a while and maybe haven't discover anything new yet. but you'd like to make sure that you've really found everything before you leave. how do you accomplish this? how do you make sure that found everything? and this sort of is a notion of exploring a graph. you've got theses rooms, they're connected by passageways. and you want to make sure that everything, at least everything that's reachable from where you started, you can actually get to it and find, explore the entire region. and this actually sort of, this of related questions are actually very useful in a number of applications. for example, if you have a map and you want to find a route between some location on the map and some other, it often depends on this sort of exploration of the graph, making sure that you can find some sequence of roads to follow to get you where you want to go. this could also be used if you're sort of building a road system and want to ensure that the entire thing is connected. you need to have some sort of algorithm to tell what can be reached from what. and finally, it has more recreational things, the video game example, but also if you want to solve the maze. this is very much, well, can i get from the start to the finish in this sort graph that connects things? but also various puzzles, if you think of sort of vertices as sort of possible configurations. and then edges that describe moves that you can make and want to say, can i rearrange the puzzle to end up in this location? again, this sort of exploration algorithm becomes very important. so before we get into this too much, we really want to formalize what we mean. in particular, we want to know what does it mean to be reachable from a given vertex. and basically, the idea is you start at the vertex and you're allowed to follow edges of the graph. and you want to able to see what you can end up at. and so to formalize this, we'd find a path in a graph g to be a sequence of vertices v0, v1, v2, etc., such that each vertex is connected to the next one by an edge of the graph. so we get some sequence of edges to follow. so the formal description of the problem that we would like to solve is given a graph g, and a vertex s in g, we'd like to find the collection of all vertices v in the graph such that there is a path from s to v, everything that we can reach from s. so, just to get a little bit of practice, if we have the following graph, which vertices here are reachable from a? well think about it a little bit and you find out that a, c, d, f, h, and i are all reachable. and it's easy to see that these vertices sort of do all connect up from edges, but you then can't get to b, e, g or j. and this is sort of because there are no edges that connect any of these vertices we can reach to any of these other vertices. and so there's no way to escape, except for these six. and this is sort of the actual idea behind the algorithm. what you want to do is you want to make sure that you can actually find everything that you can reach. and so what you do is you sort of expand, you find a bunch of vertices. these are a bunch that i can reach. but then if there are any edges that connect ones that you can reach to ones that you can't reach so far, well, you have to sort of explore those edges and find the new vertices on the other end, and sort of add them to the list that you know about. and you sort of keep expanding this list of vertices that you know you can get to until you can't connect to anything new and then you're done. so to formalize this algorithm a little bit, we are going to keep a list of discoverednodes. and this starts out just with the vertex axis you're supposed to start at. but then what you do is while there is an edge e that leaves this set of discoverednodes that connects to something you have discovered to something you haven't discovered, then what you do is you take the vertex at the other end of that edge and add it to your list of discoverednodes. and you just keep doing this until there's nothing new to be found. and then you return this list of discoverednodes. okay, that's a reasonable algorithm and it does work. but in order to really code this up, you need to do some work to handle the bookkeeping that's required for this algorithm. you need to do things like you need to keep track of which vertices you've discovered and which edges you've dealt with, which edges you've actually checked and which ones you haven't. you also need to know sort of which order to explore new edges in. if there are several possible edges to follow, which one do you follow next? and so, what we're going to do now is talk about a specific way to implement this and deal with these sort of bookkeeping isues. the first thing that we need to do is we need to keep track of which vertices we've already found. and for this, we're going to associate a boolean variable to each vertex visited(v) which basically tells us have we visited it yet. the next thing that we're going to need to do is we need to, most of the vertices that we visited will actually will have already sort of checked all of the edges relating to them. but some we haven't and we somewhere need to keep track of the list of vertices that still have edges hanging off of them that might connect this to something new. now this list isn't going to appear explicitly in our program. it'll actually sort of to be hidden in the program stock so this is that points a little bit sudden. we'll sort of see it once we introduce the algorithm. the final thing is we need to discover which order to discover, to follow new edges in. and for this we are going to use what is known as the depth first order. what this means is we're just going to start our initial vertex and we're just going to start following a chain of edges. we're just going to follow some really long path forward until one of two things happens. one thing is we could stumble across a vertex that we have already seen before. in which case there's no reason to have followed that edge and we'll just back up to where we were before. the second thing that could happen though is that we hit a dead end. and we actually hit a dead end and can't go any further forward, then we actually back up. and then once we back up though, we don't just back all the way to the beginning. we just back up once step and then try going forwards again from that new vertex that we found. okay, so that's the basic idea. how do we implement this? well part of the beauty about this is that we have a very simple recursive algorithm. so explore(v), the first thing you do is you set the visited marker of v to be true. we say we have visited it. next, for each neighbor w of v, for each w that's adjacent to v, if w has not already been visited, we recursively explore w. okay, so this is a very compact program. let's sort of actually see what it does on an example. right, also i should mention that in order for this program to execute reasonably efficiently, we really want an adjacency list representation of our graph. that's because we have this for loop we want to iterate over all of the neighbors of v in our graph. and for that, if you have an adjacency list, which gives you a list of all the neighbors of v, that's incredibly easy to do. if you don't have an adjacency list on the other hand, this algorithm really isn't that efficient. okay, fine. so let's look at in the example. here's a graph, we're going to start by exploring that vertex. so we mark it as visited. we then check for unvisited neighbors. and hey, look there is one. so we recursively explore that other vertex. we mark it as visited. we search for unvisited neighbors. and we have this one. so remember now we're sort of three layers into the program stack here. this is sort of a sub routine of a sub routine. but now when we're exploring this vertex it has no unvisited neighbors. so after we've done a little bit of checking, we decide that we're done exploring this guy and we pop the stack. this other guy still we visited both of his neighbors, we pock the stock back to the original explorer call. now this vortex does have some unvisited neighbors left, so let's visit one of them and explore that. mark it as visited, find an unexplored neighbor, explore that. mark as visited, unexplored neighbor, mark it as visited, unexplored neighbor. now when we explore this vertex though, once again we're stuck. so we wrap up exploring that guy, pop a level up the stack, go back to exploring this other vertex, who now actually does have another unvisited neighbor. so we're going to go visit that one. now we've actually visited everything in the graph. so all we're going to do is at each vertex, we're going to note that all of their neighbors have been visited. we're going to pop up the stack and get back to where we started and conclude. so here we actually have found all these vertices. and in fact if you look at it, we've actually figured out how to reach them all. if you look at sort of these darker edges which are the ones our algorithm sort of actually followed when you ran it, these sort of connect up to all the other vertices. and they actually tell us if you sort of follow these edges, they give you a unique path to any other vertex that we can reach. okay, so that's our algorithm. let's talk about correctness. and the theorem is that if all the vertices in our graph start as unvisited, when we run explore(v) it marks as visited exactly the vertices that are reachable from v. and the proof here isn't so bad. the first thing to note is that we only ever explore things that are reachable from v. and that's because, well, the way our recursive calls work are we either start at v, or we explore a neighbor of a vertex that we've already explored. so any vertex that we end up exploring has to be a neighbor of a neighbor of a neighbor of a neighbor of a neighbor of a neighbor of the original vertex or something. but that does basically give us a path and say that wherever we got to was reachable. the next thing to note is that a w, vertex w, is not marked as visited unless it has already been explored, which is just the only way we mark things as visited is when we explore them. but finally, we should note that if w gets explored, well, we then look at all the neighbors of w. and either those neighbors have already been visited, in which case it means they've been explored at some point, or we end up exploring them. so in other words, if w gets explored, all of its neighbors also get explored. so to finish things, suppose that we have some vertex u that is reachable from v. that means that we've got a path from v going up to u. and if we actually explored everything along this path we'd be done. we would have explored u at some point. so let's assume that we don't. assume that w's actually the furthest thing along this path that we've explored. however, by what we had on the previous slide, if you explore a vertex you also explore all of its neighbors. so the next vertex z along this path must also be explored. and so this is a contradiction. this says the only way this can work is if we actually explored every vertex along the path. but that means we've explored u, which is good. okay, so this explore algorithm is actually really great if we just want to find the vertices that are reachable from a given one. but sometimes you want to do a little bit more. you actually want to find all the vertices of the graph g, not just those coming from a given one. so for this, we're going to use a slightly more complicated algorithm called depth first search. and for this what we do is the following, first we mark everything as unvisited. then for each vertex in the graph, if it has not yet been visited, we explore that vertex. so to look at an example on this graph, we find an unvisited vertex, say that one, and we explore it. so we find a neighbor and another neighbor and then we pop back up the stack. and then we find something adjacent to our original vertex and come back. and now we're done exploring that first vertex. so now we look for a new vertex we've never visited before, like say that one. we explore its neighbor, come back, we're done exploring that guy. we find a new vertex that we haven't visited, that one. we explore its neighbor and his neighbor and then come back. and now that we've actually visited all the vertices, only now do we sort of wrap up and conclude our algorithm. so to analyze the run-time of this algorithm, we have to note a few things. firstly, whenever we explore a vertex, we immediately mark it as being visited. that's the first thing that we do. the next thing to note is that no vertex ever gets explored if it's already been visited. and in fact if you look at every time we make an explore as even a recursive call, then we always first check if it has not been visited, then we explore it. and this means that no vertex gets explored more than once. in fact, this means that each vertex gets explored exactly once because the outer loop in the dfs explores every vertex if it hasn't already been visited. but each vertex in our graph gets explored exactly once. but for each vertex, when we explore it, there's this inner loop where we have to check all neighbors of that vertex. and so we have to do work for each neighbor of each vertex. and so we have to be more proportional to the total number of neighbors over all vertices. and that's proportional to the number of edges because each edge connecting a and b says that a is a neighbor of b and that b is a neighbor of a. so it contributes to two neighbors. but the total amount of work is still o of the number of edges. so total work we do, o of one work per vertex and o of one work per edge and the total run time is o of size of v plus size of e, a nice linear time algorithm. so that is depth first search. next time we're going to talk a little bit more about reachability and graphs and give some more applications of this algorithm. so, i'll see you then. 
hello everybody, welcome back to the graph algorithms course. today we're going to be talking about notions of connectivity in a graph, in particular, talking about connecting components, and how to compute the connected components of an undirected graph. so, to begin with, last time we talked about this notion of reachability, what does it mean to be able to reach another vertex from a given one? and what we'd like to understand now is really what sort of classifies reachability in a graph? really not just from a single vertex but you sort of want to understand which vertices are reachable from which others? and it turns out that for an undirected graph, there's actually a very clean categorization of this. in particular for any graph g, we can always partition it into connected components such that two vertices v and w are reachable from each other, if and only if they are in the same connected component. so, sort of we break this graph up into islands, where any two islands are completely unconnected from each other, but within any island, you can get from anywhere to anywhere else. so just to make sure we're on the same page. if we have the following graph, how many connected components does it have? well, this graph is going to have 4. so if you look at these four things, from each of these things they're all connected up, but there are no edges between any pair of them. okay, so let's see how the proof of this theorem works. why can we always divide it into connected components? and basically the idea is you need to show that reachability is an equivalence relation. that means three things, first is that v is reachable from v which is easy. also that if v can be reached from w, w can be reached from v which sort of is just following the path in the opposite direction. and then there's the difficult one, that if v is reachable from u and v is also reachable from w, then w is reachable from u. what this says if you sort of take v and you take everything that it connects to, in fact, everything in that whole region connects to everything else in that whole region. and that last step is not so hard once you write it down. the point is that you can reach u from v, which means there's a path from v to u. you can also find a path from v to w. and if you should have glued these paths together at v then you have a path from u to w. so they can be reached from each other. okay, so that completes the proof but what we'd now like to do is do this algorithmically. given a graph g, we'd like to find the connected components of this graph. okay, fair enough. let's find an algorithm. well the basic idea of this is pretty simple. if you run explore(v), this finds the connected component of v. it finds everything you can reach from v. that's the whole connected component. and you just need to repeat this to find all the other components. it turns out you can do this actually with only sort of a slight modifications of the original depth first search algorithm tha we saw. now, in order to make this algorithm, we're about to see a little bit more cleanly. we're going to modify our goal a little bit. instead of actually returning sort of returning sets of vertices that are our connected components, what we're going to do is we're going to label the connected components, we're going to give everything in the first one a label of 1, and everyone in the second components a label of 2, and so on and so forth. okay, so how does this work? first we modify our explore procedure, we visit the vertex, we also make sure to visit all of its neighbors. we also assign the vertex a number corresponding to it connect component, the cc number. now the thing is that this variable cc is not going to change as we make these recursive calls to explore. and so everything else that gets found through the same explore will all get assigned the same number. however when we actually run our dfs first search, we're going to between different explores, between different vertices that we explore and then go off to find a new vertex, we increment this number. and so the second explore that we do everything gets the number one bigger. so to see how this actually works, on this graph our counter starts at 1. we find it on vertex say that one, it gets labeled with a 1. we keep exploring from that vertex and we find these other three, all of them get labeled with 1. but now we are done exploring that first vertex, so we increment the counter and find a new unexplored vertex, say that one. which we now label 2. we keep exploring and the other vertex that we found also gets labeled with a 2. but then we finish that explore call. we now increment our counter to 3 and find a new vertex, this one. we explore that and everything we find gets labeled with a 3. and now we're done. we've got three connected components. everything in the first one is labeled 1. everything on the second one is labeled 2 and everything in the third one is labeled 3. so sort of what we wanted. okay, to show that this is correct, we note that each new explore call finds a full, new connected component because it finds everything connected to that v. and of course, because v had not been found yet, it was not in a connected component that we had already seen. secondly, because of the way this outer loop and depth for search works, every vertex eventually gets found, because every vertex we eventually check to see if it's visited, and if not, we explore it. now the run time of this is basically just the run time of depth for search. we only made sort of cosmetic modifications to it. so the run time is still o of size of v plus size of e. okay, so that's our discussion of how to compute connected components. next time we're going to talk about some other applications of depth for search. so i'll be happy to see you in the next lecture. 
hello everybody, welcome back to the graph algorithms course. today, we're going to talk about previsit and postvisit orderings. and these are sort of some numbers that you can do when running a depth for search that sort of records some information about the order in which you visited vertices. and and we're going to discuss a little about why these numbers might be important. but mostly it's going to be that they will turn out the be very useful in some algorithms we're going to discuss later. okay, so let's talk about depth [inaudible], it is algorithm that we came up last time. and it's a little bit weird as an algorithm, because well what happens when you run depth for surge? it doesn't return anything, it does modify the state of some things. it marks vertices as visited or unvisited. but in the end it just ends up marking any vertex as being visited. if all that you wanted to do is mark every vertex as visited there are easier ways to do it. on the other hand the order in which we find these vertices and away that involves their connectivity is actually very useful. for example with slight modification, keeping track of a little bit of data we saw how to use depth for a search to compute connected components. so in general, if we want to make that depth [inaudible] useful, we need to keep track of some extra information about its execution. so what we're going to do for this is we're going to augment some of its functions in order to store additional information. so for example, let's look at explore. what we're going to do is we're going to mark the vertex as visited. but then before we do anything else, we're going to run some previsit function. this is just some extra things that we're doing, just to maybe record some information or do a little bit of extra work sort of just as we found this new vertex. then we go in this loop over neighbors and exploring all the unexplored neighbors. and then right before we're about to finish exploring v, we run some other post visit blocks. some thing that we did, do it right before we're finishing. so what are these functions going to be? they could be a number of things. we'll come up with an example shortly, but the idea is to augment our functions a little bit to keep track of some extra information. so what sort of extra information do we want? well one that we might want to do is to keep track of sort of what order are we visit vertices in. and so one way to do this is we sort of have a clock. this clock keeps track of time, it ticks forward once every time we hit a previsit or postvisit for a vertex. every time we discover a new vertex for the first time or sort of leave an old vertex for the last time. and every time we do one of these things we'll also core the time which that happens. so for each vertex, we will have a record of the previsit time and the postvisit time. so to what you seen what i mean by this, let's look again this example. the clock starts at 1, we visit our first vertex, its gives us previsit number assigned as 1. from there, we explore the second vertex which is previsit 2, and then a third vertex which gets previsit of 3. from there, we start, well, we're done exploring that vertex. all of its neighbors have already been visited. so we assign it postvisit 4 and move on, this other vertex gets postvisit 5. we now have a new vertex to explore, it gets previsit 6 and postvisit 7. and our original postvisit 8 and that wraps up our first explore. we now find a new vertex, who gives previous at 9 and his neighbor previous at 10. and then they get post visit numbers 11 and 12. finally, we've got a new guy with previsit 13 and his neighbors get 14 and 15 and they get postvisits 16, 17 and 18, we wrap them up. and we are now done and we just assigned the numbers 1 through 18 as the previsit and postvisit numbers of these 9 different vertices. so the way we compute this is actually pretty easy. we just have to initialize our clock to be 1 the beginning of our depth first search. and then in the previsit block of our explorer, we set the previsit number of v to be this clock value and then increment the clock. and for the post visit block we set post visit number of the vertex to be the clock value and increment the clock. very easy, doesn't really change the run time of anything, but allows us to compute these numbers. now, what are these useful for? well, really the previsit and postvisit numbers, the tell us something about the execution of the depth first search. and we'll see a little bit of this as we prove the following lemma. so suppose that you have any two vertices u and v. from these u, v intervals pre(u) to post(u) and then pre(v) to post(v), these are two intervals. and the claim is that these intervals are either nested or disjoint. and in a particular whether are nested or disjoint will actually tell us something useful about the sort of way in which the depth for search ran on these two vertices. so let's take a look at what we mean by this. if you have two intervals, there are a few ways in which they can intersect with each other. firstly, could be the case that one interval is strictly contained in the other one. these means that they're nested. it could also be that they're disjoint from each other, that they don't overlap at all. finally, we could have two intervals that are interleaved. they sort of overlap over part of their ends, but not over the entire interval either side. and what we're saying in this dilemma is that the interleaved case is actually not possible. so let's see the proof. we first assume that without laws of generality, the first visit to u happens before the first visit to v. we now have two cases to consider. first thing that we find v for the first time in the midst of exploring u. and this really is an indepth research tree that we produced by this exploration. we found v as a descendant of u, we found v while exploring u. the second thing is we could find v after we're done exploring u. so it's sort of v and u are different branches of the tree, they're cousins of each other. so let's look at the first case. if we explore v while we're in the middle of exploring u, then explore v is actually being run as a subroutine of explore u. and because of the way subroutines work we can't finish exploring uuntil we are finished exploring v. and therefore the post of u is bigger than the post of v and these two intervals are nested. in the second case, we explore the after we're done exploring u. so we start exploring u and then we finish exploring u, and then sometime later we start exploring v and then finish exploring v. and so these intervals are going to be disjoined. and so these are the only cases the intervals are nested or they're disjoined. and which case we're in will actually tell us something useful about the order in which we visited vertices in this graph. now just to review we've got these two tables here sort of pre and post numbers, but only one of them is a possibly valid table of pre and post numbers. which one is not valid? well the one on the right can't be correct, because these two intervals from pre to post for vertices a and b are interleaved, and that can't happen. so that sort of pre and post orders will see a lot more usefulness later, but in the next lecture, we're going to introduce something a little bit more new. we've been talking a lot about undirected graphs. here, we're going to be talking about what happens when the edges actually have an orientation to them. so please come back next time, and we can talk about that. 
hello, everybody. welcome back to the graph algorithms course. today, we're going to start talking about directed graphs versus undirected, in particular, talk about directed acyclic graphs and some of their properties. so what's the motivation for this? the point is that sometimes we want to talk about the edges of a graph that have a direction. this is just because i mean, sometimes, like pairs of things are related in a way, but the relation isn't symmetric. one of them comes before the other, or it's a one way viewership or something like that. and so we define a directed graph to be a graph, where each edge has sort of a designated start end and an end end. so what are examples where you might want to use this? for example, if you've got a city map where lots of your streets are one way roads, the direction that the road's pointing is actually very relevant if you want to navigate the city. you can't just follow any series of roads you like because you'll be driving down some one way streets in the wrong order, so you really do need to keep track of this orientation if you want to navigate the city. but then also some of the examples that we gave, links between webpages, the web graph is probably a directed graph, because usually, if you've got two webpages and a links to b, b probably doesn't link back to a. similarly, if you have followers on a social network, it depends on the social network. i mean, in facebook, friendships are symmetric. so they're sort of two-directional. that might be an undirected graph. but on lots of them, you can follow somebody without them following you, and so then you end up with wanting to have a directed relation for when someone's following someone else. a final example that we'll look at in some more detail today are sort of dependencies between tasks. so, we have this directed graph, and we've already built up a lot of this theory that works for undirected graphs with this sort of exploring dfs algorithms. but most of this sort of actually still holds for directed graphs. we can still run dfs on a directed graph. the slight modification now is that when we run our explores, we only want to check for directed edges out of v. so when we say for all neighbors w of v, if w has not been visited, etc, we really want to say neighbors where v points to w, not the other way around. and what this will do is it means that when we explore v, we find things that are actually reachable from v, using the edges in the direction that they're intended. so we're only sort of allowed to follow these one-way roads in their appropriate direction. now using this new depth first search, we can still compute pre- and post-orderings. they still have many of the same properties. the algorithm for dfs still runs in linear time. basically, everything's the same. the context is now a little bit more general. okay, so let's look at a sort of specific example, where directed graphs are important. in particular, suppose that we have the following morning routine. we've gotta do a bunch of things every morning. we need to wake up, we need to get dressed, we need to eat breakfast, we need to go to work, we need to shower, all of these things. we need to do these things in some order, but we can't do them in any old order. because well, we need to wake up before we get showered. and we need to dress before we go to work. and we need to eat breakfast before we go to work, and all of this stuff. and one way of representing these sorts of dependencies is by a directed graph. if we need to do a before we can do b, then we draw a directed edge from a pointing to b. and so, this gives us some sort of dependency relation. and it's not just these sort of trivial examples of how do i get dressed in the morning. but if you've got some sort of complicated system of libraries, where some of them require other ones in order to work, you can end up with some sort of similar graphic dependencies, which you actually do need similar techniques to handle. okay, so what do we do when we have these dependencies? well, one of the things that we'd like to do is we'd like to find the ordering of the tasks in a way that respects these dependencies. we'd like to wake up before we, well, fine. for example, suppose that we woke up at 7 o'clock and then showered at 7:05, got dressed at 7:15, had breakfast at 7:20, and went to work at 7:30. this puts all of our events in some order. and you'll note that this order respects all of our dependencies. we wake up before we shower, before we get dressed, before we go to work. and we eat breakfast before we go to work, and everything works out nice. and so, if we have one of these dependency graphs, we'd like to linearly order the vertices to respect these dependencies. now one thing to ask is, is it always possible to do this? and it turns out the answer is no. the sort of best counter example is this following chicken and egg problem. i mean, the point is that you need a chicken in order to lay eggs, and you need eggs to hatch chickens. and so you can't like put them in some order where one of them comes first. if you put chickens first, then it would just point you from eggs back to chickens going in the wrong direction. you put eggs first, there's this pointer from chickens back to eggs in the wrong direction. without someplace to have gotten started, there's sort of no way you can get this ordering and go. in fact, in general there's a slightly more complicated way in which this can fail to happen. in fact, if your graph has any cycle, a cycle here is a sequence of vertices, v1, v2 to vn, such that each one connects to the next using a directed edge. so you've got a bunch of vertices arranged. the circles just that each one connects to the next one all the way around. and the theorem is that if g contains a cycle, it cannot be linearly ordered. okay, so just to make, well, fine. let's take a look at the proof here. so suppose their graph has a cycle, v1 through vn, everything connected up in order. and suppose that additionally, we can linearly order this graph. well, if you linearly order these things, there are finitely many. one of them needs to come first. so suppose that vk comes first in this order. but now we're putting vk before vk-1, and vk-1 points to it, so we have an arrow pointing in the wrong direction, which gives us a contradiction. so, if we have a cycle, we cannot be linearly ordered. so, what this means is that in order to be linearly orderable, you need to be what's known as a directed acyclic graph or dag. and this is just a name for a directed graph that has no cycles, fair enough. now, by the above theorem, it is necessary to be a dag in order to linearly order. but one question we should ask ourselves perhaps is, is it sufficient? can we necessarily linearly order it if it's a dag? well, okay, this is a question to ask a little bit later, but for now, let's just review. we have the following four graphs. note that the edges here are sort of the same, except for their orientations. and which one of these graphs is a dag? well, it turns out that only a is. b has the cycle noted in red, and c has this other cycle noted in red. but if you work out a, you can actually see that it does not have any cycles to it. okay, but the question that we were posing was, is it the case that any dag can be linearly ordered? and the theorem that we'll actually show is that yes, if you have any directed acyclic graph, you can linearly order it. and next time, what we're going to do is we're going to prove this theorem. but not just that. in addition to proving this theorem, we're actually going to make it algorithmic. we're going to come up with an algorithm that, given a dag, actually produces this linear order. so that's what you have to look forward to next time, and i'll see you then. 
hello everybody, welcome back. today, we're going to be talking about the algorithm of a topological sort. and we're going to talk about this, we're going to show in fact that any dag can be linearly ordered, and we're going to show you how to do it. so remember from last time, we were talking about directed graphs and in particular we wanted to be able to linearly order the vertices of this graph. such that every edge pointing to a prome vertex with smaller index under this linear ordering something that's larger. now we know that in order for this to be possible, our graph needed to have no cycles, it needed to be a dag. and today, we're going to show that this is actually sufficient. so one way to look at this is if you have this linear ordering, let's think about which vertex comes last in this order. well if it comes last it can have edges pointing to it, but it can have no edges pointing out of it. and so this gives a motivation for useful definition. we say that the source is vertex in a graph that has no incoming edges. so a source can have as many edges as it likes going outwards, but it can't have any going inwards. similarly a sync is a vertex with no outgoing edges. it can have a bajillion edges coming into it, but nothing can escape. so to be clear that we're on the same page, the following graph has nine vertices, how many of them are sinks? well the following highlighted three vertices are sinks. they each have no edges coming out, but every other vertex in the graph does have a few or at least one edge leaving it. so here's the basic idea for how we're going to produce our linear order. we're going to first find a sink in the graph. there needs to be a sink, because there needs to be something at the end of the order. and when we have a sink though, we're perfectly fine putting it at the end of our ordering. and the reason for this is well, it's got edges that point into it, but as long as the vertex comes at the end, everything pointing to it is coming from before it. so once we put it at the end of the order, it's sort of dealt with and we just need to order the rest of the graph. so what we do is remove that vertex from the graph and repeat this process. so to see what this means, we have this graph on five vertices. d is a sink so we put it at the end of our ordering and remove from the graph. now we find another sink, say c. put it at the end and remove from the graph. e is next, goes at the end, then b, then a. and finally we have this ordering, a, b, e, c, d. and this it turns out, is consistent with the ordering that we had in our original graph. now this is all well and good, but it depends on us being able to find a sink. and before we even ask that, we should ask how do we even know that there is a sink in our graph. and it turns out that for dag at least, there's an easy way to show that there is, and the idea is the following. what we're going to do is just start at some vertex v1 and we're just going to follow a path. we're going to follow directed edges forward and just keep going, finding vertices v2, v3, v4, etc, and just keep going. now eventually one of two things will happen. either we could hit a dead end, we could end up at some vertex with just no outgoing edges, we just can't extend this path anymore. and if that's the case, we found a sink. we found a vertex with no outgoing edges. the second possibility though is that maybe this path just keeps going forever. but if it does since there are only finitely many vertices in our graph, eventually we'll have to repeat something. we'll have to find a vertex for the second time. but when we do that it means there was a path that started at that vertex, went forward a bunch and eventually came back to itself. that means we have a cycle, and so at least if we're dag that can't ever happen. so let's take a look at the algorithm. what we're going to do is, while g is non empty. we're going to follow a path until we can't extend it any further. that vertex at the end is going to be a sink which we'll call v. we take the input at the very, very end of our ordering and then remove it from the graph, and then we just repeat this. so on this graph here, well we started a, we follow a path a, b, c, d. now we're stuck. d is a sink, so we can remove it from the graph. we now follow a new path, a, b, c. c is a sink, we remove it from the graph. a and e, e is a sink, we remove it. path a, b, b is in sync, so we remove it. a is already in sync and so we remove it and now we're done. we have our order. now what's the run time of this outcome? we need to compute one path for each vertex in the grid and so all of the many paths. and each path could be pretty long, it include up to all the vertices in the graph. it could take all of v time. so the final run time of this algorithm is o of v squared which is not great. on the other hand we're doing something a little bit inefficient with this algorithm. we started with this vertex, we followed this huge long path until we found the sync and then we remove it from the graph. then what we probably did was we started that same vertex again, followed basically the same path until we got almost to the very end, until we get to right before that vertex that we removed. and we're sort of repeating all of this work. why do we need to follow this whole path again? we could just back up one step along the path and then keep going from there, and so that's what we're going to do. instead of retracing our entire path, we're just going to back up one step along our path. so to see what this does in our example, we go a, b, c, d. d is a sink which we can remove, but now instead of starting back at a, we start at c which is already a sink. and b which is already a sink now at a we have to follow the path e to could find the next sink and then we're done. and this algorithm reuses our steps a lot less and so it's a lot more efficient. in fact, we think about with this algorithm does, it's basically just adopt for search. we're starting in a vertex where following a path for until we can't anymore. and then finally, once we're stuck, we turn around and we sort of, stop using that vertex again. then from that one step previous we just keep going forward again. this is exactly the depth for search ordering. and in particular when ever we finish the post visit block at a vertex, we put it at the end of our ordering. so the order in which we're sorting our vertices is just based on the post order. in particular, vertices with small post order go at the end of our ordering, and ones with large post order go at the beginning. so the algorithm is now super easy. we run depth for search on the graph and then we sort the vertices based on the reverse post order. and note, we really don't actually have to do any sorting here. we just have to like, remember the order in which we left our vertices. so that's our algorithm, let's take a look at the correctness. so basically what we're saying is that if we got g as a dag. then we want to say, we can order them by the reverse post order. now for this to be consistent with our graph ordering, it needs to be the case that whenever we have an edge from u to v, u comes before v which means that the post of u is bigger than the post of v. let's prove this. there are three cases to consider. first, that we could explore u before we explore. next, we could explore v while were exploring u. and finally, we could explore v after were done exploring. of course, this last case can't happens is there is edge u to v which means that if we haven't already visited v we will visit as part of our exploration view. but okay the first two case is still possible let's take a look. firstly, if we explore v before we explore u, well it turns out that we can't reach u from v, and this is because we have a dag. if there was a path from v to u, and then we add an edge to u back to v. that would give us a cycle which we can't take. so it must be the case therefore, that we can't discover u as part of exploring v. we have to finish exploring v before we can even start exploring u, and that tells the post of u is bigger than the post of v, which is what we wanted to show in this case. now in the second case the analysis is a little bit different. if we explore v while exploring u remember that this means that our explore of v is a subroutine of exploring u, and therefore it needs to finish up first. and that again, tells us that the post of u is bigger than the post of v. so that completes the proof of our theorem and shows that this algorithm for topological sort actually works. so we've got now this nice algorithm for the sorting. next time, we are going to talk about something a little bit different, we're going to talk about connectivity in digraphs. so i'll see you then. 
hello everybody, welcome back to our graph algorithms course. today we're going to be talking about notions of connectivity in directed graphs which it turns out will be a little bit more complicated than those in the undirected case. and we'll talk about various notions then get to this idea of a strongly connected component. so in undirected graphs, connectivity is a pretty simple notion. we have these connected components where any two things in the same component you can get from one to the other. and if you've got two different components, they've got no edges between them. nothing connects at all. in directed graphs the story is a bit more complicated. so let's consider the following example graph. now in one sense this graph is connected, you can't separate it out into two islands such that you can't reach one island from the other. on the other hand if you want to look at which vertices can be reached from which others, the story is a bit more complicated. from vertex d it turns out you can reach every other vertex in this graph. however, d is a source for attacks. once you leave d, there's no way to get back to it, there are just no edges that come into it. now a just sort of, sorry, a f is sort of the opposite here. from every vertex in the graph you can reach f, but once again, once you do, you can't get back. then again, g and a, neither of them can be reached from each other. h and i, you can get between them but once you leave, again, you can't get back. it's sort of a lot more complicated if we want to figure out which things are reachable from which others. now, there are actually a bunch of possible notions of connectivity that show up in directed graphs. the first one is sort of that these two points can't be separated from each other, that you can't put them on two different islands where nothing in one island connects to anything in the other. and this sort of says that it means you can get from one vertex to the other, following edges in any all direction. and i mean, okay, this says you can't put them on different islands, but it doesn't say you can reach either from the other one without breaking traffic laws by following one-way streets in the wrong direction. so, a second notion is that maybe we want one of the vertices to be reachable from the other using the actual edges and their intended direction. and this is a more practical notion. however, it has its weird irregularities that makes it hard to deal with. so, third notion that's maybe a little bit stronger is that you should have two vertices, v and w, where not only can you reach w from v, but you can also go back and reach v from w. and this third notion turns out to be pretty nice. so when we say that two vertices v and w in a directed graph are connected, we mean this. we mean that you can reach v from w, and also can reach w from v. but the theorem now is that using this notion we actually recover much of the power that we had in the undirected case. a directed graph can always be partitioned into strongly connected components where two vertices are in the same strongly connected component, if and only if they are connected to each other. so, for example, the graph that we looked at has five strongly connected components. after you can get it all around around there, but there's no way to get from it to anything else. h and i you can get from one to the other but not necessarily to other things. a, b, c and e from any of those four vertices you can reach any other one, but once you leave again you can't get back. d and g are each their own components and that's it. within each of these components you can get from everywhere to anywhere else. but again once you leave it, you can't come back. so to make sure we are on the same page, we have the following graph. what is the strongly connect component of a in this graph? well, it turns out to be the following set. a, b, e, f, g, and h are in this component. the other ones aren't. and to see this, we can see that sort of you can actually get around any of these six. a goes to e goes to h goes to f goes to b goes back. b goes to g goes to h goes to f goes back to b. sort of you can use these to navigate around, everywhere you like. but from d you can't get to d from this other component. and c and i, once you get there you can't get back to the other vertices here. so the result is that you have a directed graph. it can always be partitioned into strongly connected components. such that two vertices are connected if and only if they're in the same component. and the proof of this is very similar to the proof of the sort of similar thing in the undirected case. again, we need to show that connectivity in the strong sense is an equivalence relation. that if u is connected to v, and v is connected to w, then u is connected to w. but while if these connections happen, there's a path from u to v, and a path from v to w. and so pushing them together, there's a path from u to w. similarly, there's a path from w back to u, and from v back to, sorry, w back to v and v back to u. and composing them again gives you a path from w back to u. and so that completes our proof. now there's something more to say though. once we've split our graph into connected components, they still have edges connecting these components to each other. so one useful thing to do is to draw what's known as the metagraph, which sort of tells us how these connected components connect to each other. so this metagraph has vertices that correspond to the connected components of the original graph. and you have an edge between the two of them if there's an edge in that direction connecting those vertices. so d connects to a, so there's an edge between d and the component of a. it also connects to g, a, b, c or e, those guys have edges leading to h including f and so they have edges into those components and so on and so forth. and this is our metagraph. now one thing to note about this metagraph is if you stare for a little while, you can actually see that it's a deck and this is in fact no coincidence. it turns out that the metagraph of any graph g is always a dag. now the proof of this is not so hard, suppose that it's not. what that means is that there's a cycle. and what happens if you have a cycle is that every node in the cycle can be reached in every other node because you just travel along the cycle until you get there. now, this would be true if they were individual vertices, but it's also true if you have a cycle between strongly connected components. because you just, since everything in one component connects to everything else, you can just get to the guy leading to the next component, and then to the vertex leading to the next component, and this sort of follows all the way around the cycle. and so everything in all of those components are all connected to each other. but since everything is connected, they should all be the same strongly connected components. however the single vertices of the metagraph are strongly connected components, so you can't have two vertices that are connected to each other and that gives us a contradiction. okay, so in summary, we can always partition our vertices into strongly connected components. we have a metagraph, describes how these strongly connected components connect to each other, and this metagraph is always a dag. so, this is what we have. next time what we're going to do is we're going to actually talk about how to compute these things. how do we compute the strongly connected components of the graph, how do we find the metagraph? so, come back for the next lecture and we'll talk about that. 
hello, everybody. welcome back to our graph algorithms course. today, we're going to talk about how to get an algorithm to efficiently compute the strongly connected components of a directed graph. so if you recall from last time, what we had was we find this notion of connectivity in directed graphs, where two vertices were connected if you could get from one to the other and back. now, we said this graph necessarily would be divided into strongly connected components, where within a component, you could get from everything to everything else, but sort of once you leave the component you can't get back. now, these components are connected to each other by what we call the metagraph. and the metagraph was always a dag, which is sort of a useful a thing as we'll see today. so the problem we're going to look at today is, given a graph, g, a directed graph, g, how do we find the strongly connected components of g? now there's a pretty easy algorithm for this, it turns out. for each vertex v, run explore on v and determine all the vertices reachable from v. once you've done that for every vertex, you know that for a vertex v what you want to do is find all the vertices u that are both reachable from v and can also reach v. and that, it turns out, will give you the strongly connected component of v. and so this gives you the strongly connected component of v, you run this for all v, that gives you all the strongly connected components. and so the runtime of this algorithm is a little bit slow but you need to explore from every single starting vertex. so the run time is something like o of v squared plus v times e. this is okay but we'd like to find something faster. and what's the idea? the key idea of this algorithm is the following. if you take a vertex v and run explore, you find everything you can reach from v. now this includes the components of v. but if there are other components downstream of that, you might find vertices from other connecting components as well. however, there's a case when this doesn't happen, if v is located in a sink strongly connected component. that means there are no edges out of that strongly connected component. so when you explore from v, you will find exactly it's strongly connected component. which is good, because we want to find the strongly connected component. so, if you actually find one, that's a good start. so how do we do this? well, we need to find the vertex in a sink strongly connected components which takes some thoughts. well, there's a theorem it turns out that will help. if c and c prime are two strongly connected components where there's an edge from some vertex of c to some vertex of c prime. it turns out that the largest post number in c is larger than the largest post number in c prime. now to prove this, we split into two cases. when we run our depth first search, either we visit a vertex in c before we visit a vertex in c prime, or it could be the opposite way, visit a vertex in c prime before we visit a vertex of c. in the first case, where we visit c first, well, from a vertex in c you can reach everything else in c, because it is the same component. there's also an edge from c to c prime, so you can also, it turns out, reach everything in c prime from that vertex. and that means that while you're still exploring that first vertex in c, you actually explore everything else in c and everything in c prime as a subroutine. this means, that because of the way subroutines work, you have to finish exploring all those vertices before you finish that last vertex in c. so, that one vertex in c has the largest post number of any vertex in either component. the second case is you visit c prime first. here the proclaim is that you can't actually reach c from c prime. because if you could reach c from c prime, and you still follow the edge from c back to c prime and you'd have a cycle. and this is not just any cycle. this is a cycle in the metagraph, which you can't have because the metagraph must be a dag. and this means that you can't reach c from c prime. when you explore c prime, you'll never actually find c in the middle of that exploration. so, in fact, you have to finish exploring c prime before you can even begin exploring c. and so, once again, the vertex with the largest post has to be in c. okay, so what does this mean? if you look at the large vertex with the single largest post order number in the entire graph, what can we say about that? well it has to come from a component with no other components pointing to it. that vertex needs to be the source component, which is great. it's almost what we wanted. what we wanted was a vertex in a sink component. well, there's a trick for doing this. so if you have a graph, we're going to define what's called the reverse graph, which is just what you get by taking a graph reversing the direction of all the edges. so if a graph on the left is g, the graph on the right is the reverse graph. the edges are all the same, they're just pointing in the opposite directions. now the cute thing here is that the reverse graph and g both have the same strongly connected components. i mean, v and w are connected if you can go from v to w and w back to v. but if you reverse all the edges, you can just follow those paths in the opposite directions, from v to w and w back to v. and so, the strongly connected components are the same, but the edges in the metagraph are different. because if you have a source component in the reverse graph, that means that you have edges coming out of it but not into it. well, when we reverse the edges to get the original graph g, the edges come into it and not out, and you find the sink component. so in order to find the vertex in the sink component of g, what you do is you run depth first search on the reverse graph, which by the way is easy to compute, you just take every edge and reverse the direction. because we run depth first search on the reverse graph and take the vertex to largest post order. okay so just to review. which of the following is true? the vertex with the largest post order number in the reverse graph is in the sink component? the vertex with the largest preorder number is in the sink component? or the vertex with the smallest number is in the sink component? well, all of these sound roughly equivalent. but if you work them out, only one of them is true. the first one. the vertex with the largest postorder number is in a sink component. the other ones sound plausible, but just don't work. okay, but this gives us an algorithm. and the point is the following. we run depth first search on the reverse graph. we let v be the vertices of the largest post number and this has to be in the sink component of the g. we now explore v and because it was in a sink component, the vertices we find are actually a strongly connected component of g. so, we take them, we remove them from g and then we repeat. so, here's our graph, we depth first search the reverse graph, the largest post number is that 18. so, we explore that, we find one vertex. that's our first component. we're now going to remove that from the graph and try again. depth first search on the reverse graph, 16 is the largest post. we explore from there, we find this new component. great. depth first search the reverse graph, find the largest post, explore that. we have a third component. depth first search the reverse graph, 10 is the largest, and we explore from that. we find these four guys as a component, and then there's the one vertex we find out. so that gives us a strongly connected components. unfortunately, this algorithm's a little bit inefficient again. because we need to run depth first search repeatedly on the reverse graph. but it turns out that that's actually unnecessary. because remember, the theorem that we had was that if you had an edge between two components, the one that was further upstream always had the larger post order. in fact, when we reversed the edges, now it's the one downstream has the largest post order, but whatever. but the point is that after you remove the sink component, and if you just then look for the vertex with the single largest remaining post order, that's going to be a sink component of the new graph. it doesn't point to anything else, except for maybe some components that you've already removed. and so the new algorithm, we just run depth first search once on the reverse graph, then we look for v in the graph in reverse postorder. any v that has not yet been visited, we explore that v, and mark the vertices that we found as a new strongly connected component. so we have this graph. we depth first search on the reverse graph. we record the post numbers. now the largest post is 18. we explore that vertex, find this component. next is 17, which we explore and find this component. then 15 finds this guy, 10 finds this 4, and 6 finds the last guy, and that's it. a much faster algorithm. in fact, the runtime, we essentially just ran a depth first search on the reverse graph and then ran another depth first search on g, which is the slight modification that we wanted to visit our vertices in this outer loop in some specific order. and also, we need to record the connected components we found. but basically, this is just two depth first searches, the runtime is linear o of v plus e, and this gives us a nice efficient algorithm for finding our strongly connected components. well, that completes our unit on this graph exploration and decomposition algorithms. these tell us how to find ways to get from one vertex in the graph to another. however, when you're actually trying to solve this problems in practice, you don't just want any route on your map that gets me from where i am in san diego to say, los angeles. the route that i find might pass me through new york and florida on the way, and i don't want to do that. what i really want to do is i want to find an efficient path. how do i get there which spends as little time as possible, or maybe as little money as possible. and this is what we're going to start talking about in the next unit where michael is going to be talking to you about how to find shortest paths in graphs. so, i hope you enjoyed this unit and will come back for the next one. 
hi, in this module you will study shortest paths in graphs and algorithms to find them. these algorithms have lots of applications. for example, when you want to go from one city to another, you don't want to switch your transport many times. and in the travel planning systems, there are algorithms that help you to minimize the number of such switches, for example, to get from hamburg to moscow with the minimum possible number of flight segments. when you start a navigation app on your smartphone to get home from work faster, one of these algorithms is used to give you the fastest route possible. also, one of these algorithms is used right now to direct the network packets with data through the internet so that you can watch this video online. in this module, we will start with the most direct route problem about flight segments. we will then consider the problem of getting from point a to point b with the fastest possible route. and in the end, we'll consider the problem about currency exchange, which doesn't seem to be a problem about graphs or shortest paths, but actually the same algorithms will help you to exchange currency in the optimal, most profitable way. let's start with the most direct route problem. it is formulated very simply. what is the minimum number of flight segments to get from one city to another? for example, if we look at this map you could go from hamburg to moscow with five flight segments, but this is obviously not optimal because you could go from hamburg to moscow with a direct flight. we can consider it as a graph problem on a graph where nodes correspond to cities and directed edges correspond to available flights from one city to another. the edges are directed because flights can be available one way and not available another one, for example, because there are no tickets left. and, of course, in the typical real world graph there are many more cities and many more possible flights but this is just an illustrative example. so here the graph problem is to get from node correspondent to hamburg to node correspondent to moscow. and one way to do that is to use these five edges corresponding to the five flight segments we saw on the map, but this is obviously not optimal in terms of minimization of number of edges. for example, we could get from hamburg to paris first, and then from paris to moscow, and that would be just two edges. or in this case, you could go just directly from hamburg to moscow, and obviously this is the optimal way, to use just one flight segment, which is not always possible. for example, to get from hamburg to helsinki, you'd need at least three edges on this graph. also notice that as soon as we formulate it as a graph problem, we don't need to name the nodes like hamburg or moscow. we can just name them to a, b, c, d, e, and then solve the problem on an abstract graph. so let's talk about paths and graphs. so, we define length of the path l(p), where p is the path, as the number of edges in this path. for example, if we consider the path from d to b, which consists of edges from d to e, then from e to s, from s to a, and from a to b, then the length of this path is 4. and note that this is an undirected graph, but we will also look at the directed example soon. another example of a path is from d to b again, but through s. from d to s to c to b, and the length of this path is 3. now the distance between two vertices or nodes in the graph is the length of the shortest possible path between them. for example, the distance between d and b is just 3 because this is the path from d to b which contains just three edges, and there are no paths that contains less than three edges. and the distance from c to a is just 2, because there is a path through s. there is another one through b, which is also of length 2, but there is no direct edge from c to a. the situation changes a little bit when the graph is directed because not all edges can be taken. and so distance from d to b in this graph, which looks like the same but has directed edges, is now 4 instead of 3 in the undirected case. this is because we cannot take the edge from d to s because it is directed in the wrong direction. so distance from d to b is 4 because there is this path in green and there is no shorter path. and the distance from c to a in this case is infinity because there is actually no path from c to a, because you cannot go from c to f. and even if you go from c to b, it cannot go from b to a because the edge is going the wrong direction. now let's consider the paths from some origin node s. it turns out that to find the shortest path from a to b is not simpler than finding shortest paths from a to all other nodes in the graph, or at least almost all the other nodes in the graph. and so we will be studying the problem of finding shortest path from one starting node s to all other nodes in the graph. when we select the starting node, we can lay out the same graph in a different way using distance layers. in this case, we have three distance layers. the first one is layer 0, which contains just node s, which is a distance 0 from itself. then we have layer 1, which contains four nodes a distance 1 from node s. and then we have layer 2, which contains just one node, b, which is a distance 2 from node s. if we added another node to this graph, for example, node f, which is only connected with b, then it will have 4 layers, and additional layer 3 contains just node f, which is a distance 3 from node s. note that there cannot be an edge from d to f in such layered graph because otherwise there would be a path from s to f of length 2, s, d, f, and then f would be in the layer 2 and there would be no layer 3. and another example is that there cannot be an edge from s to f because in this case the distance from s to f would be 1 and so f would be in the layer 1. and again there wouldn't be no layer 3. and the general property is that the only edges which are allowed in such layered graph are edges inside the layer, like edge from d to e in this example, and edges between a layer and the next layer, like edges from s to layer 1, edges from a and c to b, and edge from b to f. now let's consider the directed case. we directed all the edges downwards and so the layers didn't change. we still have layer 0 with just s, layer 1 with four nodes, and layers 2 and 3 with nodes b and f correspondingly. what about additional edges here? for example, can f be connected to d? so yes, it can be connected to d, but in this direction, from f to d, because this edge from f to d doesn't give us any shorter path from s to f. and so the shortest path is still of length 3. also, it is possible to connect f directly to s in this direction, or to connect b directly to s because the shortest paths don't change. however, it is not possible to add an edge from c to f, because if we add this edge, the layer structure will change. because the distance from s to f will be just 2, from s to c and from c to f. and the general property is that there cannot be any edge from a layer to another layer which is farther from s by at least two. so there can be an edge from a layer to the next layer. there can be an edge within the layer's edges, an edge from d to e in this example. and there can be an edge from a layer to any of the previous layers, such as the green edges in this example. but there cannot be any red edges, such as an edge from c to f, which is an edge from the layer 1 to the layer 3, which is at least farther by two from s than layer 1. so there cannot be any such edges in the layered distance graph. and in the next video, we will discuss an efficient algorithm to traverse the graph layer by layer so that in the end every node is assigned to some layer, and we know the distance to this node as the number of the layer to which it was assigned. 
hi, in this video we will discuss breadth-first search an efficient algorithm to find distances from an origin node to all the nodes in the graph. and we will use the layered distance structure of the graph from the previous video. we will start from the version of the algorithm which is easier to understand. we marked as s a blue node which will be our origin, and we will find distances from this origin to all the nodes in the graph. we'll start with a state when all the nodes are colored in white. and the node colored in white means that this node hasn't yet been discolored. during the algorithm, we'll discover some nodes, and then we will process them. when we discover the node we cover it with grey. at first, we discover the origin node s. and when we process the node we cover it with black. and of course after discovering the origin node, we start processing. we will process the graph layer by layer. we'll start with layer zero, which only consists of the node s and we'll start processing it. when we start processing a layer, we take all of the edges outgoing from this layer and we discover all of the nodes ends of those edges. so basically when we start from layer zero, we discover the whole layer one because the edges from the layer zero node, s, go to layer one nodes. of course there could be an additional edge from s to itself, but then we would ignore this edge because it goes to the node we've already discovered. in this case, there is no such edge, so all the ends of the edges going from s are new nodes, white nodes, which are not yet discovered and we color them with gray to mark the fact that we have discovered them. after we have discovered all of them, all the layer one we start processing all those nodes simultaneously. so we process all six nodes of the layer one. and to do that we take all the edges outgoing from those nodes and discover the new nodes at the ends of those edges. you may notice that there are a few red edges. and those are the edges which go from the nodes in layer one, but which go to the nodes which were already discovered previously to s and to the nodes of the same layer one. and there are a lot of bold black edges, which go also from the nodes of the layer one, which we are not processing, but they go to the new nodes, which were not discovered previously. and we mark those nodes with gray. and when we process edges from the layer one we get all the nodes of the layer two, because the edges from layer one go to layer two to the same layer and to the previous layers. when edges go to the same layer or to previous layers, we mark them with red and we don't do anything with the nodes on the ends of those edges. and the edges go to the next layer, we discover nodes of the next layer. and of course, all the nodes of the next layer, of the layer two, have some incoming edge from layer one. so after processing layer one, we've discovered the whole layer two. and after we've discovered it, we start processing it. so we process the outer circle, all the nodes from the layer two. and to that we consider all their edges which are outgoing from them. and you see that there are only red edges because all the edges from layer two go to either the same layer two or to s or to layer one nodes. and all of them have been discovered before that, so we don't do anything with those nodes. so we don't discover any new nodes. and it means that we don't have anything new to process, so our algorithm stops with that. and we now can assign each node to a layer. obviously node s, origin node, is in the layer 0. the nodes we discovered on the first step are the nodes of the letter 1. so, they're at the distance 1 from s. and the nodes discovered on the next step are nodes of the letter 2, and they're at distance 2 from s. and so, we've found distances from s to all the nodes in the graph. but actually not to all of them, because there can be some nodes which are not connected to s, which are not reachable from s. and such nodes must have infinite distance from s. and we'll solve this issue by initializing all the distances from s to all other nodes with infinity. and then, setting the distance from s itself to 0 and implementing the algorithm that i've just told you. then every node, which is reachable from s, will get some finite distance from s, and all the nodes which are unreachable from s, will stay with their infinite distance. now let's look how this same algorithm will work with an undirected graph. so we have basically the same graph, we just removed the arrows from the edges. so all the edges became undirected, and again we have the same origin s, marked with blue. we start with all the nodes being white, because they are not discovered yet, we discover node s, and color it with grey. and then we start processing it, and color it with black to process it without take all the edges outgoing from it. and you see that now there are more edges outgoing from s, because some of the edges which are incoming into s in the previous example, now are also outgoing from s because they are undirected edges. so, we discovered seven nodes instead of six, as in the previous example. and after we discovered all these seven nodes, this is our layer one. these are all the nodes of distance one from s. we discovered them. now, we start processing them. and to do that, we consider all the edges out going from those seven nodes. all black ones, but for s. and you see that some red edges appear. those are edges inside layer one, and edges from layer one to s, and there are a lot of bold black edges. those are the edges from the nodes of layer one to the new nodes, which are not discovered before. and we've discovered almost all the nodes of the outer circle, but for the one in the bottom which was discovered before. so we have discovered 11 new nodes. we have discovered them, and now we need to process them. and to do that, we consider all the edges outgoing from those 11 nodes. and all of those edges are red because they all go to the nodes we previously discovered. so nothing new is discovered and we stop our algorithm. and now we can assign distances. and again, distance from s to itself is zero. distance from s to the nodes discovered in the first step is one, this is layer one. and distance from s to the 11 nodes in the outer circle is two. this is our layer too. and again we've found all the distances from s. and we have found all the layers, and also if there are some nodes which are not connected with us initially, we initialize the distances to infinity. and so, they stay with this infinite distance to them. 
this is basically how the algorithm works, and this is more or less clear that it defines distances correctly because it just goes through the graph layer by layer. but to actually implement the algorithm, we need to do everything turn by turn. we cannot just take a couple of nodes and process them simultaneously. we need to have some order on those nodes. and now let's solve this problem. so we return to our initial example of a directed graph with s origin. and now we want to process each node one by one. to achieve that, instead of processing each layer of nodes simultaneously, we will have a queue of notes. so the nodes will get into the queue and wait for their turn. and as soon as every node which was in the queue before this node has already been processed, this node goes out of the queue, and it is being processed. so, when we discover node, we put it into a queue. and when we need to process it, we take it from the queue and process it. and it means that the nodes which were discovered earlier will also be processed earlier. and so, in general the order of layers won't change because first layer 0 nodes will get into the queue, then all the nodes from layer 1 will get into the queue. and then after they all are processed, we'll process nodes of layer 2, which were discovered after notes of layer one. now let's see how all it works. so we discovered node s, and we already know by that time that the distance to s is 0. it is layer 0 and there are no more nodes in the layer 0. now we start processing this node, and to do that, we process the edges outgoing from s in some order. it doesn't mean which is the order. it can be just the order in which they were saved in our data structure for the graph. so we chose our first edge, and we discover node to the right. and we know that the distance to this node is 1 because this is a layer 1 node we discovered. and instead of starting to process it, we go to the next edge from s and we discover another node of layer 1 and assign the distance to 1, and again, and again, again, and again. and so now, we have discovered all the nodes from the layer 1. and we put them all in the queue in the order starting from the node to the right and counterclockwise order. so now we've processed our node s and we need to process something else. what to process? the node which is the first in the queue, and this is the node to the right from s. so we start processing this node. and to go through the edges from this node in the order they were saved. i don't know that order, so let's see. okay, so the first edge outgoing from this node was to another node from the first layer. so it is a red edge because it goes to the node which has already been discovered. we don't do anything with this edge. then the next edge goes to the right, and we discover a node from layer 2, which is to the right, and we set its distance to 2. then the edge goes to the node right and up, and we also assign distance to this node to 2. and the last edge goes to the node to right and down from our node. and it also gets distance 2 because this is a layer 2 node. and what we do next? we've processed our node to the right from s and we need to process something else. and this something else is the next node from the layer 1 in the counterclockwise order. this is this node to the right and up from node s. we'll start processing it. in some order we'll process edges from it. so the first edge is a red edge to the left, and we don't do anything with it. and the second edge is a red edge to the node to the right and up from it, which is familiar too, but it has also been already discovered. and then the next edge gives us new nodes from layer 2. and the next one also gives us a new node from layer 2. so we finished processing this node and we go to the next one. again, red edge, red edge, new node discovered, new node discovered, next node from layer one. our red edge, our red edge, a new node from layer 2, and new node. and again, with another node from layer 1 and the last node from layer 1, we process it. okay, so we've processed all the nodes from layer 1. we've discovered all the nodes from layer 2 in some order. i don't even remember the order of those nodes. it is mostly counterclockwise but not completely. so let's see in the slides, what is the correct order? so the first one to be discovered was the node to the right. and start processing it, and there is a red edge from it, and another one. and there are no more edges from this node so we'll start processing the next one in the counterclockwise order. and there is a red edge from it, and it looks like there are no more edges from it, so we finished processing this node. and then we go to the node to the right and down, and there is a red edge from it, and that's all. and so, we go through the nodes in the second layer and all the edges from them are obviously red because we've discovered everything that's connected to s already. so we go and test that all the edges are red, and now we've finished, and again we have the graph which is layered. node s is in the layer 0, 6 nodes in the layer 1, and 12 nodes in the layer 2. and if we have some node which was not connected to s, then it stays with the distance estimate of infinity. so this is how breadth first search actually works, and in the next video we will discuss the pseudocode that actually implements this algorithm. 
hi. in this video, we will implement the breadth-first search algorithm from the previous video and analyze it's running time. let's look at this pseudocode. the procedure of bfs is called after breadth-first search and it takes graph g and origin node s as input parameters. it also uses array dist to store the distances from the original node s to all nodes in the graph. it can be not an array, it can be a map from nodes to distances, depending on what are your nodes. if your nodes are numbered from 0 to n minus 1, then it will probably be convenient to use an array of size n to store distances for those nodes. but if your nodes are labeled with some strings or some other objects, then it maybe wise to use a map from nodes to distances and call this data structure dist. anyway, we'll use this dist data structure to store our estimations of distances from origin to all the nodes in the graph. and we initialize all these distances with infinity, with the exception of the node as itself, which gets an estimation of 0 from the beginning. and we will also use another data structure q, which is a queue, the data structure which works in the principle of first in, first out. so the first element that goes into the queue is the first element that goes out of the queue. and we initialize this q with just one element, the origin node s. and this symbolizes that we already discovered node s and put it in the queue. so all the discovered nodes are those which are in the queue. all of the nodes which haven't been in the queue yet are white nodes which are not discovered yet. and the black nodes, in the terms of the example i showed you before, are those nodes which are already out from the queue and are being processed or have been processed before. and we will take nodes from q one by one, process them, discover new nodes and put new discovered nodes back into the queue. so we'll start with q initialized with only starting node s. and while this q is not empty, we take the first element from it using method dequeue. so we get it to variable u. so, this is the first node in the queue, and we start processing it. and to start processing it, we triggers all the edges outgoing from this node u in the graph. so we have a for loop for all edges (u,v) in the set of edges of the graph and this means that we traverse all the edges which have starting nodes u and some other node as it's end. and we'll look at node v and now we need to determine whether this node is already discovered and maybe already processed or not. and to determine that, we'll use our dist values. so if dist value of the node is infinity, then it means that this node hasn't been discovered yet, because as soon as it will be discovered, we will change the estimation of the distance to it and it will becomes finite. while it is still infinite, it means that the node hasn't been discovered. and vice versa, if dist is finite, then we have discovered this node already. so if this node was discovered previously, then we don't need to do anything with it. this is a red edge. and we don't do anything with a red edge from our currently processing node to the node which was discovered earlier. but if the edge is to a white node which was not discussed previously, we need to process this edge. and by processing it we first discover the end of this edge v and we do that by calling enqueue, so adding these v to the end of the queue. and we'll also change the estimate of the distance to this node v and we set it to distance to the current node plus one. because we know that when we process a node from somewhere, some of the edges are red because they go to the same layer of or one of the previous layers. but the edges that go to the undiscovered nodes go to the next layer. so the distance to v is equal to distance to u plus 1. and we repeat and repeat this process while our queue is not empty. and we need to make a few notes. first, this infinity thing. of course, in a real programming language, there won't be any infinity, so you will have to use something special for that. one variant is to estimate what is the maximum possible distance from origin node to any nodes in the graph, and just use a constant value or some computable value which is definitely bigger than that. for example, distance from origin cannot be bigger than the number of nodes in the graph, because every path without cycles inside it will have, at most, a number of nodes minus 1 edges. or you can just say that it is not bigger than the number of edges in the graph. because you won't use the same edge twice in the shortest path from origin node to your node. so anyway, [cough] you can assign infinity to number of nodes plus 1 or number of edges plus 1 or some other big value. and another thing you can do is you just, instead of using integer numbers for storing distances, you create a special structure which has two fields. one of them is distance if is determined. and another field is just a boolean field, which tells you whether the distance is defined or it is still not defined, and then this means infinity. and another note is that why this algorithm even stops. and the key observation is that we only put a node into the queue once, because as soon as we put this node in the queue, we enqueue it, we also change the estimation of distance to this node and it becomes finite. and so one, we will try to pull it in the queue next time, we will compare the distance to it with infinity and it won't be equal to infinity. so we won't put node into the queue again. and it means that we put at most number of nodes in the graph, elements in the queue. and on each step of the while loop, we take some node out of this queue, so its size decreases. so it can increase at most number of nodes in the graph times and it decreases in each steps. so, this means that these others definitely stops and it definitely stops after at most number of nodes in the graph steps of the external while loop. now let's estimate the running time of this algorithm more precisely. i state that the running time of breadth-first search is proportional to the number of edges plus number of nodes in the graph. why is that? first we've already discovered that each vertex, each node of the graph, is enqueued at most once. so the number of iterations of the external while loop is at most number of nodes of the graph. another observation is that each edge is examined either once for directed graphs, or twice for undirected graphs. why is that? well, the edge is examined when one of its ends is processed. the node, which is one of its ends, is processed. and if the graph is directed, then the edge is actually examined only if it's start is processed. and if the graph is undirected, then for each of its nodes, the edge is examined when this end is processed. of course, this edge will work and discover a new node at most once out of those two times it is examined. but still it will be examined twice from both ends. and of course, if the edge is not connected to the original node, it won't be examined at all. but we say that each edge in any case will be processed at most twice. and what it means is that the total number of iterations of the internal for loop is, at most, number of edges of the graph. and so adding those up and adding the constant number of operations in the start and adding the initialization of the dist values which goes in time proportional to the number of nodes of the graph, in total, we got number of edges plus number of nodes in the graph. and in the next video we will prove that breadth-first search actually returns correct distances to all the nodes from the origin node. 
hi. in this video, we will prove the correctness of the breadth-first search algorithm for finding distances from an original node to all the nodes in the graph. we will also prove some properties of this algorithm, which can be useful when you want to extend this algorithm to other kinds of problems. first, recall that node u is called reachable from node s, if there is a path from s to u. and the lemma states that the reachable nodes are discovered during breadth-first search. and they get a finite estimate of distance from s. and unreachable nodes are not discovered during breadth-first search. and they stay with infinite distance estimate, infinite distance value. first, let's prove it for reachable nodes. so, suppose for the sake of contradiction that some reachable nodes were not discovered during the breadth-first search. then select out of those nodes the one closest to s in terms of the length of the path. so, let's assume that u is the closest to s unreachable node, which was not discovered during the breadth-first search. then take the shortest path, some shortest path from s to u. it goes from s to some nodes of v1 from there to v2 and so on and up to vk and from vk it goes to u. then u will be discovered, actually, while processing vk. why is that? well, because first, we will discover and process s. then we will discover and process v1. then we will discover and process v2 and so on. and we will go up to vk. and when we process vk, u will be discovered. so this is a contradiction with the assumption we made for the sake of contradiction that some reachable node is not discovered during breadth-first search. so we proved that reachable nodes are discovered. and of course, they will find its estimation of distance. because this is how the algorithm works. as soon as the node is discovered, it gets estimation of distance, bigger by one, than the estimation for the node b process. now let's prove this statement about unreachable nodes. let's suppose, again, for the sake of contradiction that some unreachable nodes were discovered. and let u be the first such unreachable node to be discovered. now let's see when it was discovered. it was discovered while processing some other node v. and as u is the first unreachable node that was discovered and v was discovered before u, because u is discovered when v is already processed, it means that v is a reachable node. and it means that there is a path from s to v, but then it means that there is a path from s to u through v. so u is actually reachable, and this is a contradiction, and so we proved that unreachable nodes are not discovered during the breadth-first search. so it works correctly at least in the sense that it finds some distances to reachable nodes and doesn't find any finite distances to unreachable notes. now we will prove the order lemma, which states something about the order in which nodes are discovered and dequeued from the queue. it says that by the time some node u at distance d from the original node is dequeued, it started processing. all the nodes at distance at most d have already been discovered. so they have already been enqueued in the queue. and maybe some of them have already been processed. some of them are still in the queue, but at least, they have already been discovered. so let's prove this again by contradiction. suppose that this lemma is not true and considered the first time this order was broken, so that some node u at distance d has already started processing, and that's why it is filled with black. and some other node v at distance d', which is at most d has not yet been discovered. let's suppose that. so we know that d' is at most d, and we know that node u was discovered while processing some other node u'. and we know that the distance to this node u' is at least d-1, because if the distance to u' was less than d-1, then the distance to u would be less than d because there's an edge from u' to u. also, we know that the node v has an edge from some node v' with distance d'-1 from s, because there is some shortest path from s to v. and there is the previous node before v on this path, and this is v'. and distance to v' is exactly d'-1, because this is a shortest path. so, it can be less than d'-1 because otherwise, distance to v will be less than d'. and it cannot be bigger than d'-1 because then the shortest path to v would be longer than d'. so, distance from s to d' is exactly d'-1. and we know that the prime is at most d, and from that, we know that d'-1 is at most d-1. and it means that v' was discovered before u' was dequeued because we know that the first time that order lemma was wrong was with nodes u and v. and now u' and v' were before that. so that point of time order lemma still works. so we know that v' was discovered before u' was dequeued. so it means that v' was gray before u' became black. [cough] and this means that also, v' became gray before u became gray, because u became gray while processing u'. so v' was enqueued and filled with gray before u was enqueued, discovered, and filled with gray. what that means is that because our queue works as first in first out, and v' was discovered before u, it means that v' was also started processing before u. so v' was dequeued and filled with black before u was dequeued, and immediately after v' was dequeued, v would be discovered, because there is an edge from v' to v. so either v was discovered even before that, or it was discovered during processing v'. and we see that v is already discovered, and u is not yet dequeued. so it is a contradiction with the fact that when u is already black, v was still white and not discovered. so we proved our order lemma by contradiction. 
now the main result is that one node u is discovered during breadth-first search. the dist value, the estimate of distance to this node from origin, is assigned exactly the correct distance from node s to node u. let's prove this. to prove it, we'll use mathematical induction, and as a base case, we see that one node s is discovered. this is the first node to be discovered. the dist value is assigned to 0, and this is actually the correct distance from s to itself. so we'll use induction on the distance to the node. so, the inductive step is that suppose we proved our statement about correct distances for all nodes, which are distance at most k from the origin. and now we'll prove it for nodes at distance exactly k + 1. if we do that, we'll prove the lemma itself. so now, taken out v at distance k + 1 from the origin, we know that for all nodes which are closer, the correct distances are found during breadth-first search. now let's prove it for this particular node v. so we know that v was discovered, because it is reachable. and we proved that all reachable nodes were discovered during breadth-first research. so it was discovered while processing some other node u. now let's estimate the distance to u. from one point of view, we know that the distance from s to v is at most distance from s to u plus 1, plus this edge from u to v through which v was discovered. and we know the distance from s to v is exactly k + 1, and that means the distance from s to u is at least k. from the other hand, we know that v is discovered only after u is dequeued. and using the order lemma, we can state that the distance to u is strictly less than distance to v, because otherwise v would be discovered before u is dequeued. and so distance from s to u is strictly less than k + 1. and we already know this at least k. and we also know this distance is integer number. and so the only option is that distance from s to u is exactly k. and then see what happens when we assign this value for v. we assign it to this value of u + 1, which is k + 1. which is the same as the distance from original to v. so we proved our lemma by induction that when the node is discovered at that point, it is assigned correct distance estimate and it is saved in this value. and the last property we want to prove, just to understand better how breadth-first search works and to apply it to some nonstandard situation, is that the queue which we use in the breath-first search looks like this. it first has some nodes of distance d for some d, and maybe in the end it has some nodes of distance d + 1. but it doesn't contain any other distances. if the first node in the queue has distance d, then there are no nodes in the queue with distance less than d, there are no nodes with distance more than d + 1. and maybe there are some nodes at distance exactly d + 1, but they all go after all the nodes at distance d in the queue. let's prove that. so first, we know that by order lemma, all nodes at distance d were enqueued before first such node is dequeued. and nodes at distance d + 1 are only found when a node at distance d is dequeued in process. so, this means that nodes at distance d were enqueued before nodes at distance d + 1 were dequeued. also, we know the same thing for nodes at distance d- 1, so they were all enqueued before nodes at distance d. so by the time node at distance d is dequeued, all the nodes at distance d- 1 are already dequeued from the queue. so there are no more nodes in the queue at distance d- 1 or less. and regarding the nodes at distance more than d + 1, they will be discovered only when we start dequeuing nodes at distance d + 1 and more. but those nodes are all going after nodes at distance d. so at the point when the first node in the queue has distance d, no nodes at distance more than d + 1 can be in the queue, because no nodes at distance d + 1 were dequeued and so we couldn't put anything at distance more than d + 1 in the queue. so we proved this property. also, and now we have to prove that our algorithm finds correct distances to all the reachable nodes in the graph, that it finds infinite distances to the unreachable nodes in the graph, and we also know the structure of the queue at any given moment in time. and in the next lecture, we will also learn how to actually, not only find distance, but also reconstruct the shortest path from the origin to the node we need. because we don't want to just know that the distance in terms of number of flight segments from moscow to san diego is two, we also want to find are those two flight segments to actually get from moscow to san diego. so, see that in the next video. 
hi, in this video you will learn what is a shortest path tree, how to use it to reconstruct the shortest path from original to the node you needed shortest path to after finding distances with breadth-first search. and we'll need to slightly modify breadth-first search procedure for that. what is the shortest-path tree? on the left we some undirected graph with a nine nodes, and suppose we selected nodes as the origin. then on the right we see the layered structure of this graph, where as in the layer zero. four nodes in the layer one, two nodes in the layer two and two nodes in the layer three. so, we know the distances to those nodes but we also can remember is how did we come to this or that particular node during the breath's first search. so, for example, we example, we came to the node a during breath's first search, directly from s. we draw a directed edge from a to s. saying that s is the note from which we came to a when we discovered it. we also draw an edge from b to a saying that in our breakfast search algorithm we discovered b from a. so we can draw such an edge for all the nodes but for s itself, and we will get some graph. and we'll prove later that it is a tree, but to make a tree, we also need to make it undirected, so we just remove all the arrows from the edges. and this already will be a tree. so the lemma states that shortest-path tree, as we define it now, is indeed a tree. so that it doesn't contain any cycles. because the fact that this is a connected component is by construction. we only add those nodes in the tree, which we reached during breath for search. and always connect a new note we just discovered to some note, which was already in the tree, so the graph is obviously connected. we're going to need to prove that there are no cycles in this graph, let's prove that as usual by contradiction. so, suppose there is a cycle in the shortest path three. and suppose this is a cycle off length 5, a-b-c-d-e. now this is an undirected cycle but the edges of the cycle have to be ordered somehow initially. so, for example, the edge between a and b can be either from a to b or b to a but it doesn't really matter. let's assume that. go from a to b without loss of generality. now what we know is in the shortest path tree, at most one outgoing edge from each node because for each node other than s, we just saved from which node did we discovered it. there is only one such node from which this is discovered. there at most one outgoing edge from each node, and if we look at the edge between a and e, we know that there is an outgoing edge from a already. the edge between a and e cannot be an outgoing edge from a, because. a would have two outgoing edges. so the only way this can work is that there is an outgoing edge from e to a. and similarly, the edge between d and e can only be attached from d to e, and the same with edge cd and the same with edge bc. so now we see the directitude so looks like it can work but unfortunately it can not. so lets look at the distance from s to a. we know that when we go by the direct edge of a shortest path graph we go from node which was discovered from some node to the nodes from which it was discovered. and we know that the distance to the newly discovered node is assigned to distance of the nodes from which it was discovered plus one. so if we go by this edge to the parent in the shortest path tree, we decrease our distance from s by exactly one. we begin we stay in the node which is closer to s, exactly by 1. so if we go from a to b, the distance to s decreases by 1. if we go from b to c, decrease by 1. so we start with some distance d from s to a, and then when we go by edge a b, we stay at node b. and we know that the distance from s to b is at most d minus 1. and then the distance to c is at most d minus two, and the distance to d is at most d minus three, and d minus four for e, and now, we make the conclusion that the distance to a is at most d minus five. so, d is at most d minus 5 which is a contradiction, which cannot happen so, by contradiction we'll prove that they're cannot be any cycles in the shortest path tree indeed. now how to construct the shortest path tree? we've defined it in such a way that it is very easy to do. we only need to add two statements to the code of the bfs procedure. we need another array or map, which is called prev so in prev we will store for each node the previous node from which it was discovered. and initialize it with a pointer to no work with a special pointer needle, which basically means we don't have any previous node. and when we discover a node in the end of the procedure. we not only have data distance to those node, but we also save the node from which it was discovered. now this is the only moment when berev changes because the next time we won't discover this node again and we won't change its berev. so that's the whole code for constructing shortest-path tree. now for every node we know what is its parent in the shortest-path tree. 
how do we reconstruct the shortest path given the shortest path tree? so, the nice property of the shortest path tree is that it somehow contains all of the shortest paths from the origin to all the nodes. why is that? because if we go by edge of the shortest path tree, we decrease the distance exactly by what? so if we start with some node, and go by edges of shortest path tree by directed edges one by one. we will each time decrease the distance by one. and so it means that if the distance to the node was d, then exactly after d steps, we will end up in the origin node. because we will be at distance zero. so, we will make d steps. we will have a path of length d, where d is the shortest path length from s to this node. so we'll have the shortest path itself because its length is the same as distance to this node. so what do we need to do in the code? we write down the procedure reconstruct path, which takes in the origin node as the node u for which we need to construct the path, and the prev, which bfs procedure builds for us. and the results variable will store the path itself. so we will start with an empty path. and then, we'll go back from node u until we come to node s. so we start with our node u, and while it is not yet equal to s. we append it to the shortest path, and then we go by the edge of the shortest path tree by assigning prev of u to u. and the we repeat, repeat, repeat it until we come to node s. and u will be equal to s, and our while loop will start. by this time, we have the shortest path from s to u in the result variable, but for one thing. it is in the reverse order so we have first the and note which is u. then we have the prev of u, then prev of prev of u, and so on up to s. so the path is in the reverse order. so to return the actual shortest path from origin to node u. we need to reverse this path and then, we return the result. so of course this procedure works fast, it only needs amount of steps which is equal to the distance from node s to node u. which is definitely less than or equal to number of nodes in the graph for example. but in most cases, it will be even less than that. so in conclusion, we can now find the minimum number of flight segments to get from one city to another, if we have the graph of cities and available flights between them. we can also not only find this number of flights. but we can actually reconstruct the optimal path between some city and another city. for example, you can go from moscow to san diego in the minimum number of flight segments. and we also can build the tree of all shortest paths from one origin. so we not only can build a shortest path from one node to another, but we actually can build all the shortest paths from one node to all the nodes in the graph. and all this works in time for personal to number of edges plus number of nodes in the graph. 
hi, in this lecture, you will learn the algorithm for finding fastest routes, which is used, for example, when you open your navigation app to get home from work faster or by logistics companies when they want to deliver the goods to their customers in time, but use less cars and less people. so let's first state the problem. it's really easy to do. for example, what is the fastest route to get home from work, or more generally, what is the fastest route from point a to point b right now? and below, you see a screenshot from a popular navigation app, yonix navigator, which is given a task of getting the fastest route from the point you are currently in, which is marked by a circle with the letter y in it, to the finish, which is your destination. and it suggests to you a few variants. so the default suggestion is the fastest route, but there are two others. and there may be, for example, a route which uses less turns or the route which is easiest for a novice driver. and sometimes, there can be a shorter route, which is not the fastest one, but shortest one in terms of distance. so in this problem, we can represent the city or the country as a graph, where nodes are some positions in the country. for example, the crossroads and the edges are the roads that connect those crossroads. and there are some weights on those edges and the weight on an edge is, in this case, the time in seconds or in minutes you need to get from start of the edge to the end of the edge. and what you need to find is the shortest path in this graph, but not shortest path in the graph in terms of the number of edges, shortest path in terms of the sum of the weights of those edges. so if you want to get from a to b in the smallest amount of time possible, then you get those by sum paths with sum edges and sum weights on those edges corresponding to the time. and you just sum up those weights of the edges, and you get the total time it will take you to get from a to b. you want the fastest route? so you need the shortest path in terms of the sum of the weights of edges on this path. let's see now, does the breadth-first search algorithm from the previous lesson maybe help us? so let's look at this graph, here, we see that node a has a direct edge from a to point b, where you need to get. and so, what our breadth-first search would say is that you already know the optimal path from a to b because there is a direct edge, and so there is no point in getting to some other nodes, just go directly from a to b, right? well, this doesn't work, for example, in this case, let's suppose that it takes 5 hours to get from a to b, and the corresponding weight of the edge from a to b is 5. and let's suppose there is another node, c, such that there is an edge from a to c, which takes 2 hours to go through it. and another edge from c to b, which also takes 2 hours to get through it. and of course, this looks a little bit strange, but if there is a traffic jam on the road from a to b, and roads from a to c and from c to b are free roads, where there is no traffic jam, then this can happen. and so, then we see that going from a to b through c will take just 4 hours, while going directly from a to b takes 5 hours, and so our breadth-first search algorithm doesn't work. so maybe it's vice-versa, maybe it's always better to work around, to go around some node, not to go directly. but that's also not the case, because if the edge from a to b is 3 hours, and edges from a to c and from c to b didn't change, and we just removed the traffic jam from a to b and it became 3 hours instead of 5, it is now better to go directly from a to b. so there is now universal rule to go directly or not to go directly. we need something more clever than that to solve our shortest or fastest through the problem. now let's gain some intuition about this problem. so let's look at this graph below. and assume that we stay at origin node s. and we only know that the edge from s to b is going to take 3 hours and from s to c is going to take 5 hours. so can we be sure that the distance from s to c is equal to 5, that it will take 5 hours for the optimal route from s to c, okay? so no, this is not the case, because for example, the edge from b to c can have weight 1, and so, the shortest route from s to c will be from s to b and from b to c, which is only 4 hours instead of 5 hours if we go directly. so we cannot be sure that the fastest route from s to c is 5 hours. now, another question is can we be sure that the distance from s to b is equal to 3? and in this case, the answer is yes, because if we don't go directly from s to b, what other options do we have? we can go only from s to c and then go by some other edges to come to b, but if we go from s to c, we spent already 5 hours. and after that, we also spend some time to get to b. and the time is now negative. so we cannot spend less than 3 hours time in the end. we actually cannot spend less than 5 hours in the end. so going from s to b directly by 3 hours is beneficial for us. in terms of the graph and the weights of the edges in it, it means that there are no negative weights on the edges. all the weights are non-negative numbers. and so, we cannot decrease the length of the path by going through them. so if we have already a path of length 3 and all other paths start from spending more time, we cannot improve this direct buff. and in the next video, we'll use this idea to create a negative algorithm that solves our fastest route problem. 
hi. in this video, we'll solve the fastest route problem using the idea from the previous video, but we will do it using a naive algorithm in which we will need to improve in the next video. first, let's make an observation that if we have some optimal path, some fastest route or some shortest route. than any part of it between some note in the middle and some other note in the middle. it's also optimal in the same sense. let's prove it. consider an optimal path from some origin s to some note t and consider some two vertices u and v on this path. they can coincide with s or t or they can be somewhere in the middle. suppose there was some shorter path from u to v which is marked with green. then we would be able to go from s to u as there is no path. take a shorter path from u to v, take a shortcut and then go where there is no path from v to t. and in total, this new path will be shorter than the initial path, which was optimal. so this cannot happen. and so we know that actually any part of an optimal path is also optimal itself. corollary from that is that if there is some shortest path from s two nodes and u is the previous node on that path. then, distance from the origin to the final destination, t, is equal to the distance from origin to node u, which is the previous node, plus wight of the edge between u and t. and we will use this property to improve our estimates of distances from origin to some nodes gradient. you remember that in the breath research algorithm we had an area or map called dist, and we use these dist values to store. our estimation from the distance to the region to this particular now. in the reference research we found distance of started from infinity. as soon as they get update they became correct distances formal region to the corresponding note. this will not be the case in algorithm. these distances will change several times maybe before they become correct. but in the end they will become correct. and during the process these distances will be upper bounds on the actual distance. so they will be more than or equal to the real distance from origin to the corresponding node. [inaudible] the procedure called edge relaxation, and it means the following. we take the edge u, v. we check, is it better to go to v, through the optimal currently known path from s to u, and then following the edge from u, to v. does it improve our current upper ont he distance to v, or not? so, we have some upper bound dist value, dist of v, and we achieve that, maybe using u on the way or not. we might have come to v in a different way, and we might have come using the same distance as if we go from s to u, and then follow huv. or we could use longer path. so we just check whether it is possible to improve our current estimation of distance of feet. we are using distance of u plus weight of the edge to u. so here is the code of relaxation procedure. it takes one edge from u to v. as input and it checks if the current distance estimate of v is bigger than the current distance estimate of u plus weight of the edge. that means that definitely we can come to know u from s with a path with a length at most dist of u. and if we then follow edge uv with a weight w of uv we will improve the distance estimate of v. so in this case we update this distance estimate we decrease always as you see and we also remember that the node from which we came into v is now u. remember that in the breadth for search algorithm, we start in pref the node from which this node was discovered. in this case, we use the same data structure to the store the node from where we've updated the distance to our node last time. now, we're ready to suggest a naive algorithm to solve our fastest route problem. procedure naive takes graph g and origin node s as inputs. it uses dist values and prev values the same as in the breth first search, and we initialize these values with infinity as dist[u]. and the prev[u] value with point resting no where, we also initialize the dist of the original nod withe zero. and then, the only thing we do we relax all the edges. more specifically, on each iteration of the do while loop, we try to relax each of the edges in the graph. and if at least one is effective, that is, some dist value changes, then we continue to the next iteration. and we only stop when the whole iteration, after going through all the edges in the graph, couldn't relax anything. and we state that this naive algorithm works, basically that it stops and when it stops it finds correct distances to all the nodes. to prove that assume for the sake of contradiction, that at some point no edge can be relaxed and there is a vertex v such that the distance to the vertex is bigger than the actual distance from origin to the note. we know that this estimation of distance cannot become less than the actual distance because this is always an upper bound. it starts from infinity and it is only decreased when we find a better path from origin to this node. so there is some path from origin to this node of length exactly dist[v], so it cannot be less than the actual distance from origin to this node. and this also means that there can actually be no such situation that we do relaxations [inaudible] and we do many iterations and we don't stop at any point. because after any successful relaxation, some distance estimation is decreased by at least one. and if we started with infinity then the value just becomes finite, but that can happen at most a number of nodes times. and if the value was already finite it just decreases by at least one. and if we started with some number of finite values, which are bigger than the actual distances and that each iteration we decrease at least one distance by at least one. this process cannot be infinite. it will at some point come to the stage when the distance, and the distance estimate are the same, and so this edge cannot be relaxed anymore. and if that happens for all the edges our algorithm will stop. so our algorithm definitely stops. the question is whether it comes to exactly the same dist values as distances from origin to the corresponding nodes. so for contradiction we assume that it does not. and for that for at least some node v dist value is bigger than the actual distance from our agent. and then, we consider some shortest path from s to this node v. v definitely is a broken note, in the sense that [inaudible] is bigger than the correct distance, but there can be some other notes on this path from s to v, which have the same property, which are broken. u, v the first note of counting from s on this path, which is broken in some sense. u is definitely not the same as s, because for s we know that the [inaudible] value is zero, and the correct distance is zero. u is at least the second note on the path, or maybe much later then. there is a previous note on the path before u. and what is denoted by p. and let's look at s, p, and u. what we know is that p is not a broken node, and so its dist value is the same as the distance from origin to this node p. and so we know that the distance from s to u is actually equal to the distance from s to p plus weight of the edge from p to u. why is that? because the part of path from s to u is optimal because it is a part of an optimal path from s to v. and also part of path from s to p is also optimal, and so this equality it's true that distance from s to u is equal to distance from s to p. and then add the weight of the s from p to u, but we also know that distance from s to p is equal to this value of p. and so, the second quality is true that it is equal to dist value of p plus weight of the s from p to u. but we also know that note u is broken and the dist value of u is strictly bigger than the correct distance from s to u. is equal to dist value of p plus weight of the edge from p to u. but this inequality is exactly the property we checked to determine whether an edge from b to u can be relaxed or not. and so, from one hand we know that this edge can be relaxed and from the another we know that we cannot relax anymore edges in our graph and our nef algorithms stopped and this is a contradiction. so now, we proved by contradiction that our nef algorithm returns correct distances from orignal to. so now it's in the graph. we won't analyze the running time of this algorithm, because in the next video we'll improve this algorithm and then we'll estimate its running time. 
hi, in this video, you will learn dijkstra's algorithm, which is an efficient algorithm to find shortest paths from original node in the graph to all the nodes in the graph with weighted edges where all the weights are non-negative. first, let's improve our intuition. so let's say we're in the node s, and this is the origin node. what we know exactly is that the distance from this node to itself is 0. we don't need to see anything else to decide that because all the edges are non-negative, so it's better just to stay in s, and this is the shortest path from s to itself, okay? and now, let's look at all the edges outgoing of node s. and let's relax all those edges. so now we see that there are edges of length 5 and 10, and they go to nodes a and b. and so, we can determine that the dist value of a is 5, and the dist value of b is 10 after relaxation. now if there are no more edges outgoing from s, we already know that the distance from s to a is exactly 5. because we cannot go around it and spend less time. so we're sure about distance for a. so now, let's relax all the edges from a. and we will see that the distance to b will improve, and the distance to b becomes 5 plus 3, which is just 8. and also, we'll discover two more nodes, c and d, and we will relax the edges from a to them. and c will get an estimate of 12 because it's 5 plus 7, and d will get an estimate of 6, which is 5 plus 1, which is the way of edge from a to d. so the question is what is the next node for which we already know the correct distance for sure? and this node is d. because it has the smallest dist value estimate, and it means that there is no way to go around it. we can go directly to it from s through a in distance 6, but if we go through any of the other nodes, it will take us at least 8 to get there and also some non-negative amount after that. so we cannot really improve this dist value estimate of 6. so we now know exactly the distance to d is 6, and we can continue by relaxing edges from d. while for b and c, it is still possible that their dist values are larger than actual distances. for example, if there is an edge from d to b of length 1, then the dist value of b will improve even better, it will be 7. and if we get an edge from d to c of length 1, then the distance to c will be just 7, also, which is much less than 12. so now we have this intuition that at any moment of time, if we have relaxed all the edges outgoing from some known set of nodes for which the distances are already known correctly, then the node with the smallest dist value estimate is also a node for which we know the distance correctly. so, the main idea of dijkstra's algorithm is that we maintain some set r of vertices for which dist value is already set correctly. we call this set r, known region. and initially, we only have node s, the region in the set r, because for this node, we definitely know that the distance to it is 0. and then on each iteration, we take the node outside of this region with a minimum dist value out of all nodes in this outside region, and we add this node to the known region and relax all the outgoing edges. and so, after a number of iterations, which is equal to the number of nodes in the graph, we will know all the correct distances from origin to all the nodes in the graph. let's see how it works in an example. here, we have six nodes, some edges, and the edge weights are written nearby the corresponding edges. and on top of each node, and under the bottom nodes, we have the current dist value. so we start with infinities in all the nodes, but the origin node to the left, which has dist value of 0. now we add this 0 node to the known region, and we now know for sure that the distance from the origin to it is 0. and so we color the dist value by green, because we are sure that this is the correct distance from origin to this node. and then we color our node in black, and we start processing it. so we color nodes in black when we start processing them in the same way we did in the breadth-first search. now relax all the outgoing edges from this node. first relax the edge of length 3, and we improve the dist value from infinity to 3. and next, we traverse edge of length 10 to improve the dist value from infinity to 10. now what do we know? we know that the node with a minimum dist value outside of the known black region is the node with dist value of 3. so this node actually has a distance of 3, and we color the distance with green and the node with black, and we start processing this node. so we process the outgoing edges. first, edge of length 8, which goes to the node with distance 10, but new estimate is 3 plus 8, which is 11, which is bigger than 10. so, nothing changes because of this edge. and i also have an edge of flag 3, and we improve the dist value from infinity to 6. and there is also an edge of flag 5, which improves the dist value from infinity to 8. and now, what we see is that the node with the minimum dist values, the node in the top right corner, which has this value of 6. so we color it with black color and the distance with green and process it. this edge gives us improvement from infinity to 8. this edge gives us improvement from 8 to 7, and this edge gives us an improvement from 10 to 9. now the best node is in the bottom to the right and has distance 7. and it improves the distance estimate for the right most node from 8 to 7, and notice that there is an edge of weight exactly 0. this is allowed because 0 is non-negative. we'll only forbid the edges from being negative, but non-negative, including 0, is okay. so now, we don't have any more outgoing edges from these nodes to the right and the bottom. and so it's like the next node process, which is the right most node with an estimation of 7. but we don't have any outgoing edges from it, so we process the last remaining node, which has distance of 9. and we can try to relax some edges, but they don't work because this is the farthest node from the start. so now we know all the distances from the origin to all the nodes in the graph. and this is how dijkstra's algorithm works on this example. 
now let's implement this algorithm. so the procedure dijkstra takes again graph g and origin node s as input. it will also use dist values and prev values. it will initialize dist values with infinities, but for the origin node s for which dist value is 0. and it will initialize breadth values with pointers to nowhere. it will use also some data structure which can be an array or a priority queue. it depends on your choice of data structure. and the writing time will depend heavily on that but any way this is a data structure, for which we need just three operations. to create it from a set of these values. to extract the minimum value from it to get the node outside of known origin with a minimum dist value. and, to change the dist variable of some node which is currently in this beta structure. so we will talk later about which data structure to choose, but for example, we can use an array here just to create an array from all the dist values and then work with it. so while this data structure is not empty, we'll take out of it the node with the minimum dist value, and this data structure will contain only those nodes which are not in the known region. so it will contain initially node s and all the nodes, but then after we make the first iteration we will extract node s from this data structure h. because node s has dist value of 0 and all other nodes have dist value of infinity, so node s has the minimum dist value. so we extract minimum from it, which means that we take the node with the minimum dist value and remove it from the data structure. and then we'll process this node u, basically we take all the outgoing edges from u in the graph and we try to relax them. to relax them, we check if it's possible to relax the edge. and if it's possible we update the distance to the end of this edge, the dist value to the end of this edge. we update the prev value of this node v and most importantly we change the priority of this node v in our data structure h. what it means is that we improved the dist value for this node v. and now this node could become potentially the node with the minimum dist value, for example. so we need to account for that. so you know in data structure we need to do something, for example if we store everything as an array, we just need to decrease the value of this array corresponding to node v. and then if we store everything in an array then when we do extract mean, we just go through all the values of this array and find the minimum value and the node corresponding to this minimum value. so this all happens while our data structure is not empty. but basically it will happen at most the number of nodes times, because it starts with all the nodes in the data structure. and then just extract next node for each time, and the size increases one by one. 
now let's prove that dijkstra's algorithm actually finds correct distances from the original node to all the nodes in the graph. and the lemma states that when a node is selected as the node with a minimum dist value out of all nodes outside of a known region, it's dist value is actually equal to the correct distance from origin to this node. let's prove that. again, by contradiction. let's assume this rule is broken at some point. let's select first such moment. so this moment, we have some known region, r, which contains node s and maybe some other nodes colored in black. and all the nodes colored in white are outside the known region. so c, d, e, f, and g, in this case. and they have some dist values which are written in blue near the corresponding nodes. what will happen next is node c will be selected as the node with the minimum dist value out of all the nodes outside known region. and we suppose that this dist value of six is wrong. what we know about dist values is that they are upper bounds on the correct distances, that is if we have some dist value then we can also show some path of exactly the length equal to this dist value. and so the shortest path to this node is less than or equal to this value. so there can be only two cases. dist value is equal to the correct distance, but we assume it is not. and in this case it is strictly bigger than the correct distance. so we know that dist value of six is strictly bigger than the length of the shortest path from origin to node c. now let's think about this shortest path. it starts in the node s, which is inside known region, and it ends in the node c, which is outside of the current known region. so there is some edge which goes from inside known region to outside of the known region in this shortest path. let's consider this edge. this can be an edge, for example, from b to f. it can also go directly into c, that doesn't matter. what matters is what happens next. after it goes from known region to outside, it also goes through some path of known negative length, or maybe this path is empty with the edge from known region outside of it goes directly to c, but anyway this rest of the path is non-negative. what it means that if we just considered the first part of this path which ends in the end of the edge from known region to outside, now this part of the path is also strictly less than the dist value of c, strictly less than six. so we know that there is a path which goes from s, goes somewhere inside the known region and then goes outside and its length is strictly less than six. but it means that actually some of the edges, it means that this edge could be relaxed in the current situation, because we know for sure that the distance estimate for node b is equal to the shortest path from s to b, because the first moment when this rule breaks is with node c, it was not broken with node b. okay? so, the distance estimate of node b is exactly equal to the shortest path from s to b. when we add the length of the edge from b to f to it, we will get this same thing as we get when we go by shortest path from s to c, but take only the part of it, which adds in the node f. so if we add to the value of b, the value of the length of the edge from b to f. we will get value very which is strictly less then six. and so it is strictly less than this value of f because this value of f is greater than or equal to the dist value of e. so we could relax this edge but we didn't do it for some reason. that's we know that we've relaxed all the edges from the known region to outside of the known the region. so this contradicts our algorithm. now this contradiction proves that this rule is actually not broken and when we select the node with the minimum distance value outside of the known region, it's dist value is actually equal to the correct distance from origin to this node. and this proves that dijkstra's algorithm finds correct distances from the region to all the nodes in the graph. 
now let's estimate the running time of dijkstra's algorithm. in the beginning it just initializes dist values and prev values and that takes time proportional to the number of nodes. but our estimate will be bigger than that, so we just ignore this part. and other than that, it creates a data structure to store dist values and extract minimums from it. it extracts the minimum node from this data structure. v times, where v is the number of nodes in the graph, and also it examines the edges. each edge is examined, at most once, when the start of this edge is processed. and during processing, it updates dist value, pref value, and changes priority in the data structure. updating dist value and pref value is constant time, so the main part is in the changepriority part. so now this running time estimating depends on how do you actually implement the data structure from which you do extractmin, changepriority, and we should build by makequeue. one way to implement it is just using an array. actually we'll need two arrays. first array is just the size v, where you store in the cell number i, you store the dist value of the node number i. but you also need to remove nodes from this array, so you'll use another boolean array, where you store a flag. whether this node is still in the data structure or is is not already. so to build such arrays you need time proportional to the number of nodes, because it's basically, write down these values for all the nodes and you write all the flags, you say that they are true. then, each extractmin operation in the time proportional to the number of notes. because you need to go through the whole array, check whether this particular node is considered to be in the data structure or not. just look up in the secondary, and then if it is then try to proof the minimum, and after you found the minimum, you just mark the flag as false, it's now it's no longer in the data structure, and you take the minimum value from there array. so each extractment of operation works in time proportional to v and we make v times, so this is v squared time. now, change priority operation is very easy in case of array, because to priority you just need to change the disvalue of one node so you just get the number of that node and you just change the value in the first array corresponding to that node, so that takes constant time. and so in this case, the total complexity is v + v squared + e. and v squared is bigger than v obviously, and also v squared is bigger than the recall to e, because there can be at most one edge between each pair of nodes so v squared is the leading term and so our running time is big o v squared for the array implementation; and there is another way to implement this data structure using a binary heap or priority cube built on top of binary heap. we know that to build a binary heap from an array of size v we need time proportional to v. we now that extractmin works in the logarithmic time, so the total time for extractmin will be v log v. and the tricky part is changepriority changepriority operation can be implemented in the binary heap, but it is a little bit tricky. so instead of implementing additional operation in the binary heap, we can actually cheat and we can just insert a new element in the heap each time we need to decrease the dist value for some node. so we need to improve our dist value and to improve it means to decrease it. what we can do is we can just insert another element, a pair of a node and its dist value estimation into the priority queue. so, then when the time comes to extract that node the pair with the minimum dist value will be selected and you will extract it with the minimum dist value which was found by this time. and if at some other point of time you will also try to extract this same node but with different dist value, you can just ignore it, you can just write down that you already processed this node. extract it again from the q, and just don't do anything with it. so, this will increase the size of the heap and so refreshes will be slower a little bit, but they won't be much slower because how many elements can be in the and in this priority queue? it will be no more than the number of edges because each time you change priority you examine some edge, and you examine some edge at most once. so total number of change priority operations is the number of edges. and so you will add at most e new elements in the heap. in addition to the values which were there initially. and we know that e is at most v squared. so logarithm of v is on the same, it's like at most two logarithms of v because logarithm of v squared is just two logarithms of v. so the last part, the total time for changepriority is proportional to e multiplied by logarithm of v. and the final estimation is v + e logarithm v. which is much better than v squared in the case when there are no edges. because there are very many edges like on the order of v squared than v squared will be equal to v squared log v. which is worse than in the case of area implementation. but if the number of edges less than v squared the graph is far from being. full graph with all the edges, then this can be much, much faster than the array-based implementation. in conclusion, you now know the algorithm to find the minimum time to get from work to home for example. and actually you also know the algorithm to find the fastest route itself, because the reconstruct path algorithm will be the same as you have in the prev first search. because you have the same prev values and you can just reconstruct the path from them. and you know that this algorithm works for any graph with non-negative edge weights. so you can not only find, fastest routes but you can also find shortest routes and routes which are minimum in some other senses and you know that it works either in time quadratic and number of nodes in the graph or in time v plus e log v depending on the implementation. either array based implementation or binary heap based implementation. 
hi, in the previous lesson you learned to find shortest paths in the graph with non-negative edge weight. in this lesson, you will learn to find shortest paths even in the graphs where some of the edge weights can be negative. we will explore that using a problem about currency exchange, which doesn't seem to have anything to do with shortest paths. but we will soon find out it does. so the problem about currency exchange is that you are given some possibilities that you can convert currency into another. for example, us dollars to russian rubles, or rubles to euros, or euros to norwegian crowns. and what you want to do is take your initial $1,000 and convert it into russian rubles. potentially doing many conversions on the way, such that you get as many russian rubles as you can in the end. so one question is what is the maximum amount of rubles you can get? and another question is actually can you get as many as you want? and the same question about us dollars. can you get as many us dollars as you want? it may seem that if you can get as many rubles as you want, then you will also be able to get as many dollars as you want. but that's not always the case, it depends on your restrictions. for example, if you lived in the ussr, you might be able to get some rubles for dollars you got from your foreign country trip. but it's very unlikely you could buy some dollars inside ussr. so in this problem we assume that you have some restrictions on which directions you can convert currencies and what are the rates. so this is an illustration from wikipedia article called triangular arbitrage which illustrates the fact that sometimes it is possible to make three trades and exchange first dollars to euros, then euros to great britain pounds. and then pounds to us dollars, so that you generate some profit. in this case, given $5 million, you generate a profit of 25,000, just by doing those three trades. these triangular arbitrages exist very rarely and only due to some market inefficiencies. but in theory, this is possible. and this example has some numbers on the edges. for example, the rate of conversion from dollars to euros is 0.81. and you get 1.19 euros per british pound. and you get 1.46 dollars per british pound. so given these conversion rates and of course those should also count for commissions and any other money that you actually don't get. so given those numbers, you determine exactly how many dollars will you get from 5 million initial if you do all the trades shown on the picture. so we will be discussing this problem, how much you can get of each currency and if it's actually possible to get more of the currency you had initially doing some trades. so how does this conversion work? if we have some path, if we convert first from dollars to euros then euros to pounds then some other conversions, then we come to norwegian crowns and then we finally convert them to rubles. then how many rubles do we get per 1 us dollar? well, it is determined by the product of the conversion rates written on the edges. so for example, if for $1, we get 0.88 euros and for 1 euro we get 0.84 pounds, and so one, and for 1 norwegian crowns we get 8.08 rubles. then for 1 us dollar we will get the product of all those numbers in rubles. that's how conversion rates work, if you account, again, for commissions and other payments in each trade inside this number, which right on the edge. so what are the possibilities for exchanges? it may look that there is only one way. you can just convert from dollars to rubles, or maybe use some intermediate currency. but actually, there is an infinite number of possibilities. for example, in this particular case, you could exchange dollars to euros, euros to pounds, pounds to dollars, and go through this cycle for as many times as you want, then convert to rubles. or convert to euros and then to rubles. so there are many, many possibilities, and the result of each such path through the graph is the product of the numbers written on the edges of this graph. so the problem formulated in mathematical terms, is that if you're given a currency exchange graph with weighted directed edges denoted by ei, between some pairs of currencies. maybe you can convert some pairs of currencies but you are not able to convert some others. and this graph doesn't have to be symmetrical or undirected. so it is possible that you can convert dollars to rubles but not vice versa. and each edge has a conversion rate corresponding to this pair of currencies and it written as a weight of this edge r with index ei. and what you want to do is maximize the product of the weight of the edges over all positive paths. and a potentially infinite number of paths, from the node corresponding to us dollars to the node corresponding russian rubles in this graph. okay, and we could substitute these two currencies by any other pair, it doesn't matter. 
so now, i want to reduce this problem to the problem about shortest paths, and we will do that using two standard approaches. first, we don't know what to do with this product, so instead of products of weights we want sums of weights, like in the shortest paths problems. and we will replace products with sums by taking logarithms of weights, and i will show that in a minute. and another problem is that we need to maximize something in this problem, while shortest paths are all about minimizing. so we'll have to also negate weights to solve minimization problem instead of maximization. first let's talk about taking the logarithm is a known rule that x is the same as two to the power of logarithm of x over two. so, we can take any product of two number, like x and y and rewrite x is two the logarithm of x and y as 2 to the power of logarithm of y. and then, x y is equal to 2 to the power of logarithm of x by 2 of the power of logarithm as y, and now we know the rule of summing the powers. so this is equal to 2 to the power of logarithm of x plus logarithm of y so if we want to maximize product effects on y. this is actually the same of maximizing the sum of logarithm of x and logarithm of y because if he sum becomes bigger then 2 to the power of the sum becomes bigger. and if the sum becomes smaller, then 2 to the power of sum also becomes smaller. this is true not only for 2 numbers. for example, if we have 3 specific numbers. 4, 1, and 1 or 2 which one to multiply. on one hand, we get 2 which is 2 to the power of 1. on another hand, if we sum up the logarithm terms of 4, 1, and one-half, we get sum of 2, 0, and -1. and the logarithms can be both positive and negative. they're positive when the number is bigger than 1 and they're negative when the number is smaller than 1, and they're 0 when the number is equal to 1. so in this case, we get the sum of 1, which is the same as the power to which we exponentiate our 2. so you see that it works not only for 2 numbers, but also for several numbers. and in general, to maximize the product of k numbers, r would ej. it is the same as to maximize the corresponding the sum of logarithms of these numbers. and now that this only works if all these numbers are positive, because we cannot take logarithms of negative values and we also cannot take logarithms of 0. but if all the exchange rates are positive numbers, hopefully then we just take logarithms and we reduce our problem of maximizing product of some numbers to maximizing sum of some numbers. now, we want to go from maximization to minimization but that is easy. if you want to maximize the sum of logarithms, it is the same as minimize minus the sum. and we will also want to work just with the sum, not with the minus sum, so we can insert this minus inside the sum incorporated. and so finally, we get that maximizing the sum of logarithms is the same as minimizing the sum of minus logarithms. trading those two ideas, we've got the following reduction. we replace the initial edge weights, conversion weights, rei by minus algorithm of rei. and we find the shortest path between the nodes corresponding to usd and the nodes corresponding to rur in the graph. and this is equivalent to the initial problem of how many rubbles you can get from $1000. so now, it looks like we've solved the problem because we can create the currency exchange graph with the conversion rates, we can replace those rates with logarithms. and we can find the shortest path from usd to rur using dijkstra's algorithm, which were learned in the previous lesson. and then, we can just do the exchanges corresponding to the shortest path in the graph which you found. however, that doesn't exactly work. because dijkstra's algorithm heavily relies on the fact that the shortest path from s to t goes only through vertices that are closer to s than t. and this is because the edge weights are positive, but if edge weights can be negative, this is no longer the case and the example is below. if we used dijkstra's algorithm as soon as it saw only two edges from s to a and to b. one of them with weight five, and another with weight ten. it would decide that the shortest path to s to a is exactly five, because we cannot improve it. in this example, we can improve it. we go from s to b, then from b to a, and the path will be, already, minus ten, which is much less than five. so dijkstra's algorithm doesn't work in such cases. such an example is also possible in the currency exchange problem. here is a graph with realistic conversion rates between ruble, euros and u.s. dollars. and our goal is to convert rubles into us dollars in the most profitable way. and it turns out that if we take minus logarithms of these conversion rates, then although the number on the edge from rubles to us dollars is less than the number from rubles to euros. it is still beneficial to go through euros to us dollars because of the negative edge between euros and us dollars. and it's true that if you multiply the conversation rate between rubles and euros, and then between euros and dollars. it will be slightly bigger than if we convert directly from rubles to dollars. so, all problems in the graphs with negative weights come from negative weight cycles. for example, in this graph, we have a negative cycle abc. what it means is that we can go from a to b than from b to c, than from c to a. and if we add those weights we get -1. so the sum of the edges on the cycle is negative. and because of that, if you want to convert for example from s to a, if you want to go from s to a and find the shortest path, this is not possible. because we can go from s to a, use distance of 4 but then we can just go around the cycle a b c, a b c, a b c many, many times, as many as we want. and the distance will only decrease. so the distance from s to node a is actually minus infinity, is not defined. you can do as short a path as you want. and the same is true about nodes b and c of course because they are on the cycle. so you can do the same thing with them. and the same is also true about node d because it is reachable from the cycle. so we can go to the cycle and make a lot of round trips on the cycle and then go to node d from either from b or from c. so all these nodes. are the infinitely close to s, like the shortest path is minus infinity. and it turns out that in the currency exchange problem a cycle can potentially make you a billionaire, if you are lucky and if you have enough time for that. but you'll learn how to do that a little bit later in this lesson. 
hi, in this video you will learn bellman-ford's algorithm, which is an algorithm for finding shortest paths in graphs where edges can have negative weight. actually, do you remember the naive algorithm from the previous lesson about dijkstra's algorithm? it turns out, it's not so naive and bellman-ford's algorithm is almost the same as the naive algorithm. so, that naive algorithm just relaxed edges while dist changed, and at some point it stopped. we didn't estimate the running time of that algorithm. but it turns out, that this algorithm has benefit over dijkstra's algorithm that it works even for negative edge weights. it is a little bit slower than dijkstra's algorithm but it works in graphs with any edge weights. so, here is bellman-ford's algorithm. and, it takes as input, graph and original node again. and we will find shortest paths from this original to all the nodes in the graph. and there is an additional comment that this algorithm assumes that there are no negative weight cycles in g. otherwise, it will work still but it won't return correct distances for some of the nodes, so. so, if you know that there are no negative weight cycles in g, this algorithm will give you the answers you need. if there are negative weight cycles, we'll discuss later what to do in this case. so, this algorithm uses the same dist values and prev values as b f s and dijkstra's algorithm. it initializes them the same way, infinitely for distances apart from the origin node s and prevs just point to nowhere. and then we repeat exactly v- 1 times. v is the number of nodes in the graph. we relax all the edges in the graph in order. so, this is the bellman-ford's algorithm. and actually, this repeat v- 1 times is excessive. actually, we can just return to the naive algorithm interpretation when it repeated relaxation of all the edges until no relaxation is possible. so, this can be done, and this will even work faster than bellman-ford, in the case where there are no negative weight cycles. however, we write this pseudo-code in this form just because it is easier to prove the algorithms correctness this way. but, you can just know that if at some iteration during these v- y iterations nothing changed, no edge was actually relaxed, we can just stop there and the distances will already be correct. now, let's estimate the running time of this algorithm. and i state that it's proportional to the product of number of nodes and number of edges. so, that is longer than dijkstra's algorithm which was v squared in terms of array based implementation and even e + v log v in terms of heap, binary heap implementation. so, this is longer than that but it works with negative edge weights, so it's good. so initially, we just initialize dist and prev values in time proportional to number of nodes. and then we'll do v- 1 iteration. each iteration takes time proportional to the number of edges because relaxation is a constant time procedure. so, totally we get time proportional to ve. and now, we'll look at an example of how bellman-ford's algorithm works on a real graph. so the original is, again, s. and the numbers 0 and infinity in blue are the dis values. and the numbers near the edges are the edge weights. so, what we do is we take all the edges in order starting from the edge with weight 4. and we try to relax it. and we improve the dist value for a from infinity to 4. we take the next edge with length 3 and we improve the dist value of b from infinity to 3. we take the edge with weight -2 and we further improve the dist value for b from 3 to 2. then, we consider edge from a to c and improve infinity to 8. then, we consider edge from b to c and improve it further from 8 to -1. then, we consider edge from b to d and improve from infinity to 3. and, we consider the edge from c to d and improve from 3 to 1. this is just one iteration of bellman-ford's algorithm. now, let's see what happens on the next iteration. we consider edge from s to a, it doesn't improve anything. this edge, doesn't improve anything. this edge doesn't improve. this edge also doesn't improve. and this doesn't improve. and this doesn't improve. and this does improve. and so already, on the second iteration, nothing can be improved. so, we actually can stop the algorithm just after one iteration. and it will take, instead of ve operations, just e operations because we made just one iteration over all the edges. and often, it will work just this way in practice. but for some graphs, of course, we'll need many more iterations. so now, we already know the final distances that the bellman-ford algorithm returns. and in the next video, we will prove that this algorithm returns correct distances in the absence of negative weight cycles. 
hi, in this video, we will prove that bellman-ford's algorithm returns correct distances from origin node to all the nodes in the graph in the absence of negative weight cycles. first, we need the following lemma. after k iterations of relaxations inside bellman-ford's algorithm for any k, then if we take any node u, dist[u] after these k iterations will be equal to the shortest path length from s to this node u, but among all the paths that contain, at most, k edges. so not all the possible paths, just paths which contain 0, 1, 2, or, at most, k edges. for example, after one iteration, we state that dist values of all the nodes will contain the best possible shortest path, which consists from 0 or 1 edges. we'll prove this lemma by mathematical induction. and the base case is after 0 iterations, all the dist values are infinity and dist[s] = 0. and this is correct because for s, there is a path of 0 edges, which has lines to 0. and correct distance from s to s is 0. and dist value is also 0. and for all the other nodes, there is no path that contains 0 edges. and so the shortest path that contains 0 edges and goes through those nodes is infinity. now the induction step is if we proved for paths with length at most k, then we need to prove it for paths with length at most k + 1. so, we know that before iteration number k + 1, dist[u] is the smallest length of a path from s to u, which contains at most k edges. so what do we do on the k + 1-th iteration? each path from s to u goes through one of the incoming edges, or like from some node v into node u. and also, parts of length k+1 go this way. they go k edges to some known v, and then they go through this edge from v to u. so when we try relaxing edge from v to u, we compare the current dist value, which is the smallest path length out of the paths, which contain at most k edges, we compare it with the smallest length of a path from s to u, which contains at most k + 1 edge and goes through v. so all parts which contain at most k+1 edges and go to u, they go through one of the incoming edges (v, u). and so we will compare with all the possible paths that contain at most k+1 edges and go from s to u. so if initially, we had the best paths, which contain at most k edges, after new iteration, we have the best paths, which contain at most k+1 edge. because we add one last edge from v to u to the best path, which contains at most k edges from s to v. so the lemma is proved, and now we have two corollaries. first that if a graph doesn't have any negative weight cycles, then bellman-ford algorithm correctly finds all distances from the starting node s. why is that? because it does v-1 iterations. and after v-1 iterations, for each node, its dist value contains the shortest path out of all paths that contain at most v-1 edges. but if there are no negative cycles, then any path, any shortest path contains at most v-1 edge. because if it contains at least v edges, then there will be a cycle inside it. and the cycle will be non-negative, so we can just remove this non-negative cycle from the path, and it will improve or stay the same. so any shortest path contains just v-1 edges or less, and so bellman-ford algorithm correctly finds the best shortest paths for each node. another corollary is harder, even if there is a negative cycle in the graph, that doesn't mean that there is no correct distance estimation from origin to some particular node because that particular node may be not reachable from any of the negative weight cycles. so if there is such node u, then there's no negative weight cycle from which it is reachable. then bellman-ford algorithm will correctly find distance from after this node by the same reason because if we have any shortest path from s to u, it cannot contain cycles because u is not reachable from any negative weight cycles. and so the cycle on the path from s to u must be non-negative, and so we can just remove it from the path, and the path will improve or stay the same. so, any path from s to u doesn't contain cycles, and that means that any path from s to u, which is the shortest path, has at most v-1 edges. and this means that bellman-ford's algorithm will return actually a correct value for the distance from s to u. and in the next video, we will look into what to do with the negative cycles when they're present in the graph and how to use it for your own profit, potentially making yourself a billionaire based on currency exchange 
hi, in this video you will learn to deal with negative cycles in the graphs. you will learn to detect whether there is a negative cycle in the graph and to find the negative cycle if you know there is one. and you will use that in the next video to achieve infinite arbitrage. but before that, you really need to learn to detect negative cycles in the graphs, and this lemma will help you. so, it says basically that there is a negative weight cycle in the graph if and only if you make one additional iteration of relaxation in the bellman-ford's algorithm, and some edge is relaxed. first, let's prove that if there is some change in the last step of bellman-ford's algorithm, then there is a negative cycle. assume for the sake of contradiction that there are no negative cycles. then it means that all shortest paths from s to any node in the graph contain at most, v- 1 edges. we've already discussed that. if they contain at least v edges, then there is a cycle. and this cycle is non-negative so it can be removed, and the path will be either improved or just stay shortest path. so, know this value can actually be updated on iteration number v. because we already found the optimal shortest path length with paths that contain, at most, v- 1 edges, and we cannot improve anything after that. so this is a contradiction with the fact that the last iteration actually relaxes some edge. and so if it does, then there is a negative cycle in the graph. now let's prove, in another side, that if there is a negative weight cycle, then there will be a relaxation on the iteration number v. suppose again, for the sake of contradiction, that there is a negative weight cycle. for example, a cycle of length three a b c a, but there are no relaxations on the iteration number v. now let's look at the following three inequalities. these inequalities basically state that edges (a, b), (b,c), and (c,a) cannot be relaxed on the v-th iteration. but if we sum these inequalities, on the left side, you will have sum of these values for a, b, and c. and on the right part, we will have sum of these values of a, b, and c plus sum of weights of the edges of the cycle. and from this sum of inequalities that follows, that the sum of the weights of edges is non-negative. but we know that the cycle is a negative weight cycle, so this is a contradiction and so we prove by contradiction that there will be some relaxation on iteration number v. so we proved our lemma, and now we know how to detect negative weight cycles in the graph. we just do one additional duration of bellman-ford's algorithm, and it doesn't increase its asymptotic complexity, its training time. and now we can determine whether the graph is good or bad, whether it contains negative cycles or not. now, not only to detect what are the reason negative cycle or not, also want to find some negative cycle itself, at least one. maybe they are many of them, but we want some negative cycle. so first, we still need to run executive v iterations of bellman-ford's relaxation. and as we know, if there is negative cycle, then for some node v, it will be relaxed. this value will be decreased on the last iteration, so save this node. and i state that this node is definitely reachable from a negative cycle. because if it was not reachable from the negative cycle, then any shortest path to this node will contain at most v- 1 edge, and it won't be improved on the iteration number v. so it is reachable from a negative cycle, and also it is reachable from a negative cycle with at most, v steps. because if you have shortest path from s to v, which contains negative cycle inside, even if there are more than the edges from this negative weight cycle to v, then there is another cycle on the shortest path. and it also has to be negative, because otherwise we can remove it from the shortest path. so v is definitely reachable from a negative cycle, with at most v steps. and we remember the previous node from which each node was updated last time. we store it in the prev values. so if we go by prev links from v, and we do that at least three times, we will be already definitely on this negative weight cycle. because we will already return to the cycle by at most the steps, and then we will just go around the cycle for some amount of duration. so if we go by v times back, we will be on the negative cycle. then, only thing we need to do is just to save the position where we started and then go once around the cycle until we come to the same node and we will know that the nodes we saw during this round trip are the nodes of the negative cycle. so this is the algorithm, and it works in the time of bellman-ford's algorithm plus proportional to the number of nodes. so basically, the same running time as bellman-ford's algorithm. so now the question is, again, if you have a negative cycle in your graph of minus logarithms, it doesn't mean that it is always possible to get as many rubles as you want from $1,000 dollars. and unfortunately, that's not always the case. for example, in this graph, there is a negative cycle between euro, british pounds and norwegian crowns. and you can check that if you multiply the conversion rates on the edges of this triangle you will get more than one and it means that, for one euro can get more than one euro. and this means in turn that you can get infinite arbitrages if you just make trades along the edges of this triangle for as many times as you want. but there is no possibility, in this particular example, to exchange euros for dollars. so you cannot exchange rubles for euros then make many loops around this negative weight cycle. and then exchange euros to dollars. so this is not sufficient, and you cannot actually get any dollars from rubles, you cannot get any rubles from dollars in this particular case. and in the next video, you will learn how to determine whether it is possible to have an infinite arbitrage from rubles to dollars, or from any other currency to any other currency. and not only to detect that, but to actually find a way to implement it if it's possible. 
hi! in this video, you will finally learn how to detect whether infinite arbitrage from some source currency to some target currency is currently possible. and not only that, you will also learn how to actually implement it if it is possible. but first, let's learn a criterion for the existence of infinite arbitrage. the lemma states that if we consider some source currency s, which we want to exchange into some target currency u, and get as much as we want of currency u from a unit of currency s. and it is possible if and only if the node u corresponding to the currency u in the currency graph is reachable from some node w, which was relaxed on iteration number big v. where big v is the number of nodes in the currency graph or just the number of different currencies which we consider. so we run the bellman-ford algorithm for exactly big v iterations and if some node w was relaxed on the last iteration, and some node u is reachable from it, then it is possible to get as much as you want of u from the source currency s, and vice versa. if it is possible to get as much as you want of currency u, then it must be reachable from some node w, which is going to be relaxed on the last iteration on the bellman-ford's algorithm. so let's prove this lemma. first, we'll prove that if some currency u is reachable from w, which was relaxed on iteration number v, then the infinite arbitrage is possible. to see that, first note that, as w was relaxed on iteration number v, then we know from the previous video that w is reachable from some negative weight cycle, which in turn is reachable from the node s. it corresponds to the picture on the slide. and so we can get as much as we want of currency w from currency s. because we can go from s, to the node x of the negative cycle, which is reachable from s, then go as many times as we want, through the negative cycle, return to the node x, and then go from this node x, to node w. and using this way, we will get as much as we want of currency w, and then we know that u is reachable from w. we know that w is reachable from the negative cycle. the negative cycle is reachable from s. so we can use this hallway from s to negative cycle, then go through the negative cycle as many times as we want. then go to w from there, then go from there to u and this way we will get as much as we want of currency u. so infinite arbitrage is actually possible. now let's prove the other way around. that if we can get as much as we want of currency u from currency s, then u must be reachable from some node w, which is going to be relaxed on the last iteration of the bellman-ford algorithm. so first, let l be the length of the shortest path from s to u with at most v- 1 edges. then we know that after v-1 iterations of the bellman-ford's algorithm, dist[u] is equal to exactly l. and we know that exists infinite arbitrage from s to u and so there exists some path shorter than l, strictly shorter, because we can get a path for any length which is arbitrarily small. and so in particular there it is, the path which is shorter than l. and so the dist[u] will be decreased at some point. it will become less than l also. but we know that by the iteration number v- 1, it is still equal to l. it is not less than l. so it will be still decreased even more on some iteration after iteration v- 1. so it will be decreased either on iteration number v or on some of the further iterations. now let's note that if at some point, some edge from node x to node y was not relaxed, and also node x was not relaxed itself, then edge (x, y) will not be relaxed on the next iteration because nothing has changed. the dist[x] is the same. and dist[y] is not bigger than dist[x] plus length of edge (x, y) and this didn't change from this iteration to the next iteration. so this edge won't be relaxed on the next iteration. and this means that if node y was relaxed at some iteration, then there is some node x from which there is an edge to this node. and this node x was relaxed at some previous iteration. so basically, only those nodes which are reachable from nodes relaxed on previous iterations can be relaxed at the current iteration. and so we know that the node u will be relaxed at some iteration starting from iteration v and further, and this means that u is reachable from some other node or maybe the same node. but from node which was relaxed on iteration number v, note that u can be this node. for example, u can be relaxed exactly at iteration number v. and as we consider node to be reachable from itself, then this is a particular case, in which u was relaxed on iteration number v, and u is reachable from itself so, it is possible to get infinite arbitrage to u. but in any case, we'll prove that if infinite arbitrage from s to u is possible, then u is reachable from at least some node, maybe the same u which was relaxed on iteration number v. so now we have a criterion for existence of infinite arbitrage. how to apply that criteria. so this is an algorithm to detect infinite arbitrage. so we first do exactly v iterations of bellman-ford algorithm, and we save all the nodes which were relaxed on the last iteration, iteration number v. and let this set of nodes be denoted by a. now we'll put all the nodes from a into the queue, and we'll use this queue for a breadth-first search. so we'll start our breadth-first search not from a queue which contains just the first node, from which we want to know all the reachable from it. but we will put all the nodes which were relaxed on iteration number v into the queue, and then start breadth-first search. and then this breadth-first search will find us all the nodes which are reachable from at least one of the nodes from the set a. so those will be exactly those nodes for which infinite arbitrage is possible. so, all those nodes and only those can have infinite arbitrage. and this is a way to find all target currencies for which infinite arbitrage from a fixed source currency s is possible. but this is not enough. what we want is to actually if infinite arbitrage is possible, is how to actually implement it. and here is the next algorithm. so let's suppose we already detected that the infinite arbitrage to some target currency u is possible, and we determined that using breadth-first search from the set a. so we need to augment this breadth-first search and we will remember the parent of each visited node, so that when a breadth-first search discovers a new node, it discovers it from some node which was discovered previously. this is the parent of this node. and this is what allows us to reconstruct the path from the source node to the end node in the regular breadth-first search algorithm. so we will do the same thing, and this will allow us to reconstruct the path to the target currency u from some node w, which was relaxed on iteration number v. then we'll use the algorithm from the previous video to find the negative cycle from which w is reachable. because we know that w was relaxed on iteration number v, and so it is reachable from some negative cycle, which is in turn reachable from s. and if we go back by the parent pointers from w, we will find this negative cycle. so we'll find both this negative cycle and the path from it to w. and of course we can also find a path from the source currency to this negative cycle by, for example, launching a regular breadth-first search from s until we encounter some node of that negative cycle. and then combining all that, we will have a path from s to the negative cycle, through which we can then go by as many iterations as we want. then we'll go by a path from this negative cycle to w. and then we'll go by a path from w to u, which you already know. and that gives us a way implement infinite arbitrage from the source currency s to the target currency u. so in conclusion, we can now implement the best possible exchange rate, in the case it exists and there is no infinite arbitrage. and we can determine whether actually infinite arbitrage is possible currently. and we can actually implement infinite arbitrage from a given source currency to given target currencies. and in a more general and abstract way we can find shortest paths, not only in the graphs where all the edges are positive, like graphs of navigation systems with positive times to go through an edge and length of the edge, but also in the graphs where negative edge weights are possible, such as the graphs of currency exchange. so we can basically find shortest paths correctly in any graphs with weighted edges. that's what we've learned in this lesson. 
hello and welcome to the next module, in which we will be talking about minimum spin entries. to [inaudible] this title, this problem considers the following toy example. assumes that we have six machines in our office and we would like to join them. in doing that whereby putting wires between some pairs of them such that each machine, each machine is reachable from any other machine. i assume further that we can put wires only between some pairs of our machines. and for each [inaudible] we're now [inaudible] responding [inaudible] why between them. for example in this case we are allowed to put a y between these two machines and of course this five. and we are not allowed to join these two machines by wire. okay. wire of the optimal solutions in this case is shown here on the slide. it is not difficult to check that in this case indeed any machine is reachable from [inaudible] machine. for example to reach the right machine from the left machine we would go as follows. first by this edge, then by this edge, and then by this edge. the total cost of the shown solution is equal to two plus one which is three plus three which is six plus four. which gives us ten plus two which gives us 12. so the total cost is 12, and this is actually not the only solution in this case because instead of using this wire, we may use this wire. the result in solution is shown for you on the slide and it is not. again, difficult to check that, in this case, any machine is reachable from any other machine and that the total cost is equal to 12. we will soon learn read the algorithms that will allow us to justify that in this, for example, the optimal total cost is indeed 12. these two algorithms will also allow us to solve very efficiently in practice instances consisting of thousands of machines. in our second example we have a collection cities and we would like to build roads between some pairs of them such that there is a path between any two cities. and such that the sum of the lengths of all the roads that we are going to build is as small as possible. okay? in this case the solution looks like this. and again we will soon learn how to find the solution efficiently. formally the problem is stated as follows. [inaudible] graph h [inaudible] and we assume that this graph is connected. okay and it is given together with positive edge weight. what we're looking for is that subset of edges, e prime, such that if we leave only these edges in the graph. then the resulting graph is connected and also the total cost of folds and edges at e prime is as small as possible, okay? so why is the problem is called minimum spending tree? well minimum corresponds to the fact that we are looking for a subset of edges or minimum total cost or minimum total weight. okay. it is called spanning because we're looking for a subset of edges such that if we leave only these edges, then the result in graph is still connected so it spans all the vertices in our graph. and finally, the word tree corresponds those effect that in each solution, the set e prime is going to be. is going to form a tree. we will prove this fact on the next slide. before proving that any optimal solution for minimum spanning tree problem any optimal solution in prime forms a tree. let me remind you a few useful properties of trees. first of all, just by definition, a tree is an undirected graph that is connected and is acyclic, that is it contains no cycles. recall that we usually draw trees as follows. so we draw them level by level and look like this. in this case, we actually. we're actually talk about rooted trees. this is a rooted tree and in particular, this is a root of this tree. at the same time, this graph is also a tree with only difference that there is no root in this tree and it is not drawn level by level. so once again, this graph is also connected and there are no cycle in this graph so it is a tree. there is no root in this tree, however we can take a vertex, declare this vertex as a tree and then hang by this vertex. and then this will allow us to draw this graph level by level,okay? the next property is that if we have a tree with n vertices than it necessarily has n minus one edges. why is that? well let me illustrate this by drawling something. initially we have just one vertices and zero edges. so initially the property has satisfied the number of edges. is equal to the number of vertices minus one. let then introduce some new vertex. so we'll attach it by an edge to a previous vertex. then we introduce another vertex, probably another one, and another one. and so on. so each time when we introduce a new edge introducing new [inaudible] we also have a new edge. so initially we had one vertex, and zero edges [inaudible] vertices and now we have, i'm sorry five vertices, now we have six vertices and [inaudible] five edges. so still the property is satisfied. note that we cannot introduce a new edge without introducing a new vertex. because if we introduce a new edge that would mean that we are connecting two existing vertices by edge. for example like this, but this will necessarily produce a cycle which means that this will give us a graph which is not at three. okay? the next property is that, actually any connected graph with the number of edges equal to the number of vertices minus one, is necessarily a tree. well, let me emphasize that it is important that here we are talking about connected graphs. because for example if a graph has the property that the number of edges is equal to the number of vertices minus one but it is not connected, that it is not necessarily a tree. a counterexample is the following. assume that we have four vertices and there is a [inaudible] and three vertices and there is one isolated. one isolated vertex. in this case, we have four vertices, and three edges, right? so this is not a tree, but at the same time this graph is not connected. there is no pass for example from this vertex to this isolated vertex, okay? and the last property says that an undirected graph is a tree if and only if there is a unique path between any two vertices. and this is also not difficult to see first of all if we have a tree then of course there is a unique path between two its vertices. on the other hand if we have to vertacise, and there are at least parts between them, then this gives us a cycle, which means that this is not a three. okay, now let me get back to proving that any optimal solution [inaudible] problem is indeed a three, for this, consider some [inaudible] solution. so in this case we have six vertices, and we consider some way of joining them into some way of connecting them. i assume that our solution looks like this. so you see that this is not a tree because there is a cycle. on these four vertices. well, in this cycle for any two vertices in this cycle, there are two paths. right? for example, for this vertex and for this vertex we can go either this way or this way. and this is true for any cycle, if we have a cycle and two vertices on this cycle we can either go this way or this way. and this means that there is some redundancy here. in this particular case, we can remove, for example, this edge from this cycle. or from this solution. this will only decrease the total weight of this solution and the resulting set of fedres will still be connected. great this proves that any optimal solution must be acyclic, and since any solution is required to be connected, this proves that any optimal solution is in fact a tree. 
the goal of this whole lesson is to present two greedy algorithms for the minimum spanning tree problem. before going in to the details of this algorithms, let me present you the higher level ideas of both these algorithms. so the first one is the kruskal and the second one is you need to prim. the high level idea of the algorithm by kruskal is the following. so, we go through all edges in order of increasing width, and we repeatedly at the next lightest edge which doesn't produce a cycle. alternatively, the main idea of the prim's algorithm is to grow a tree repeatedly so initially it contains just one vertex, then we attach a new vertex to it, then a new vertex to it. and we always attach the new vertex to the current tree by the lightest available edge. let me illustrate this again on a toy example. so for kruskal's algorithm, again, we repeatedly add the next lightest edge that doesn't produce a cycle. initially, our solution is empty, so we just add the first lightest edge. in this case, the lightest edge has weight 1, so we just put this edge into our solution. and there is also another edge which has cost or weight 1, so we also add it to our solution. the next one has weight 2. we add it. at the same time, the next one, the next lightest available edge has weight 3. however, if we added it to our current solution, this would produce a cycle. so we skip this edge. the next lightest available edge has weight 4. we add it because it doesn't produce a cycle. then, again, we try to add the edge with weight 5 because it is the next lightest available edge. however, it produces a cycle. so we skip this edge, and instead we add the edge of weight 6, right? this gives us a solution, and we will soon justify that this method indeed gives an optimal solution. now, to the prim's algorithm. it works in a different way. so it repeatedly grows just 1, 3. for this, it will select a root for this tree. so i assume that this highlighted vertex is going to be the root of the tree that we are going to construct. at each iteration we are going to attach a new node to this tree, and we would like to do this by using the lightest possible edge. so for this vertex, we have four edges going out of this node. one of weight 4, one of weight 5, of weight 6, and of weight 8. in this case, we select, of course, the edge of weight 4. so we attach it. now our tree contains two nodes, and we would like to attach a new node to this tree by lightest edge. so in this case, this is the vertex in the middle. and it has weight, the corresponding edge, has weight 1 so we attach it. the next one has weight 2. the next one has weight 6. and finally, the last node is attached by an edge of weight 1, right? so, in the next part of this lesson, we will present the implementation details of both these algorithms, but we first will prove that both these algorithms are correct. namely, that they produce an optimal solution. 
mostly considerate to algorithms construct the solution iteratively. namely they start with an empty solution, with an empty set of edges. and at each iteration they expand the current set of edges by one edge. but they use different strategies for selecting the next edge, namely the kruskal's algorithm. select the next lightest edge that doesn't produce a cycle. while the prim's algorithm selects the next lightest edge that attaches a new vertex to the current tree. what we need to prove to show that both of these algorithm's optimal is that this strategies are in some sense safe. namely, if at the current step we have a subset of edges which we call e prime which is a subset of some optimal solution. then by adding this edge according to one of these two strategies to the set gives us also a set of address which is also a part of some optimal solution. this will justify at the end when we have a three, this three is also a part of some optimal solution, which means that this is just optimal. so in this video, we're going to prove a lemma which is called cut property, which will justify that both these strategies, i indeed save. the formal statement of the lemma is the following. assume that we have a graph g with a set of edges v and the set of edges e. together with this graph, we are given a subset of its edges that we call x. x is a subset sum, subset of the set of edges of our graph for which we knows that it is a part of some optimal solution. it is a part of some minimum spanning tree. i assume also that the set of vertices is partitioned into two parts. one part is the subset of vertices s, and another part all the remaining vertices v-s. so we're given also a set of vertices s and this set of vertices s satisfies the following property. no edge from the set x joins two vertices that such as one lies intercept s and the other one lies intercept v-s. no edge from the set of x crosses between s and v-s. finally, assume that e, the edge e of the initial graph satisfies the following property. it is a lightest edge that joins the vertex from s with the vertex outside of x with the vertex from v-s. e is a lightest edge across this partition. then what dilemma states is that if we add the edge e to our current set x, then what we get will also be a subset of some minimum spanning tree. in other words, adding e to x in this case is a safe move. so since this is a long statement, let me repeat it once again using a small example. so i assume that this is our graph g, so we are given a graph g. together with this graph g we are given some subset of its edges shown here on the slide in blue, it is the set x. so this is just subset of the set of edges of how we graph and assumes that we know that it is a part of some minimum spanning tree. so the set x is a subset of some minimum spanning tree, then consider some partition of the set of vertices into two parts. see, this is one part and this is also remaining vertices, this is the second part. then this partition is required to satisfy the following property, no edge from x on our picture now blue edge joins to vertices from different parts. in this case, it is satisfied. indeed, any blue edge here joins two vertices that lie in the same part. next, we consider the lightest edge in our initial graph that joins two vertices from different parts. assume that this edge is e, shown here in red. then what lemma states is that if we this edge to our set x, then the resulting set of edges will also be a part of some minimum spanning tree. so this tells that adding e to our current subset which we grow repeatedly by one edge to the subset is a safe move then i will proof the cut property. this is our graph g and this is the subset of edges x shown here in blue and we assume that this subset x is a part of some minimum spanning tree which we denote by t. in other words, x is a subset of edges is a subset of some minimum spanning tree t. now, we also have a partition of the set of vertices into two part. we said the vertices s and the set of vertices v-s, all that remains. e is a lightest edge in the initial graph which joins two vertices from different parts of partition. so e joins the vertex from s with the vertex from v-s. what we need to prove is if we add the edge e to our current set x, then what we get is also a part of some minimum spanning tree. once again, what we assume about x. we assume x is a part of some minimum spanning tree t, and e joins two vertices from different parts of partition. and what we need to prove is that x with e added 3 is also a part of some possibly different minimum spanning three. so the minimum spanning tree which contains x plus e is not necessarily the same as t. so to prove this, we consider two cases. so first, if e happens to be a part of the minimum spanning tree t, then there is noting to prove. once again, we assume that x is a part of minimum spanning tree that we denote by if e is also part t sub x plus e is a part of minimum spanning tree t. in this case, there is nothing to prove, we are just done. so i assume that e is not a part of the minimum spanning tree t. let's then consider the wall tree t. so it contains x, so now the edges showing in blue show the wall tree t. know that if we add the edge e to the tree t then it produces a cycle. because in the tree t, there is a path between the vertices, between the endpoints of e, and when we add the edge e, it produces a cycle. in this case on our example, this is the cycle. this is the cycle, i'm sorry. so this is a cycle and the edge e in the cycle joins to vertices from different parts. since this is a cycle, so the edge e joins for example the left part with the right part. since this is a cycle it must eventually at some point go from right part to left part. which means that there must be an edge in the tree t which also joins two parts. and we denote this edge by e prime and in our case this is this edge, so this is e prime. now, i claim is that if we replace the edge e prime by the edge e in the current tree, is then what we get is an optimal spanning tree. so why is that? first of all, the resulting tree is still connected, because we just removed some edge, edge e prime from a cycle. so this is still connected, it was connected before. and when we remove an edge from a cycle, it cannot disconnect the graph. at the same time, the weight of the edge e prime is at least the weight of the edge e, because e is the lightest edge that joins different part of partition. which proves that the resulting set of edges is a tree of minimum possible weight which in turns, proves the cut property. 
we're now ready to present all the details of the kruskal algorithm. namely, we will prove that the kruskal strategy is optimal, it produces a spanning tree of minimum total weight, and we will also present implementation details of this algorithm. recall that the idea of this algorithm is the following. we start with an empty set x, and we repeatedly add to this set the next lightest edge that doesn't produce a cycle. so it is not difficult to see that at any point of time the set of edges x forms a forest. that is a collection of trees. let me illustrate this. assume that we have some set of vertices and initially, the set x is empty, which means that each of our vertices forms a tree of size 1, namely a tree that contains 1 vertex and no edges. initially each vertex is isolated in our set x. now, we start adding edges. probably, this is the first one, then we add this edge, then this edge, then this edge, then this edge, for example. at this point of time, our set x consists of three trees. so this is the first tree, t1. this is the second tree, t2. and this is the third tree, t3. in particular, the tree t3 contains just one vertex. it is an isolated vertex, okay? assume also that the next vertex that the next edge that kruskal's algorithm is going to add is the following. [noise] so, it is the next lightest edge that doesn't produce a cycle. the first thing to note is that the edge e must join two edges that belong to different trees, right? because if they were in the same tree, this would produce a cycle. okay, now we need to show that adding e is a safe move. for this, we need to use cut property, right? and in turn for using cut property, we need to construct, we need to show a partition of the set of vertices such that e is the lightest branch in our graph that joins vertices from different parts of partition. to construct such a cut, let's just take all the vertices from one of these trees as one part of this cut namely this is going to be the set s so this is one part of our partition and all other vertices is the other part of this partition. in this case, we see that e is the lightest edge that joins two vertices from different parts. which means in turn that cut property justifies that adding e in this case is indeed a safe move. in other words, if our carbon set tax is a part of some optimal spanning tree, then x with e added is also a part of some minimum spanning tree. once again, initially, the set x in the kruskal algorithm is empty, which means that each vertex of our initial graph forms a separate connected component. so this is how initially the set x looks like. so each vertex lies in a separate connective component. then we start adding edges to x. this creates [noise] a forest. in this forest currently, we have three trees. this is the first tree. this is the second one. and this is the third one. assume now that the next lightest edge that kruskal's algorithm considers is the following one. first of all, we need to be able to check whether it joins two vertices that lie in the same tree or in other words, that lie in the same connected component. in this case, they lie in the same connected component, so kruskal's algorithm will not edit through the set x, because otherwise, it would produce a cycle in our set x. now, assume that next set that kruskal's algorithm tries is the following. again, we need to check whether the corresponding two end points lie in the same connected component. in this case, it is not a case. they lie in different connected component. so we add this edge and to this point, we should update the data structures that we use to indicate that now we actually merge trees t1 and t2. so what we need to check in our data structure is whether two given vertices lie in the same set or in the same connected component, and also if they lie in different connected components, we need to merge the corresponding two trees. so the perfect choice for data structure for this algorithm is, of course, the disjoint sets data structure. once again, to check whether two given vertices lie in different connected components, we just check whether find of one endpoint is equal to find of the other end point of this edge. if they are different then they lie in different connected component. and when adding an edge to the set x, we need to merge the corresponding two tree and this is done by calling the method union of the corresponding two end points. we will now illustrate this on a toy example. this is a toy example where we have six vertices. let's first call them a, b, c, d, e, f, and let's assume that we have a data structure disjoint set and let me show the contents of this disjoint sets of this data structure. so initially, each vertex lies in a separate set. no we start processing edges in order of non-decreasing weight. so the first lightest edge is ae. we check whether a and e, at this point, lie a different connected components. for this way, we call find of a and find of e. this gives us different ids because they stay in different sets. so we add this edge to our current solution and we also notify our data structure that now a and e actually lie in the same connected component. so now it looks like this. the next lightest edge if the edge cf. again we ask our data structure whether c and f belong to the same set and each replies that they do not belong to the same set because find of c is not equal to find of f, so it is safe to add this edge to our solution. we also notify our data structures and c and f now lie in the same set by calling union of c and f. so now c and f lie in the same set. the next edge is a, e, d and we see that a and d lie in different connected components so we just add this etch to a solution and also notify our data structures that we need to merge sets that contain the vertex a and the vertex d. so now, we have three different disjoint sets in our data structure, which actually corresponds to vertices of our three trees. so the first tree contains vertices aed, the second one contains the vertex b, and the last one contains vertices c and f. now, the next lightest edge is de, it has weight 3. however, we see that d and e belong to the same connected component. this, in turn, means that if we added the edge de to our current solutions, this would produce a cycle. so we just keep the edge de, and we continue to the next lightest edge. the next one is ab, and we see that a and b lie in different connected components, so it is safe to add the edge ab to the current solution. we also need to merge the corresponding two sets. so after this merge, our sets look as follows. now, the lightest edge is the edge be, it is of weight five, however, b and e belong to the same set, so we skip it. and the last edge that we actually add to our solution is the edge bbf. it is of weight 8 and, at this point, we also nudge two sets. and now, all our vertices lie in the same connected component, which means that we constructed an optimal spanning tree, that is a spanning tree of minimum total weight. the pseudocode of the kruskal algorithm looks as follows. first, for each vertex in our graph, we create a separate disjoint set. we do this by calling makeset method of disjoint sets data structure. then we initialize the set of edges x by empty set. the next step is that we sort the edges, all the edges of our graph, by weight. then we process all the edges in order of non-decreasing weight. this is done in this is fold. then for reach such edge, we need to check whether adding in the x safe or not. namely, whether it produces a cycle or not. to do this, we just check whether u and v belong to different connector components. for this, we need to check where to find a few equal to find a v or not. if they're different, then they lie in different connected components. in this case, it is safe to add the edge u, v to the set x and produces in this line and also in this case we need to notify our data structure that all the vertices that before that lied in connected component with u and three, now lie in the same connected components, because we just joined these two trees, and this is done by calling union of of u and tree. finally, in the end, we just return the resulting set x. it remains to estimate the running time of the kruskal's algorithm. we start by sorting the edges, so this requires big o(e log e) time, right? this in turn can be rewritten as big o(e log of v squared), just because a simple graph has at most v squared edges. this, in turn, can be rewritten as just e times 2 log v. again, log of v squared is equal to 2 log v, so we rewrite it as follows, and this is math analysis is just big o of e log v. so this is an upper bound on the running time of the first step. then we need to process all the edges. for this, we make two equals to find that region. why two equals, well, just because we process all the edges and follow each edge, we make two calls to define that region mainly, for one endpoint and for the other endpoint. then we also make at most v minus one calls to the union procedure. why v minus one? well, just because initially we have n connected components. namely, when the set x is empty, each vertex of our graph forms a separate connected components. then each time when we call union, we actually merge two connected components. and in the end of the run of our algorithm, we have just one connected component. so all the vertices lie in the same tree. so initially, we have n connected components and then we have 1 and each time we call union, we reduce the number of connected components by 1 which means that we make exactly v minus 1 calls to union procedure. okay, so we have roughly e calls to find and roughly v calls to union procedure. now recall that if we implement the disjoint set data structure as a forest or as a collection of disjoint trees and we use union by rent. heuristic than the running time that abound on the running time of each iteration is just log v, which gives us that amount e plus v times log v. recall also that in our case, the graph is connected, which mean that e is at least v minus 1, which in turn means that e plus v, is at most 2e. which allows us to rewrite it as follows. so the upper bound on the running time of the second step is actually the same as for the first step. it is o of e log v, which gives us that the upper bound on the running time of the whole algorithm is big o(e log v). now recall that we actually know that if, for our implementation of disjoint sets data structure, we use both union by run heuristic and past compression heuristic then we can state a strong upper bound. that is, instead of using log v here, we actually have log star of v, the iterated log, right? this gives us a stronger upper bound for the second step. unfortunately, this doesn't improve the total running time because still the upper bound for sorting all the edges delineates the upper bound for the second step. however, for some applications, the edges are given to us already in sorted order, which means that, in this case, we do not need to spend e log v time for sorting the edges. so that the total running time in this case becomes equal to e times log* of v. which makes the kruskal algorithm in this case even more efficient. so in this case, the running time is upper bounded by e times log star of v, which is nearly linear. 
in this video, we can see the prim's algorithm. recall that in this algorithm, the set x always forms a tree, and we regularly grow it. at each iteration we attach a new vertex which is not currently in the tree to the current tree by the lightest possible edge. and this is in fact very similar to dijkstra's algorithm. recall that dijkstra's algorithm finds the shortest paths between two nodes by constructing the tree of shortest path from a given source node. we will now illustrate this on a toy example. consider the following toy example that we've already seen. we are now going to grow a tree which will eventually become a minimum spanning tree. at each iteration we're going to select the vertex such that it can be attached to the current tree by the lightest possible edge. to select such a vertex we will use the priority queue data structure. so initially, we know nothing about the graph, so the cost of attaching each vertex to the current tree is just equal to infinity. so initially the priority was a cost of attaching each vertex to the current tree is equal to infinity. that is, we show here the priorities of all other vertices in the priority queue data structures that we're going to use. then we declare some vertex, just randomly picked vertex, as a root of the tree that we are going to grow. assume that we consider this vertex as a root. now we can change its priority to be equal to 0. it costs us nothing to attach this vertex to the current tree. it is the root. now we need to process all address going out of this tree. namely, we change the priority of this vertex to 1 because we see that there is an edge of cost 1 that attaches this vertex to our current tree. and for this vertex we change its priority to 8. okay, now we have five vertices in our priority cube, all of the vertices except for the root. and the cost of one of them is equal to 1 because of priority. the priority of the second one is equal to 8, and the priority of the remaining three vertices are equal to plus infinity. so we select the vertex with the smallest priority, this is 1. so we attach it to our current tree. so our current tree looks as follows. now we need to process all the address that grow out of this tree. namely, when we add a new vertex, we need to check whether new vertices can be attached to this vertex. when we see this, first we see that there is a match of weight 6 so we need to change the priority of this vertex to 6 because it now can be attached to the current tree by the edge of weight 6. also we change the priority of this vertex to 9. okay, now we have four vertices in our priority queue data structure. and we select the vertex with the minimum priority, which is the vertex with priority 6. so now this vertex is also in our tree which is joined by this edge to our current tree. now we need to process all the edges going out of this vertex to the vertices that are not currently in our tree. when processing the edges, we change the priority of this vertex to 4. and we change the priority of this vertex to 5. namely, we just found a cheaper way of connecting this vertex to our current tree. okay, now we have three vertices in our priority queue so we extract the minimum value. this gives us the following vertex, so we include it in our tree, which gives us, and attach it by this node. then we need to process all the edges going out of this vertex to vertices that are currently not in the tree. we see that we need to change the priority of this vertex to 1 because there is a vertex of, because there is a match of weight 1 that connects this vertex to, there's a vertex which is currently in the tree. and we need to change this priority to 2, right? okay, great, now we have two vertices in our priority queue, one of priority 1, and one of priority 2. so we select this vertex. now it is in our tree, and it is connected by this edge, right? when processing all the edges going out of this vertex, we see that there is a match of weight 3. however, it doesn't decrease the course of attaching the remaining vertex to the tree. so we keep the perimeter of this vertex unchanged. so in the last step, we extract the only remaining vertex which is this one. and we see that the cost of attaching this vertex to the current tree is equal to 2, and this is done by this edge. so, now, let's compute the total weight of the resulting tree. it is 2 plus 1 plus 4, which gives us 7. plus 6, which is 13. plus 1, so this gives us the tree of total weight 14. we now provide the full pseudocode of the prim's algorithm. as we've discussed before, it is very similar to dijkstra's algorithm. so, once again the prim's algorithm gradually grows the tree which eventually turns into a minimum spanning tree. we used the priority queue data structure to keep for each vertex the minimum cost of attaching it to the current tree. at each iteration we use priority queue to quickly find the vertex which can be attached to the current vertex by the lightest edge. so specifically we do the following. initially we have for each vertex we do not know the cost of attaching it to the current tree so we just put infinity into the array cost. we'll also going to keep array parent which we'll actually define as a tree, okay. initially we do not, for each vertex of our graph, we do not know its parent. then we select any vertex of our graph, u0. and we declare this vertex as the root of the tree which we are going to grow. we then update cost[u0] to be equal to 0. so, we say that attaching this node, this vertex to the current tree is, the cost of attaching this node is equal to 0 because it is already in our tree. we then create a priority queue. so at this point and we use costs as priorities. at this point all the vertices lie in priority queue. and the priorities are all of them except for u0 are equal to plus infinity, while the priority of u0 is equal to 0, okay? then we do the following. at each iteration, we extract the vertex with the minimal priority out of our priority queue. this means that we're actually attaching this vertex to the current tree. when we do this, we also need to go through all the neighbors of the vertex v and to check whether the edge that leads from v to this neighbor of v, it has actually cost less than the current cost, or than the current priority of this neighbor. specifically we do the following, when we add the vertex v to our current tree, we iterate through all its neighbors. this is not in this for loop. so we iterate through all neighbors z of the vertex v. and we check if z is still in the priority queue, namely, if z is not in our current tree, and if the current cost of z is greater than the weight of the edge from v to z. then, this means that we've just found a cheaper way to attach the vertex z to the current tree, right? and in this case, we just update the value of cost at z. so we assign cost at z to be equal to the weight of the edge v z. and we also say that in this case if we attach the vertex v to the current tree, then this will be done through the edge v, z, which means that v is going to be the parent of z. finally, we notify our priority queues that we need to change the priority of the vertex z to the updated value cost of z. since the prim's algorithm is very similar to dijkstra's algorithm, its running time is also similar. so essentially, what we are doing here is the following. we make v calls to extractmin procedure, right, and also we make at most e calls to changepriority method, right? and the total running time depends on the implementation of the priority queue data structure. if we just use array to store all priorities and at each iteration, this means that change in priority is very cheap, because we just change the corresponding value in our array. however, for finding the minimum value, we need to scan the whole array. in this case, we get v squared upper bound on the running time. why is that? well once again, because extractmin in this case has running time big o of v. so this is v times big o of v, while changepriority has running time big o of 1, right. it is a constant time operation. since the numbers of edges in the graph is at most v squared, this gives us big o of v squared. if on the other hand we implement our priority queue data structure as a binary heap, then both these operations have running time big o of log v. in this case, we have the following running time, v + e times log v. and since, in our case, again the graph is connected, e is at least 3 minus 1. so this allows us rewrite the whole expression as big o of e times log v. so once again the running time depends on implementation. if we use array to implement our priority queue, this gives us v squared. if we use binary heap, this gives us e log v. which one is better depends on whether our graph is dense or not. we are now ready to summarize. in this module we considered two greedy algorithms for the minimum spanning tree problem. and it uses the following idea. at each iteration we add the next lightest edge to our current solution if this edge doesn't produce a cycle. to check whether the current edge produces a cycle or not, we use disjoint sets data structure. namely, for the current edge uv we just check whether u and v belong to the same connected component. if they do, we just skip the current edge. the next algorithm is due to prim. it uses a slightly different but still greedy strategy. namely, we greedily grow a tree. at each iteration we attach a new vertex to the current tree by a lightest possible edge. to find such a vertex quickly we use the priority queue data structure. 
hi, in this module, you will be working on a project based on advanced shortest paths algorithms. the project will be organized as a series of programming assignments similar to those you had in the previous modules. however, in these programming assignments, you will be working with real world data, based on raw networks and social networks. also, you will have more freedom to come up with your own creative ideas about how to improve your algorithms. you even can compete with your fellow learners on the forums on how fast does your algorithm solve a particular problem in the problem assignment. you've learned the classical shortest paths algorithms in two of the previous modules. and you've learned the breadth first search algorithm, dijkstra's algorithm and bellman ford's algorithm. the advanced algorithms can be faster thousands of times, or sometimes of million of times in practice. but those are optimizations which work for particular kinds of graphs. for example, some algorithms are very good for road networks and some others are very good for social networks. all the advanced algorithms that we will tell you in the lectures of these modules are some optimizations based on the classic dijkstra's algorithm. so, it's good to go back and recall how dijkstra's algorithm works, and to make sure that you can solve the problems based on dijkstra's algorithm in the previous modules before starting on the project. 
in this lesson, we will study the idea of bidirectional search. and we will come up with the bidirectional version of the dijkstra's algorithm. but first, let's recall what is the problem that we are solving. in mathematical terms, we are given a graph g, which can be either directed or undirected. but it has nonnegative edge weight. this is important. in all the lectures of this module, we will work with only the graphs with nonnegative edge weights, such as travel times or distances. we're also given a specific source vertex as, and the target vertex t. and we want to find the shortest path between s and t in the graph g, so the first question is why not just use the dijkstra algorithm that we already know? it has pretty good worst case complexity, e plus v times log v, this is pretty good, right? can we even do better in the worse case? well, it turns out that we can. mikkel thorup came up with an algorithm in 1999 that solves this problem for undirected graphs in linear time. linear times in terms of number of edges and vertices of the graph. such algorithm is still not known for directed graphs, but for undirected graphs, we can definitely do better. and for directed graphs, we can do better also. but we just still don't know the linear time algorithm. however, we're not just interested in the worst time complexity. we're interested in the practical performance of our algorithms. and in practice, the classical dijkstra's algorithm, on a graph of usa, with 20 million variances and 50 million edges. will work roughly, for several seconds, on average. several seconds, sounds like a good thing, but when you use, for example, google maps, you don't want to wait for several seconds. you want the results on your screen in a blink of an eye, which in practice means that you want them under 100 milliseconds. and not only you, but millions of people simultaneously want those results in a blink of an eye. so, that's why we need something significantly faster for such implications. now, let's consider how dijkstra's algorithm progresses through the graph. you remember that it processes vertices in order of increasing distance. it starts with the source vertex s, and then it processes it by considering the outgoing edges and finding the vertices on the other end of those edges and relaxing those edges. and so, it goes to these vertices at distance one. and the number near the edges are the weights of the edges. and the numbers inside the nodes are the distances to those nodes. and then it goes to another layer of nodes at distance two, and so, you see that dijkstra's algorithm actually goes in circles around the source vertices, and this is not coincidence. you remember the lemma that when a vertex u is selected via the extractmin procedure in the dijkstra's algorithm. the estimate of distance, to this node. just a few. it's guaranteed to be equal, to the actual distance, from the source vertex to protects you. and also, all the nodes, on the distance smaller than that, have already been processed by that moment. this is guaranteed in the dijkstra's algorithm. so, actually, a circle of processed vertices grows. and this is how it looks if we want to find a shortest path from source vertex s to the target vertex t. we start growing the circle around the node s. until it touches point t, and as soon as it touches point t, we can actually stop the distrust algorithm. this is called earnest stopping. instead of scanning all the vertices in the graph and finding the distance from s to all other vertices in the graph, we only find the distances to the notes we need. to actually find the distance to the node t, and the shortest path to node t. this is an optimization of distress algorithm already, but this is the simplest one. and here's the idea for the bidirectional search. instead of going from s and growing the circle until it touches point t, we want to go simultaneously forward from s, and backward from t until we meet. and as soon as we meet, we can find the shortest path between s and t by combining the half of the path from s to the mid-point, and the half from the meeting point to t. in the algorithm, we won't go literally simultaneously. we will take alternating turns from s and from t. we'll do a typical digstress algorithm from us. we will do one turn of that algorithm, and then we will alternate to t, and we will make one turn of the digstress algorithm backwards from t. to do that, we will need not only to know the outgoing edges from each node, but also all the incoming edges to each node. but other than that, we will do the regular dijkstra search from s forward, and the regular dijkstra search from t, but backwards. and we will alternate the turns of those dijkstra's. so, then we will make one more turn from s, and one more turn backwards from t. and then one more turn from s, and then one more turn backward from t. and now, we see that the vertex in the middle was discovered by both algorithms. the forward one and the backward one. and now, we can reconstruct the shortest path from s to t, and we can fill in the correct distances from s to all the nodes on that path. now, let's see how it works compared to the usual dijkstra algorithm. so, the usual distance algorithm will cover and scan the nodes in this big circle with the center in s and touching point t. what will happen in the directional search is we will be growing two circles of roughly the same radius until they touch. and this area, covered by these two smaller circles, is roughly proportional to the number of vertices scanned during the bidirectional search. so, let's denote the big circle by c1, and the two smaller circles by c2 and c3. and the distance from s to the meeting point v, which is roughly equal to the distance from the meeting point v to the target node, t, we'll denote by r. now, let's estimate the number of covered vertices by the area of the corresponding circle. so, the area of the circle c1 is pi times the radius of c1 squared. the radius of c1 is 2r, so its square is 4r squared. and in the end area of c1 is four pi r squared. and the area covered by the directional search is the sum or areas of c2 and c3. the areas of c2 and c3 are roughly equal. and the area of c2 is pi r squared. so, the total area is two pi r squared. and we see that the total area covered by bi directional search is roughly twice smaller than the area usually covered by the usual distress algorithm. so, let's see what happens actually when the real world networks when we use the simple distress algorithm, and the bi directional search. this is a part of the map of usa with 1.6 million vertices and 3.8 million arcs. and we're finding the best shortest path in terms of the travel time. so here, we try to find the shortest path from the small blue rectangle to the small green rectangle in the right edge of the map, and we color it with green. all the notes scanned by the regular dextrose algorithm while it was looking for the surest path. and on the next picture here, we see the result of the bid directional search. and you see that there is something similar to what we draw on the previous slides. so, there is something like a blue circle around the starting point, and there is something like a green circle around the target vertex. the green circle is cut in half because of the edge of the map, but in general, this is the picture we imagine. and here is another example on the map of the full us. and we see a full circle around the green point, although it is not like a real circle, it's more like a related square. but this is due to the way we humans build the roads, because we build them from north to south. and from east, to west very often, and that's why the circle is more like a square rotated by 45 degrees. and the blue point is surrounded by something more like a half a circle, which is cut by the shore. so, roughly we get a 2x speedup on the road networks, which is good, but not great. we were hoping for something like thousands of times speed up. but this is true for road networks, but what if we look at social networks? and in the next video, we will see that indeed for social networks, the bi directional search idea works exceptionally well. and it will work thousands of times faster that the regular dijkstra's algorithm. 
hi, in this video we will apply the idea of bidirectional search to social networks data. and we will see that it will work exceptionally well, going up to thousands of times faster. but first, let's learn an interesting fact about our world. so in 1929, hungarian mathematician, frigyes karinthy, made a small world conjecture, which you probably heard of already. so the conjecture was that basically when we need to pass a message from any person on earth to any other person on earth, using only personal connections in between, we need at most six handshakes to do that. so that we need only at most five intermediary people to pass a message from anyone to anyone else on earth. this is somewhat surprising, but it turned out to be close to truth in different experiments which people made after that conjecture. and this is called the six handshakes or six degrees of separation idea. now let's look at the most popular social network, facebook, given that six handshakes idea. so suppose an average person on facebook has around 100 friends. then if we consider friends of friends of that person, it will be 100 times 100 roughly, that's 10,000 friends of friends. if we consider friends of friends of friends, that will be 100 times more or 1 million. and if we progress with that, we'll get that at six handshakes, we will have 1 trillion people. well, that's not actually possible, as there are only roughly 7 billion people on earth. so it will be much less than that, but you get the general idea, that the number of friends grows exponentially when we increase the distance. so now we want to find the shortest path from michael to bob via friends connections. so we want to know, actually how many handshakes do we need to send a message from michael to bob. and if michael and bob turned out to be the two farthest people on the facebook social network, then the regular dijkstra's algorithm would have to look through all roughly 2 billion people which are on facebook to find that shortest path. now what will happen if we consider bidirectional search? so we'll only consider friends of friends of friends of michael and friends of friends of friends of bob. and if we know that the six handshakes conjecture is true, then there will be a common person among friends of friends of friends of michael and friends of friends of friends of bob. if we find that person in common, then we'll find the shortest path of friends, at most, six between michael and bob. and to do that we can create a hash table, for example, of all the friends of friends of friends of michael or a sorted list. and then search the friends of friends of friends of bob in that hash table or ordered list, one by one, to find the common person. and how many people will we actually consider in this case? we'll have roughly 1 million friends of friends of friends for both michael and bob, so in total, 2 million people. and this will work in linear time if we use hash tables. so this will be actually 1,000 times faster than using dijkstra's algorithm, at least in the number of friends scanned. so this is going to work really thousands of times faster on social networks. and what we've just seen is a particular case of a more general idea called meet in the middle, and this applies not only to graphs. so, in general if you need to search for some best object of some kind, for example, the best shortest path or a best subset of people to create a team, then instead of searching for all possible objects, all the possible shortest paths or all possible subsets of people, we can somehow divide all the possible objects into halves. for example, when we are searching for shortest paths, we can divide any path on the first half from the source and the second half to the target. and when we are talking, for example, about subsets of people, if we have n people, we can divide any subset of n people into two halves. first half consists of a subset of the first n of our two people, and the second half consists of a subset of the second n of our two people. and then we can search all the variants for the first half separately, all the variants for the second half separately. and then we just need to find the compatible halves. so for example, we again create a hash table or an or at least of the first halves which are possible. and then for each particular second half, we try to find whether there's a compatible first half to it in this hash table. and this is in general how meet in the middle works. and typically, instead of searching through all possible big n objects, this works in time proportional to square root of n, because only square root of n halves of those objects. square root of n of the shortest paths halves and square root of n for first halves of the subset of people. so this idea can be applied in many, many different problems to take a significant speed up. so in conclusion, we understood that the regular dijkstra's algorithm goes in circles of increasing radius around the starting point. and the bidirectional search idea can reduce the search space. we can scan less vertices than if we go from one source and grow the circle. and on the raw networks it works roughly twice faster than the regular dijkstra's algorithm. but if we use this meet in the middle idea on the social networks, or actually on many other problems, then it will roughly work as a square root of n instead of big n if big n is the total number of objects we need to search through. and in social networks, it can be thousands times faster than the regular dijkstra's algorithm. and you will actually see that in one of the problems of the programming assignments of this project. and in the next video, we will actually come up with a bidirectional version of dijkstra's algorithm. 
hi in the previous videos we've introduced the idea of bidirectional search. but now we need to actually specify, how exactly will we expand the dijkstra algorithm to become bidirectional. first let's recall how dijkstra's algorithm works, so it needs to find the shortest path from some vertex s to vertex t. it initializes an auxiliary area dist, with a values of infinity for all the nodes but for the starting node s. and for the starting node s, it initialize the distance estimate to 0, and then it goes in iterations. on each iteration it calls extractmin, to chose the unprocessed vertex u with the smallest distance estimate. and there is a guarantee that, by the time vertex u is extracted that way, it's distance estimate, is equal to the actual distance from the starting vertex as to the vertex u. and then we process u, meaning that we try to relax all the edges outgoing from u. and then we repeat this process, until t is extracted and processed, so then we know that the distance to t is already equal to the distance estimate, in the distance array. now, we need to introduce a notion of reverse graph, for our bidirectional search. so the reversed graph g, r for a graph g is the graph with the same vertices. and reversed edges. you have an example in the bottom of the slide. when we have vertices a, b and c. and the graph g is on the left with edges going from a to b, from a to c and from b to c. and in the graph gr, the vertices are the same, but the edges go the other way around, from b to a, from c to a, and from c to b. so this is very simple, and to build the reversed graph, we can just copy the vertices. and then, go through all the edges of graph g, reverse them, and add them to the new graph gr. so the bidirectional dijkstra's algorithm, we'll start with building the reversed graph. so that it can do the backward search from the target vertex. and then, it will start dijkstra's algorithm in the forward direction from, the source vertex as in the graph g, and also the backwards dijkstra's algorithm from vertex t in the reverse graph gr. so in the reverse graph gr, the regular dijkstra's algorithm will be the same as the backwards dijkstra's algorithm in the initial graph of g. and then will alternate between dijkstra steps in the forward search in g, and in the backward search in gr. and we will stop, as soon as some vertex v is processed, and remember that, process means that it is extracted from the q with variances. both in the forward search and in the backward search. as soon as that happens, we can compute the shortest path between s and t. and i will show you how, in a minute. but first the question, so let's consider this meeting point v. the first vertex that is processed in both the forward search and the backward search. we said previously, that as soon as we meet in the forward and backward search, we can re-construct the shortest path from s to t by taking the first half from s to the meeting point. and then from the meeting point to t. so, is it right, is it always the case that as soon as we meet in some vertex, v, there's the shortest part from s to t that goes through v? so, what do you think? well, it turns out that this is not the case, and here's an example. s and t, we do alternating terms from s and from t backwards, then from s, then from t backwards. and inside the vertices, we have the distances on the left from s and on the right to t. and then we meet, in the middle vertex. and we see that the distance from this vertex 2t is 5, and the distance from s to this vertex is 5, and we can actually reconstruct the path of length 10 from s to t. but what if there was an edge, this green one, i'll find 4. this wouldn't contradict the progression of neither the forward dijkstra search, nor the backward dijkstra search because, this edge has length 4, which is bigger than 3. and so the middle vertex will be considered before, the vertex to the right from it in the forward search. and also, the middle vertex will be considered before the vertex to the left from it in the backwards search. so, we will really meet in the middle vertex before even considering. the edge between the vertex to the left of the middle, and the vertex to the right from the middle. but, here we can see that the shortest path from s to t, doesn't actually go through the middle vertex. it goes around it, and it is a length 8, instead of the path which goes through this middle vertex and has length 10. so it is not always the case, that the shortest path go through the meeting point. so what exactly then do we need to do, to compute the distance? when we found the common meeting point. 
it is not always the case that the shortest path go through the meeting point. so what exactly then do we need to do to compute the distance when we found the common meeting point, and is it actually possible? well it is possible, but it is little bit more complicated. so let's denote by dist of u the distance estimate in the forward dijkstras from source s in g and by dist r of u. the same but the distance estimated in the backward dijkstra from t in the gr. so dist r is an estimate of the distance from u to the target vertex t. so after some node v is processed in both g and gr. i say that there exists some shortest path from s to t which passes though some node u which is processed either is the forward search or backward search or in both. and the distance from the s to t is equal to the distance estimate of u in the forward search, plus the distance estimate of the backwards search from u. what it means is that there exist some vertex u that chooses path from s to t, at least some of this path from s to t goes through this vertex u. and this u is either processed in g, or is processed in gr. and also, the distance estimates for that vertex are already correct for both forward search and backward search. so of course, if it is processed in the forward search, for example, then we know that the distance estimated to this node is already correct. that the distance from source vertex to this node u is equal to it's distance estimate dist of u. but if it's not yet processed in the backwards search, in the reverse search, then this is not guaranteed in the general case for the dist r. for the distance estimate in the backwards search. but we can show that for some node u is actually also a correct estimate which is equal to the distance to the distance from u to the target vertex t. so there exists some nodes u and some shortest path that this node u lies on the shortest path and both distance estimates from source to u and u to target are already correct by the moment our forward search meets our backward search. now, let's prove this dilemma. so, we have these two gray circles, which show us this can't vertices by the forward search from s, and by the backward search from t. and they touch in the vertex v, which is the meeting point. now first, let's prove that if there is some node u which is not processed by forward search and is not processed by the backward search. and there is the shortest path from s to t going through this node u, that then there is also shortest path which goes from s to t through v. to see that, first let's compare the first half of the shortest path from s to t going through u to the shortest path from s to v. we know that v is already processed in the forward search and it means that the distance from s to v is at most the distance from s to u. so here the green part is less than or equal to the red part. also if we consider the backward the extra search, we know that v is processed, but u is still not processed in this backward search. so, the right half, the green right half is also less than or equal to the red half. so, in both halves, the green parts are less than or equal to the red parts. and this means that the path from s to t going through v is shorter or of the same length as the shortest path from s to t going through u. so it means that actually the path from s to t going through v is the shortest path. and so in this case, our theorem is proved. if there is at least some one vertex u which is not processed by forward and not processed by backwards search. and there's a shortest path going through the vertex then there is a shortest path, another one which goes through the vertex v. which is already processed by both forward and backward search. and of course both distance estimates of v are already correct. because it's already processed in both backwards and forward search. now let's consider another case when there are no varices which are unprocessed by both forward and backward searches. but we already meet a point v. so let's consider then any shortest spot from s to t, and let's consider the last vertex u on such shortest spot that is processed by the forward surge. so u is the last vertex which is on the shortest path, which is processed by the forward search. and then the distance estimates of u in the forward search is already correct. this is equal to the distance from s to u. let's consider the next edge of the shortest path. it goes to some vertex w. and this vertex w has to be processed by the backward search. why? because w is not processed by the forward search because u is the last vertex of the shortest path which is processed by the forward surge. and w cannot be unprocessed by both forward and backward search, because it lies on the shortest path. and so w is indeed inside the gray circle around t, it is indeed already processed by the backward search. and so the distance estimate of the backwards search for w is already correct. it is already equal to the distance from w to t. now, what we know is that the distance between s and t is equal to the distance from s to u. which is equal to the distance estimate of u, dist of u plus length of the edge from u to w. plus distance from w to t which is already equal to the distance estimate of the backward search, dist r of w. and now what we want to prove is that this is also equal to the sum of distance estimate of u and the reverse distance estimate of u. so the first sum is the same. and now i claim that the distance estimate in the reverse direction is equal to the sum of the two other summons in the first equation. why is that? well from one point of view we know that the distance estimate of u is always bigger or equal to the real distance from u to t. because the distance estimate in any moment of dijkstra's algorithm is greater or equal to the real distance. from the other hand, we know that the vertex w is already processed in the reversed search. and when it was processed, in particular, the edge from u to w was relaxed in the backwards search. and when it was relaxed it made sure that the distance estimate of u in reverse direction is at most this sum of length of this edge plus the distance estimate for w. so we know that distance estimate of u is at most. a length of edge from u to w plus distance estimate of w. but this sum is the length of the shortest path from u to t going through w. so in fact, we know that distance estimate of u is both greater or equal than the true shortest path from u to t. and also is less than or equal to the shortest path from f from u to t. and this means that it is exactly equal to the shortest path from u to t. and that the distant from s to t is actually equal to the distance estimate a u plus the reverse distance estimated of u. 
so now we've proven that theorem. and we can implement the algorithm as follows. we do the alternating turns of forward search and the backwards search until we meet at some point v. and we remember which vertices are processed in forward search and which vertices are processed in the backward search. then, we take all those vertices which are processed at least in one of them, and for each of those vertices, we minimize the sum of distance estimate of the forward surge plus the distance estimate of the backwards surge. and for the node for which this sum is minimal, we know that there is a shortest path going through this vertex. and its length is equal to the sum of these two distance estimates. and then to reconstruct the path itself, we can reconstruct the shortest path from the source vertex to this middle vertex in the forward search. and separately, we reconstruct the path from this vertex to target vertex t in the backward search. and then just join those two parts into a single shortest path from s to t. i won't show you the psuedocode in the lecture, but you can see it in the slides which are uploaded. so in conclusion, a few remarks. first, the worst-case running time of bidirectional dijkstra is the same asymptotically as for the regular dijkstra algorithm. and we saw that it can differ, for example, twice for route networks, but this is not an asymptotic difference. and speedup in practice depends on the graph, so for the route networks it's roughly 2x speed up. but for social networks, it can be thousands of times faster. from the other hand, the memory consumption is somewhat worse, but again, it is not asymptotically worse. it's just 2x to store two copies of g, the g and the reversed g. and also we'll need to store some auxiliary arrays for forward search and for backwards search, so twice the memory for those also, so roughly 2x memory consumption. and you'll see the actual speedup on social network's graphs in one of the problems of the programming assignment of this project. 
hi, these lesson rules tell you the a-star algorithm. it is another optimization of the dijkstra's algorithm for the case when you have a specific source and specific target, and at any point, we have an estimation of how long does it take to get from this point to the target. this estimation doesn't have to be exactly precise, but if it is good enough it can speed up your search a lot. an example of such estimations is very simple. if you have the coordinates of your target point, and you have a phone with gps controller, and you know the gps coordinates of your current point, you can have an estimation based on the distance by the straight line between those two points and the speed of your car. so,this is the kind of information we can use to greatly improve the performance of the dijkstra's algorithm. so, let's first understand the notion of the directed search, and to accomplish directed search, we will need the notion of potential function. so, we can take any function mapping vertices to real numbers, and it will be called a potential function, and then, the value of this potential function on a vertex is called this vertex' potential. this potential function defines new edge weights. if we had edge weight l of u v on the initial graph, then we can denote by l pi of u v the new edge weight, which is equal to the old edge weight minus the potential of the starting vertex, plus the potential of the ending vertex of that edge. and it turns out that if we replace the old edge weights with these new edge weights, it doesn't change the shortest paths. well, of course it does change the distances between vertices. what i'm saying is only that the paths themselves, which were shortest in the initial graph, they're also the shortest paths between the same pair of vertices in the new graph and vice versa. the shortest paths in the new graph with new edge weights are also the shortest path between the same pair of vertices in the initial graph. and to prove this, i need this lemma. for any potential function mapping vertices to the real numbers, for any two fixed vertices s and t in the graph and any path p between them, the length of this path in the new graph with new edge weights is equal to the length of this path in the initial graph minus the potential of the starting vertex, and plus the potential of the ending vertex. why is this important? well, because if you fix the starting and ending vertex, then for any path it slacks in the initial graph, and it slacks in the new graph, differed by a constant. because this constant only depends on the starting vertex and the ending vertex, and we fixed that. and so, if some path worth the shortest path between s and t, then it will stay the shortest path between s and t in the new graph, because all the paths just add a constant to their length. and so, the paths which were shorter than others, they stay shorter than others. and the same goes vice versa, if the path was shortest in the new graph, it was also shown in the initial graph. and now let's prove this lemma, we consider some particular path p from s to t, and we also denote s as v1 and t as vk and the intermediate vertices v2, v3 and so on, vk minus one are the vertices of this path. now we consider the length of this path in the new graph, with the new edgeways. by definition, it is equal to the sum of the length of the edges of this path, which is l pi(v1,v2) + l pi (v2,v3), and so on. and then we can rewrite each l pi from vi to vi plus 1 by definition, and we know that l pi of v1 to v2 is equal to l of v1 to v2 minus the potential of v1 plus the potential of v2. so, this first line is equal to l pi (v1, v2). and each of the next lines is similar, it corresponds to the length of the edge in the new graph with the new edge weight. so we write each edge on its own line, and now we can notice that there are somes which cancel out. for example pi(v2) here, and pi(v2) here are with different signs, so they cancel out. and the same will happen with pi(v3) and pi(v3) here and the same and the same will happen with pi (vk-1)- pi(vk-1). so, most of the terms, including this pi(vk-2) will cancel out. the only thing that is left is the sum of the length of the initial edges minus the first potential of the starting verdicts minus pi(v1) and plus plus pi(vk), the last verdicts. those are the only two summons of the form pi of something which don't cancel out. and then, we can rewrite this as the first summoned is the length of this path in the initial graph. l(p), minus the potential of the starting vertex, because v1 is the same as s, and plus the potential of the last vertex, and vk is the same as t. so, the length of the path in the new graph is equal to the length of the path in the initial graph, minus the potential of starting vertex, plus the potential of the target vertex. so, we've proved our lemma, so we know that the shortest path in the initial graph and in the new graph with new edge rates, are the same. although their lengths differ by some constant depending on the starting and ending vertex of the path. now, we can formulate an algorithm which can be called dijkstra with potentials. so, we take some potential function pi, and we'll launch the dijkstra algorithm with new edge weights l pi instead of the initial edge weight l. and the result endurance path is also a shortest path in the initial graph. so, the result of this algorithm is the same as the result of the initial dijkstra algorithm. but, depending on the result of the potential function pie maybe it will come from the vertex to the target vertex t faster than with the initial edge weights, because the order of vertices that this algorithm considers can be different. so, this is the idea of optimization, but does any potential function feed us in this case, if we want run dijkstra algorithm using the new edge weights? well, the answer is no, because for any edge (u,v) the new length l pi(u,v) must be negative, so that we can apply dijkstra algorithm for these edge weights. such pie, such potential function for which this is true that all the new edge lines will be non negative, is called feasible. now, the intuition behind the potential functions the following. pi(v) is going to be an estimation of the distance from current the vertex v to the target vertex t, so how far is it from here to t? if we have such estimation, we can often avoid going to wrong direction, and that's why we'll get directed search. and typically in practice, pi(v) a lower bound on the distance from the current point to the target. for example, on a real map a path from the current point to target cannot be shorter than, if you manage to go directly on a straight line from the current point to the target. this is often not possible, because of houses or mountains, but if this is possible, this is the shortest path possible at all. and our real path cannot be shorter than the straight line segment between the current point and the end point. so a* algorithm is basically this djikstra with potentials. what does djikstra with potentials do? it goes in iterations, and at each iteration it picks the vertex v which minimizes the current estimate of distance to v. and what will be equal to? we know that any path in the new graph differs from the same path in the old graph by minus potential of the starting vertex, plus potential of the ending vertex. so, we are going to pick the minimizing dist of v, which is the distance estimate if we ran the regular djikstra algorithm on the same graph, which is basically, the length of the best known path from the source vertex to v minus pi(s) plus pi(v). now notice that pi of s summoned is the same for all v. so, vertex v which minimizes this expression is the same v that minimizes just dist[v] + pi(v). in the some sense, this is basically the most promising vertex, because pi of v is an estimate of the distance from v to the target. and so, we pick vertex v with the minimum current estimate of the distance from source vertex to v plus the distance from v to the target vertex. and this is in term, an estimation of the shortest path from s to t going through v. so this is why the search is directed. we always go, not just to the closest vertex to s, which is still not unprocessed, but we also want it to be close enough to the target. and this is why while we're searching, we avoid going away directly from the target. and then, as soon as we find the vertex t as the vertex minimizing this expression in the dijkstra's algorithm on this new graph, we stop. so, of course, this algorithm could still go through the whole graph because before it gets to the target. but in practice, if you have a good estimate of the distance left from the current point of the target, we will get to the target and extract it much earlier. and on the way there, we avoid a lot of turns in the wrong direction. 
talking about the performance of a*, it can only be seen in practice. it is an empirical algorithm, and it depends on the quality of the potential function. and, in the case when this potential function gives a lower bound on the distance from the current point to the target, which is most often the case, we have the following picture. the worst case is if the potential function is always equal to zero for all the vertices. this is always feasible potential function, but then the algorithm will work the same as the dijkstras algorithm. it will not work better, but it will work the same. and the best case, is if your lower bound on the distance to the target is always actually equal to the distance to the target from the current v. then, the edge weights will be zero, if and only if this edge lies on the shortest path to the target vertex. so, it will be very easy to distinguish the edges through which you should go, and the edges through which you shouldn't go. if the edge has void zero, then it leads you to the target. otherwise, it is not optimal to go through this edge. so you only go through the edges with void zero, and then you will only visit edges which lie on the shortest paths from the source to the target. and so, you will explore the minimum possible number of edges at all before getting to the target, so this is the best case. and in practice, depending on the quality of your lower bound, you'll be somewhere in between. and it can be actually shown that the better your lower bounds, the tighter they are, the better will be the performance of the a* algorithm. and in the next video, we will see how to actually implement a bi-directional version of this a* algorithm to improve it even more. 
hi. in the previous video, you've learned the a* algorithm. which is an optimization of the regular dijkstra shortest path algorithm using additional information. in the previous lesson, you've learned that you can use a bidirectional search to optimize dijkstra's algorithm. now, we're going to join those two ideas to optimize the a* algorithm further. and to get the bidirectional a* algorithm. so bidirectional a* algorithm is basically the same as bidirectional dijkstra. but with the use of potentials. so now we need two potential functions. one potential function pi f(v) estimates the distance from the current point to the target. this is needed for the forward search. but also we need an estimation for the backward search using dijkstra's algorithm from the target. so we need another potential pi r(v). which estimates the distance from the source to the vertex, v. now we have a problem because, under these two potentials, we can have different edge weights. because the new edge weight according to pi f is the initial edge weight minus potential of u plus potential of v. and the new edge weight under the reverse potential is the initial weight minus potential of v plus potential of u. and we need these two variables to be actually equal. because in our bidirectional dijkstra's algorithm. we use the fact that we're going through the same graph. but just in different directions, forward and backward. but for it to work correctly, we need the fact that the edges have the same weight going in forward direction or backward direction. so, how to solve this problem? so we need that l pi f (u, v) = l pi r of (u, v). and if we rewrite by definition, we will see that this is equivalent to having pi f(u) + pi r(u) = pi f(v) + pi r(v). for any pair of vertices connected by an edge. actually, we can make an even stronger claim. that we need a constant value of pi f(u) + pi r(u) for any u. if this is true, then, of course, for any two different vertices u and v. pi f(u) + pi r(u) = pi f(v) + pi r(v). and then all the edge weights will be equal both in forward and backward directions. and the solution for this is to take any feasible potential, pi f, for the forward search. take any feasible potential function for the reverse search. and then create new potential functions. pf(u) = half the difference of the forward and backward potential. and the reverse potential, pr(u), is just =- the potential for the forward search. if we pick such potentials, then the sum of pf(u) and pr(u) will be equal for any u. so this will be constant. and the only thing that we need to prove is that these potentials are both feasible. the forward one is feasible for the forward direction. and the reverse one is feasible for the backwards direction. we will prove this for the forward direction and for the reverse direction. it is just symmetric. so lemma states that if pi f is feasible potential for forward search. and pi r is a feasible potential for the reverse search. then their half difference is a feasible potential for forward search. let's prove this. first, we know that pi f is a feasible potential. and so for any edge, (u, v), length of this edge minus potential of u plus potential of v is greater equal to the 0. also, we know that pi r is a feasible potential for the backwards search. and so the length of this edge minus potential of the ending vertex, now, v. plus the pi r(u) is also greater or equal than 0. notice that, in the first line, we subtract by f(u) and add pi f(v). and in the second line, we subtract, vice versa, pi r(v) and we add pi r(u). this important. now, we can sum up those inequalities. and we'll get the 2l of the edge from u to v -pi f(u)- pi r(u) and + pi f(v)- pi r(v). all this is greater than or equal to 0. now, if we divide these inequality by 2. we'll get that the length of the edge minus half difference of potentials on u. plus half difference of potentials on v is more than or equal to 0. and this means that the potential, which is pf. which is half the difference between pi f and pi r, is a feasible potential, indeed. now that we've proven our lemma, we have a very useful tool. if we have some kind of graphs with additional information for which we have good potential function. you can first take the graph with its regular potential function. then we can reverse the graph. take the potential function for the reverse graph. and then we can combine those two potentials into new potentials. which lead to equal edge weights under both forward and backward search. and it allows us to launch bidirectional a* algorithm as bidirectional dijkstra with potentials algorithm. and it will be the best of both worlds. it will be both bidirectional and directed surge. and so it will work probably faster than both the just bidirectional dijkstra. and adjusted a* based on regular dijkstra's algorithm. the question which is left is, where to get those potentials? and in the next videos, we will discuss some of the examples of where we can get good potentials from. 
hi, in the previous videos we've introduced a new algorithm called a*, and we even created it's bidirectional version. but this all was somewhat abstract because to implement this algorithm in practice, we need some potential function. and we don't know yet where to get that potential function so that it is feasible and good enough. so first we'll prove a lemma. remember we were talking about lower bounds, we were talking that most of the time, potential function of a vertex is a lower bound on the distance between this vertex and the target. so now we'll prove that actually if potential function is feasible, and the potential of the target vertex is less than or equal to 0, then this potential function is indeed a lower bound on the distance between the current vertex and the target vertex for any vertex v. to prove that, we should know that first l pi of (x,y) is non-negative for any x,y if pi is feasible, by definition of feasibility. and so also l pi of any path p, the length of any path in the new graph is also non-negative. then, we take any shortest path between v and t, and call it p. and then we know that zero is less than or equal to the length of this path because the length of each path in the new graph is non-negative. and this length of the path in the new graph is equal to the length of the path in the initial graph minus the potential of the vertex v plus the potential of the target vertex. but now remember that potential of the target vertex is non-positive. so, this is less than or equal to length of the path and initial graph minus potential of vertex v. and we know that this final expression is bigger than or equal to zero. which means, in turn, that the potential of v is less than or equal to the length of the path in the initial graph. which is the shortest path between v and t, so it is equal to the distance from from v to t, so we've proven our lemma. now it's time to introduce at least one specific potential, and we'll start with the one we already discussed several times, we'll just prove that this is really a good physical potential. so we consider a road network on a plane map where any vertex has its coordinates x and y. then the potential given by euclidean distance and the euclidean distance between two points is basically the length of a line segment connecting those two points. it is given by the form of pi of v is equal to the euclidean distance from v to the target. which is in turn equal to the square root of the sum of squares of differences of the coordinates of v and the target. so we stated this potential is feasible and that the potential of the target vertex is zero. and if we prove this lemma this in turn means that given the previous lemma that this potential is always a lower bound on the actual distance in this road network from v to the target. but this is also obvious in self because this potential is equal to the straight line segment length between v and the target. and of course no path in the road network and be shorter then the length of the straight lines segment between current vertex and the target vertex. so our previous lemma makes sense in this case. now lets prove that this is a good potential. so, we know for any edge (u,v) the length of this edge is greater then or equal to the euclidean potential to the euclidean distance between these two nodes. because segment is the shortest path between two points on the plane. now we see that the potential of any node u, is equal by definition to the euclidean distance between this node u and the target node. and then we can use the triangle inequality for the euclidean distance, to state that the euclidean distance between u and t is less than or equal to the distance between u and v, plus the distance between v and t. now we know that the euclidean distance between u and v is less than or equal to the length of the edge between u and v. and the euclidean distance between v and t is exactly equal to the potential of v. so we see that pi(u) is less than or equal to l(u,v) + pi(v). and we can write this as l (u, v)- pi (u) + pi (v) is non-negative. and this is by definition the length of the edge in the new graph with new edge weights. and we see that all the edge weights in the new graph are non-negative. and that's why the potential pi is really feasible. and also it's obvious that the potential of the target vertex is zero because it's basically the distance from the target vertex to itself and it is equal to zero. so to implement an actual specific a* algorithm on a plane map, we need to find the shortest path from source vertex to the target vertex, so for each node v we can compute, either pre compute or compute on the fly, the potential. which is equal to the euclidean distance from v to d, to do that we just need the coordinates of v and coordinates of t and this computation works in constant time. and then we just launch dijkstra's algorithm with potentials pi of v, and this will be our a star. and even with euclidean distance potential, this will work faster than the regular dijkstra's algorithm. and you will see that in one of the problems of this project. 
hi, in this video we'll consider another better way to find a good potential for a star. it will be based on landmarks. so first, a landmark. let's fix some vertex a and we will call this vertex a landmark. then we choose the potential function, pi of v equal to distance from this landmark to the target, minus distance from this landmark to the node v. and by distance, i mean the distance in the graph we're looking at. so we can of course not know this distance beforehand because to find this distance we need to launch some shortest path algorithm. but let's suppose we know the distance from the landmark to all other vertices beforehand. then we can use such potential. and i say that this potential is feasible. and that the value of this potential on the target vertex is zero. so let's prove this. first we write the length of the edge with this potential by definition. and then we write it as length of the initial edge, minus distance from landmark to the target plus distance from landmark to distance u. then plus distance from the landmark to the target. and so this cancels out with the minus distance from the landmark to the target. and minus distance from the landmark to node v. and this is in turn equal to distance from landmark to u plus length of the s from u to v minus distance from the landmark to v. and now this is non-negative due to triangle inequality because distance from a to v cannot be bigger than distance from a to u plus length of the edge connecting u to v. so we have just proven that the edge weight for any u and v and the new graph is non negative. so the potential we provide is indeed feasible. and also the value of this potential on the target vertex is just difference of d from a to t, with d from a to t which is zero. so this is a physical potential with zero value on the target. and so this is actually a lower bound for any node v, on the distance from v to the target. so what can we do with this. we can select several landmarks which are basically any vertices of the graph. so we can select them any way we want.and we can optimize how good do we select those landmarks and hopefully our reason will become better because of that. so we select those landmarks, maybe 10 landmarks for example, and we precompute the distances from those landmarks to all other vertices. so to do that we'll need to launch something like algorithm which will find the distances from one node to all the others. we cannot really save anything on this one, because we need distances to all other vertices. so bidirectional storage or a star won't help us here. we will need to find distances to all other vertices, and we will need to scan the whole graph. but this can be done on the pre-computation state. and then for anyone marked a. we have the following inequalities. the distance from any vertex v to the target is greater than or equal to distance from a to the target minus distance from a to v due to triangle inequality. and also due to another triangle inequality, we have another low end bound, that distance from v to t is greater than or equal to distance from v to the landmark, minus distance from target to the landmark. we can use both if we can precompute the distances not only from landmarks to all the other nodes but also two landmarks from all other nodes. but to do that we just need to reverse the graph and run algorithm from landmarks from each of the landmarks again. we can use the tightest lower bounds out of all those lower bounds for each node v. so we take maximum over all landmarks a that we selected of both differences, and this will be the tightest lower bound we have. and this will still be visible but i'll show, we can show that and you know that the tighter the bounds, the better the algorithm works, the faster your a star search will find the target verdicts. so by choosing the landmarks wisely and maximizing the lower bounds over them, we can speed up our a star search a lot. and now let's think how should we select the landmarks. the intuition is that a good landmark appears either before the starting vertex or after the target vertex and then the lower bound induced by this landmark will actually improve something. and this is if we fix the source vertex v and the target vertex w, then the landmark is the one which is either before before v or after w. but we need to select a set of landmarks that will work best for any query. so for any query from service to target, we need some landmarks which are before s and some landmarks which are after t. and so it makes, for example, to choose the landmarks on the border of our map. for example, here on a map of us, we have landmarks in the rectangles, which are red and yellow. and in this particular search, in the middle, from green area to blue area, we used the four yellow landmarks, meaning that only the lower bounds from those four landmarks actually improved our search in some way. and this just supports the idea that the landmarks should be before the starting point and after the ending point in some sense. and here's another example of a start with landmarks. and again, red and yellow rectangles are landmarks. and again, yellow rectangles are those landmarks which are used in the search. and you see that how much less vertices do we really visit with an a star search, which is by the way also bi-directional a star search, than with the regular bi-directional search. because in the regular bi-directional search we had those a fat circle surrounding each of the starts and targets rectangles. and here we have only small tiny streams of scant vertices colored in green and blue. so this set of only something like 21 marks significantly reduces each sequential search but with the cost of pre-processing. so to do fast we need to pre-process our graph and to compute distances from 20 vertices to all other vertices in the graph. so either we have a separate time to pre-process our graph and save the results, or this is only applicable if we know that we'll get a lot of queries in our problem. so if we get just one query it's faster to find the shortest path from source to target directly without first trying to find a shortest path from 20, versus through all other vertices on the graph. but if you have 1,000 queries to find the shortest path, then already we're competing for 21 marks can be beneficial because it will only take us 20 searches from those landmarks. and then each of the subsequent 1000 searches which were giving us queries will be significantly faster. in the problems of this project, in the programming assignment, you will have both problems where you need just to compute the results for queries. and you will have problems where you will have a separate time to pre-process your graph as you want. and then you will answer the queries. and there we will offer you another algorithm in the next lesson, which works really well for huge rod networks. but you could also try, instead of or in addition to implementing data algorithm, to also implement this a star with landmarks algorithm. and you can try different ideas on landmarks selection. and it is known that really, a very good choice of landmarks can lead to very fast algorithms for subsequent queries on the shortest path. in conclusion, we've introduced a star algorithm, which is basically a direct search. and direct search can scan fewer vertices, a lot fewer, as you just saw on the pictures. a star is a direct search algorithm based on dijkstra and potential functions. and we can also make a star be directional to make it the best of both worlds, both a bidirectional and a with potentials. the euclidean distance is one example of a potential for road network on a plane. and, we can also use landmarks to create a good potential function. and if we create several landmarks, and we choose them wisely, so that there are landmarks before and after source and target verdicts, for any source and target verdicts, then, this can speed up our algorithm even more significantly. 
hi, in this last lesson of the advanced shortest paths module, we will study the contraction hierarchies algorithm. in the previous lessons, we've learned the idea of bidirectional search. and we've studied the bidirectional dijkstra's algorithm, which can be thousands of times faster when applied to social network drafts. however, for road networks, it typically gives just roughly 2x speedup. and in this lecture, we'll start an algorithm that gives much better speedup, thousands of times and more. and the idea for such speedup is the following. long-distance trips go mostly through highways. for example, if you consider this long trip by car from san francisco to new york, and if you look at the navigational directions, then you'll see that basically, you need to first merge into the highway. then you need to merge from that highway into a bigger interstate highway, go through it, then exit to a smaller highway, and then exit to a street, and then get to your address. and this is the general case. when you need to get from some point a to point b, you need to first merge from streets to a highway, then merge from there into an even bigger highway, and repeat that maybe several times, and then start exiting. you exit to a smaller highway, then to an even smaller highway, and then exit to your street, and then go to your address. and so less important roads merge into more important roads. and there is some kind of hierarchy of those roads, which is obvious to humans. but we don't know that hierarchy when we're given the graph. so the idea is to use the structure of the shortest paths in the real road networks. and this will allow us to actually avoid scanning many small roads, many not important vertices in the graph. because if you go from san francisco to new york, then most probably you don't need to go through small streets somewhere in las vegas or chicago. most of the way, you'll be going through a big highway. and you won't go into small streets in the middle of your trip. and there are algorithms which are based on this idea, highway hierarchies and transit node routing by sanders and schultes. and transit node routing is one of the fastest known algorithms for shorter paths in real road networks. they can be millions of times faster than the classical dijkstra algorithm. but those algorithms are pretty complex. and in this lecture, we'll study the contraction hierarchies algorithm, which is at least thousands of times and more faster than dijkstra. but it is pretty simple, and you will be able to implement it as part of the project of this module. and you will be even able to play with it and implement different, your own ideas and heuristics to improve it even further. so the idea of the contraction hierarchies algorithm is to order the nodes by importance. instead of ordering the highways by their importance, we order the nodes. and the idea is that the importance of the nodes on your shortest path should first increase and then decrease back. and for example, these could be points where a highway merges into another highway. and it can't really get away without passing through this node. and then if this is true that the importance first increases and then decreases back, we can use bidirectional search from source to the most important node on our shortest path. this is a forward search. and then from the target to the most important node on our shortest path, this is a backward search. and then when they meet in the common middle point which has the biggest importance on the path, then we have the path between source and the target. so the idea is about which nodes are important, which are not. first, many shortest paths involve important nodes. if you look at this map of the us, you will see that there are many, many big routes coming out from big cities. and that's no coincidence. and as most of the shortest paths, which are long-distance go through these big roads, they also go through these big cities. so big cities are important nodes. and many shortest paths go through those important nodes. another idea is that the important nodes are somewhat spread around. because in each region of the map, we have some big cities and capital of the state or some bigger city in the state, which is important node, such that many shortest paths again go through it. it can't just have one single important node, and everything else is unimportant. you always have them spread out through all the map. and another thing is that important nodes are sometimes completely unavoidable. for example, for many, many different source points in san francisco on the left shore and for many target points in oakland on the right shore, there is no other way to get from the source to this target via some shortest path and bypass this treasure island in between. and so this treasure island node is very important because you cannot just avoid it with the shortest path. and the contraction hierarchies algorithm uses this scheme of shortest paths with preprocessing. when you don't just get a graph, and then instantly start to answer queries, how long does it take to get from a to b? now instead you first get the graph and you get some time to preprocess it. it can be a long process. it can take a few hours or even days. but then when you're ready, and you've saved the results of your preprocessing, you can answer the queries for distance and shortest paths much faster. and you work with this preprocessed graph to do that. and this is a very practical case. for example in google maps or in yanix maps, that's what people do. they first preprocess the graph. and then they push the preprocessed graph and the algorithm into production. and then they can answer your queries millions of times faster without you noticing it. and it doesn't matter that we have to spend a few hours or a few days before pushing the graph into production because it happens behind the scenes. and so the general schema is that you prerpocess the graph then implement a special algorithm to find distance and shortest path in this preprocessed graph. and then you reconstruct the shortest path in the initial graph and show it to the user somehow. in the next videos, we will first study how the preprocessing works for the contraction hierarchies. then we'll study how to implement the distance and shortest path queries. we'll prove the correctness of the overall algorithm. and we'll point out some very important optimizations of this algorithm. and in the end, we will understand how we can actually measure the importance of the nodes that this algorithm uses and how to implement this importance in your algorithm. 
hi. in this video, we will study the preprocessing phase for the contraction hierarchies algorithm. so the general idea is that we will be eliminating nodes of the initial graph, one by one, in some order. when we eliminate the node, some of the shortest paths that existed in the initial graph can be gone because they were passing through this node. and in this case, we will need to add some shortcuts so that we preserve the distances. although the distances between any two nodes that are still in the graph are the same the distances between these nodes, in the initial graph. so we'll add some shortcuts some new edges to the graph. and in the end, we will get the augmented graph which has the same set of vertices as the initial graph. it also has all the edges of the initial graph. but apart from that it has all the added shortcuts as edges. this is the augmented graph. basically the graph with augmented set of edges, and also we'll output the order of the nodes that we used in this preprocessing. this is the general scheme. now how does the node contraction work? so let's look at this very simple line graph, all nodes in one chain. the nodes are numbered. and they are numbered already in the order in which we are going to contract the nodes in this example. and let's look what happens when we contract the nodes. so first, we contract node number one, it goes down, means that we contracted it. and actually what's on the top line is the graph, which is contracted. and now it is in green because it is already added into the augmented graph. and you see that there was a path from node 6 to node 4, which went through node 1. when we contracted it, it's no longer in the graph. so there is no path from 6 to 4 in this graph. and then we need to reconstruct that path. and so we add a new edge between 6 and 4, which is colored in blue in the picture. and the length of this edge should be equal to the length of the path between 6 and 4, which went through 1, because that was the only path. so it was the shortest path between 6 and 4. so we add a new edge of length 5 because the length of the shortest path between 6 and 4 was 5, and now after adding this edge it is again 5. and all the shortest path, all the distances between the nodes which are still in the graph on the top they are preserved. now what happens when we contract node 2? well nothing really happens because no shortest paths between the nodes which are left in the graph went through the node 2. so it's an interesting case. now let's contract node 3. and again, there was the shortest path between 4 and 5 going through node 3, and it had length through each. so we add a new blue edge of length 3 directly between nodes 4 and 5. when we can track the node 4, it is the most interesting node because it already has two blue nodes. but nevertheless, when we contract it we remove the path between node 6 and 5, which runs through 4 through two blue edges. and we need to reconstruct the path. so we add a new edge directly between node 6 and 5 of length 8, because the path was of length 8. and notice that in the initial graph there was a path from 6 to 5, which went 6 to 1 to 4 to 3 to 5 and the total length was 3+2+1+2, which is 8. and now we just added one edge of length 8 between 6 and 5. and in the end we can track node 5. nothing changes because no shortest paths between other vertices which are still in the graph which is basically only node 6 went through node 5. and we don't need to contract node 6 because it's the last node in the graph. and you see that the nodes in the new picture are different heights. and this just symbolizes that the higher is the node, the later it was contracted. and the higher the node, the more important it is. so we first contract or remove the least important nodes. and the nodes which are left in the end are the most important nodes. and we draw down from bottom to top in the order of increasing importance. now let's see what happens in general when we contract node v. so, there can be some edges incoming into v from some nodes u1, u2 and maybe some others. there can be some edges outgoing from v. for example, two nodes w and w2 and maybe some others. also there can be some undirected edges connect to v and then there will be both an incoming edge from the other ends to v and outgoing edge from v to the other end of this edges. and also, there could be directed edges from the same nodes to v and from v to this node. so some nodes can be both in the bottom part of the example and in the top bars of the example. but that doesn't really matter much, and we will discuss everything on this particular example. so what happens if v is contracted? then v will be deleted, and the edges from u1 to v and from v to w1 will also be deleted. and let's suppose that this path from u1 to w1 of length 2, 1+1, was shortest in the initial graph. then we will need to add a new edge directly from u1 to w1 so that we preserve the distances. so what would be the length of this edge? and the length of this edge will be of course 2 because the length of the new edge must be equal to the length of the corresponding incoming edge plus the length of the corresponding outgoing edge. and for every pair of edges, u1 to v, v to w1. or u2 to v and v to w2, or u1 to v and v to w2. for each such pair, we need to add a new edge with a shortcut, but we don't have to add shortcuts always. so let's consider this another example, with nodes u2 and w2. there is this path from u2 to w2, going through v, of length 5, 3+2. however, it could happen so that there is another path from u2 to w2, which doesn't go through v, and has a length of just 3. then, it is shorter than the path going through v. and then, if we delete node v, the shortest path from u2 to w2 doesn't change, because it doesn't go through v. and so in this case we don't have to add a shortcut. and in practice, we don't want to add a shortcut when we can avoid that. because when we add a shortcut, we increase the number of edges in the preprocessed augment draft. and then our queries will work slow. less edges, faster queries. so we really want to avoid adding shortcuts, and we need to find such paths, and such paths which don't go through the contracted node, and are shorter than the corresponding path going through the contracted node. they are called witness paths, because they are witnesses that we actually don't need to add a shortcut. and in the next video, we'll discuss how to efficiently search for those witness paths so that we don't have to add shortcuts in the preprocessing phase. 
hi, in this video, we'll learn how to find those witness paths that we need from the previous video. so just to remind you, when we contract node v, then for any pair of incoming edge from some node u to node v and outgoing edge from v to some node w, we want to check if there is some witness path from u to w which doesn't go through v, has length at most equal to the sum of those length of two edges. if there is such path, then there is no need to add a shortcut from u to w after contracting v because the shortest path between u and w will stay the same even if we don't add this shortcut. and search for such witness path is called a witness search. so first a few definitions, let's call a node u a predecessor of v if there is an edge from u to v, either directed or undirected. and also let's call w a successor of v if there is an edge from v to w again, either directed from v to w or undirected. now, witness search is actually very straightforward. so we need to check for each predecessor of v, and for each successor of v, whether there is a witness path between this predecessor and the successor that doesn't go through v and that has a length which is smaller or equal to the length of the incoming edge plus the length of the outgoing edge. so for example, in this picture, for u1 and w1, we need to find some path. and it is drawn to the left, which doesn't go through v and has length at most 1 plus 1 which is 2. and also for pair u1 w2, we need to find also a path, which is drawn to the right under v, which doesn't go through v again and which has length equal to or smaller than 3, 1 plus 2. to do that, for each predecessor ui of v, we just run dijkstra's algorithm, regular dijkstra's algorithm from that ui. and in that dijkstra's algorithm we'll always ignore node v. so we don't add it to the queue. we don't extract it from the queue because we want only the paths that don't go through node v. so this is essential for good query performance. because again if we don't do this witness search, we don't find witness paths. and we will have to add shortcuts for every pair of neighbors of v, for every predecessor and every successor. and this will add a lot of new nodes in our augmented graph. and in a graph with a lot of edges, it's very hard to find shortest paths. it will take a long time. so it is essential for us to find as many witness paths as we can and don't add the corresponding shortcuts. but we also want the preprocessing to work really fast. and so we want to optimize this witness search. so two ideas for optimization, first is we can stop dijkstra's algorithm when the distance from the source becomes too big. basically, if we search starting from some predecessor and the distance from this predecessor to the node currently extracted is very big, that cannot be a sure witness path because it cannot be already shorter than the sum of the incoming and outgoing edge. and this is one idea. we'll discuss it in detail in a minute. and also another idea, we can limit the number of hops. so we can only allow the dijkstra's algorithm starting from some predecessor to go through at most, let's say five edges. if it cannot find a path to some successor, we just say that there is no such path, and we add a shortcut. of course, we won't find all the witness paths this way. but we will do it fast. and this is a trade off between the number of edges, shortcuts added in the augmented graph, and the speed of the preprocessing. so first with the stopping dijkstra, so in this example if we found, in our dijkstra running from the node ui some node x, to which the shortest path is of length 4, then it is already useless because this length 4 is more than the maximum sum of incoming edge into v plus our going as from v. so it's 4 is more 1 plus 2 which is 3. and so there is no way there can be some witness path going through x to either w1 or w2 because it will be already longer than 4. and so it will be longer than the path from ui to the corresponding w through v. so in general, if when we extract node x, extract mean from the queue, the distance from the source to this x is already bigger than the sum of the maximum incoming edge and the maximum outgoing edge. there is no witness path going through x. and so there is no point to exploring further from x. we can just stop the algorithm here because all the nodes which will extracted further in the dijkstra's algorithm will be even further than x by the property of the general classic dijkstra's algorithm. and so basically, what it means is that we can limit the distance by the sum of two edges, the maximum sum of two edges. and as soon as dijkstra finds node which is farther from the source, we just stop the dijkstra's algorithm. there is another small improvement. but it can make it significant in practice. consider any predecessor w prime of any successor w of v. so for example, there can be an edge from w prime to w2 of length 1. and then let's see. for example, we found some path ui as the source of djikstra's algorithm to w prime. and that path has length just 1. then we already know that there is a witness path between the ui and w2 because the path from ui to w2 through v has length 2, 3. and now the path from ui to w2 through w prime has length at most 2 because there is a path from ui to w prime of length 1. we go through it. and then go through edge of length 1. and we get a path of length just 2 which is shorter than the path to v. so this is indeed a witness path. so using this idea, we can just go up to nodes which are previous to us, which are predecessors of our successors of v and then stop there. so basically what it means is that we limit our distance by the difference of the sum of the two longest incoming and outgoing edges minus the length of the last edge on the path, from some predecessor to some successor of v. and then there are two criteria during the dijkstra's algorithm. if we found some w prime, which is connected directly to some successor of v, and the distance to w prime plus the length of this edge is smaller than the corresponding path from the source to the corresponding successor, then we've just found the witness path. and if the distance to some node is already bigger than this maximum of maximums in the bottom of the slide, we just stop the dijkstra's algorithm because we cannot already find a witness path, because all the nodes are already too far. another idea for optimizing the witness search is to limit the number of hops in dijkstra. and by hop, i mean just the jump through an edge. so we consider basically only shortest paths from the source from some predecessor of v, which contain at most k edges. as soon as our dijkstra's algorithm considers a node, which is at least k plus 1 edges from the source, we don't actually process this node. we ignore it. and if a witness path is not found using this dijkstra's algorithm, we just consider it as if there is no witness path. and we add a shortcut in the preprocessing phase. and so this is a tradeoff between preprocessing time and the size of the augmented graph. if k is small, then we won't find many witness paths. and then we will add many shortcuts. but our preprocessing will be faster. in practice, you can change k gradually from the start to the end. and typically, k is smaller in the start, for now k = 1. and then in the a, in the end, k = 5. and this can be based on the degree of the vertices that you're contracting. so these are the ideas for speeding up the witness search. and this is all for preprocessing. so in the next video, we will study how to actually do the queries in the preprocess graph, after we preprocess it by contracting the nodes and adding the shortcuts. 
hi, in this video we'll study how to actually find the distance between nodes in the augmented graph after pre-processing it using contraction of the nodes. so the main idea is that we will use bidirectional dijkstra. and this bidirectional dijkstra will only use some of the edges of the augmented graph. so you remember that when we contracted the nodes, we placed them from bottom to top in the order of increasing importance. and also in the order of contraction, so the later the node was contracted, the more important is this node. and so we drew this arrow with node order from bottom to top. and we will consider only the upward edges. so for example if we need to find the shortest path between node two and node three in this graph, the path will look like this. it will go first, by edges, which go up to some node with the biggest importance. for example, in this case, it will be node 6. and then we'll go by edges down from that node 6, into node 3. and to find such a path, we will use bidirectional dijkstra, which we'll start both from 2 and from 3 and from 2 it'll go by forward edges which go up. and from 3 it'll go by backward edges which also go up. and then they will meet at some node and we'll find some path which goes first up and then goes down. this is the general idea but it is someone difference still from the classical bidirectional dijkstra variant. so in the bidirectional dijkstra as soon as we find some node that is process both by the forward search and the backward search, we stop and compute the shortest distance. in this case, we don't stop when some node was processed by both searches. we stop only when the extracted node is already farther than our current estimate of the distance to the target. this is the pseudo code code for the function computedistance which takes as inputs service node s and target node t and returns the distance between them. of course it also takes as input for example the graph and the reverse graph for the bidirectional search but i just omitted those details. note however that you only need to store in the graph the edges which go upwards. so we have a node a and a node b and an edge from a to b. if a was contracted earlier than b, then you need to store this edge. but if b was contracted earlier than a, then this edge goes downwards and so you don't need to store this edge. you can just delete it because it won't be used at all in the queries. and so, you don't need to store it in the pre-process, in the augmented graph at all. we start with initializing variable estimate which will store our current estimate of the distance between the source and the target. and we initialize it with plus infinity because currently, we don't know of any path between source and target. then we do the standard initialization of the bidirectional dijkstra data. we fill the arrays with distance estimates for the forward search and the backward search with plus infinities. and we'll also set the distance in the forward search from the source to zero. and the distance in the backwards search from the target also to zero. and also have the lists of the processed nodes for the forward and the backward search. then we'll have the main loop and this loop goes while we still have some nodes to process, either in the forward dijkstra or in the backward dijkstra. which means that basically in our priority queue that we use for the djikstra algorithm, there still are some nodes. either in the queue for the forward djikstra search or in the queue for the backward djikstra search. while there are still such nodes, we try to extract node in the forward search if there is still something in the queue for the forward search. and this node will all ready have the distance estimate which is equal to the distance from source s to this node v. so if the distance from s to v is already bigger than the estimate of the distance from s to t, we don't need to process node v. and we don't need to put it into queue and to process the nodes which are the successors of v. so we only do that if the distance estimate of v is smaller or equal to the current estimate. then we process, basically we do the same, we do in the regular dijkstra step. and then, we need to also update our estimate. so if v is processed in the reverse search which act that by if v in proc r, then it is processed in both searches. because v has just been processed in the forward search and it is also in the list of processed nodes in the backward search. so it is processed by both forward and backward searches. this means that its distance estimates both in the forward and the backward search are already equal to the corresponding distances from s to v and from v to t. and so we know the length of the shortest path going from s to t through v. this path has length equal to dist of v plus dist r of v. if this length is smaller than our current estimate of the distance between s and t we need to update it. and that's what we do in this if statement. and then we try to do the backward dijkstra search. and it is completely symmetric to what we have here for the forward dijkstra search, so i don't write the code for that. and this while loop goes and goes until all the nodes and all the queues for forward and backward search are processed. and then in the end we just return our current estimate of the distance between source and the target. now it seems that we already know everything. we know how to pre-process the graph using the contraction of the node. we also know how to implement queries using bidirectional dijkstra, so are we done? can we actually go and implement this algorithm? well of course we're not done because for example we don't know yet whether this algorithm works correctly or not. so commute distance procedure returns some estimate of the distance between source and the target but is it always correct estimate? is it always equal to the distance between s and t in the initial graph or not? we don't know that. also, this can be distance procedure only returns us the estimate of the distance between source and the target but it doesn't return the actual shortest path between source and the target in the initial graph. but that's more of a technical moment. using the standard bidirectional dijkstra algorithm we can reconstruct the path in the augmented graph between source and target using the special ra with the previous node in the shortest path from source to this node. and the same for the backward search. and then some of the edges could be shortcuts, instead of the edges of the initial graph. but then while we do the pre-processing we should also store in the shortcut not just the edge but also which two edges were there before we ended the shortcut. now we can recursively take the shortcut and replace it with the two edges that were before the shortcut. and then if some of those two edges are also shortcuts, we recursively get the edges that were there before these shortcuts were added, and so on. and then, we will eventually reconstruct the path in the initial graph, which only contains the edges of the initial graph. and in the next video we will actually prove that this algorithm works correctly, that the estimate from the compute distance procedure will always return the correct distance between source and the target. 
hi, in this video, i'll will prove that the algorithm suggested in the previous video for finding the distance between two nodes in the augmented graph always returns correct result. first, let's formally define the augmented graph. so augmented graph g+, has the same set of nodes, v, and has the augmented set of edges, e+, which contains all the initial edges e of the graph g along with all the shortcuts added at the pre-processing stage. and the lemma states that the distance, d+(s,t) between any two nodes s and t in the augmented graph. is equal to the distance between the same nodes in the initial graph. and note that this makes sense, because actually the only thing we are doing is we are adding shortcuts. and those shortcuts are not shorter then the path which was initially between those two nodes. just has the same length as the path through the contracted node v in the initial graph. so, always when we add the shortcut, we don't do anything but the path. but, more formally, first thing to know is that edges are only added to g. so, if there was some shortest path between s and t and g, it will also be present in g+. and so distance in the augmented graph is, at most, distance in the initial graph. but also, if we add some shortcut from u to w, then at that moment there was a path from u to w through v of length of the h from u to v plus length of the h from v to w. but this is equal actually to the length of the shortcut edit. so, there was a path before adding this shortcut from u to w through v. so, if there is some shortest path from s to t, going through this edge (u,w) in the augmented graph. then in the initial graph there was a path going from s to u, then from u to v, from v to w and then from w to t, which has the same length. so there is a path of the same length in the initial graph. and so the distance in the augmented graph is the same as the distance in the initial graph. now [cough] let's also define the rank of vertex, which is the position of this vertex in the node order returned by the pre-processing stage. so, basically, it's the importance of the node. so, if our node was constructed first, it has rank one. if it was constructed last, it has rank n, where n is the number of nodes. and the higher the rank, the more important is the node, the later it was contracted. also, lets denote increasing and decreasing path in the augmented graph. so a path is increasing ff the ranks of the nodes of this path in order increase. and also it is decreasing, again, if the ranks of the nodes in this path decrease. and what we want to say is this lemma, that for any source and target there is a shortest path in the augmented graph. which is first increasing and then decreasing. so, it contains two halfs and the first half is a path which is increasing, the second half is a path which is decreasing and those two halfs meet in some node v. this is a justification of our bidirectional search, which goes only through edges, which are going upwards. so the fourth search goes through edges upwards to v, and the backwards search goes from target through backward edges, which go upwards, also to v. and then they meet, and if we prove that there is always the shortest path of this form, then our algorithm from the previous video works correctly. now let's prove it. but first, let's look at this proof of the idea. so again, we draw here nodes such that nodes in the bottom are less important than the nodes in the top. and want to prove that for any source as in any target t, there is a path of this form when it first goes up and then goes down. but what if that's not the case, what if there is some shortest path but it's not always going first up then down. then there is some node, uk, for which it goes first down, then again up. and so both its neighbors are higher. then lets consider the moment when this node uk was contracted. both its neighbors are higher so they were contracted later than uk. and so when uk was contracted, both those notes were on the graph. and those are neighbors of uk, so when it was contracted, two cases. first, there was a new shortcut edit in blue, then what we can do is we can remove uk from the shortest path and then just use the blue edge instead of it. the length will stay the same because the length of the shortcut is equal to the length from uk-1 to uk and then from uk to uk+1. and there is no more problem with this node uk. which is smaller than both neighbors. the second case is that no shortcut was added but then a witness path was found and this is drawn in green. but then all the nodes in the witness path were present when node uk was contracted. so they were contracted later. so they're actually higher than node uk. so when we remove uk again we can replace it with a witness path and we will remove our problem with uk. this doesn't prove yet the general case because there still can be other places where this going first up then down is violated. so now let's prove formally. so assume for the sake of contradiction that no such path from source to target which as far goes up and then goes down exists. then for any shortest path from s to t, there is some node ui of this path such that it's rank is smaller than the ranks of both neighbors, as in the example. and recall all such nodes, local minimums. then, for any shortest path between s and t, denote by m(p), the minimum rank of a local minimum of this path. so there can be several local minimum on this path. but we denote by m(p) the minimum rank of all of those. now let's consider the shortest path p* with the maximum m(p) among all the shortest paths between s and t. and then consider the local minimum (uk) which has the rank equal to this m(p). and then there are two cases, when we're contracting (uk) either there was a shortcut added between (uk-1) and (uk+1). then there is the shortest path with the shortcut instead of the current path from uk-1 to uk to uk+1. and this shortest path, p' doesn't contain uk. and uk has the minimum rank of all the local minima in the path p*. so m(p') is stricted bigger than m(p*). and this is a contradiction with the choice of p*, which has the maximum m (p) of all the shortest paths. in other case, when we were contracting uk, there was a witness path from uk-1 to uk+1. and it was comprised of nodes which have high rank, higher than rank of uk because they were contracted after uk. and so there is another shortest path p'' with this witness path instead of the path uk-1, uk, uk+1. and again m(p'') is strictly bigger than m(p*). which is again a contradiction with the choice of p*. so now we have pre-processing via node contraction. we have an algorithm for querying via bidirectional dijkstra in the augmented graph. and we have proven that this algorithm always works correctly. so, are we done now? well, it turns out we are not done yet, because the question is how to select the node order. our algorithm, so far, doesn't specify how does it order the nodes for contraction. and it turns out that the ordering of the nodes influences the pre-processing time and the query time very significantly. and so in the next video, we'll finally discuss how to order nodes in such a way that both pre-processing and query work fast. 
hi, in this video you will learn how to construct a good node ordering for the contraction hierarchies algorithm. so far if you think about it, our algorithm works correctly for any node ordering. if you just choose any order and pass it to the processing procedure, it will contract the nodes in this particular order and return an augmented graph. if you then run queries on this augmented graph using the bidirectional dijkstra approach from the previous videos, everything will work correctly because the correctness proof doesn't depend on any particular order. however, this order will influence the order of operations in the bidirectional dijkstra algorithm for the queries. and actually, it will heavily influence the time required for queries and for preprocessing. so we want to choose a good node ordering, so that our preprocessing and our queries are really fast. first thing we want from the order is we want to minimize the number of added shortcuts. because each time you add a shortcut, you add an edge to the augmented graph in the end. and the more edges, the slower your dijkstra algorithm will work. so we want less shortcuts. second, you want to spread the important nodes across the map. because from the common sense, it follows that there are important big cities in every sufficiently large region of the country, of the map. and then in each state, if you divide it into sub-regions, there are still relatively big cities which are important nodes and so on. and this should work well in practice. and third, you also want to minimize somehow the number of edges in the shortest paths in general in the augmented graph. so basically, you want to make your graph look less like a road network graph and more like a social network graph, where bidirectional dijkstra algorithm is so much faster than the regular dijkstra's algorithm. because any two nodes in the social network are at most six edges away. and then bidirectional dijkstra finds, with forward surge and backward surge, in just three steps they meet each other. you want the same in the augmented graph and you can somehow become close to it by adding shortcuts in the right places. so in general, you want these three things. now we'll discuss how to do that. so we want to order nodes by some importance, so we need to introduce a measure of importance for that. and then, we will at each step contract the least important node. however, when we contract a node, the properties of the nodes which are neighbors of this node can change. and then importance can also change for those nodes after that. so in general, the algorithm will work as following. we'll keep all the nodes in a priority queue ordered by importance, so that we can extract the least important nodes very fast. but of course, priority will be something like minus importance so that the hat of the priority queue contains the least important node, not the most important one. then, on each iteration we will extract the least important node. and we want to contract this node, but first we need to recompute its importance. because it could have changed because of the contractions of the other nodes. then, we need to compare its new updated importance with the importances of all the other nodes. and to do that, we will take the hat of the priority queue. because we've already extracted these nodes from the priority queue, it's no longer there. so we will compare the importance of this node with the smallest importance of all the nodes which are still in the queue. if the importance of the extracted node is still minimal, then we can safely contract it, it's the least important node. otherwise, we will need to put it back into the priority queue so that we can extract a node with even smaller priority from it. of course this is only approximate, because at some point we could extract the node's recomputed importance and then decide that it has really the least importance of all the nodes in the queue. however, there is some other node which is still in the queue which has a smaller importance, but we haven't yet recomputed it and so we don't know about that. and that's okay, we will try just to order the nodes approximately by this importance. also, it can seem that this process can go on and on indefinitely. but we'll show that it will actually stop after a finite number of iterations, always. why is that? well, if we dont contract the node, we update its importance and it actually changes. and after that, its importance is corresponding to the current set of nodes which are still not contracted. and after at most number of nodes attempts, all the nodes will have updated importance. and then, the node with the minimum updated importance will be contracted after that, because it will be extracted and it will have the smallest importance at that point. and then when we recompute its importance, it will turn out to be the same because it already has updated importance. and then we will just contract this node. so eventually this kind of iteration always contracts a node. and in practice, it usually takes one or two iterations usually, to actually contract something. so this works typically fast. now, we'll talk about the importance criteria. so first is edge difference. it is related to the number of shortcuts added and minimizing the number of edges in the augmented graph. second one down is number of contracted neighbors. and this is related to spreading the nodes across the map. so if there are already many neighbors of this node which already contracted, then we will probably contract some node in some other place. because we want to spread important nodes and also spread unimportant nodes. we don't want everything to be clustered in one region. also, shortcut cover is connected with how important that node is, how unavoidable it is. do you remember the picture with an island between san francisco and oakland? the treasure island which was unavoidable if you go from san francisco to oakland, if you want to go with the shortest path? so shortcut cover will somehow approximate how unavoidable a node is. and the last one, the node level, has something to do with the number of edges in the shortest paths in the augmented graph. so now we'll go through all of these in detail. so edge difference. you basically want to minimize the number of edges in the augmented graph. so when we try to contract the node, v, we can actually compute how many shortcuts would be added if we actually contract this node. we can also always compute the number of incoming edges, and number of outgoing edges of any node. and then we can compute the edge difference. the number of shortcuts added minus number of edges deleted, which is the degree of this node. and this is called edge difference. and the number of edges increases by edge difference after contracting v. so this influences both the number of edges in the final graph and also the number of edges in the graph being preprocessed. which is important because it influences the preprocessing time. so what we want is to contract nodes with small edge differences first. contracted neighbors, i already talked about this. we want to contract a node with small number of already contracted neighbors, so that the nodes which we contract don't cluster in one place, but they're actually spread out in all the regions of the graph. shortcut covers, very important. basically it says how many nodes are there, such that we cannot avoid visiting our node if we want to go to those nodes. more formally, if we have some node, v, we can do the number of neighbors, w of v, such that we have to create a shortcut, either to w or from w in the case we need to contract v. and this basically means that when we contract v, we'll lose some shortest path to w. and that's why we need to create an additional shortcut. and so v is important for w, and if v is important for many, its neighbors, then it's important. and so, we need to first contract nodes with small importance, with small shortcut cover. node level is an upper bound on the number of edges in the shortest path from any node s to node v in the augmented graph. it can be shown formally that this is an upper bound, but i'll tell you the intuition. so we initialize the level with 0. and then when we contract some node v, we update the level of its neighbors. so for a neighbor u of v, either we could get to u somehow without going through v, and then contracting v doesn't influence its level. or we could go to u through node v. but then to go to node v, we needed at most, level of v edges, and then one more to get from v to u. so that's why l(u) needs to be maximized by l(v) + 1. so in the end, l(v) is an upper bound on the number of edges on the shortest path to v. and so we want to contract nodes with small level, because first, we need to contract non-important nodes. in the end we proposed the following importance measure. we just sum up all those four values. note that this is a heuristic value, and you can actually play with weights of those four quantities in the importance and see how preprocessing time and query time change. but we think that just summing them up will already work really well. however, each of the four quantities is necessary for fast preprocessing and queries. if you avoid implementing at least one of them, probably preprocessing or query time will be too long. but then you can further optimize with the coefficients. for example, two times added distance plus three times contracted neighbors, and so on. so it also means that you need to find a way to compute all of these quantities efficiently at any stage of the preprocessing. both in the beginning, and also when you need to contract a node but you've already contracted some other nodes. this is not very hard and we'll leave this as an exercise to you. so, we did a lot of work. we've come up with a preprocessing stage. we need a particular node ordering with that and we've come up with a complex importance measure for nodes for that. and also we need to do bidirectional dijkstra, but not classical bidirectional dijkstra. we need to do some tweaks in the query stage. so, is it all worth it? it turns out that yes, it is well worth it. if we take for example, a graph of europe with 18 million nodes, and launch the classical dijkstra's algorithm on it. and we generate random pairs of vertices in different places of europe and then compute the shorter distances. then on average, it will work for 4.3 seconds. however, if we launch contraction hierarchies on the same graph with the same pairs of vertices and we tweak the heuristics inside the contraction hierarchy as well. it will work on average just for 18 milliseconds, which is almost 25000 times faster. so it is very well worth it. for example, for a service like google maps or yandex maps, working 25000 times faster means that you actually can provide your service to millions of users and do it blazingly fast. and the speed-up will only increase if you increase the size of a graph. for example, if you go from the graph of europe to the graph of the whole world, the speed-up will only increase. so now, it seems that we know everything. we know how to preprocess by contracted nodes, ordered approximately by their importance and we have some importance measure for that. we know how to query using bidirectional dijkstra algorithm. and our importance function is heuristic, but works really well on road network graphs. and this algorithm works 1000s of times faster than dijkstra. if you tweak it really well, it can work even tens of thousands times faster. but at least 1000s of times faster, this is what is guaranteed for your own implementations. you will try to do that in the programming assignments of this project for this module. and you will see that your solutions are actually much, much faster than the classical algorithm. and you will need them to work 1000s of times faster to actually pass the assignment. and also, we actually encourage you to compete on the forums on whose solutions is the fastest. so you can take the result by the grader and then post it on the forums, and someone can organize the result table with the top results. and it's very interesting how you can come up with your own heuristics for improving this algorithm. because you see there are a lot of heuristics in the last part of this lecture, and you can come up with your own ideas and maybe they will be so good that you will speed up everyone else's solution 10 times or 100 times. so really try to do that. we will also show you some pointers to where you can get the data for real graphs of the road networks of some parts of us and the whole us. and in the end, this could become your own open source project which you could show, for example, to potential employers. so we wish you good luck with advanced shortest paths. 
hello. i haven't seen you for a long time, since we were working on the change problem. but today we'll work on a completely different topic called string algorithms. string algorithms are everywhere. every time you spell check your documents, or google something, you execute sophisticated string algorithms. but today, we walk about very different application of string algorithms. sam berns gave a fantastic pep talk when he was 16. he was talking about his life and a year later, he died. sam was suffering from a rare genetic disease called progeria. children with this progeria often will be above average intelligence, look old already at the age ten and usually die in their teen years. but for many years biologists have no clue of what causes progeria. but in 2003, they figure out that progeria is caused by single mutation, on chromosome one. to understand what pattern matching has to do with progeria, we need to learn something about genome sequencing. when my children were young, that's how i was explaining them how genome sequencing work. i was using an example of the newspaper problem. take many copies of the identical new york times newspaper, then set them on a pile of dynamite. don't do it at home, and then wait until explosion is over, and collect the remaining pieces. of course many pieces will burn in the explosion, but some pieces will remain. and so your goal is to reconstruct the content of the new york times. a natural way to solve the newspaper problem, is to consider it as an overlapping puzzle. look at different pieces and try to mix them together like this. and then slowly but surely, hopefully you'll be able to assemble the whole gem. and that's roughly how the human genome was assembled in 2000. and here, bill clinton is congratulating craig venter, one of the leaders of the human genome project, on completion of this $3 billion mega science project. we don't need to know much about genomes for the rest of these talks. the only thing we probably need to know is a genome is simply a long strand in a, c, g, t alphabet. i will try to explain how the newspaper problem translates into genome sequencing. they start from millions of identical copies of a genome. then they break the genome at random positions using molecular scissors. these molecular scissors don't look quite like this one shown in this picture. then we generate short substrings of the genome called reads, using modern sequencing machines. of course, during this generation some reads are lost. and the only thing left is to the assemble the genome from millions, or maybe even billions of tiny pieces. the largest jigsaw puzzle humans ever attempted to solve. today, i won't be able to tell you about algorithms for genome assembly. but if you are interested in learning about this algorithm, you can attend or bionformatics specialization at coursera. or read the book, bioinformatics algorithms. assembling human genome was a challenging $3 billion project and afterwards the era of genome sequencing began. but in the first ten years after sequencing human genome, biologists were able to sequence only about ten other mammalian genome because it was still difficult. however, five, six years ago, so-called next-generation sequencing revolution happened. and today, biologists sequence thousands of genomes every year. why do biologists sequence thousands of species? there are many applications. for example, the next big science sequencing project after the human genome was mouse genome. because we can learn lot about human biology and diseases from mouse gene. an important application in agriculture, for example by sequencing rice genome, biologists are able to develop new high yield crops of different plants like rice. and there are many hundreds and hundreds of other applications. but recently, in addition to sequencing of many species, there is also much effort on sequencing millions of personal genomes. the things that make us different are mutations. however, there are surprisingly few mutations that distinguish my genome from your genome. roughly one mutation per thousand nucleotides. however, this mutation make big difference. they account for height or they account for more than 7,000 known genetic diseases. five years ago, the era of personalized genomics started and nicholas volker is a foster child of personalized genomics. he was so sick that he went through thousands of surgery, but doctors still were not able to figure out what is wrong with this kid. however, after he is genome sequence can reveal a mutation in a gene linked to defect in the immune system. doctors applied immunotherapy and nicholas volker is now a healthy child. however, sequence in personal genomes, from scratch, still remains difficult even today. what biologists do today however, they do so called reference base human genome sequences. let's start from craig venter genome assembled in 2000, call it reference genome. and then let's start sequencing my genome by generating all reads from my genome. here's some of the three perfectly match to genome, but some of them don't. and based on these reads that do not match, we will be able to figure out what is my genome. for example, we can find a mutation of t into c and deletion of t in my genome as compared to. it brings us to a number of computational problems. the easiest one is the exact pattern matching. given a string pattern and a string text, we want to find all positions and texts where pattern appears as a substring. but our goal is to find mutations, and therefore we want also to solve approximate pattern matching problem. where input is a string pattern, a string text, and an integer d. and we want to find all positions in text where pattern appears as a substring with at most d mismatches. i think you already have some good ideas on how to solve this rather simple problem. but think about this, even if you have fast algorithms for solving this problem, would you be able to solve the next problem? to answer the question where do billions of reads generated for me, match the reference genome from. and this leads us to multiple pattern matching problem given a set of strings, patterns and a string text, find all positioning texts where a string from patterns appears as a substring. 
so let's develop the first brute force approach to pattern matching. first thing, let's get our pattern into a car and let's drive this car along the text. of course while we are driving, each time we see our pattern appear with in the text we report that there is an occurrence of pattern in the text. so how it works? let's try to drive nana along panamabananas. so, first letter doesn't match, therefore we need to drive further. first letter again doesn't match, so we need to drive further. now, n matches n. a makes an a but m doesn't make n so we need to drive further. we continue, continue, continue, continue, continue and now there is again a match. n, a, n, a, we found the pattern and then we continued further, problem solved. this approach is actually very fast. it takes only all of text time pattern time because every time we drive along the text, it may take us up to pattern symbol comparison to figure out. where pattern matches the text at a given position. and mischa will tell you later how knuth-morris-pratt algorithm allows to speed up this naive brute force algorithm and by running o of text time independently on the lens of the pattern. so looks like they succeeded. it can go home, right? but wait a second. let's see how this algorithm would work for billions of patterns. and it turn out that for billions of pattern it will take time text times patterns. to where patterns is the total lengths of all patterns and if we apply it to my genome then text will be 3 billion nucleotide long and the total length of the pattern will be maybe as large as 10 to the power of 12. so, the naive algorithm, or even knuth morris pratt algorithm will not work. should we give up? 
adds brute force approach to pattern matching is slow when we try to match billions of patterns. let's try to figure out why. so, what is happening if we put each pattern in its own car, then the first dives the first car, then the second car, then the third car, the next car and next car, and that's why it takes a lot of time. here's a new idea, let's pack all patterns in a bus, and let's drive this bus along the text. but how do the constructors bus? let me show you how we can construct the bus from multiplied patterns. let's start from the first pattern and represent it as a pass in a tree. continue to the next button, continue this next button, and continue this next button. so far it was easy and not interesting. we have four patterns and we constructed for passes from the root of the tree. let's go to the next one, antenna. now the first letter in antenna actually already appears on the way from the root it is right here. the second letter also appear away from the root. and then, we need to branch the previous pass into two passes to construct the pass for antenna. now let's do bandana. so bandana we press it further. and now we again have to branch the pass. continue with ananas, again branching. and finally continue with nana, branching again. and of what we've constructed is actually our bus. and which is called trie of patters. how do we use this bus? how would we drive? after constructing this bus, how would we drive with along text? well, you'll use triematching. and we'll drive this whole trie along the text, and at each position of the text, we will walk down the trie by spelling symbols of text. a pattern from the set patterns matches text each time we reach a leaf. let me show you how it works. so our bus, in now looking at the first letter of the text, p, so if we walk along the edge, labeled by p, the next letter is a, we walk along this edge, the next letter is n, we walk along this edge, and we found that part of pan appears in panamabananas. next, we move to the next letter of the text and we start my chunk again. a, n, a and now there is no match so at this position there is no match between, patterns and texts. continue further, n, a. once again, there is no match. a, once again, there is no match. for m, from the very beginning, there is no match. we continue further. a. once again, there is no match. let's try here. b, a, n, a, n, a we came to so we found the pattern, but we have to continue further. further. further. further. found the pattern again. continue further. again found the pattern. and now, there is no more match. so, we found, in a single run on our bus, we found all matches of patterns against the jacks. actually, i haven't finished yet. we also have to match n, a, s. no. a? no. now we are done. our bus is very fast, recalls at runtime of our brute force approach was o text time patterns. where pattern says the total lens or for pattern. that 's why it was slow the total length of all patterns is huge. in the case when we tried to match reeds against the genome. but the run time of triematching is only o of text time the length of the longest pattern. and typically in modern sequencing pattern, the reeds have lengths only 200, 300 nucleotide. so it looks like finally they are done. should we go home? we are not ready to go home just yet. note that trie we constructed has 30 edges. but in general, the number of edges for a trie is o of the total length of the patterns. and for the human genomes, the total length of the patterns will be in trillions. so unfortunately, our algorithm will be impractical for read matrix. should we give up? 
we saw that using tries dramatically improves on the brute force approach with respect to speed but becomes impractical with respect to memory. what should we do? let's try a different crazy idea. instead of packing patterns into a bus, let's pack text, into bus. let me explain how we can do this. we'll generate all suffixes of text. and form a trie out of all the suffixes. it will be larger. a rather large suffix trie. for each pattern, we can check if it can be spelled out from the root downward in the suffix trie. so we are building a very large bus this time. let's see how this idea works for panamabananas. let's start by adding a dollar sign to the end of panamabananas and i will explain later why i add this strange dollar sign to the end of my string. so we start from the longest suffix of panamabanana$ and builds a corresponding parse in the trie continue continue continue further continue continue so far there is no branching. continue continue and now the first branch in our suffix trie appears. continue further there are new branches showing up and then finally be constructed something that we call suffixtrie of text. how can we use this for part lecture? well let's take our pattern once again, put it in the car and let's drive along the branches of our suffixtrie. first we match first symbol in pattern. the next symbol, the next symbol. and finally, we found a match over the pattern to one of suffixes of the text to which we found a match of a pattern in the text. we use banana it will go like this. we found banana with this nab, it goes like this. unfortunately, we didn't find it because there is no continuation for b. let's see for antenna, we go this way and finally have to stall because there is no match for t. so it looks like this suffixtrie idea worked for us. but there is one important question we forgot to answer. where are the matches? how do we find our patterns match the text? there's no information in the suffix trie yet that allows us to answer this question. here's an idea. let's try to add some information to the leaves of our tree. but what information to add? let's say for every leaf, let's add information about the starting position of the suffix, that generated this leaf. for example, for bananas, we will let position six, because bananas, start at position six of the text. let's see how it works. so for panamabananas$, we will be adding zero because this suffix starts at position zero. then we will be adding for anamabananas$ we will be adding correspondent position. continue, continue, continue, continue, continue, continue, continue, continue, continue. and finally our tree, leafs on our tree, get decorated to this positions of the suffixes in the text and finally when we looked at all suffixes our tree got all the information we need to figure out where the positions of patterns are and that is actually what is called suffixtrie. original suffixtrie as i described earlier, decorated with the position of all the leaves in the text. however, getting information about position of suffixes to the leaves of the suffix try doesn't yet help us to figure out where the string bananas appears in the text. so, what we want to do, once we find a match, like match of bananas, we want to walk down to the leaf or maybe leaves, in order to find the starting position of the match. let's see how it works. so for banana, we ended in the middle of the trie, but we'll continue walking, continue walking, and finally, we find where banana start at position six. for ana the continued working but there are three ways to continue working towards the leaves. this is the first one. this is the next one. and this is another one. so in this case we find that baana actually appears three times in the text and the three positions are shown on the top. so it looks like we finally solved the problem of finding positional patterns in the tree, which means we now have a fast algorithm for solving the problem. we saw that suffix trie results in a fast algorithm for the part i mentioned, but let's take a look at the memory footprint of suffix trie. suffix trie is formed from text suffixes of text. the average length of the suffixes is roughly text over two. and therefore, the total length of those suffixes is length of the text, multiplied by length of the text minus one, divided over two. for human genome it appears huge impractical memory footprint. should they give up? 
is so that pattern matching with suffix tries is fast, but impractical with respect to the memory footprint. how about this idea? we saw that bananas takes a lot of edges in our suffix try. can we compress all these edges into a single edge? that's very easy to do. so let's simply do this, and do this with every known branching pass in our suffix tree. very quickly our tree gets much smaller. so if you're almost done, continue, continue and finally, we construct something that is called suffixtree(text). and since each suffix adds one leaf and at most one internal vertex to the suffix tree, then the number of vertices in the suffix tree is less than two times text and therefore memory footprint of the suffix tree which is proportional to the number of edges in the suffix tree is simply all the lines of the text. this sounds like cheating! because we haven't answered the question, how do we store all the edge labels? they will take the same total length of pattern space that all labels in the suffix tree took. however, let's try to do the following. so instead of storing the whole string bananas as a label of our edge, let's notice that bananas start at the position six of the text and has lance eight. and therefore instead of storing bananas on the edge, we will only store two numbers, 6 at the starting position of bananas and 8 the last of bananas. that will be sufficient to reconstruct the entire label bananas. and we will do it for all edge labels. and as a result, now you see that suffix tree is indeed a very memory efficient way to code all information about suffixes of the text. you may be wondering, why did we add this silly dollar sign? to panamabananas. i added it because i wanted to make sure that each suffix corresponds to a leaf. but why do we want to make sure that each suffix correspond to leaf? i suggest you try to construct suffix tree for papa without adding the dollar sign and compare with the suffix tree for papa, with dollar sign and you will see why the dollar sign is important. the sos of suffix trees are a fast and memory efficient way to do pattern match. however, construction of suffix trees is not for faint hearted because they need to combine all suffixes of the caps into the suffix tree. and the name of it for doing this takes quadratic o text squared time. however there is an ingenious linear time algorithm for constructing suffix trees called and it was developed over 40 years ago and this algorithm amazingly has linear write of text to construct the suffix tree. so it looks like we are done, finally, after all the effort. and now, i want to tell you about the big secret of the big o notation. something that sasha, daniel, misha, and neil, forget to tell you about. indeed, suffix trees enable fast exact multiply pattern matching run time. all of text plus patterns, and memory of text, that's the best we can hope for. however, big o notation hides constant, and the best known implementation of suffix tree has large memory footprints of 20 time text which reaches very large memory requirement for long genomes like human genomes. but even more importantly, we want to find mutations. and it is not clear how to develop fast approximate multiple pattern matching category using suffix tree. so once again we are facing an open problem that we have to solve. 
the previous lecture ended with a rather difficult algorithmic challenge that we will try to solve using the burrows-wheeler transform and suffix array. let's start with the burrows-wheeler transform. allow me to slightly change the focus. instead of pattern mention, we'll talk about text compression. and run-length encoding is the simplest way to compress text. where a run of a single symbol is substituted by the number of times the symbol appeared in this run, followed by the symbol itself. you may be wondering why we want to do run-length encoding for genomes because genomes don't have many runs. but they do have many repeats. for example, more than half of human genome is formed by repetitive dna and the lion's share of many plant genomes is formed also by various types of repeats. but here's an idea. let's convert test into something else so that our repeats will be converted into runs. we'll start from the genome then we'll turn it into convertedgenome and then we will apply run-length encoding to the convertedgenome. because our hope is that convertedgenome will have many runs. let me show how we can accomplish this. so let's consider all cyclic rotations of our favorite string, panamabananas$. we start from this one, and this one, and this one, and continue to form all cyclic rotations of this string. and then after you generated all the cyclic rotations, let's sort them. the dollar sign is viewed as the first letter of alphabet, even before a, so we'll start with $panamabananas. continue, continue, continue, continue, and finally we'll have a sorted list of all suffixes of the text. you might be wondering why we are doing this, but look at this strange sync. if we look at the last column of the resulting array and the last column of the resulting array is called burrows-wheeler transform of the text. that you will notice that also our regional string, panamabananas$ did not have many runs. the burrows-wheeler transform of this string actually has many runs. for example here's a run of five a in the burrows-wheeler transform of our original text. how have we achieved it? let me explain it by using an example from the famous double helix paper by watson and crick where they first presented the structure of dna. and these are just some consecutive strings in burrows-wheeler transform of this book. and you see that there are many runs of a in the burrows-wheeler transform of this text. why so many runs of a? well one of the most common words in english is and. and every time you have and in the text, it is likely to contribute to a run of a in the burrows-wheeler transport, as you see in this example. so our goal now is to start from the genome, apply burrowswheeler transform to the genome. and we can now, hopefully, comprise burrowswheeler transform of the genome. and after you apply this compression, we will greatly reduce memory for storing our genome. but it totally makes sense if we can invert this transformation. and from compression version of burrows-wheeler transform, we can easily go back to the original burrows-wheeler transform. but can we go back from the burrows-wheeler transform of the genome, to the genome itself, is it even possible? 
we saw in the previous lecture that the burrows-wheeler transform idea will not work unless we figure out how to invert the burrows-wheeler transform. let's try to reconstruct the text banana from its burrows-wheeler transform, annb$aa. we know the last column of the burrows-wheeler matrix. we also know the first column, because the first column is simply sorting all elements of the burrows-wheeler transform. and therefore, we know all 2-mers in the cyclic rotations of our string. and if we sort them then we will get the first two elements in each row of our burrows-wheeler transform matrix. now after we know the assorted 2-mers we once again return to the original burrows-wheeler transform matrix. and for each such 2-mers, they actually know the symbols that precede every 2-mer, and therefore, we know all 3-mers. we sort them and now they appear in the same order as they appear in burrows-wheeler matrix. we once again return to the original burrows-wheeler matrix, and we know again the symbols that precede every 3-mer, we'll continue. we generated all 4-mers, we sort them again, repeating the same, thing again. and this way we generate 5-mers, we once again sort. and with the final step, we generate all 6-mer, sort them, and we now know our string, banana, banana. so we now know the entire matrix, and therefore, symbol in the first row of this matrix after the dollar sign spell banana, exactly what we need. we saw how to invert the burrows-wheeler transform, but it was a very memory intensive algorithm. indeed, for reconstructing text from the burrows-wheeler transform, we needed to store text, cyclic rotations of the string text. can we invert the burrows-wheeler transform with less space and without text rounds of sorting? to develop a faster and more memory efficient algorithm for inverting the burrows-wheeler transform, we'll start from an interesting observation. let's take a look at all occurrences of a in the first column and in the last column. and let's ask a question, where is the first a in the first column? it's hiding along the circle which represent text. and you can see that it's hiding right after panam, shown in green. so next question i want to ask, where is the first a in the last column? it's hiding along the circle. and maybe this is just coincidence, but strangely, it is hiding exactly in the same place. let's ask the same question about the second a. and second a is hiding once again in the same position along the circle, bothe for the first column and for the last column, right after pan. the next question we will ask, where is the is hiding, and it's once again hiding in the same positions. the same here, the same here, and the same here. so it looks like that i-th position of a in the first column is hiding at the same position along the column as i-th position of a in the last column. and if we look at the appearances of n, the same rule you can check, they're the same rule will apply. so is it true in general? let's try to answer this question. well let's number all occurrences of a in the first column. and then let's chop off the first a, the sorted six strings that appeared in the burrows-wheeler transform matrix remain sorted. because we simply remove the first identical symbol from all of them. and now let's add this chop symbol to the end of each of the strings. we added them, and of course, the strings remain sorted. but these are exactly six strings that end in a in our burrows-wheeler matrix. which means that they follow in our matrix in the same order than the order we started from. and that result in the so-called first-last property of the burrows-wheeler transform. the k-th occurrence of symbol in firstcolumn and the k-th occurrence of the same symbol in the lastcolumn correspond to appearance of this symbol at the same position of text. and it's shown in the string. our move is the first-last property, let's try to invert the burrowswheeler transform again. let's start with the dollar sign that is located in the first row, in the first column in the first row. it corresponds to s1 in the last column. we know where s1 is located in the first column, let's move there. and s1 in the first column correspond to a6 in the last column, so let's move there. and we know where a6 is located in the first column, so let's move to this position of a6. a6 in the first column corresponds to n3 in the last column. and we know where n3 is located in the first column, so let's move here. and we'll continue moving through the matrix according to the first-last property. and slowly but surely we will be spelling our original text. and when we finish, we notice that the memory we took is only 2 times text. and the time we took is simply following these pointers that i defined by the first-last property. which is also all in the length of the text. so we are done. the only question left, where is pattern matching in the burrows-wheeler transform? 
okay, let's learn how to do pattern matching with burrows-wheeler transform. let me first summarize what we learned about pattern matching with the suffix tree. the runtime is equal to o(length of the text + total length of all patterns). memory is, in the best implementation, known today is 20 * length of the text, or which is high for live strengths like human genome. so it would be in nuclear diclonk. so the question we will try to address in this lesson is, can we use burrows-wheeler transform to design a more memory efficient linear-time algorithm for multiple pattern matching? so let's see how we can do this. let's search for ana in panamabananas. well, we'll definitely start by noticing that there are six rows that start from letter a. but when we look, and please notice also that we are currently matching the last symbol in ana as in the first one. this will be important. so there are six rows starting from a, but only three of them are ending in n. what we need, because we are looking for matching the last two symbols now of ana, which is na. so the mental attention to these three symbols, and using the first last property, we can figure out where these three n's hide in the first column of our burrows-wheeler matrix, here, here, and here. after they found where they appear in the first column, we know where na appears. in the string can be actually found, three matches of na. this is the last two symbol in ana. let's now try to match the first symbol in ana, and we know where to look for this first symbol. we'll look for them correspondingly in the last columns of these three strings. and after we found a in these three rows, then using again the first last property, we find where these three occurrences of ana appears in the beginning of our cyclic rotation. as a result, we found three matches of ana. let me specify some details of the algorithm that we just discussed. we will use two pointers, top and bottom, that specify the range of positions in the burrow-wheelers matrix that we are interested in. in the beginning, top will go to 0 and bottom equal to 13, to cover all positions in the text. in the next iteration, the range of position we are interested in is narrowed to all position where a appears in the first column. then, what do you do afterwards? we are looking for the next symbol, which is n in ana, and we are looking for the first occurrence of this symbol in the last column among positions from top to bottom, among rows from top to bottom. and likewise, afterwards, we are looking for the last occurrence of the symbol. as soon as we found the first and last occurrence of this symbol in this case, and the first last property will tell us where this n and all n's in between are hiding in the first column. as a result, the pointers top and bottom equal to 1 and 6 are changing into 9 and 11, they narrow the search. and then we continue further, and that's how we find the positions of ana in the text. the algorithm that i just described translate in the following bwmatching pseudocode. and you can see lines in green describe what we have been doing with these top and bottom pointers. note that we are using last to first array and given a symbol at position index in lastcolumn, lasttofirst index defines the position of the symbol in the first column. so it's implement first to last property. it looks like now, finally, we are done. we have a very first pattern matching algorithm based on burrows-wheeler transform, and it has good memory footprint. the only problem, though, is that bwmatching is very slow. it analyzes every symbol from top to bottom in the last column in each step. what should we do? the trick here is to introduce the count array. and the count array describes the number of appearances of a given symbol in the first i positions of the last quote. this slide shows the count array, and ardent is the count array. we can design a better version of bwmatching by substituting four green lines that we discussed before by two green lines that are using the count array. and as you can see, we don't need anymore to explore every symbol between top and bottom indices in the last column. if you are wondering about the details of transformation from the previous four lines into two lines using the count array, check our coursera course or get details from our books that describes this transformation. so it looks like finally, after all these complications, we are done. but there is still one question we found quanza. where are the matches that they found? where do they appear in the text? 
at the end of the last lecture, we faced the challenge of finding positions of the pattern in text when they tried to develop pattern matching with burrows wheeler transform. now i will explain how we will use suffix arrays for solving this problem. suffix array simply holds starting positions of each suffix. for example, the first suffix start at position 13, the second suffix start at position 5, the next suffix starts at position 3 and we can fill up the end of the suffix array. here it is. so, when suffix array is constructed, we can very quickly answer the question where the occurrences of the part are not. in this case, and the case of ana, our pattern appear at position 1, 7 and 9. the challenge, however, is how to construct the suffix array quickly. because the naive algorithm for constructing suffix array that is based on sorting all suffixes of text requires all of length of text logarithm length of text comparison. and each comparison also may take time. there is a way to construct a suffix array if you're already constructing a suffix tree. because, as you can see from this example, a suffix array is simply a depth-first reversal of the suffix tree. indeed, you start from lead 5, continue to lead 3. one, seven, nine, continue further and by simply traversing all leaves in the suffix tree in order to reconstruct the suffix array. to summarize, if the construct suffix array by the depth-first traversal of the suffix tree, then it takes all of text time and roughly 20 times text space. and misha will also explain later how to quickly construct suffix array without relying on suffix tree. in fact, manber-myers, in 1990, constructed the first algorithm for the first linear timeout for suffix array that require o of text time four time text space. however, for genomics application, even this reduced space four times length of the text, is still large. what can we do to reduce it? here is a trick for reducing the memory for suffix array. i will first ask a question. can we store? only a fraction of the suffix array, but still do fast pattern matching. for example, can we store only elements of the suffix array that are multiples of some integer k. shown here, if integer k is equal to five, then we only store elements five 10 and 0 of the suffix array. we have no access to other members of suffix array. how it can be useful? let me show you how to use the partial suffix array to find the position of matches. when we have complete suffix array it is trivial, we simply look up at the highest point elements in the suffix array. but what do we do when we only have partial suffix array? where are these ana occurrences appear in the text? well, we do not know, because there is no corresponding k element, let's say, for a2na. how do we find where it appears? indeed, we don't know yet where it appears, but we can also ask a question where is b1 a n a appears. once again, we don't know how to answer this question because there is corresponding [inaudible] in the partial suffix. but, using the first last part of it we can ask different question. where is the string a 1 b ana appears? and now we can answer this question. because in this case, the element of the partial suffix array is present. it is fast. so, we know where there is a1bana appears, now the thing left is to figure out where ana appears. and it's easy to do because if aabana appears at position 5, then b1ana appears at position 6. and ana appears at position 7, so we figure out how to use suffix array for fast pattern matching. of course, the time to search for pattern will be multiplied by a factor of k, because if you store, we potentially can search for up to k position before you find the fill element of the partial suffix array, but k is a constant in this algorithm. 
so far, we were focusing on exact measures, which will help us to answer the question, where is the positions on my journal and in the reference journal identical? but we, of course, are most interested in approximate pattern matching, because we want to find positions in my journal that are different from the reference journal. and approximate pattern matching problem is given a string pattern, a string text, and an integer d, try to find all positions in text where the string pattern appears as a substring with at most d mismatches. i'm sure you can come up with a fast algorithm for solving this problem, but i'm really interested in a more difficult problem. multiple approximate pattern matching, a set of strings patterns, a string text, and an integer d is an input. and we want to find all positions in text where a string from patterns appears as a substring with at most d mismatches. let's try to use the burrows-wheeler transform again to find approximate matches of ana in panamabananas, and we will allow up to one mutation in ana. we will start again with finding all rows in the burrows-wheeler matrix that start with all a, here they are. and amongst them, we want to find rows that contains na. and among six rows that start with a, only three of them actually end with n. here are these rows, and we are interested in them. they form exact matching of the last two symbols of ana to our text. but that's not the only thing we are interested in. in the past, it was the only thing, but now, they're actually interested in all six rows starting starting from a, because we are interested in approximate matches as well. and to find approximate matches, we need to retain all the six rows. and specify the number of mismatches for each of these rows, here they are. after we found all rows we're interested in, we use again the first last property to find where the symbol in the last elements of these rows appears in the first column. and they will be appearing here. once again, we check which of these six appearances will match ana with, at most, one notation. and it turn out that one of them actually doesn't satisfy this property, because there is a match, but it's a match with two mutations, which is beyond our maximum allowable number of mutation. and that's why we are not interested in this row anymore. and then by applying last to first property again, we find all occurrences of ana, where there is up to one mutations in text. how do we find where all these five approximate occurrences appear in the text? well, you can use suffix array, or more precisely, the partial suffix array. and you can figure out how to use the partial suffix array to find approximate occurrences as well. i tried to hide some details of approximate pattern matching with burrows-wheeler transform, to make it a little bit easier for you to understand how it works. in reality, it is a bit more complex. if you wanted to learn about the details of approximate pattern matching with burrows-wheeler transform, you can find those details in our bioinformatics algorithm course on coursera or in our book. sam berns had a very rare genetic disease. in fact, there are less that 1,000 people on earth with this progeria. however, there are over 7,000 of such rare genetic diseases. and as a result, about 10% of human population have a rare genetic disease. we now learn how pattern matching will help the doctors of the future to learn about mutations in our genome and will allow them to diagnose many of these mutations. however, even if a child is diagnosed with a disease causing mutation, in the case of progeria, there is no cure. and, this is the next challenge for personalized genomics, moving from diagnostic to new drugs aimed at specific diseases. and to finish this lecture, i will tell you just about one case of a very successful drug that biologists developed based on exact knowledge of specific mutation implicated in a disease. i will talk about more complex type of mutations. so far, we've talked about a point mutation when one nucleotide is changing into another nucleotide. but there are more complex mutations that work as an earthquake operating on the genome. in this particular case, i'm talking about so-called philadelphia chromosome that is formed from two normal human chromosomes. pieces of these two normal chromosomes exchange position. as a result, two chimera chromosomes are formed as shown here. biologists figured out how to detect this event, and it turns out that it is a biomarker for chronic myeloid leukemia. and based on exact knowledge of biological maheners, admittedly, it's more complex as the mutations restarted, but it's once again, a mutation in the human genome. biologists were able to develop a miracle drug called gleevec that is very efficient for chronic myeloid leukemia. 
hi, in this module, algorithmic challenges, you will learn some of the more challenging algorithms on strings. we will start with the knuth-morris-pratt algorithm for exact pattern matching. it allows to find all occurrences of a pattern in the text, in the time proportional to the sum of the length of the pattern and the text. then, we'll proceed to learn the algorithm for building suffix array of a string in time n log n. and after that, you will learn how to build a suffix tree, given a suffix array of the string in linear time. of course, you already know how to build a suffix tree of the string ,but the algorithm you know is n squared. and that doesn't allow you to take really long strings of millions or billions of characters and build suffix array for them in reasonable time. and the n log n algorithm will allow you to tackle that. so first, let's recall what exact pattern matching means. the problem is very simply formulated, you are given a long text and the pattern that you need to find in it. and you need to find all the positions where the pattern starts in the texts, all the positions, and when number positions from zero in the whole module. you learned brute force algorithm for that, which basically slides the pattern down the text, and the running time of that algorithm is the product of the length of the text and the length of the pattern. what we are going to do in this lesson is to improve this time to the sum of the length of the text and the length of the pattern, but first, let's recall how brute force algorithm works. it first aligns the pattern and the text such that the pattern starts from the zero position in the text, and tries to match the pattern by comparing it character by character to the corresponding characters of the text. and in this example, we find pattern right away in the position number 0. so we add position number 0 to the output, and then we slide pattern one position to the right. and we compare the first symbols and they don't match, so we slide the pattern again, and again, no match. and again, and then we just slide the pattern. we compare the symbols and if they don't match, we slide the pattern to the right. and then in the last possible position, we again find the occurrence of the pattern in the text. and so our output is a list of positions 0 and 7, and those are all the positions where the pattern occurs in the text. so the question is now, can we avoid some of the positions where we tried to align the pattern with the text? but it actually didn't made sense given what we already know about the previous comparisons. and the answer is yes, we can. in this particular example, we've already found the pattern in the text starting from the 0 position, and that means that when we slide the pattern to the right and try to align it to the first position in the text. we're going to compare the prefix of the pattern without the last character with the suffix of the same pattern without the first character. and they don't match, so there is no occurrence of the pattern starting from the first position. but, if we somehow pre-processed the pattern and knew that the prefix without last character is not equal to the suffix without the first character, then we could just keep all this alignment with the first position in text at all. and the same is true about the next position of the pattern. if we knew that the prefix of the pattern of length two, is not equal to the suffix of the pattern of the length two. then we could skip also positioning the pattern against the second position on the text, and then, when we slide one position to the right again. we compare this prefix of length one which is letter a, with the suffix of length one which is also letter a. and these are equal, so this position, we cannot skip according to this rule. but it means that instead of comparing the pattern to the text in positions zero, one, two and three, we could just safely move the pattern from the position zero to the position three, skipping positions one and two. and in the more general case, we could skip even more positions, depending on what is the life of the pattern, and how much of its prefixes coincide with their corresponding suffixes. another example is when we don't even find the whole pattern in the text. we still can skip some of the positions. so in this example, the longest prefix which is common for the text and the pattern consists of six characters and the pattern is longer. so we cannot compare prefixes of the pattern with suffixes of the same pattern and then decide that we don't need to check some of the positions in which to align pattern of the text. but instead we need to do the same thing with the string marked in green. we need to compare prefixes of this string with suffixes of this string. and we can notice that the first position where the prefix of a string can coincide with the corresponding suffix is position number four. so we can just move the whole pattern from position zero to the position four in the text. and then, try to compare the pattern with the text and we find an occurrence. we couldn't find an occurrence earlier, because no longer prefix of the string a, b, c, d, a, b, coincides with the corresponding suffix. and another example, again we find the longest common prefix of the pattern in the text. it has length 6, and is the string a, b, a, b, a, b. and for this string the longest prefix which coincides with the corresponding suffix is a, b, a, b of length four. and that means that we can move the pattern two positions to the right, and skip the alignment at position one of the text. now we again find the longest common prefix of the pattern, and the suffix of the text starting in position two. it again has length six, which means that there is no occurrence of the whole pattern in the text in position two. but we need to consider the string a, b, a, b, a, b, which is the longest common prefix. and again, compare the prefixes of this string with the suffixes. and we already know that the longest prefix which coincides with the suffix is a, b, a, b, of length four. so we can again move the pattern to the right, so that the prefix and the corresponding suffix match. and now we find the occurrence of the pattern in the text. to make an algorithm from these observations, we will need the definition of a border. so border of a string is a prefix of the string which is equal to the suffix of the string of the same length. for example, for string arba, a is a border, because the prefix a is equal to the suffix a. and ab is a border of a string a, b, c, d, a, b, which we saw in the second example. and a, b, a, b, is a border of a, b, a, b, a, b. and do you notice that the prefix a, b, a, b intersects with the suffix a, b, a, b? and that is okay and we just mark the fact that they intersect with an orange color. but actually we notice that not just a, b is a border but also a longer string of line four which is a, b, a, b is also a border. however string a, b is not a border of the same string a, b because we require that the border doesn't coincide with the whole string. we need only those prefixes and suffixes which are shorter than the initial string. now, let's consider shifting all the pattern along the text in the general station. so, the first thing we do is we find the longest common prefix of the pattern with the suffix of the text to which we've just aligned our pattern. then we find w, the longest border of u, so that there is a w in the beginning of u, and there is one in the end of u, and also mark both w's in the text t. now, i suggest to move pattern p to the right in such a way that the first w in p coincides with the second w in t. and that is the way i suggest to skip some of the positions where we don't need to align pattern with the text. now you know that it is possible to avoid some of the comparisons that the brute force algorithm does. and i've suggested a specific general way to do that. but we don't want to miss any of the occurrences of the pattern in the text. so the question is, is it really safe to move the pattern in the suggested way? you will learn that in the next video. 
hi. in this video, you will see that the method i suggested in the last video for shifting the pattern on the text is safe, in the sense that we won't miss any of the occurrences of the pattern in the text by shifting it that way. but first we need suffix notation. we denote by s with index k, suffix of string s which starts in the position k. for example, for s = abcd, the suffix starting at position 2 is s2, and it is cd. and for string t, which is = abc, the suffix starting in position 0 is t0, and it is abc. note that, again, we use indexes starting from 0 for all the strings. suppose the pattern is aligned with position k in the text. let's denote by u, the longest common prefix of the pattern, and the suffix tk. then, select the longest border w of the string u. in the last video, i suggested to move the pattern to the right in such a way that the left w in the pattern coincides with the right w in the text. and now, we'll prove that there cannot be any occurrences of pattern in the text in the red area between the current position k and the start of the right w in the text. this will prove that the shift suggested in the last video is safe, we won't miss any occurrences by shifting the pattern this way. suppose the pattern occurs in the text in some position i between k and the start of the line w. let's move the pattern to align with that position i. then we can notice that there is a prefix, v, of the pattern that is also a suffix of u in the text. and so this string v, which is both a prefix of p and a suffix of u, is actually both a prefix and a suffix of u. and also this v is longer than w, because it started before w in the text and ended in the same position as the right w in the text. and so v is a border of u, because it's both a prefix and a suffix. and it is also a border which is longer than w, but w was the longest border of u. so we got the contradiction with the assumption that our pattern p occurs somewhere between the current position k and the start of the right win the text. now you know that it is actually possible to avoid many of the comparison that the brute force algorithm does by shifting the pattern along the test and skipping some of the positions in which the brute force algorithm tries to align the pattern with the text. but how to actually determine the best shifts of the pattern, how to compute those longest borders and the common prefixes, that you will learn in the next videos. 
hi, in the previous video, you've learned that we need to quickly compute longest borders of different prefixes of the pattern. and in this lecture you will learn the notion of the prefix function, which helps to do exactly that. and we will study some of the properties of the prefix that allow it to compute it fast. prefix function of a pattern p is such function that for each position i in the pattern, returns the longest border of the prefix of the pattern ending in this position i. lets consider an example. here's a string p and we consider the first prefix of this string, a, for this prefix there is no border. so, prefix function is 0, now for the prefix a b, ending in the position number 1, there is also no border, because a is not equal to b, and so, the prefix function is again zero. for prefix a b a, ending in position 2, the longest border has length 1, and it is border a. for the string ab, a b the longest border is a b of length two, and for the next prefix the longest border is already of length 3. and although the prefix aba, and the suffix aba intersect, this is still a valid border. and for the next string, the longest border is a b a b of length 4. and then we meet character c, and the prefix function begins from zero again, because there is no border for the string a b a b a b c. and for the next string, the longest border is a. for the next one, again, a, and for last one ab. so here is the prefix function. now we will prove a useful property of the prefix function that the prefix ending in position i has a border of length s(i+1)-1. to see that, let's first consider the longest border w of the next prefix ending in position i+1. w has length exactly as of i+1, by definition of the prefix function. let's consider the last character of w and cut it out. what's left, denoted by w prime, is a border of the prefix ending in position i and it's length is exactly s(i+1)-1. so we've just proved the property, so we see the prefix ending in i has a border of length as, i + 1- 1, and it means that the longest border of the prefix ending in position i, is at least s(i + 1)- 1. from this we get an immediate corollary that the prefix function cannot grow very fast when moving from position to the next position. in particular it cannot increase by more than one. as we saw in the example, it can of course decrease, or stay the same, but it cannot increase by more than one from position to the next position. in the algorithms to follow, we will need to efficiently go through all the borders of a prefix pattern and this lemma helps us with that. it says that all the borders of some prefix of the pattern, but for the longest one, are borders of this longest border in term. and to see that, let's look at the longest border, and some border u of the prefix, which is shorter than the longest border. and then we can see that this border u is both a prefix, and the suffix of the longest border, and it was also shorter than the longest border. so u is indeed a border of p[0..s(i)- 1]. and the useful corollary from that is that all the borders of p[0..i] can be enumerated by simple ways. first we take the longest of the borders, and then we find the longest border of that string, and then we find the longest border of that string, and so on until we get an empty string as a border. and by the end, we have gone through all the borders of the initial prefix of the pattern. and to go from any prefix to its longest border, we just need to use the prefix function. and then again, the prefix function for that prefix of p, and then the prefix function for that prefix of p and so on. so if we know the prefix function of the pattern we can go through all the borders of any prefix of the pattern in an efficient way. going only through the borders and not encountering any other positions in the pattern. now lets think how to compute the prefix function. we know that s(0) is 0 because the prefix ending in position 0 has length 1 and has no known empty borders. now to compute s(i + 1) if we already know the values of the prefix function for all the previous positions, let's consider the longest border of the prefix ending in position i. and let's look at the character and position i + 1 and the character right after the prefix with length s(i). if those characters are equal than s(i) + 1 is at least s(i)+1 because we can just increase the length of the border. and that means that s(i+1) is exactly equal to s(i)+1 because we've learnt that the prefix function cannot grow by more than one from position to the next position but if those characters are different then everything is a bit more complex. so we know that there is border of the prefix ending in position i that has length exactly as i+1-1.so if we find that border then the next green character after it will be the same as the character in position i+1. it will be the same x. so what we need to do is we need to go through all the boarders of the prefix ending in position i by decreasing length. and as soon as we find some boarder that the next character after it is the same as the character and position i plus one, we can compute as of i plus one as the length of that boarder plus one. so now we basically have the algorithm for computing all the values of the prefix function. we start with initializing s(0) with zero, and then we go and compute each next value of s. if the character s(i+1) is equal to the character right after the previous border. then we just increase the value of the prefix function by one and go ahead. if those characters are different, we go to the next longest border of the prefix ending in position i using prefix function and look at the next character after it. if it coincides with the character in position i + 1 then we found the answer. otherwise we again go to the next longest border using the prefix function and look at the next character after it and so on. at some point, we may come to the station that the longest border is empty. and then we'll need to compare the character in position i + 1 with the character in position 0. and either they are the same, and then the prefix function is 1. or they are different, and then the prefix function has the value of 0. now you know a lot of useful properties of the prefix function but we still don't know exactly how to compute all of its values and you will learn that in the next video. 
hi, in this video, you will learn how to efficiently compute all the values of the prefix function in an efficient way. we'll start with an example. we're given a pattern, p, and we want to compute its prefix function, s. we'll start with position 0, and we'll fill in 0. and s of 0 is always 0, because the prefix of pattern ending in position 0 has length 1, and has no known empty borders. why don't we move on to the character in position one, which is b. and we compare it with the next character after the end of the previous border. the end of the previous border was before the pattern p starts, so the next character is a. and we compare a with b, they are different. and so the value of the prefix function is again zero, because we cannot take border of the empty border. and so we have to acknowledge that the prefix function is again zero. now we'll look at the next character a and we'll look at the next character after the end of the previous border which is again the a in position 0. and we see that these characters are the same. so we increase the previous value of s by one. and our value of prefix function for position 2 is 1, and now our current border is of length 1 and it contains just the letter a in position 0. and we look at the next character which is b. and we need to compare it with the character right after the end of the current border, which is the b in position one. and those characters are the same, so we increase the length of the previous border by one. and we write down that s of three is equal to two. and our current border is of length two, and it is ab. now we'll look at the next character a. we need to compare it with the next character after the current border, which is a in position two. and they're the same. so again, increase the value of our prefix function by a and we increase the length of the current boarder and it becomes aba. the next character is b, we need to compare it with the next character after the end of our border and they are again, the same. so, we increase the value of our prefix function by one and it becomes four. and our border is abab. now we'll look at the character c, we need to compare it with the next character after the end of the current border. and they are different. so what we need to do is to take the longest border of our current border, and we'll look at position three, and s of 3 is two. and so the longest border of abab is just ab. and now, we need to compare our current character c with the next character after the end of that border, which is a. and they're again different, so we need to take the longest border of our current border, and that has length 0. so, that will be empty string. and so now, we need to compare our current character with the first character of the pattern, which is a. and they're again different. so we'll write down that our prefix function is 0. we move on to the next character a. we compare it with the next character after the end of the border which is the first character of pattern. and they are the same so we write down 1 as the value of our prefix function. now the border has length 1 it is just string a. we look at the next character a, and we compare it with the character right after the end of the current border. they're different, so we need to take the longest border of our current border, which is empty string. and so we need to compare current character with the first character of the pattern. they're the same, so we write down 1 as the value of our graphics function and the current border has length 1. and finally, we go to the symbol b in the end of the pattern and we need to compare it with b in position 1. they are the same so increase the value of the prefix function by 1 and write down 2 as the value of the prefix function for the last position. this was how the computation of the prefix function values works on an example. and now let's look at the pseudo code which can compute these values for any string p. it will return an array of values s, which has the length the same as the length of the string, and which will contain the values of the prefix function for positions from 0 to length of the string minus 1. we start by initializing this array, and we'll write down s of 0 equal to 0, as i explained. and we also need additional variable border which will contain the length of the current border at all times. and then this for loop where position i goes from one to the last position in the string will actually compute as of i. at first we need to look at our current character p of i, and we need to compare it with the character right after the end of our current border. for example if the length of our current border is 2 then the border itself contains characters in positions 0 and 1. and so the next character right after the end of the current border is in position two. this is why we compare character in position i, p of i, with the character in position border. p of border. so we compare them and as soon as they are the same, we know what to do. but, if they're different we need to take the longest border of our current border. how to do that? our current border ends in position border minus 1. we know that the length of its longest border is contained in the corresponding value of the prefix function. so we assign value s of border minus 1 to our variable border. and we only end this while loop if either current character p of i is the same as the character p of border or our border became empty string. after that we compare our current character again with the character in position border. and either they are the same, then we increase our current border by one. and that is the value we need to assign to our prefix function. or, they are still different after border became zero. and so, our current prefix function value should be zero. and in the event of the for loop iteration, we just assign the value to the prefix function. and that will return the array with values of the prefix function. and we state that this algorithm runs in linear time. why is that? well, let's first forget for a moment about the inner while loop. then what we are left with is initialization which obviously works in linear time. and the for loop with linear number of iterations. and everything inside for loop, but for the while loop, for which we don't account now, runs in constant time. so, everything but for the inner while loop works in linear time. now on each iteration of the four loop, the while loop could take in theory as many as linear number of iterations. and if we sum all that up, that would be quadratic in the length of the pattern. and now, we will bound the total number of the while loop iterations by big o of the length of the pattern. and to do that, we will consider the graph of the values of the prefix function. here, the horizontal axis contains positions in the string. and vertical axis contains values of the prefix function. the graph always starts in .00. because s of 0 is 0. and it can stay the same, it can increase, it can decrease, but it never becomes less than zero because prefix function is always non-negative by definition. and also, it can never increase by more than one on each iteration. and our variable border in the code behaves the same way. it can be increased by at most one on each four loop iteration. but after each successful iteration of the inner while loop it gets decreased by at least one, because we consider the current border. we look at the next character and if it's different from the character in position i we take the longest border of our current border which is strictly shorter. so our border value decreases. and so all in all, border can increase at most length of the pattern times. and it is decreased by at least 1 on each iteration of the while loop. and border is also always non-negative. it means that it can be decreased at most, we go off length of the pattern times. and so there are at most linear number of while loop iterations in total. now you know how to compute prefix function efficiently in linear time in the length of the pattern. but how to actually solve the initial problem? how to find the pattern in the text? that you will finally learn in the next video. 
hi, in this video, you will find and learn the knuth-morris-pratt algorithm that allows to find all the occurrences of a pattern in the text in the time linear in terms of the length of the pattern and length of the text. so instead of the product of those lengths as in the brute force algorithm, we will need just some of those lengths to find all the occurrences. the algorithm goes as following. first we create a new long string s which consists of the pattern, the text and between them we insert a special character called dollar. which is basically. any character that is absent from both pattern and text. it cannot be specifically the character dollar. it is just a placeholder for some character that is absent from both pattern and the text. after we've assembled this longstring s, we need to compute it's prefix function. and after we've computed its prefix function, we need to look in the positions in the string s, which are inside the text part of it. so we'll look at all positions i such that i is more than length of the pattern. so after the pattern and after the dollar, and if the prefix function for that position is equal to the length of the pattern, then we know that there is an occurrence of the pattern in text ending in that position. for example, here we have a prefix function value of four. and we have an occurrence of the pattern ending in the corresponding position. we need to find all the positions where the pattern starts in the text. so from the position where it ends we need to compute the position where it starts, and to do that we need to subtract length of the pattern -1, but that would be the position in s. and to compute the position in the text, we also need to subtract the length of the pattern and one for the dollar. so in total, we need to subtract two length of the pattern from the position in the string s to find the starting position of the pattern in the initial text. and there is another place where prefix function of string big s is equal to the length of the pattern. and again, there is an occurrence of the pattern ending in that position. and why does algorithm even works? first, we need to notice that the prefix function for this string big s is always less than or equal to the length of the pattern. because of the dollar sign it occurs right after the end of the pattern so when the border is bigger, we would need to have another occurrence of dollar in the string big s. but dollar is only between pattern on the text and is absent from the text. so prefix function cannot be bigger in under the life of the pattern. if we look at the position i, which is to the right from the dollar and the prefix function is equal to the length of the pattern. and that means that the pattern is a border of the corresponding prefix of s. and so it ends in position i. and we only need to determine the position in which it starts in the text t. and to do that, we need to do a few computations. and we will see that this position is i minus two length of the pattern. however, if the prefix function in some position i is strictly less than the length of the pattern, then it means that the pattern doesn't end in that position in the string s. and that means that it doesn't end in the corresponding position in the text. and that means that we've found all the positions in which the pattern ends in the text. and so we've of course also found all the positions where it starts in the texts by subtracting two lines of pattern from each side position. so the codes for this algorithm is already pretty simple. we take as input pattern p and text t, we assemble string s by pattern with special symbol dollar and with the text, then we compute the prefix function of this long string s. we initialize the resulting list of positions where pattern occurs in the text. and we go through all the positions, i and s, which are to the right from the dollar sign. and if, at some position, we see that the value of the prefix function is the same as the length of the pattern, we just append i minus two length of the pattern to the result. and we return this resulting list of positions in the end. this algorithm is already pretty simple, and it works in time proportional to sum of the length of the pattern and the text. to prove that, we know that string s can be built in the time proportional to sum of the length of strings p and t. computing prefix function is done in the proportional time. and the four loop runs through part of the string. so it also runs in time proportional to sum of the length of the pattern and the text. in conclusion, you now know the knuth-morris-pratt algorithm for exact pattern matching. you can find all occurrences of the same pattern, in all of the text in time linear in terms of the length of the pattern and length of the text. you can also compute prefix function of any string in linear time. and you can go through all the borders of any string in the order of decreasing length using prefix function. and in the next lessons, we will learn how to build suffix array and suffix tree in time which will allow you to find many different patterns in the same text even faster than if you use algorithms like knuth-morris-pratt's. 
hi, in this lesson you will learn how to build a suffix array of a string in time and log n. suffix arrays are useful data structure that you already used in the previous modules but now you will learn how to build it really fast and first we'll recall what is a suffix array. so the problem of construction of a suffix array is very simple you're given a string and you need to sort all of it's suffixes in lexicographic order. however as we will soon see you won't need to actually compute all the suffixes and then solve them and output all of them because that will use too much both time and memory. you will just need to know in which order are those suffixes. and the suffixes themselves sorted in lexicographical order are only in our head. they're not stored anywhere in the problem. so we assume that the alphabet from which our strings are built are ordered, so that any two characters we can say which one of them is smaller. for example, in english we can order all the characters from a to z in a binary alphabet we just have zero and one and zero is less than one. by definition a string s is smaller than a different string t if either s is a prefix of t or s and t coincide from beginning up to some character and then the next character in s is smaller than the corresponding character in t. for example, if s is ab, and t is bc. then they don't coincide. but the first character is already different. and the character in s is a. and the character in t is b. a is less than b, so s is less than t. and in the second example, s and t coincide for the first two characters. and then the third character c is less than character d. so s is smaller than t. and in the third case, s is a prefix of t, but it is different from t, so s is smaller than t. and here is an example of suffix array. we have a string, s, and all suffixes ordered in lexicographic order are a, aa, and so on. so, here are exactly six suffixes because the length of string s is six, and so we have six different suffixes. we want to avoid this case when s is a prefix of t and that is why s is less than t because this case is different from all others and usually you just compare s and t from the first character and go the right until they differ. and then see which character is smaller. and this is a corner case when you go up to the end of the s, and then you see that there is nothing there and so that is why s is smaller. so to avoid using that rule at all, we will append a special character called dollar to the end of the string for which we'll build suffix array. so all the suffixes will have this dollar on the end. and now if initially some suffix was a prefix of another suffix. now it is just smaller by the usual rule, because as soon as it ends, and it is still coinciding with the prefix of the bigger suffix, the next character in the smaller one is taller, which is smaller than all other characters. and so we can determine by the usual rule that the smaller suffix is actually smaller. so how the suffix array changes in this case. we have initial string s, ababaa and we append dollar to the end and we get s prime. and now all the suffixes in the lexicographic order of string s prime are $, a$, aa$ and so on. and if we just remove dollar from the end of each of these strings we will have the suffix array of the initial string s, preceded by an empty string. so building a suffix array for s prime is giving us a suffix array for string s right after removing the empty string from it. what about storing the suffix array, suppose we have some algorithm to computed. how are we going to store? we want our suffix array to be stored in a linear memory, but the total length of all suffixes is some of arithmetic progression from one to the length of the string, which is quadratic. and so, it will take too much memory to store the suffix rate, even if we can compute it fast. so, we need to store only the order of the suffixes, not the suffixes themselves. and the order is just a permutation of numbers, and the number of those numbers is the length of the string. so that will take [inaudible] of your time. and so that is what we mean by suffix array. this order. so it will be just an array of positions. and all the positions are from 0 to length of the string minus 1. and the array has length equal to the length of the string. now let's look at an example of such order. so we have initially string s which is ababaa$. and we number all the suffixes by their starting positions. for example ababaa$ is 0 and abaa$ is 2. and we will start the order of the suffixes in an array order. so the smallest suffix is just $ because $ is smaller than any other characters. so the first suffix and the order is suffix number 6. and the next one is a$ which is number 5 and then aa$ which is number 4 then ababaa$ which is 2 then 0 then 3 and then 1. so this is the kind of array which we call suffix array. which is the order of all the suffixes of the initial string. and, we don't store the suffixes themselves. however, if we need to look at, for example, the third character of the second suffix in order we can first go into the area order, find out which suffix is number two, and that will be the first position of that suffix in the stream. and if we then add two to that we'll get character with position two in that suffix. so in theory we can look at any character of any suffix really efficiently although we don't store those suffixes directly. okay, now you know how to store the suffix array and how to manipulate it efficiently. but you probably wonder how to actually construct it. and you will learn that the next few videos. 
in this video you will learn in general how to efficiently construct suffix array; what are the steps and the substeps, and we will work out the details in the next few videos. but first, we need to go from suffixes to cyclic shifts. a cyclic shift of a string is a string we get if we write our initial string around the circle. and then start from any position and go through the whole circle. so for the initial string ababaa$, we can write down all the seven characters around the circle and then we will have the following cyclic shift. the initial string, the string babaa$a, which we'd gather if we start from character b and so on, moving around the circle. so these are the seven cyclic shifts of the initial string. and if we sort cyclic shifts instead of suffixes, let's see what happens, so these are all the cyclic shifts. let's suppose we somehow manage to sort them in lexicographic order. and then we remove all the characters after the first occurrence of $, in each of those cyclic shifts. what we get is actually the suffix array we wanted. all those strings in the third column are suffixes of s in the sorted order. this is actually always true, we have a lemma that after adding to the end of string s character $, which is smaller then all other characters in that string. sorting cycle shifts of the string and sorting suffixes of the stream is equivalent. we won't prove this lemma, we'll leave this as an exercise, we'll just use it in the following. apart form total full cyclic shifts, which go the full circle starting from some position in the circle or string s, we'll also need partial cyclic shifts. and those are basically sub strings of cyclic string s. so it can take any position in the cyclic string and go a few characters in the order in the clockwise order from there, and you will get a partial cyclic shift. for example, if we take the same initial string and want to build all cyclic shifts of length 4, you can start from the first character a and you will get abab, or we can start from b and get baba, and so on. we will have again, seven different cyclic shifts of length four ending in the $aba. now that we only go in the clockwise order, because this is the order when you try our cyclic string s as three a's written around the circle. so the general strategy for constructing a suffix array of the string s, is we start with a simple task, we sort all the single characters of the string s and those single characters are actually partial cyclic shifts of length one. so we assign l to one and now we have our base. we have sorted all the cyclic shifts of length l and then we will do iterations while l is still less than the length of the string, we will use our order of cyclic shifts of length l to sort the cyclic shifts of twice the length. and we will do that efficiently using the order of the current cyclic shifts of length l and thus will increase the length of the source suffixes twice. and then we will again increase it twice and so on and at some point l will be greater than or equal to the initial string s. and then we will have sorted the cyclic shifts of length greater than or equal to the length of the initial string and the order of those cyclic shifts is the same as the order of the full cyclic shifts of the initial string. so, this is the general strategy how we'll build the suffix array of the string. now let's look at an example of application of these general strategy in practice. we use the same string in all the examples. so this is our string, and we start with sorting the partial cyclic shifts of length 1, which is just single characters of the string. and the smallest is $, then go four instances of letter a and then two instances of letter b. and six is the position of dollar in the string and 0, 2, 4 and 5 are the positions of letters a and 1 and 3 are the positions of letters b. in this case, the order of the partial cyclic shifts is 6, 0, 2, 4, 5, 1 and 3. and on the next step we go from cyclic shifts of length 1 to cyclic shifts of length 2 and their ordered changes. first goes $a, because $ is the smallest character and then goes a$, and then aa, and then two instances of ab, and then two instances of ba. and the order in this case is already 6, 5, 4, 0, 2, 1, 3, and that's because dollar is in position 6. and then a, after which there is a $, is in position 5. and then a, after which there is an a, is in position 4, and so on. on the next step we go from cyclic shifts of length 2 to cyclic shifts of length 4 and we get $aba and so on up to baba. and the order changes again it is 6, 5, 4, 2, 0, 3, 1. and on the last step we go from these partial cyclic shifts of length 4 to partial cyclic shifts of length 8, which are already longer than the initial stream. and we start from $ababaa$ and end with babaa$ab. and the order actually didn't change in this case. and now we have the order of cyclic shifts of length a which is the same as the order of the full cyclic shifts of the string s, which is in turn the same as the order of the suffixes of this string. so if we remove everything after the first occurrence of $ in all those partial cyclic shifts, we'll get the order of the size fixes of the initial string s. and so order is now our suffix array. and in the next video we'll start working through the details of this general strategy. 
hi, in this video you'll learn the algorithms used in the initialization phase of the suffix array construction. and those are sorting of the single characters of the initial string and also competing equivalence classes of those characters. first we know that we assume the alphabet is finite. and we can use thus counting sort to compute order of the characters. you probably remember a counting sort from the first course, algorithm tool box. if you don't, i encourage you to go through those lectures once again, because we will use counting sort twice in the construction of the suffix array. in the initialization phase and in the transfer phase. here is the pseudo codes for the procedure sortcharacters, which takes string as an input and returns the order of the characters of that string as the output. we start with initializing that order as an array, of size equal to the length of the string. and we'll also need another array count, used in the counting sort. which will initialize with zeroes, and which has size equal to the size of the alphabet, not the size of the string, but the size of the alphabet. and then what you have in the following two for loops is just the familiar code for the recomputation phase of the counting sort. we count the number of occurrences of each of the characters in the string and then we also compute the partial sums of that array. and then in the end, we go from the right to left in our string s. we look at the character and we know that the partial sums array contains the position after the position where this character should be in the order. so we decrease the counter by one and we save our character position in the corresponding cell of the array order and in the end we just return the order of the character. so this is just an implementation of the counting sort as applied to characters of string has. and it works in time proportional to length of the stream plus size of the alphabet. because we know that this is the running time of the counting sort for length of s items, each of which can take only size of the alphabet, different values. and i need to note here that typically the size of the alphabet is small like for example, four letters, four streams in a genome, or 26 characters. if we are only working with the english words, or maybe alphanumeric characters. then there will be 26 small letters, 26 big letters, and 10 digits. but sometimes the alphabet can be very very big, such as unicode. and in this case counting sort might not be appropriate. if your string for example has only 1000 characters but those are all unique code, and the alphabet size is a few million character, then maybe you could sort the characters of this string in a more efficient way. apart from sorting the characters, we will also need additional information to make the following steps of the algorithm more efficient. and to do that, we introduce equivalence classes of the partial cyclic shift. so we denote by c with index i, partial cyclic shift of length l, where l is the current length of the cyclic shifts, which we already have sorted. and initially, we have sorted single characters. so l is equal to 1. and then on the further phases of the algorithm, l will increase from one to two, to four, and so on, twice in each iteration. so, some of the cyclic shifts can be equal to different cyclic shifts starting in different positions. ci can be equal to cj and then they should be in the same equivalence class. so to assign equivalence classes, we define the area class. and class of i is equal to the number of different cyclic shifts of length l that are strictly smaller that the cyclic shift starting at position i. so for different cyclic shifts which are equal, the value of class[i] and class[j] will be the same. because the same other cyclic shifts are smaller than these two equal cyclic shifts. and we'll need to compute this array class to increase the speed of the next phase. and before computing this array class, we assume that we have already sorted all the cyclic shifts of the current length l. so, how to actually compute the classes of the cyclic shifts when we already know their order. let's look at the example of sorted characters of the string. so we know already that the characters are sorted, and their order is 6, 0, 2, 5, 1, and 3. now let's assign classes. we want to assign class 0 to the smallest of the cyclic shifts of the current length. which is dollar, which is in position six. so, we write 0 in position six of the class. and we initially set up a class to be of length equal to the length of the string of course. the next, smallest cyclic shift is letter a and it is different from the previous smallest one which is dollar. so we need a new equivalence class for a. and so, we assign 1 to the equivalent class of a which is in position 0 in the initial string. so we assigned 1 to class of 0. and the next one is also a which is already in position two. but it is equal to the previous one. so we are saying the same equivalence class to it. so we'll write down 1 as the value of class of 2. the next one is also a. it is also equal to the previous one. so we assign 1 to class of 4. and the same one we do with class of 5. and the next one is b which is different again from the previous one and so we assign new class which is bigger by one which is two so we assign 2 to the value. value of class of one because b we find in position 1. so class of 1, we assign to value 2. and then the last one is also b it is equal to the previous one so again we assign 2 to class of 3. and now we know the classes of all the single character cyclic shifts. we know that the smallest one is dollar. and it is the only one that's equals 0. we know that 4 a's are in the equivalence class 1. and we know that 2 b's are in the equivalentce class 2. here is the pseudo code for the algorithm computecharclasses which takes its inputs string s, and the order of the characters. and computes the equivalence classes just for single character cyclic shifts of the string s, given their order. so we initialize the array class with just an area of size equal to the length of the string s and that will be our return value. also initialize the first value of this class array. but we don't initialize class of 0. we initialize class of order of 0, because order of 0 is the position in which the smallest character in the string occurs. and we initialize this character with class 0. so we assign class of order of 0 to 0 saying that the character in position order of 0 has equivalent class of 0. and then starting from second character in order up to the end, we go through the characters of string in order and we assign classes. to assign a class to a new character, we compare it with the previous one in the order. if it's different from the previous one means it's bigger because we go through them in the order. and so we need a new class. and so we just take the value of the previous class, increase it by one and assign to the class of this character. otherwise, if this character is the same as the previous one, we don't need to create a new class. we just assign the same class as the class of the previous character. and in that, we return the array with the classes. and we state that the running time of this algorithm is linear. which is obvious, because we only have initialization of the array. and then for loop, which runs for linear number of iterations with constant number actions performed in each iteration of the for loop. and that's all for the initialization phase of the suffix array construction. and in the next video, we'll learn the transition phase from the current length to twice the length of the cyclic shifts. 
hi, in this video you will learn how to implement the transition phase of the suffix array construction algorithm. in the transition phase, you assume that you have already sorted cyclic shifts of some length, l. and you know not only their order but also their equivalence classes, and you need to sort based on that cyclic shifts of length 2l. the main idea is the following. let's denote by ci cyclic shift of length l starting in position i, and by ci prime the doubled cyclic shift starting in i. that is, cyclic shift of length 2l, starting in position i. then, ci prime is equal to ci concatenated with ci + l. so we just take string ci, we take string ci + l, put it after string ci. and the total string of length 2l is equal to string ci prime. and so to compare ci prime with cj prime it's sufficient to separately compare ci with cj, and ci + l with cj + l. and we already know the order of the cyclic shifts of length l. so instead of comparing them directly, we can just look in the array of their order and determine which one is before which one. and that one is going to be smaller or the same as the other one. and also, we have the area with equivalence classes. and so we can determine whether two cyclic shifts of length l are really equal, or they're different, by looking in the array for equivalence classes and comparing their equivalence classes. so basically we can compare two cyclic shifts of length l in constant time. and that is why we can sort the doubled cyclic shifts faster. for example, if s is our initial string, ababaa$, and the current length l is 2. and position i is also 2, then ci is c2, is a cyclic shift starting in position 2, which is ab. ci + l is c2 + 2 which is c4, which is aa. and ci prime is equal to abaa. so this is the distinction between cyclic shifts of length l and 2l, and how we combine c2 and c4 to get c prime 2. so now we have to think about the following problem. we need to sort pairs of numbers basically, because each cyclic shift of length l corresponds to its number of position in the order of all cyclic shifts of length l. and we first need to sort by second element of pair, and then we stable sort by the first element of pair. and if we do these two steps, then our pairs will be sorted because they will be sorted by first element. and also inside the equal first elements it will be sorted by the second element, because it was initially sorted by the second element and the sort is stable. so we didn't break the order of the second element in the case when the first elements are the same. so, this is the idea for sorting pairs of objects. and let's look at this example. so let's suppose our current length is 2, and we already sorted all the cyclic shifts of length 2, and they are to the right in the sorted order. now for each of the cyclic shifts of length 2, let's look at the cyclic shift of length 4 which ends in this cyclic shift of length 2. so we take the two previous characters and add them to the left. so c4 prime and in c6, and also we'll look at c5. we take the two previous characters, and c3 prime ends in c5, and so on. so we go by two characters to the left from each of the cyclic shifts of length 2, and we get a set of cyclic shifts of length 4. now we have highlighted in yellow the first elements of the pairs, which are also cyclic shifts of length 2. those are not sorted, but we know their starting positions and we know what are the correct starting positions in the sorted order. so we can reorder this list of cyclic shifts of length 4 by the order of the first halves of the elements in this list using the known order. and we will need to do so in a stable sort fashion so that if, for example, c2 prime is before c0 prime, and the first half of c2 prime and c0 prime are the same. they need to stay in the same order in the final sort. and the same goes about c3 prime and c1 prime. they both start in ba. so when we sort by the first half, c3 prime has to stay before c1 prime. that's our requirement. so suppose we manage to sort the first halves in such a way, and we started the whole cyclic shifts of length 4 accordingly, what do we get? we actually get the sorted list of cyclic shifts of length 4. and of course for those which differ in the first half, it's obvious that they compare in the correct order. but for those which are the same in the first half, their second half is also sorted because it was sorted initially in the second column and we implemented a stable sort. so c2 prime is still before c0 prime, and c3 prime is still before c1 prime. so this is the idea. for sorting double cyclic shifts we take ci prime, which is a double cyclic shift starting in position i, and we know that there's a pair of ci and ci + l. and already know that the single cyclic shifts are already sorted. c order[0] is the smallest one, and c order |s|- 1 is the biggest one. now let's take the doubled cyclic shifts starting exactly l to the left, counter clockwise from those. and then c prime order of [0]- l, c prime order of [1]- l, and so on are sorted by second element of the pair of single cyclic shifts, sorted already. and when i decrease order of 0 by l, i mean decrease modulo the length of the string because this is a cyclic string. so we need to do a cyclic subtraction. so we get these c prime order [0]- l and so on sorted by the second element of a pair, and we need only a stable sort by first elements of pairs. but we know that counting sort is stable, and we know equivalence classes of single shifts for the counting sort. and we know that there are not many different single shifts. at most their number of different single shifts is length of string. so we can again use counting sort to sort those equivalence clusters of the single shifts. 
now lets consider the pseudocode for the procedure sortdoubled which will sort the doubled cycled shifts of length to l, given the string s. the current length l, the order of the current shifts of length l, and their equivalents classes in the array class. we'll start with initializing the array count with the zero array of size equal to the length of the string this is the standard array for counting sort, but as oppose to sort characters procedure it will sort not characters, but equivalents classes of cyclic shift of length l. and there are at most length of s difference including classes that's why we initialized the array with size length of the string. as opposed to sort characters where we initialized it with the size of the alphabet. we'll also need another array new order, which will store our answer. it will be the order of the sorted doubled cyclic shift. we initialize it with area of size, length of s. the next two four loops are standard four loops for the counting sort. when we first count the number of occurrences of each equivalence class of single cyclic shifts and then we compute the partial sums of that counting array and the last four loop of the counting sort needs to go through the array. we're going to sort from the end to the beginning and that is important for the sort to be stable. so, we need to go through the array of double cyclic shifts which are initially sorted by their second half in the reverse order. but you don't want to actually build this array of doubled cyclic shifts and then go through it in reverse order. we want to only build this array in our head and in the code, we just want to go through this array in the reverse order. so how to do that? remember that we have the array order, and if we go in the direct order of this array, we'll go through all the cyclic shifts of length l in increasing order. what we need instead is, first, to go not through cyclic shifts of length l, but through cyclic shifts of length to l which starts exactly l counter clockwise from those. and that is why we decrease order i by l and at length of the string and take modulo s, just because we're going through a circle. and we need to go downwards from the last i to the first i, because we need to go in the reverse order. so these two lines for i from length of s- 1 down to 0. and the last line which assigns variable starts to order i minus l plus length of string s module s. what they basically do is they go with variable start in the reverse order through the array of double cyclic shifts. sorted by their second half. so start goes through the starts of those double cyclic shifts in the reverse order. now, everything else that happens in this for loop is just regular counting sort. we take the class of this start position, which is the class of the first half of the corresponding doubled shift by which we want to sort. then we go and decrease the partial sum corresponding to that equivalence class in our counting array. and then we just put our start in the position which the counting sort prescribes to it. so these three lines from getting the clust of the start position decreasing the partial sum and assigning the start to the position counter of clause are the three standards lines of the counting sort. the complexity here is that start is going in the reverse order. through the array of double cyclic shifts sorted by their second half and that we instead of comparing characters or something else we compare equivalence classes of the single cyclic shift. so this is what this last forlob does. and in the end what we have is the array new order. which contains the double cyclic shifts which were initially sorted by their second half and then we sorted them by count and sort, by their first half. and so now they are sorted by the first half and the count and sort was stable. so in case when their first part, first half is the same, they're also sorted by the second half, because they were sorted by second half initially. so new order finally contains all the dabbled cyclic shifts in the correct order, in the sorted order. so this is the function that sorts all the doubled cyclic shifts. and the running time of this procedure is linear because this is basically the regular counting sort. although it sorts very complex objects, in practice in the code, it just sorts integers, the equivalent classes of the single cyclic shifts and it does so in the running time of the counting sort which runs in the time number of items plus number of different values. number of items is equal to length of the string and the number of different values of a clauses is also to smallest length of the stream. so all in all, those three for loops run in linear time. in the next video, we will talk about how to update the classes of those double cyclic shifts after they are sorted, and how to finally build the suffix array from scratch. 
hi, in this video you will learn how to update the equivalence classes of the double cyclic shifts after sorting them. and that will be the last step before we can actually present the whole algorithm for building the suffix array. so to update classes, we need to compare the pairs of single shifts which constitute the double cyclic shifts which we have just sorted. we have already sorted the pairs. so, we just need to go through them in order and compare each pair to the previous pair. if it's the same, then we need to assign it to the same class. if it's bigger, then we need to create a new class and assign it to this pair. to compare the pairs, we can compare them separately by first element and then by second element. of course the elements of the pairs are cyclic shifts and we don't want to compare them directly character by character. but, for that we already know their equivalent class is of the single cyclic shift, and we can just compare the equivalence classes instead of the cyclic shifts themselves. so we can compare any two pairs of single cyclic shifts in constant time. let's look at an example. s is our initial string and suppose we've already sorted the doubled cyclic shifts of length 2, and our initial cyclic shifts were of length 1. so we have our array class of the equivalence classes of the cyclic shifts of length 1, which is basically letters. and remember that this array has one element which is equal to 0 which corresponds to the dollar, and it is in position six. we have four elements which are equal to 1 which correspond to letters a in positions 0, 2, 4, and 5. and we have two elements which are equal to 2 which correspond to letters b in positions 1 and 3. so these are the equivalence classes of the single cyclic shifts. now for the double cyclic shifts we can write them down in the order because we've already sorted them. and we know the new order which is the order of the double cyclic shifts. they go 6, 5, 4, 0, 2, 1, 3. from $a to ba. and along with each doubled cyclic shift, we'll also write down the pair of the equivalence classes of its halves. for example, for $a, the equivalence class for dollar is 0 and the equivalence class for a is 1. so it corresponds to pair 0, 1. and for ab, for example, the equivalence class of a is 1, and equivalence class of b is 2. so we write down the pair 1, 2. these are the pairs of the equivalents classes of the single cyclic shifts. and now we need to compute the equivalence classes of the doubled cyclic shifts. and write them down into the array newclass. to do that, we go through the double cyclic shifts in the sorted order using array neworder. and we start from the first one, which is $a. and we write down value 0 for its class in position 6 because it is in position 6 as we see from the array neworder. then we'll proceed to the next doubled cyclic shift. and to assign class to it, we need to compare it to the previous one. and of course in this picture, we could compare directly these double cyclic shifts the previous one, and determine that it's different. but in practice, in general stage we don't want to do that. and instead of comparing the cyclic shift directly, we compare the pairs of numbers written to the right from them and we see that the pair 1, 0 is different from the pair 0, 1. and we do this comparison just by two comparisons of numbers instead of comparing full cyclic shifts. as far as this double cyclic shift is different, we need a new class for it. and we assign it to class 1. and write it into position 5 because this is the position for this double cyclic shift as we see from array neworder. now we proceed to the next one which is aa. we again compare it with the previous one, by pairs. 1, 1 on 1, 0 are different pairs, so we write down a new class again, class 2 in position 4, as given in the array neworder. then proceed to ab. it is again different. 1, 2 is different from 1, 1. so we create a new class 3 and put it in position 0 as given by array neworder. then we'll look at ab again and it is the same as the previous ab. as we see from pairs 1,2, 1,2 which are equal. so, we dont need to create a new class. we write down the same class 3 into position 2 as given by the array neworder. now look at ba, it is different from ab. so, we create new class 4. and then the second ba's of course are equal to the previous ba, so we write down 4 in position 3 as given by the neworder array. so this is how it works, updating of the classes. now let's look at the code. so the procedure updateclasses does exactly the same as we did in the example. it takes as input array neworder, the order of the double cyclic shifts. it also takes classes of the single cyclic shifts. and also it takes the length of the single cyclic shifts as inputs. and it will return the array with equivalent classes of the double cyclic shifts as a result. first we initialize variable n with the size of neworder. basically n will be equal to the length of the string but we don't have string as an input so we need variable to compute it's length. and we initialize the array newclass with an array of size n. and first we assign class 0 to the smallest double cyclic shift which is given by neworder of 0. and then we go through all the double cyclic shifts from position 1 to n-1, and we need to compare the double cyclic shift number i with the double cyclic shift number i-1. to do that, we first compute their starting positions. cur is the starting positions of the doubled cyclic shift number i, and prev is the position of the previous one. and also need to compute the positions of their middle of the position where their second half starts. so, we need to compare them half by half. so, cur and prev are the starting positions of the doubled cyclic shifts and mid and midprev are the starting positions of their second halves. to compute them, we just take the position clockwise to the right by l. so we add l and take everything modular n. which is the length of the string. and now we do just what we did in the example. we compare the classes of the current position and the previous position. and the classes of the starting positions of the second halfs. if at least one of the halfs is different, it means that the pair is different from the previous one. and we need to create a new class, increase the current class by 1 and assign to the current position. otherwise, the pair is the same as the previous one, and we don't need to create a new class. we just assign the same class to the current position. and we then return the array with the new classes of the double cyclic shifts. we state that the running time of this algorithm updateclasses is linear. and that's easy to prove because, well basically, we only have one for loop with linear number of iterations and constant time operations happening inside. 
now to the full algorithm for building the suffix array, finally. so procedure buildsuffixarray takes in only string s and returns the order of the cyclic shifts or of the suffixes of this string. we assume that s already has $ in the and, and $ is smaller than all the characters in the string. we start with sorting the characters, single character cyclic shifts of s, and save the result in the right order. and also compute the equivalence classes of those characters and save the result in the right class. and we initialize the current length as one. and then we have the main loop, which proceeds while the current length is still less, strictly less than the length of the string. if it is, then we first need to sort the double cyclic shifts of length to l. and then we also need to update their equivalence classes so that the next iteration can use them to again sort the doubled cyclic shifts. and then we just multiply l by 2, and go on in our while loop until we get to the station when l is more than or equal to the length of s. and by the time array order will contain the correct order of all the full cyclic shifts of the string s, which is the same as the correct order of all the suffixes of the string s if it has a $ on the end. and the running time of buildsuffixarray procedure is length of s times logarithm of that, plus size of the alphabet. so the size of the alphabet is because of the counting sort of characters in the beginning. because we're sorted them in time proportional to length of the string plus size of the alphabet. but if we wanted, we could just sort them in time s log s without using the count and sort to sort the characters. so we could actually remove the plus alphabet from the buildsuffixarray asymptotics. although in practice usually the alphabet is very small, so we don't need to do that. and using counting sorts is better than actually sorting the characters in s log s. and also compute the classes of the characters in linear time after that. in each while loop iteration, we do both sorting of the double cyclic shifts and update their clusters in linear time. and we have only logarithmic number of iterations, because l is doubled every iteration, and as soon as it gets at least s or more, we stop. so it's on a logarithmic number of iterations, so all in all, the while loop runs for s log s. and adding to that, the initialization cost, we get s log s plus size of the alphabet. so now you finally can build suffix array of a string s in time length of s times logarithm of that using linear mode mode of memory. and you can do not only that, but you can also sort all cyclic shifts of a string in the same time. and you know that suffix array handles many fast operations with the string. and also, in the next lesson you will learn to build suffix tree of the string from its suffix array in linear time. and that combined will give you an algorithm to build suffix tree in time s log s. and of course you already knew how to build a suffix tree in quadratic time, but s log s is much, much better than that. so you will learn that in the next lesson. 
hi, in this lesson, you will learn how to build suffix tree of a string given its suffix array in linear time. at first we'll explore some connections between suffix array and suffix tree, and then we'll learn to compute some additional information to the suffix array. and then finally we will use suffix array and the traditional information called lcp array to build a suffix tree. first recall the problem. it's very simple. you're given a string s and you need to compute its suffix tree. and you already know how to do that actually. but the algorithm you know works in square time, and so it will work only for short strings, maybe up to 1,000 or 10,000 characters. and if you want to build suffix tree for strings of length of millions or billions, you will need a much faster algorithm. and after you learn this lesson, you will know how to build suffix tree in time, length of string times logarithm of these lengths, because you can build suffix array in this time and then construct suffix tree from the suffix array in linear time. so the general plan is to construct suffix array in time as log s, then compute some additional information called lcp array in the linear time. and then given both suffix array and this additional information, construct the suffix tree in linear time. first, let's explore how suffix array and suffix tree are connected. here we have a string s, ababaa$. and again, we insert $ in the nth which is smaller than any of the characters of the string both to build suffix array and then to build suffix tree from it. and on the left, we have in the column. all the suffixes of the string s sorted in lexicographic order. so that is basically the suffix array. and on the right, we have the fully built suffix tree of the string, which is already compressed so that you see that on the edges we have not single letters but whole sub strings of string s. and by the way, interesting question is how do we store suffix tree? we shouldn't, of course, store the sub strings that are written on the edges directly because that could lead to quadratic memory usage and we want linear memory usage. so instead of storing the sub strings themselves, we just store the index of the start, and index of the end index of the corresponding sub string. so for each edge, we store two indexes, the start of that edge in the string and end of that edge in the string. and to store the nodes, we just store, for example, an array of pointers to the children nodes. and that array is indexed by the first character of the edge outgoing from this node into the child. and we can store the information about the edge itself in the node for which this edge is going from its parent. this is one of the ways to store everything but you may organize everything in another way. the important thing is that you shouldn't store edges as substrings. so what corresponds in the suffix tree to the suffix array elements? let's take the first element of the suffix array. actually, it is corresponding to suffix in the string s and that is corresponding to a leaf in the suffix tree and also to the path from a root vertex to the corresponding leaf vertex in the tree. so the first element of the suffix array corresponds to this route highlighted in blue, and then if we go to the next element of the suffix array, we get another route from route vertex to the leaf number 1. and then if we go to the next element, we get route from the route vertex to the leaf number 2. and note that the indexes of the leaves, and the indexes of the suffixes are just in the sorted order. so, those are not positions in the string s. those are numbers of the suffixes in the increasing order, from 0 to number of the suffixes minus 1. so, each of the elements of the suffix array corresponds to some path from root to leaf in the suffix tree, that is what we know. that is unfortunately not yet sufficient to build the tree from the suffix array because there are many ways to create some paths from root to different nodes. which corresponds to suffixes of the suffix array. so we will need some additional properties. and this additional property we will need is call longest common prefix, or often it is just said as lcp. so lcp of two strings s and t is the longest such string u, that it is both a prefix of s and of t. and we denote by big lcp(s, t), the function which returns the length of the lcp of strings s and t. for example, lcp("ababc" and "abc") is 2 because their longest common prefix is ab, and it's length is 2. and lcp("a","b") = 0 because their longest common prefix is empty. now let's look again at the suffix array and suffix tree and also take into account lcp between the neighboring elements of the suffix array. so when we look at the first element of the suffix array, we just have an edge corresponding to it in the suffix tree. and when we have the next element, we have a path from root to another vertex. but if we compute the longest common prefix of this element of the suffix array with the previous element of the suffix array, we'll see that this longest common prefix is empty. and that corresponds to empty intersection, between the previous path and the new path. the only common node is the root node, and they don't have any edges in the intersection. however, if we proceed to the next suffix, it has a common prefix of length 1 with the previous suffix. and it corresponds to the common path in the tree highlighted in yellow, starting in the root node and going through edge a to another node which is still a common node for the current suffix and the previous one. so this is how lcp corresponds to the tree. if you go to the next suffix, it again has the same longest common prefix with the previous suffix. so we have the same common path from root to the next node by edge a and then the part of the path is different from the current suffix and from the previous one. if we go to the next suffix, their longest common prefix with the previous one is even longer, and so the common part of the path is now consisting of three nodes and two edges. a root node, next node by edge a and next node by edge ba. and the rest of the path is unique to the current suffix. if we go to the next suffix, it again doesn't have any common prefix with the previous one so the only common in the path is the root node. and the next suffix has longest common prefix of ba, and that's why we see this path from root to another node via edge ba. and this is the common part of the path for the current suffix and the previous one. so we see that basically all the nodes but the leaves are corresponding to the longest common prefix of the neighboring suffixes in the suffix array. and this is how we can actually build the suffix tree by first computing the longest common prefixes of the neighboring elements in the suffix array. and then building those internal nodes. and then, in the way of that, we will also build the leaves as the ending points of the paths corresponding to the suffixes from the suffix array. so this is the plan of what we'll do. but first, we'll need to compute those longest common prefixes for the elements of the suffix array. 
so we define lcp array and let's consider suffix array a of string s in the raw form that is that a[0] is a suffix, a[1] is a suffix and so on up to a[s-1], all those are suffixes of s in lexicographic order. then lcp array of string s is the array lcp small of size length of s-1. it contains fewer elements than the suffix array and then the string itself. besides that, each element lcp[i] is just equal to the longest common prefix length. between a[i] and a[i+1]. so it's the longest common prefix of two neighboring elements in the suffix array and what we want is to compute the values of this array. for example if we have our string ababaa$. then, we first compute the longest common prefix of $, and a$, which is 0. then, we compute the longest common prefix of a$ and aa$, which is a of length 1. then it's again a of length 1. then it's aba of length 3. then it's empty. and then it's ba of length 2. so the lcp array for this string is 0, 1,1, 3, 0, 2. and the central lcp array property which will enable us to compute it fast is that for any end assist i and j in the suffix array, where i is less than j. the longest common prefix between a[i] and a[j] which are far from each other, is not bigger than the lcp of i, which is basically the longest common prefix of i and the next element. so what i'm saying with this lemma is that the lcp of two neighboring elements is always at least as big as the lcp of the first one of them with any of the next elements. and the same goes the other way. the lcp of two neighboring elements is at least the same as lcp of the second of them with any of the previous ones. and to see that let's look at some hypothetical example that we have some long suffix array and elements i and i+1 are here. and also there is some element j farther in the suffix array. and we see that really the common prefix of suffixes i and i+1 is pretty long. it's not so long with suffix j, it's only of length two. but this example doesn't yet prove anything. so maybe for some other situation with a suffix number i+1, it could be solved that the common prefix of i and j will be bigger than common prefix of i and i+1. so let's suppose that, and we don't know what is suffix i + 1, so we just replace it with many x. x is an unknown letter. we know that the lcp of i and j is equal to 2. so let's consider k which is the length of the longest common prefix of a[i] and a[i + 1]. and we suppose that it is smaller than 2 in this case. so how can that be? one variant is if a[i + 1] is shorter than 2, and then a[i + 1] is actually a prefix of a[i]. but in this case, a[i+1] is smaller than ai which contradicts the property of the suffix array. that the suffixes are sorted. and if suffix i+1 is sufficiently long then it follows that it's kth character is different from the kth character of both ith suffix and jth suffix. and in this case there are again two cases. in the first case is that this character in suffix i+1 is bigger than the corresponding one in strings i and j. but from this it immediately follows that suffix i+1 is bigger than suffix j which contradicts the suffix array properties, so it is impossible. and another case is that this character is less than the corresponding character in both strings i and j. but in this case it immediately follows that a[i] is bigger than a[i + 1] which again contradicts the suffix array property. so in all cases we found the contradiction. and so, it is not possible that the longest common prefix of i and j is bigger than the longest common prefix of i and i+1. and we proved the lcp array property because for this symmetric case, the proof is a null x. now how do we compute the lcp array? one variant is to go for each i, compare a[i] and a[i+1] character by character and compute the lcp directly. but this will work in linear time for each i. and in total it will be length of the string squared. and we want to compute everything in linear time. so how to do this faster, and you will learn that in the next video. 
hi, in this video, you will learn how to compute lcp array in a linear time. and the main idea is the following. we'll start by computing lcp of the first two smallest suffixes directly by comparing them character by character. but then, on each next iteration, instead of going to next pair of suffixes in the suffix array, we move the smaller suffix in the stream one position to the right and then compute its lcp with the next suffix in the suffix array. so we won't go in good order in the suffix array, we will go in some strange order. but this order is good. it will show that if we go in this order through the smaller suffixes, then the lcp of the smaller suffix and the next suffix will decrease by, at most, one on each duration. and so, we will know that most of the characters of the two new suffixes, we have already compared many of them, and we don't need to compare them again. we'll start from there, and we will convert the next character and the next one directly, and the lcp itself will be very easy to compute, because we will still do that by direct comparison of characters with characters. we will just avoid some of the comparisons, because we will know from the previous durations that the common prefix has at least such length, and we don't need to compare the first such many characters. and in the end, it turns out this will work in linear time. so we will denote by a and of pi the suffix, starting in the next position in the stream, after suffix ai, in the suffix array. so the next one in the suffix array will be ai plus one, but we won't know that one, but the one which starts in the string one position to the right. so here's an example. we have a string, which is ababdabc, and the smallest suffix is ababdabc. the whole string is actually the smallest suffix, and the next one in the suffix array is abc. and their longest common prefix is ab, and here we see it. and we compute this longest common prefix, which is equal to two, by the length, directly. and then, we will know that if we move to the next two suffixes in the stream to the next one after a zero and the next one after a one, those will both start with letter b. so the last of the common prefix decreased by, at most, one, cuzboth suffixes just moved one position to the right, and we cut away only one position of the longest common prefix. of course, these two suffixes are not, probably, two neighboring suffixes in the suffix array in the general situation. it might be so that there is some suffix between them. but because of the property of the lcp array, the longest common prefix of the smaller suffix with the next suffix in the suffix array will be even bigger or at least the same as its longest common prefix with the next suffix in the string, because the next suffix in the string is bigger than the smaller one. and by the property of lcp array, the common prefix with the next element is the same or bigger than the common prefix with some element farther away in the suffix array. so now, we can move to the next element from the smaller one and then take the next one to it in the suffix array, and compute their l speed directly but remembering that the first several characters, we don't need to compare exactly those, which are in the lcp of the previous pair. so, this is basically the algorithm. we compute lcp(a[0] and a[1]) directly and save it's value as variable lcp. and then on each iteration, first suffix in the pair, which is smaller, goes to the next in the string, then we find which one is the next in the suffix array in the order, and we compute their longest common prefix knowing that we don't need to compare the first lcp- 1 characters. and then on each comparison, if it's successful, we increase lcp, and we go to the next comparison, and we repeat that until we feel the whole lcp array. and the idea is when we make each comparison, we increase lcp. and when we move to the next pair, we decrease lcp by at most one. and this is why we cannot do too many iterations. so the lemma states that this algorithm computes lcp array in linear time. and to prove that is now easy, because each comparison, we do between one suffix and another. it either finishes the iteration, and number of such comparisons is at most number of iterations, and we have at most length of the string iterations, or if it is a successful comparison, then it increases the current value of variable lcp. and the variable lcp cannot be bigger than the length of the string in any moment, and at each duration, lcp decreases by at most one. so if we start from zero, we cannot go higher than length of s, and at each iteration, we decrease by at most one. we cannot do more than linear time of increasing lcp, and so we cannot do more than linear number of comparisons. and this is why this algorithm works in linear time. and as soon as we can now compute the lcp array, we can proceed in the next video to construct the suffix three given the suffix array and the lcp array. 
hi. in this lecture you will finally learn how to build suffix tree of a string, given its suffix array and the lcp array. and we will do everything on this example. we have our string, we have our sorted suffixes in the order, and we start building the tree from just the root vertex. we consider the first suffix, and we create a leaf in the tree corresponding to the suffix. and we connect it with edge to the root node. when we go to the next suffix, we see that the longest common prefix of the suffix with the previous one is empty, we know that from the array. and so, the only common part of the path from root to the new leaf and to the previous leaf is the root note. so we don't need to create any new nodes, other than the leaf node for the new suffix, and we create an edge from root directly to this new node. and we write down the corresponding suffix on this edge. when we go to the next suffix however, there is already a common prefix of length one, which is a, so we'll need to create a new node in the middle of this last created edge. and we'll divide it into edges with letter a and with letter $. and this is what happens. so now we have a new node which is connected with letter a to the root and from which, there are two outgoing edges, one with the letter $, and another with string a$, corresponding to the last considered suffix. now when we consider the next suffix, the longest common prefix with the previous one is again just a. so we don't need to create a new node other than the leaf node. so we create a leaf node for the new suffix abaa$ and we connect it with the yellow node which corresponds to the longest common suffix prefix with the previous suffix, with an edge where we write baa$, everything that is left in the suffix. when we consider the next suffix the longest common prefix with the current one is of length 3, so we'll need to subdivide the edge again. and this is what we get. the yellow node is the node corresponding to the longest common prefix aba of this suffix and the previous one. and then we create a new leaf node for the suffix ababaa$ and the node with number 4 corresponding to position 4 in the suffix array. then we can consider the suffix baa$, it doesn't have any common prefix with the previous ones, so it starts from the root and goes into the new node. and then the next one has a common prefix with it, ba. so we create a new node after ba, subdivide the edge, and create a new leaf node for the last suffix with edge baa$. so this is basically what is going to happen. how do we implement this creation of new nodes and subdividing of edges? the following way. when we build an edge to the leaf for some suffix, we go and sit in that leaf node and when we consider the next suffix, we go up from that node using the pointer to the parent node in the tree. until we are high enough, so that the longest common prefix is below us. as soon as we jumped to the longest common prefix or higher, we stop. how do we whether we're higher than the longest common prefix or not? we need to also store the depth in the nodes and the depth is the number of characters on the path from the root to this node. this is easy to keep during the suffix tree construction, so i'll just assume we have it. so we go up from the leaf until we are in the longest common prefix or above it. if we're exactly in the longest common prefix with the previous suffix, we don't need to build any new nodes. we just build a leaf for the new suffix, and connect it with the current node. however, if we are higher than the longest common prefix, then we'll need to create a new node in the middle of the current edge, going down from our node in the direction of the longest common prefix. so we divide our edge in the middle. we create a new node which is corresponding to the longest common prefix with the previous suffix. and we create a new leaf node as usual for the suffix and connect it with its new note. so this is the whole algorithm. to repeat. to build the suffix tree from scratch, we first build suffix array and then build lcp array from that. we start building the tree from only root vertex. we grow the first edge for the first suffix just from the root. and then for each next suffix, we're sitting in the leaf we just built for the previous suffix, we go up from the leaf until we jump higher than lcp with previous suffix. and then we build a new edge and a new leaf for the new suffix and maybe, depending on where we are in the tree, we need to subdivide the current edge. i state that this algorithm runs in linear time and that is pretty easy to see. we know that the total number of edges in the suffix tree is linear from the previous modules. and during this process for each edge we go at most once down when we do this edge, and then we go at most once up when we go to find the l sub e. and then we go down, we don't go through the same edge again. because we've already been there. so, for each edge we go through this edge at most twice, disregarding maybe additional one per iteration, and we have only linear number of iterations of the whole algorithm. and the time to create a new edge or a new leaf or subdivide an existing edge's constant, so in total this algorithm works in linear time. so now you are fully equipped to build suffix structures, such as suffix array and suffix tree. and you can do that pretty fast in time slogs where s is the length of the string and that's cool, because you can solve very complex problems using these data structures which are basically impossible to solve without them. and some of them will be in the programming assignment. so see you there. 
hello everybody, welcome to our course on advanced algorithms and complexity. as the first unit in this course, we're going to talk about network flow algorithms and, in particular, in this very first lecture, we're just going to give an example of a problem to give you a feel for what types of things we're going to be talking about in this unit. so to set things up, suppose that you're a disaster relief manager and you're trying to, among other things, you have this city and you want to know how quickly could it be evacuated in the case of an emergency. well, to do this you have to look at the roads leading out of the city, and so you see that there's the main highway out that will handle 5000 cars an hour. of course, this isn't the only road leading out of the city. there are some secondary highways that can each handle 2000 cars an hour. of course, things are a little bit more complicated than that. these other roads, they each bifurcate into two halves. each of these halves can handle 1000 cars an hour. so you're maybe okay so far. but it turns out that two of those halves merged together a little wise down into just a single road that can only handle 1000 cars now. and so you can imagine that in real life, there are many, many, many more roads than this in their full road network, but this is a toy example. and we'd like to know, given that this is the network, how quickly can we evacuate? well, it's not hard to start playing around with this. we can take 5000 cars an hour and send them out along the main road. we can send another thousand cars an hour along this northern path here. another thousand cars an hour can go along up on the northern road and then split off and join in on the merged road, and finally, another thousand cars and hour can go off on the third highway. now, putting this all together, we have a total of 8000 cars an hour that we can evacuate, but we'd like to know, is this the best that we can do or can you do better? well, if you play around with this a little bit, there's no obvious way to make an improvement, and you might suspect that this is the best you can do, and in fact, you'd be correct. one way to show this is that if you draw a river, suppose that there was a river where this blue line is on the diagram, you'll note that there are only four bridges that cross that river. and the total capacity of all the bridges is only 8000 cars an hour. so if only 8000 cars an hour can cross this river and you need to cross the river to get out of the city, only 8000 cars an hour can evacuate the city. and that proves that this plan that we have for evacuation is really the best you can do. it's bottlenecked at the river, you can't do any faster. so network flow problems are problems that will allow us to study things like this problem. and this is what we're going to be talking about in this unit. and next lecture, what we're going to do is we're going to take a little bit of a more careful look at this problem. we're going to come up with a formal framework to discuss this kind of issue, and then we're going to discuss some examples of where these sorts of problems might show up in real life. so that's what you have to look forward to in the next lecture. i hope to see you then. 
hello everybody. welcome back to our network flows unit. today we're going to be talking a little bit about sort of formal definitions and sort of getting a concrete definition of our problem and then some examples of sort of what sorts of problems fall into this category. so, remember last time we discussed the disaster management problem. today what we're going to do is we're going to have a sort of formal framework for talking about this problem and some similar problems. so to begin with, we're going to want to define what a network is. a network should be thought of like this network of roads that we saw in this previous example. so a network is a directed graph g representing all the roads, but with some additional information. each edge e is assigned a positive real number called its capacity. this is how much traffic can be handled by that road. additionally one or more vertexes labeled as a source, this is the city, this is the place where the traffic is coming from. and then one or more vertex is labelled a sink, which is sort of the edges of the graph where everything is going to. so flow goes from sources to sinks along these edges that each have a capacity. so with the example from last time, we have this network of roads, we can turn it into a graph where the city is a node, each of the intersection gives us another vertex, and then we have some extra vertices sort of at the ends where the cars escape to. the city itself is the source of our flow. the four exits give us sinks, labeled t here. and each of the edges has a capacity which says how many cars an hour can drive along that. fair enough. the next thing we want to be able to discuss are flows. we want to be able to talk about flows of traffic through this network, and talk about what's a valid flow and what isn't. and before we get into anymore detail on that, in the last example we actually talked about sort of exactly which routes different cars take. you know a thousand cars traveled this route and another thousand traveled this route and so on and so forth. but this is a little bit more complicated than we want to do. rather than talking about where each individual car goes, we're instead just going to concern ourselves with the total number of cars, the total amount of flow along each edge. so in particular, we're just going to figure out how much flow goes along each edge. but this, of course, needs to satisfy a couple of conditions. it can't just be any number we like. the first of these is rate limitation. for any edge e, for one thing the flow along that edge needs to be non-negative. you can't send a negative number of cars along this road. and secondly the flow needs to be at most the capacity of the edge. you can't run more cars along the road than the total capacity of the road fits. the second thing is a little more subtle, it's conservation of flow. the idea here is that if a car drives into an intersection, then eventually it needs to drive out of the intersection. and what this says is that at any vertex except for sources where flow gets created and sinks where flow gets destroyed it needs to be the case that the total flow of cars into that vertex is the same as the total flow coming out of that vertex. so for every vertex, the sum over all edges pointing into the vertex of the flow is the same as the sum over all edges going out of that vertex of the flow along that edge. and so putting this together formally, we define a flow on a network is just an assignment of a real number, f sub e, to each edge e. such that these two conditions hold for each edge e the flow on that edge is between 0 and the capacity of the edge and for all vertices v except for sources and sinks the total flow into that vertex is the same as the total flow out of that vertex. so that's what a flow is. so for example, in our example from last time, we have all of these cars traveling in various directions, and on each road, we can compute the total amount of flow, the total number of cars flowing along that road. so there are 5,000 along the main highway, the northern highway going up has 2,000 cars an hour, half of them going one way, half of them spooling off the other way. and for each road, you just label how many cars an hour are traveling along that road. and if you look at this for a while, you can actually determine that yes, these satisfy the properties that we want. no road has more flow than its capacity and flow is conserved at each of these three vertices that aren't sources or sinks. so to make sure that we're all on the same page here, we have a network listed up above, and then three possible assignments of flow. which of these three are valid flows for the given network? well, you look at these for a while, you compare to the definitions, and you'll find out that c is the only valid flow. a has the problem that it doesn't conserve flow at the denoted vertex. it has six units of flow going into that vertex but seven units of flow coming out. b conserves flow everywhere, but the edge that's highlighted has six units of flow whereas the capacity is five. on the other hand if you look at diagram c, everything works out. flow is conserved, nothing goes above capacity, it's all great. okay, so this is what a network flow is, but network flows are actually very useful to study because they actually model a large number of real life phenomena. we've already talked about flows of goods or people along a transportation network which fit very cleanly into this model, but you can also look at flows of electricity along a power grid or flows of water through pipes or even flows of information through a communications network. all of these are going to be examples of network flows in which this sort of formalism that we've developed will be useful for analyzing problems. now, what types of problems are we going to be studying? well, the big one that you want to know is the size of a flow. you want to really know how much stuff is actually flowing. how many cars are actually evacuating the city? how many can we get to evacuate the city? and for this, we need to define the size of a flow. and it turns out this can be computed by looking only at the sources. and the idea is that any flow, it gets created at the source, they sort of drive until they hit a sink and then they go away. but if we could just measure how much flow is coming out of the sources, that will tell us how much there is in total. so given the flow after we defined its size to be the sum of all edges that leave a source of the flow coming out of that source minus the sum over all edges going into a source of the total flow through those edges. and so it's the total flow going out of source minus the total flow going into sources, that's the size of the flow. now, it turns out you can equally well compute this by looking only at sinks. the lemma says the size of the flow is equal to the sum of flow going into a sink minus the sum of flow going out of a sink. and the argument here is pretty nice, we'll be seeing similar things a lot. so the thing to note is that if you take the sum of all vertices of the total flow going into that vertex minus the total flow going out of that vertex, that's actually zero because each edge, some flow leaves the vertex but then goes into another vertex, so the two terms cancel out. on the other hand, if we take vertices that aren't sources or sinks, but conservation of flow, that inner term is zero. so this is the same as the sum only over sources and sinks of the total flow into that vertex, minus the total flow out of that vertex. now if we look at the sum only over sources of the flow into the vortex minus the flow out of the vortex, that's minus the size of the flow. and the other term is just the flow into sinks minus the flow out of sinks. and since the sum is zero, the total flow into a sink minus total flow out of a sink is the same as the size of the flow which what we were trying to prove. okay so that's what the size of the flow is. the big problem that we're going to be trying to solve, and we'll really discussing how do you solve this problem for the next several lectures, is how much flow can you fit through a network? formally this is called the maxflow problem. the input should be a network g, so a graph with these capacities and some designated sources and sinks, and the output should be a flow f for the graph g such that the size of the flow f is as large as possible. and this is the problem that we're going to be spending the next several lectures on. so, come back, next lecture we'll start talking about some of the tools that will be useful in designing these algorithms. so i'll see you then. 
hello everybody, welcome back to our network flows unit. today we're going to be talking about some tools that are actually very useful. a tool called the residual network for coming up with new flows, or adding a little bit of flow to an existing flow. so remember last time we formally defined these things. we defined what a network was, and we defined the flow on this network was, and then defined what the maxflow problem, which is the one we're working towards solving. there's a very basic technique to solving maxflow, that is basically what we are going to be working towards for the next bunch of lectures. and the idea is to build up your flow a little bit at a time, and this is really what we did in the original example in the first lecture, where we routed a bunch of cars along one road, and then routed some more cars along another road and so on and so forth, and built up the final flow as the sum of a bunch of little flows. so how do we this in practice? well suppose that we have the following network. here, all the edges have capacities 1, for simplicity, and what we can do is, we can just add flows of those together a little bit at a time. we can know, hey, we can send the unit of flow along this top path, and if we just have a unit of flow on each of these edges, everything balances. but after we do that, we can send another unit of flow along the bottom path, and then another unit of flow along the middle. and once we've done this, we now have a maximum flow, but we built it up in nice convenient little pieces. okay, so let's consider another example, this one's actually a little bit simpler. we have our network here, the maximum flow is 2, as we've shown here, but we're going to try and add flow increment. so let's start by adding flow along this path, it's a perfectly valid path, we can route a unit of flow through it. and now we want to try to add our second unit of flow and there's a bit of a problem. we can't readily add a second unit if we've already used up these piece edges, the remaining edges just don't connect to each other we can't actually get the flow to work. now it turns out this away around this, which of course there is since the maximum flow is 2, and it involves with the rounding flow along with this blue path, which is a little bit weird since we can not actually do that. we can't actually send flow down along the middle edge since there was not an edge there, but if you think about it in the right way, you can think of sending flow down this middle edge as cancelling out the flow that we currently send in the up direction. if the flow going up and the flow going down are thought to cancel each other, then once we add these two flows together, we just get this flow, which is perfectly valid, because there's no flow running along the middle edge. and so,the moral of the story is that if you want to be able to appropriately add your little bit of flow, sometimes it's not enough to just add flow along new edges but sometimes you also have to let your flow cancel flow along existing edges. so given a network g and a flow f what we're going to do is construct what's called the residual network, g sub f, and this is a new network that represents the places where flow can be added to f. but this includes not just edges where there's more room for extra flow to go along that edge, but also places where we could cancel out existing flows. so to define this formally for each edge e of our graph our residual graph, our residual network is going to have edges, well it's going to have an edge along e. and the capacity is going to be the capacity of the edge, the original capacity of the edge, minus the flow along that edge. and the point is this is the amount of remaining capacity that we have of course, if the flow is equal to the capacity we can ignore this edge because it would have no capacity. we also need to have an edge opposite e with capacity equal to the flow along e, because this will represent the amount of flow that we can cancel going in the other direction. so for example, up top we have the following net, we have a network, and it has a flow assigned to it, there are various units of flow assigned to various edges. down below will give us the residual network, so if you look at the edge on the left, for example, well we used up all five units of its flow. so what does this mean? well, we've got no edge left pointing down, because there's no extra flow that we can push in that direction. however, we do have a new edge pointing up with five units of flow, saying there are five units of flow going the other way that we might cancel out later. if you look at the top edge we use five out of seven total units of flow, so there's two units of flow left. so there's this one edge up top with two units of flow, and then there's this additional edge going the opposite direction representing the five units of flow that can be still be cancelled. and so we do that also for all the other edges of the graph, and this gives us the residual network. so if we look at what this does to our previous example, we have this graph, we route flow like this. now we can't add to it directly, but if you look at the residual network, we're actually going to have an edge going back in the opposite direction from each of these. and in this residual graph there is actually a path that supports user flow, it involves this middle edge that says that we're cancelling out flow along the middle. okay, so given a network g and a flow f, any flow g on the residual graph it turns out can be added to f to get a new flow on the original graph. so, the point is that if you have flow along this edge in the same direction that you had in the original graph, that's saying, you should add that much flow along that edge. however, if you got flow sort of in one of these opposite direction pointing edges, that's saying that that much stuff should be cancelled from the flow that you had before along that edge. so just to make it clear, let's look at this problem. so we have a network with a flow on it, f this upper left corner. down below it we show what the residual network is corresponding to that flow. now in the upper right we have a flow, little g, for the residual network. and the question is if we want to compute the sum of the flows, f plus g, what is the flow of f plus g along this highlighted edge from source to. well, what do we get? the original flow along that edge was two, we need to add the flow of g along that same edge. that's four extra units of flow and we need to subtract off the flow in the canceling direction, so that's plus four minus two, that's a total of four units of flow from s to t in the residual. and you can compute the other edges and yes, f + g does give you a valid flow for the original map work. in fact, the theorem is as follows, if you have a graph g and a flow f and then have any flow you like g on the residual map work, a few things happen. firstly, f + g is always a flow on the original network which is nice. if you want to look at the size of the flow, the size of f + g is just the size of f plus the size of g. and finally and importantly, any flow on the original network you can always get by finding some appropriate residual flow like adding it to f. now the proof of this is actually not that hard, if you want to look at conservation of flow, conservation of flow of f, and conservation of flow of g, if you combine them,imply that you have conservation of flow on f + g. next if you want to look at the total flow f + g sends through an edge, well the flow it sends through edge e is equal to at most the flow of f along that edge plus the flow of g along that edge, which is at most the flow of f plus the capacity of that edge in the residual. but that capacity is just the original capacity minus the flow that you sent from f and so that's just the capacity of our original network. on the other hand, you can't end up with negative flow along an edge because g isn't allowed to cancel more flow along that edge than you had originally. and so, putting this together, f + g has to be a flow. next, if you look at the flow of f plus g out of a source, this can be shown to be the flow of f out of that source plus the flow of g out of that source. so combining this, the sum of the size, the size of the sum of the flows is the sum of the sizes. and finally if you're given any flow h for our original network, it's not hard to construct a g that's somehow h- f, that's a flow on the residual graph. and so you can then write as h as f + g for some appropriate flow on the residual graph. so, in summary, flows on the residual network, so it exactly correspond to ways to add flow to our original f. and this is very useful because our big picture idea for our algorithm is going to be start with some flow, and then add little bits of flow, and the residual graph will tell us exactly how we can add little bits of flow. so that is all we have for this lecture, come back next time and we will talk a little bit about how to show that we actually have the best flow when we do. 
hello everybody. welcome back to our unit on flows and networks. today we're going to be talking a little bit about sort of how to bound the size of our flows. and in particular, i mean, we've got this problem. in order to find maxflows, we're going to need a way to verify the flows that we have are actually optimal. so, in ,particular what we're going to do is we're going to need techniques for bounding the size of a maximum flow. and it turns out we actually had a way to do this. so, in our original example, we said we have the city we're trying to evacuate. and if we have a river at a particular location, if you just look at the total amount of capacity of all the roads that cross the river, this gave us an upper bound on the rate at which we could evacuate the city. because everyone evacuating the city needs to cross the river at some point. and this is going to be our basic idea for bounding maxflows. the idea is, we want to find a bottleneck in the flow. we want to sort of find some region where in order to cross from one side of this bottleneck to the other, there's not a lot of capacity. and the total capacity across this bottleneck will give us a bound on the flow. so to make this a little bit more rigorous, we're going to define a cut. so given the network g, a cut, this is going to be a set of vertices of g. and you should think of these as sort of the set of vertices sort of on the source side of river, on the same side of the river as the c. so this is a set of vertices such that c contains all the sources of our graph and none of the sinks. now the size of the cut is given by the total capacity of all edges that leave the cut, that go from inside the cut to outside the cut, which is the sum of all that capacity. and so, for example, in this network that we had corresponding to our city evacuation problem, we can define a cut that contains these four vertices. and size of the cut, well, is the sum of the capacities of these four roads, which ends up being 8000. okay, so to make sure we're all on the same page, here is a pretty simple network. there is a cut, which is this blue square that contains four vertices on the inside. what's the size of this cut? well, you just have to look at which edges cross from inside the cut to outside the cut. these have capacities one and two and three. and so you add those up, and you get six as the answer. okay, so the important thing though is that your cuts provide upper bounds on the size of the flow in and out. in particular, for any flow f and any cut c, the size of f is at most the size of c. and this was sort of exactly the argument that we had, any piece of flow needs to cross the cut. there's only so much capacity that lets you cross the cut, and so that's an upper bound on the flow. now, to make this rigorous, let's give a proof, the flow is the sum of our sources of the total flow out of that vertex minus the total flow into that vertex. now for vertices that aren't a source or sink, this term is zero. so we can extend this to a sum over vertices inside our cut of the flow out of that vertex minus the flow into that vertex. on the other hand, you'll note that, i mean, if you have an edge that stays within the cut, it comes out of one vertex and into another and cancels out of the sum. so this is the same as the sum over edges that leave the cut of the flow through that edge, minus the sum over edges that go into the cut of the flow through that edge. now of course, the flow of edges leaving the cut, that's at most the capacity of the edge, the flow of edges into the cut is at least zero. and so this things is at most the sum of the edges that leave the cut of the capacity of that edge, which is exactly the size of the cut. so this proves the theorem. and what this says is that if you have any cut c, that gives you an upper bound on the maximum flow. the size of the maximum flow is at most the size of the cut. now it's good we've got some upper bounds, but the question is, is this good enough? i mean, there are lots of ways to prove upper bounds. but what we really want is a sharp upper bound, one that good enough that once we found a maximum flow, we'll have a matching upper bound that will tell us you actually can't do any better than this. and, somewhat surprisingly, bounds of this form are actually good enough. so the big theorem here is known as the maxflow-mincut theorem. for any network g, the maximum over flows of the size of the flow is equal to the minimum over cuts of the size of the cut. in other words, there's always going to be a cut that's small enough to give the correct upper bound on maximum flows. so to prove this theorem, let's start with a very special case. what happens when the maximum flow is equal to zero? if this is the case, it has to be the case that there's no path from source to sink. if there is any path from a source to a sink, then you could ride a little bit of flow along that path, and your maxflow would be positive. so what we're going to do is we're going to let c be the set of vertices that are reachable from sources. and it turns out there can't be any edges out of c at all because if there were, if there was an edge that left c, then wherever you ended up, that would also be reachable from the source. and it should be in c as well. now, since there are no edges leaving c, the size of the cut has to be 0. now, in the general case, we can do something similar. we're going to let f now be a maximum flow for g. and then, we're going to look at the residual graph. now, if the residual graph, which is a way to talk about ways of adding flow to f, if that had any flow that you could put in it, f couldn't be a maxflow. so the residual graph has maxflow zero. and what that means is there's a cut c with size zero in this residual graph. and i claim this cut c has size exactly equal to the size of our flow f. and the proof isn't hard. the size of f for any cut is actually the total flow out of that cut minus the total flow into that cut. but if c has size 0 in the residual graph, that means that all the edges leaving the cut need to have been completely saturated, they need to have used the full capacity. and the edges coming in to c had to have no flow, because otherwise the residual graph would have an edge pointing on the opposite direction. and so the size is just total sum over edges leaving c of their capacity minus the sum over edges in the c of zero, which is just the size of the cut. and so we what found is we found a flow f and a cut c where the size of the flow is equal to the size of the cut. now, by the previous limit, you can't have any flows bigger than that cut, or any cuts smaller than that flow. and so this is the maximum flow, and it's equal to the minimum cut size. so in summary, you can always check whether or not a flow is maximal by seeing if there's a matching cut. in particular, f is going to be a maxflow if and only if there's no source to sink path in the residual graph, and this is a key criteria that we'll be using in our algorithm that we'll be discussing next time. so, i hope to see you for the next lecture. 
hello everybody, welcome back to our flows in networks unit. today we're actually going to, finally, give an algorithm to compute maximum flows. so the idea of this algorithm is very much along the lines that we've been sort of hinting at the entire time. we're going to start with zero flow, in our network, so the trivial flow, no flow along any edge. and we're going to repeatedly add a tiny bit of flow, sort of building up the flow a little bit at a time, until we reach a state where it's impossible to add anymore flow, and then we'll be done. so how do we add flow? you have some flow f. we then compute the residual network, gf. and this really does represent the ways in which flow can be added. so any new flow that we would have would be of the form f + g, where g is a flow in our residual network. so if we want to replace f by a slightly larger flow, all we need is a slightly positive flow in the residual network. and to do that, all we want to do is see if there's a source to sink path in this network. so, what happens if there's no path? if there's no source to sink path in our residual network, then the set of vertices that we can reach from the source defines a cut of size 0. that says there's no flow in the residual of positive size. and so any flow f + g has size at most the size of f and f is a maximum flow. and so if that's the case, we're done. we already have a maximum flow and we can just stop. now if there is a path, it turns out we can always add flow along that path. what you do is if you add x units of flow to each edge along that path, well, you have conservation of flow, there's x units in an x units out of each vertex on that path. and as long as x is at most the minimum capacity of any of these edges in the residual graph, this is actually a flow in the residual network. so if we do this, we find some flow g for our residual network with the size of g is bigger than 0. then we'll replace f by f + g, we found a new flow where the size of f + g is strictly bigger than the size of f. we found flow that's slightly bigger than the one we had before. so to make this formal, we produced what's known as the ford-fulkerson algorithm for max flow. you start by letting f be the trivial flow. and then you repeat the following. you compute the residual graph for f. you then try and find an s to t path, p, in this residual graph. if there is no such path, we know that we already have a max flow so we can just return f. otherwise, what we're going to do is we're going to let x be the minimum capacity of any edge along this path in the residual network. we're going to let g be a flow, where g assigns x units of flow to each edge along this path. and then we're going to let f be f + g. and when we do this we increased out flow by a little bit and we just keep repeating until we can't increase our flow anymore. so, for example, we've got the network here. here's our residual network. how much flow do we end up adding in one step? well to figure this out you have to do two things. you first have to find your s to t path, which is this one. and then you say, well how much capacity are there on the edges? which edge has minimum capacity? and that's this edge of capacity 4. and so in this case you'd route four units of flow on your first step. but, to really see how this algorithm works let's take the following example. so, we have a graph up top. initially we have no flow running through that graph so the graph below is the residual, is the same network that we started with. and now what we want to do is we want to find paths in the residual network. so here's an s to t path. the minimum capacity along this path is 5, so we route 5 units of flow along each of these edges. now this updates the residual, we have a couple, we've got a new edge, we got an edge that wasn't there before, whatever. we now want to again find an s to t path in the residual graph. this one works out pretty well. again, the minimum capacity of these edges is 5, so we route 5 more units of flow along each of those edges and we update the residual graph again. once again, we find an s to t path in the residual graph. this one works pretty well. the minimum capacity here on these edges is 2. so we route 2 more units of flow along each of those edges. and, at this point, once we've updated the residual we will note there is no s to t path. in fact, there's a cut right here that prevents us from routing any more flow. and so given that cut you can actually see that this flow which routes 12 total units of flow is actually a maximum flow and so we're done. so before we get into analyzing the run time of this algorithm, there's an important point to make. we should note that if all the capacities that we have are integers in our original network, then all the flows that we produce are also integer. because every time we try and augment our flow along some path, we look at the smallest capacity, which is always an integer. and so we put an integer amount of flow everywhere and everything remains integer if we started with integers. and there's an interesting lemma that we get out of this, which actually will prove useful to us later, that says if you have a network g with integer capacities, there's always a maximum flow with integer flow rates. and you can get it just by using the ford-fulkerson algorithm. okay but now let's look at the analysis. and for this analysis to work i'm going to have to assume that all capacities are integers. now what does this algorithm do? every time through this loop, we compute the residual graph and then we try to find a path p in it. and each of these run in o of number of edges time. now, every time we do that, we increase the total flow by a little bit, in fact by at least 1. so the number of times we do it is most the total flow on our graph. so our total runtime is bounded by the number of edges in our graph times the size of the maximum flow. now this is a little bit weird as a runtime, because it depends not just on sort of the structure of the graph that we're working on, but also the capacities of the edges and the size of the maximum flow. this leads us to a problem, where, potentially at least, if we have numerically very, very large capacities in our graph, it could actually take us a very, very long time to compute the flow. one other thing i should note about this algorithm is that it's not quite a full algorithm. what it says is at every step i need to find some source to sink path in our residual. now, there might be many valid paths to choose from, and the ford-fulkerson algorithm, as i've stated, doesn't really tell you which one to use. now you might just want to run depth-first search because it's very fast, but maybe that's not the best way to do it. and as we'll see a little bit later in fact, finding the right way to pick these augmenting paths can actually have a substantial impact on the runtime of the algorithm. but that's for a little bit later. that's all for our lecture today. next time, we'll talk a little bit more about the runtime of this particular algorithm. so i hope to see you then. 
hello everybody, welcome back to our network flows unit. today we're going to be talking about sort of an example of an algorithm. network on which the ford-fulkerson algorithm might not be very efficient. so last time we had this great algorithm for maxflow called the ford-fulkerson algorithm. the runtime was all of the number of edges of the graph times the size of the maximum flow. now, this is potentially very bad if the size of the flow is large. on the other hand, this is sort of a theoretical problem at this point. we don't know for sure whether or not this is ever actually a problem. so today we're going to consider the following example. here is a graph, some of the capacities are pretty large a bunch of them have a million capacity and then there's one edge with only capacity one. so the max flow here is big, we can route a million units of flow over the top and another million over the bottom, so the max flow for this graph is two million, fine. let's look at possible executions of the forward focus and algorithm on this graph. in particular, one in particular. so we start with no flow, we have a residual graph, let's look for a source to sync path. here's one. what's the minimum capacity on this path? well it's one coming from that middle edge. so we're going to route one unit of flow along that path. update the residual, find the source to seek a path. here's one. one unit of capacity along the middle edge. so we route one more unit of flow along this path. update the residual, find the path, one more unit of flow, residual, one more unit of flow, and we can keep going like this for a while. so the question here is, if we keep iterating the ford-fulkerson algorithm like this. how many iterations will actually take to compute the maximum flow?. if assuming that it keeps augmenting paths is according to this pattern. well quite a lot actually. each step here adds only one unit of flow because we're keeping limited by this middle edge. in order to find a max flow, we need a total of two million total units. so that ford-fulkerson algorithm requires something like two million iterations before it converges on this graph. and that's a really big number for a graph with only four vertices. on the other hand, if you think about it, it doesn't need to be this bad. i mean here's another perfectly valid execution of the ford-fulkerson algorithm on this graph. we've got no flow. let's find a path in the residual. there's this one. we can write a million units of flow along that path, update the residual. here's another path. put a million units of flow along that path and suddenly we've got a cut. we're done. and so there's a big difference between these two different executions of more or less the same algorithm. and what would be really nice is if we had a way to ensure that we always had something that looked more like the ladder execution than like the former execution. and next time we're going to be talking about sort of a way to go about this. a sort of principled way of choosing our paths to guarantee that we don't have the type of problem presented by the first of these examples. so that's what we will be discussing next time. i hope to see you then. 
hello everybody and welcome back to our network flows unit. today we're going to be talking a new algorithm for network flows, or maybe just a version of the old algorithm, that will do a little bit better than what we had previously. so last time, we were still talking with the ford-fulkerson algorithm for maxflow. the runtime, in general, is o of the number of edges times the size of the flow. and last time we showed that this can actually be very very slow on graphs with large capacities. and in particular, we had this example, where sort of every time, if you routed flow, at least if you were picking the wrong paths, then you just got one unit of flow every iteration and it took millions of iterations to actually finish. fortunately though, we know that the ford-fulkerson algorithm gives us a choice as to which augmenting path to use. and the hope is that maybe by picking the right path we can guarantee that our algorithms won't take that long. and so in particular what we want to do is we want to find sort of a principled way of picking these augmenting paths in order to ensure that our algorithm doesn't run through too many iterations. and one way to do this is via what's known as the edmonds-karp algorithm. the idea of the edmonds-karp algorithm is as follows. we'd like to use the ford-fulkerson algorithm but we're always going to be using the shortest possible augmented path. that is, shortest in the number of edges that are being used. and, basically all that this means is that if we want to find our augmenting paths, we want to use a breadth-first search, rather than a depth-first search. so, for example, if we're trying to run edmonds-karp on this example, then we can't use the zig-zag path with three edges. we're required to pick this augmenting path with only two edges instead. after we've done that there's another path with only two edges, and after we've done that there's nothing left to be done. so at least on this example the edmonds-karp algorithm gives us the good execution rather than the bad one. now to really look into how well this works, we need to analyze these augmenting paths. so if you have an s to t path. you'll note that when you add your augmenting flow it always saturates some edge. that is, uses up all the available flow from that edge. and this is because the way we decided the amount of flow to run along this path was we took the minimum capacity of any of these edges in the residual graph. and so which ever edge had only that much capacity left got saturated. now, once we add this augmenting flow, we have to modify the residual network. we end up with edges pointing backwards along each of these places because we can now cancel out that flow we just added. and, at least the one edge that we ended up saturating, we destroyed that edge, we used up all of the remaining flow. okay, so we'd like to now analyze the edmonds-karp algorithm. and the basic idea is that whenever we have an augmenting path, we always saturate some edge. and we're going to show that we don't have too many different augmenting paths by showing that no edge is saturated too many times. now we'll note this really fails to hold in the bad case that we looked at because the middle edge kept on being saturated over and over again, it just flipped from going pointing up to pointing down in the residual graph over and over again. and this was the real thing that was limiting to us to adding one unit of flow per iteration. okay. so that's the idea of our analysis and the way we're going to show that this works is we're going to start with a critical lemma. the edmonds-karp algorithm is very concerned about distances in the residual graph because it looks for short paths there. and so we'd like to know how these distances change as the algorithm executes. because as you run your algorithm your residual graph keeps changing, and so the distances inside the residual graph change. now the lemma that we want is the following. as the edmonds-karp algorithm executes, if you take any vertex v and look at the distances from the source to v, those distances only get bigger. similarly look at the distances from from v to t or the distance from s to t, again those can only increase, never decrease. and the proof is not so bad but it's a little subtle. so, whenever we have an augmenting path, we introduce a bunch of new edges that point backwards along this augmenting path. now the augmenting path sort of by assumption was always the shortest path from source to sink. and what that means is that the new edges point from vertices that were further away from s to vertices that are closer to s. and the key observation is that new vertices of that form never give you any faster paths from source to v. and this is because, well if i told you someone introduced a great, fast, one-way highway that went from a city 1,000 miles away from your house to a city 10 miles away from your house, it would not actually be useful for you to get anywhere from home. now it would be incredibly useful getting back from this other place, but if you wanted to get to this place 10 miles away, you could just drive 10 miles instead of driving 1,000 miles and taking the new highway. similarly these edges that only point from distances farther from s to vertices closer to s, they never help you get to places from s any faster than you were before. now the saturated edges that got removed might make things become slightly further away than they were before, but the new edges never make anything closer. and that basically completes our proof. the fact that distances at vertices to t increase is completely analogous, as is the proof that vertices from s to t increase. so, with that under our belts, the critical lemma now is the following. we want to show that there's a limit on how often edges can be resaturated. and so we have the following lemma. when running the edmonds-karp algorithm, if an edge e is saturated, that edge cannot be used again in any augmenting path at least until the distance between s and t and the residual graph has increased. now the proof this is a little bit subtle, so we're going to first consider this path that caused us to saturate the edge. so the path went from s to u, this had length x, then from u to v which was our edge. and then from v to t which had length y. now, this had to be a shortest path. and so the path from s to t had to be x+y+1. now, when we use that edge again, we use this edge from v back to u. well, we need to have some path from s going to v then to u and then from u to t. now this has to be the shortest path. now what's the distance from s to v? the distance from s to v is at least what the distance from s to v was before, which was x + 1. then the distance from v to u is one and the distance from u to t is at least what it was before, which is at least y + 1. so that means when this edge gets used again, the distance from s to t had be at least (x + 1) + (y +1) + 1, which is at least x + y + 3. which means that when this edge gets used again, it has to be the case that the distance between s and t was bigger than it was before. and that completes our proof. once we have this lemma the rest of this analysis is actually pretty easy. the distance between s and t in the residual graph can only increase and it's never more than the number of vertices. so it can only increase the number of vertices times. now between times that it increases, no edge can be saturated more than once because once it's saturated you can never use it again. and so between times you can only have o of e many saturated edges. but each augmenting path has to saturate an edge. you can only have o of e many such paths between increases in this distance between s and t. and that can happen only o of e many times. so there are only o of size of v times size of e many augmenting paths used by this algorithm. each path here takes only o of e much time. and so the total run time, is at most, o of v times e squared. now, this is maybe not so great, because, o of e times e squared, this might be number of vertices to the fifth, or number edges cubed. but it is polynomial and it has no dependence on, or it sort of doesn't become very, very, very big when our actual size of our flow becomes very, very large. okay. so one problem, sort of a quick review properties of this edmonds-karp algorithm. which of the following are true about the edmonds-karp algorithm? one, that no edge is saturated more than size of v many times. two, the lengths of the augmenting paths decrease as the algorithm progresses. or three, that changing the capacities of edges will not affect the final runtime. well, it turns out that only one of these is true. yes, edges only become resaturated after the distance between s and t increases, which only happens v many times. however, the lengths of the augmenting paths increase as the algorithm progresses, not decrease. and finally, although the runtime does not have an explicit dependence on the edge capacities, like it did in the ford-fulkerson algorithm, they can still affect the runtime. if all the capacities are zero, you don't need to do any augmenting paths. if the capacities are weird, they might make you do a little bit more work than you'd have to do otherwise. but the nice thing about edmonds-karp is that there's a bound to how bad it can be. so in summary if we choose augmenting paths based on length it removes this sort of, at least bad dependence that we had on the numerical sizes of the capacities. we have a runtime we can write down that we can run independently of our total flow. and now max flow is an incredibly well studied algorithmic problem. there are actually better more complicated algorithms that we're just not going to get into in this course. the state of the art is a little better than what we had, it's o of number of vertices times number of edges. and if you want to look it up, i mean, feel free to look up these more complicated algorithms. but this is all that we're going to do in this course. the next two lectures, we're going to sort of talk about some applications of these maxflow algorithms to a couple other problems where it's not quite obvious that this is the right thing to do. so i'll see you next time. 
hello, everybody. welcome back to our network flows unit. today we're going to talk about an application of some of these network flow algorithms we've been discussing, to a problem called bipartite matching. so to get started on this, suppose you're trying to coordinate housing in a college dormitory. so what you have is, you've got n students and m rooms. each student is giving you a list of rooms that they consider to be acceptable, and what you'd like to do is place as many students as possible in an acceptable room. now, of course, there's a limitation here that you can't place more than one student in the same room. okay, so this is the problem. how do we organize this data? well, i mean, you got a bunch of students. you got a bunch of rooms. and there's some pairs of students in rooms, without students willing to be in that room. and so a great way to organize this data pictorially is by with this bipartite graph. a bipartite graph is a graph g whose vertex set is partitioned into two subsets, u and v, students and rooms. they're sort of two types of vertices, so that all edges in the graph are between a vertex of u and a vertex of v, so all the edges that connect the student to a room now connect the student to a room to a room. and so if we just redraw that graph and call two sides u and v instead of students and rooms, it's exactly a bipartite graph. so what we'd like to do on this graph is find what is called a matching. we want to find a bunch of pairs of different rooms, that's a bunch of edges in a graph, but it needs to be the case that each student gets assigned only one room, and each room is assigned to only one student, and that says that no two of these edges that we pick can share an end point. so in our example if you look at the blue edges here, that will give us a matching. we've got a bunch of pairings of students get paired to rooms that they were acceptable to be paired with. and each student assigned only one room, and each room is assigned to at most one student. so the big problem we're going to try and solve is known as bipartite matching. given bipartite graph g, we try to find a matching of g that consists of as many edges as possible and ideally one that pairs up all of the vertices with each other. so, just to be sure that we're on the same page, if i give you the bipartite graph, what's the size of the number of edges in the largest possible match? well, you have to play around with it for a bit. you can find that you can actually get matchings of size three here and it takes a while, but you should be able to convince yourself that it's not actually possible to get the matching here of size four, five. so, let's talk about applications. bipartate matching actually has a bunch of applications. one thing, need be is matchmaking. suppose you have a bunch of men, women, some pairs of them are attracted to each other and you would like to sort of pair them off into as many possible couples as possible, such that nobody is dating more than one person at the same time. now, we have to be a little bit careful here. if there are gay people, then this doesn't quite fit into the context of bipartite matching, because there are men attracted to men or women attracted to women. the graph is no longer bipartite. and there's nothing wrong with this necessarily, but it will make the problem computationally more complicated. another example that you might want to consider is maybe a scheduling problem. you have sort of a bunch of events that need to be scheduled at different times. each event has some blocks of time that would work for it and you need to make sure that no two events get the same time block. once again, sort of a bipartite matching problem. so how are we going to solve this problem? they key idea is there's a connection between this problem and network flows, sort of what you want to solve in bipartite matching is you want to connect nodes on the left to nodes on the right without putting too many connections though a given node. this sounds sort of like a flow problem. you want to have flows running from left to right without too much flow running through any given node. so to make this work, you add source nodes and connect them to the left and have the right node to connect to a sink node and build up a network. so in particular, we start with our bipartite graph. you direct all of the edges left to right. we're going to add a source and sink node. we're going to hand the source node to the vertices on the left and connect the vertices on the right to the sink and we're going to define all the edges of this graph to have capacity one. this gives us a network associated to our bipartite graph, and it turns out that for every matching in our bipartite graph there's a corresponding flow on the network. and so to be formal about this, if g is the bipartite graph and g prime the corresponding network, there's actually a one to one correspondence between bipartite matchings on g and integer value flows on g prime. and just to prove this, well, if you have a matching, we can produce a flow by running flow through each edge of the matching. then to make everything balance out, each vertex of u, which we had flow running through it, we need to have flow coming to that vertex from s, and then that edge goes to some vertex and v and that needs to extend through the edge to t, and that will give us a flow. now if we have a flow and wants to go back to a matching, you just look at these middle edges between u and v and say which ones of them have flow? and those edges, we use in the matching. now, we can't have two edges coming out of same vertex of view because there wont be enough flow going into that vertex. there is only one unit of flow going in at most and so there can't be two units coming out. and we also can't have the edges sharing the same vertex on v for basically the same reason. and so there's a relationship between bipartite matching and integer valued flows. however, you'll note that it was a lemma that we proved that you can always find an integer valued maximum flow. and so our max flow algorithm sort of already worked for solving this problem. so this gives a very simple a algorithm for solving bipartitematching. you construct the corresponding network g'. you compute a maxflow for g' in such a way that gives you an integer maxflow. you then find the corresponding matching and return it. that solves the problem. now, we could just say that we're done here, but there's something very interesting going on. so maxflow-mincut, relating to maximum flow to the minimum cut, which is sort of nice as a theoretical tool. but here these bipartite graphs, the maximum matching relates to a maxflow and lets see what these cuts relate to. so if we have the network corresponding to a matching and look at a cut in this network, well, this cut contains the source and it contains some set x of vertices on the left and some set y of vertices on the right. and we'd like to make this cut as small as possible. now if we fix x, the vertices on the right will sort of, when do they contribute to the cut? well, vertices in y, they have edges to t which produces sort of one edge to the cut. but if you had an edge from x to a vertex not in y, then that would also give you one edge that breaks the cut. and because of this it actually can be shown that you can basically afford to just let your elements in y be exactly the elements in the right hand side that are connected to by some element of x. now if we do that, what edges break the cut? well, you've got edges from s to elements of u that aren't in x. now, by the way we constructed these, vertices in x can only connect to vertices in y that are also in the cut. but vertices in y they connect to t and those also give you edges out of the cut. so the total size of the cut is the size of u minus x plus the size of y. however, you'll note that all edges in g connect to either a vertex in y or a vertex in u minus x. so one way to find a bound on your matching is by finding a set of vertices such that every edge in your graph connects to one of those vertices. and working this out gives us what's called konig's theorem, which says if g is a bipartite graph and k is the size of the maximal matching, then there has to be a set s of only k vertices of the graph, such that each edge in g connects to one of these vertices of s. and you'll note that if you have such an s, that gives you a bound in the maximal matching, because each edge needs to use up one of those vertices and no two edges can share a vertex. and so konig's theorem says that sort of the maximum matching on the graph is the same as the minimum when it's called vertex cover set s of vertices that connect to all edges. so, for example, if we have the following graph, you'll note that these four vertices connect to every single edge in the graph. so that says immediately the maximum match can size at most four and it turns out that in this case its tight. now theres one more special case of konig's theorem that's worth mentioning. that the case where g is a bipartite graph with n vertices one each side. one thing that you might want to do is produce what's called a perfect pairing on g. that is a match that uses every single vertex on both sides. now, it's a theorem that you should have specialized konig's theorem to this case, you can show that there's always a perfect pairing, unless there's some set of only m vertices on the left hand side, such that the total number of vertices that they connect to is strictly less than that. so you can always pair up your n men with your n women, unless there's some collection of m men that pair with a total of fewer than m possible women, and if that's the case, these m men can not all simultaneously have distinct dates, and so it's clearly not possible to produce your perfect pair. so in summary, we've got this interesting problem of maximum matching, and we can solve it by resulting it to a problem of finding maximum flows. furthermore, maxflow-mincut gives us some interesting characterizations of the sizes of this maximum action. so, that's all i have to say about bipartite matching. come back next session, we'll talk about one more problem that you can solve using this maxflow technology that we've developed. 
hello, everybody, welcome back to our flows in networks unit. today we're going to be talking an interesting problem on image segmentation. this is a problem in image processing, and we'll actually show that there's some surprising connections to this max-flow min-cut type of things that we've been talking about. so the problem we're trying to solve is image segmentation. given an image, separate the foreground of the image from the background. and we don't want to get too much into image processing, so here's the basic setup. the image is a grid of pixels. we need to decide which pixels are in the foreground and which are in the background. and i don't know much about how you actually process images, but we're going to assume that there's some other program that gives you some sort of idea about which pixels are in the foreground and which are in the background. so, in particular, there's some other algorithm which looks at each pixel and makes a guess as to whether it's foreground or the background. it assigns this pixel two numbers, av, which is sort of a likelihood that it's in the foreground, and the bv, which is the likelihood that it's in the background. so the simple version of this algorithm, the input are these values a and b, and the output should be a partition of the pixels into foreground and background. so just the sum over v in the foreground of a sub v plus the sum over v in the background of b sub v is as large as possible. so to be sure that we're on the same page, here's a really simple version. we've got three pixels and we've got some a and b values. what's the best possible value that we can get out of this problem? well, it turns out that this problem is actually not that hard to solve in general. basically, for any pixel, if you put it in the foreground, you get a points, and if you put it in the background, you get b points. so if a is bigger than b, it goes in the foreground, and if b is bigger than a, it goes in the background. so what you do is, well, 1 should go in the background and gives us 4. 2 goes in the foreground and gives us 5, 3 goes in the foreground and gives us 6. and so the answer is 4 and 5 and 6 is 15. very well. now, this problem is maybe a little bit too easy. but let's take a little bit more information into account. we sort of expect that nearby pixels should be on the same side of the foreground-background divide. they're not going to be sort of randomly spattered throughout the picture, they tend to be more or less connected regions. so for each pair of pixels v and w, we're going to introduce a penalty pvw for putting v in the foreground and putting w in the background. so the full problem is the following. as input we take a, b, and p. again, we want a partition of our pixels into foreground and background. and now we want to maximize the following. the sum of v in the foreground of av and the sum of v in the background of bv, as before. but now we subtract the sum over all pairs, where v is in the foreground and w is in the background, of pvw. and now we want this thing to be as large as possible. now, before we get into too much depth on this, i'm going to do a tiny bit of algebra. i'm going to subtract the sum over all vertices v, all pixels v, of av plus bv. and the point is that this is a constant that doesn't depend on our foreground-background split, so this doesn't really affect our maximization problem. it just changes the numbers around a bit. we now want to maximize negative the sum over v in the foreground of bv and then v in the background of av, and then pairs v in the foreground w in the background of pvw. now, instead of maximizing a negative quantity, of course, we can try to minimize this positive quantity. okay. that changed things around a bit. what do we do now? well, the thing to note is that we want to split the vertices into two sets. and we pay a cost. and the cost is mostly based on the boundary between these two sets, sort of pairs where we break across the boundary, that's where we pay this big penalty. and this looks like kind of a familiar problem. this looks a lot like a minimum cut problem. so to make this all formal, let's try and build a network so that this is a minimum cut problem. the first thing we have to do is add two new vertices, a source and a sink. now, we add edges from a source to vertex v with capacity av and an edge from v to t with capacity bv. we also add an edge from v to w with capacity pvw. and this gives us a network. now, if we have a cut in this network, the cut contains s and not t, and then some of the other pixels and not some of the others. now, what's the size of this cut? well, if v's inside our cut, there's an edge from v to t with capacity bv. if v is not in our cut, there's an edge from s to v with capacity av. and then if v is in our cut but w isn't, there's an edge from v to w with capacity pvw. but if you stare at this for a bit, you'll note that if we just let the foreground be this thing is in the cut and the background be the thing is not in the cut, this is exactly the thing that we're trying to minimize. so the original problem of this image segmentation boils down exactly to solving this minimum cut problem. and now, maybe we don't know directly how to solve mincut, but we know that mincut is equal to maxflow. and so we're just going to use our maxflow algorithms. we're going to construct this network, compute the maximum flow, and then find the corresponding minimum cut. so the algorithm for image segmentation is really not that hard. you construct the corresponding network g. you then compute a maxflow f for g using edmonds-karp or whatever other algorithm you want. then we need to find the corresponding flow, so we compute the residual network. and you let c be the collection of vertices reachable from the source in this residual network. then the foreground should just be the same. c, the background, should be everything else. that is the optimal solution to our image segmentation. and so in summary, we started with this basic problem in image processing, we found a nice mathematical formulation, and then we noted that it looked a lot like minimum cut. and we're able to construct a network, use the relationship between maxflow and mincut, and then use our existing maximum flow algorithm to just solve this problem. and so this is one final application of these flow algorithms that we've been discussing. that's really all that we have to say for the moment about these flows and network algorithms. come back next time, we'll start another unit on linear programming problems, where we'll discuss some problems that are actually somewhat more general than the ones we've been discussing here that turn out to be very useful in practice. so i hope to see you then. 
hello, everybody, welcome back to our course on advanced algorithms and complexity. today we're starting a new unit, we're starting to talk about linear programming problems. and in particular today we're going to give just a simple example of the sort of problem that we will be trying to solve during this unit. so imagine that you're running a widget factory and you'd like to optimize your production procedures in order to save money. now these widgets, you can make them using some combination of machines and workers. now you have only 100 machines in stock, so you can't use more than that. but you can hire an unlimited number of workers. however, each machine that you're trying to use requires two workers in order to operate it. additional workers can be building things on their own, but machines they are using require two workers on them. now in addition to this, each machine that you use makes a total of 600 widgets a day. and each worker that's not currently involved in operating a machine makes 200 widgets a day. finally, the total demand for widgets is only 100,000 widgets a day. so if you make any more than this, they just won't sell, and that's no good for anybody. so writing these constraints down in a reasonable way, if we let w be the number of workers that we have. and m the number of machines, we have a bunch of constraints. the number of workers should be non-negative, the number of machines should be between 0 and 100. the number of workers needs to be at least twice the number of machines. and then finally, 100,000 is at least 200 times the number of unoccupied workers. that's w minus 2m, plus 600 times the number of the machines. and so these constraints sort of constrain which allowable combinations we can have. now we can try and graph these constraints. so here we've got a plane of possible values of m and w that satisfy these constraints. now if we're just starting where m and w both need to be non negative, we have this quadrant as the allowable values. but when we require that m needs to be at least 100, we're reduced to being in this strip here. when we look at our constraint based on the total demand, we find that m + w is at most 500. and so we're now constrained to this region. and we add the final constraint that the workers need to be at least twice the number of machines. we finally come to this diagram of possible configurations of machines and workers that we can use. what's next? profit, well suppose that profits are determined as follows. each widget that you make earns you a $1 but each worker that you're hiring costs you $100 a day. so the total profit that you get, then, in terms of dollars per day. well it's number of widgets, 200 workers minus twice machines, plus 600 times number of machines, minus the total salaries you paid to workers, 100 times the number of workers. so that's 100 times the number of workers plus 200 times the number of machines. and if we want to plot that on our graph we can do it as follows. so these lines that i've drawn are lines of equal profit. there's a line with $30,000 a day, and then $40,000 a day, and then $50,000 a day. and sort of as you go from left to right, or from bottom to top, you make more profit. so what we're trying to do now is we're trying to say well, what can we do to get the most profit? and it turns out, the best you can do is at this point here. note that it's a corner of the allowable region, it's where we have 100 machines and 400 workers. and the total profit is $60,000 a day. now it's clear from this diagram that this is the best that you can do. but if you actually want to prove it, there's a clever way you can do that. so two of the constraints that we have, one of them is that the number of machines is at most 100. and another is that 200 times the number of machines plus 200 times the number of workers is at most 100,000. now if we take 100 times the first constraint and add it to a half times the second constraint, what you find is that 200 times the number of machines plus 100 times the number of workers has to be at most 60,000. and that says the profit that we make has to be at most 60,000. and so this is a very convenient way to prove that the 60,000 that we could attain is actually the best we can do. so in summary, what we did is we solved this problem where we maximized this function, 200m + 100w. subject to the following list of five constraints. and because the thing we're trying to maximize is a linear function, and the constraints we have are linear inequalities, this makes this an example of the type of problem we're going to be looking at. that is, a linear program. so come back next lecture and we'll sort of formally define this problem and get us started on our investigation. 
hello everybody, welcome back to our unit on linear programming. today, what we're going to do is we're sort of going to put everything on sort of a more solid, rigorous basis. so remember last time what we did is we had this factory problem, where what we wanted to is we wanted to maximize. in terms of m and w, this 200m + 100w, this is linear expression. subject to the following list of linear inequality that they had dissatisfied. and so in general, basically, just this is where the linear programming is. it says we want to find real numbers, x1 through xn that satisfy a bunch of linear inequalities, so a11x1 + a12x2 +..., is at least to b1, and then a bunch more of those. and subject to these constraints, we would like a linear objective function, v1x1 + v2x2 + etc., to be as large or possibly as small as possible. to clean up the notation a bit, we're really going to store this by having a matrix a that encodes the coefficients of all these inequalities along with vectors b and v. and our output should be a vector x and rn. such that a times x is at least b. and what i mean by this is that if you multiply the matrix a by the vector x, then you get a new vector where the first component of that is bigger than the first component of b. the second component is at least the second component and so on and so forth. and note that if you just unroll what that means it's exactly the system of linear inequalities that we had on the previous slide. now subject to this constrain, we would like v * x to be as large or as small as possible. so linear programming turns out to be incredibly useful because them are an extraordinary number of problems that can be put into this framework. to begin with the factory example that we solved in the last lecture was exactly of this form. optimize a linear function with respect to some linear inequality constraints. but there's a ton more problems that fit into this. one of them is the diet problem, which was studied by george stigler in the 1930s and 40s. and intuitively, it's a very simple problem. how cheaply can you purchase food for a healthy diet? this is really important if you say, need to feed and army full of soldiers for example and you want to be cheap. so how do you do this? well, you got a whole bunch of variables for every type of food that you could possibly eat. you need to know how many servings per day of that food you're going to have. so you've got a variable x of bread, x of milk, x of apples, so on and so forth. and then you've got a bunch of constraints. firstly, for each type of food, you need to have a non-negative number of servings of that food. it wouldn't do to have minus three servings of bread a day. and additionally, you need to have enough nutritional content, you need to have sufficiently many calories per day. so many calories do you have? that's just the (cal / serving bread) x bread + (cal / serving milk) x milk + so on and so forth over every type of food. and this should be at least 2,000 or whatever your minimum calories per day for your diet is. and in addition to this constraint, we have another similar looking constraint for each other nutritional need. vitamin c, protein, what have you. and so we have a bunch of linear inequalities as our constraints on these variables. and subject to these constraints, we want to minimize our total cost. and the cost of our diet is the (cost of serving a bread) x bread + (cost of serving milk) x milk+ so on and so forth. so we want to minimize a linear function of these variables, subject to a bunch of linear inequality constraints. this is a linear problem. you can solve it and it would tell you in some sense the cheapest diet that you could live on. unfortunately, i should worn you that actually doing this is maybe not the best idea. when you solve this, the solution tends to optimized for a few very efficient foods for getting calories and protein. and then maybe a few random things to like fill in your other dietary needs very cheaply. but i mean it might say that you should eat mostly potatoes and peanut butter and then a bunch of vitamin pills or something. and so, these tend not to produce diets you'd actually want to consist entirely on. but maybe if you want to think about what can i do to eat more cheaply, it's something to look at. so another problem that fits very nicely in this linear programming formulation is network flow. it turns out that the network flow problems that we discussed in the last lecture are actually just a special case of linear programing problems. so if you want to solve max flow, then you've got a bunch of variables, f sub e, the flow along each edge e. and they satisfy some constraints, for each edge e, f sub e if between 0 and the capacity of the edge. and then for every vertex that's not a source or a sink, you have conservation of flow. so the total flow into that vertex is the same as the total flow out of that vertex. now when you first look at this, this might not seem to be an inequality, it's an equality, not an inequality. but you could actually just write it by writing down two linear inequalities. you could say the flow into the vertex is at least the flow out of the vertex. and on the other hand, the flow into the vertex is at most the flow out of the vertex. and so we put these two inequalities together, it's equivalent to this one equality. so once we put these constraints on, we now have an objective function we'd like to subject to these constraints, maximize the flow. that's the total flow going out of sources minus the total flow going in to sources, which is a nice linear function. and so this maximal problem is just a special case. when you phrase it this way, it's exactly a linear problem. now a lot of the time, when you look at a linear program, it's exactly what i said. you're subject to these constraints, there's a unique maximum value which attains the best possible value of the objective function. however, there are a couple of edge cases that you need to keep in mind where things don't quite work out this way. the first is you could actually have a system where there is just no solution. you could have constrains say x is at least 1, y is at least 1, and x + y is at most 1. if you have the system of constrains which is graphed here, there's actually no solution. because if x and y are each at least 1, x + y needs to be at least 2. so there's no solution to the system, so you can't even start with trying to find a maximum. it could also be the case that even though your system has solutions there's no actual optimum. and a way that this could happen is as follows, if you have the system where x is at 0, y is at least 0, x- y is at least 1, there's actually no maximum value for x here. basically, the region is graphed here, but it says you could higher and higher and higher up. x is actually unbounded in this system. and so in some sense, your solution should say that there's no maximum. now just to review this i've got three pretty simple systems here, each with two equations and two unknowns. now one of these systems has no solution. one of them has solutions but no solution with a maximum x value, it's unbounded. and the third one actually does have a unique maximum x value. and so i'd like you to take a little while to think about which one of these is which. okay, so if we actually graph these three systems, a it turns out has no solution. one equation says we're supposed to be bigger than one, the other one says we have to be less than zero, you can't do both of those. b if you write it down does have a unique maximum at i think x equals one and a half, as plotted there, it's the red point. and c, although it does have plenty of solutions, if you graph this region. you'll note that you sort of slide up along this line, x equals y, you can make the x value as large as you want. and so there is our r solutions, but there's no max. in any case, that's all that i had to say about this basic introduction to linear programs. come back next time and we'll start by looking at a special case of dealing with linear equalities rather than inequalities. 
hello everybody, welcome back. today we're talking more about linear programming, well actually we're not. we're looking at sort of a simpler problem first. so linear programming talks about dealing with systems of linear inequalities. today we're going to look at sort of a simple special case of this where we look at systems of linear equalities. so for example we have a system of linear equalities x + y = 5, 2x + 4y = 12 and we'd like to be able to solve this for x and y. so the very general way to do this is by what's known as the method of substitution. you use the first equation to solve for one variable in terms of the others. you then take that variable and substitute it into the other equations. you now have a bunch of equations with n-1 variables and you recursively solve those. then once you have the answer to those equations, you substitute them back into the first equation to get the value of the initial variable. okay, let's see how this works in practice. so, x + y = 5. 2x + 4y = 12. using the first equation, we solve x as 5- y. we substitute that into the second equation and we find that 12 = 10 + 2y. solving that for y, we find out that y = 1. and substituting back into the first equation, x = 5- 1 = 4. so, x = 4y = 1, that's the solution to the system. now, just to make sure we're on the same page, if we have the system x + 2y = 6, and 3x- y = -3, what is the value of x in the solution to that system? well the answer is 0 here. so from the first equation, we get x = 6- 2y. substituting into the second, we get that -3 is 18-7y. solving that tells us that y = 3, so x = 6- twice 3 = 0. and that's the answer. okay, so that was our first example. let's look at another example. we have a system of linear equations x + y + z = 5. 2x + y- z = 1. so we solve this by substitution, great. from the first equation, x = 5- y- z. we substitute that into the second equation and we solve for y. we find that y = 9 + 3z. great, we now know what y is and we want to solve for z but we can't. there are no equations left. we've already used the first equation to solve for x and the second to solve for y. we can't solve for z because there's nothing left. but this is actually fine for us. it turns out that any value that we assign z will give us an actual solution. you give me any value for z, we set y = 9 + 3z and then x is 5- y- z or -4- 4z. and any value of z gives us this solution. so there's an entire family of solutions. we can let z be a free variable. and for any value of z, we have a unique solution. so in general, your solution set will not necessarily be a point, but it will be a subspace. you'll have some free variables and no matter what settings you give those, your other variables will be functions of your free variables. now this subspace has a dimension which is just the number of free variables, the number of parameters you need to describe a point on it. and generally speaking, each equation that you have gives you one variable in terms of the others. and so generally speaking, the dimension of your set of solution is going to be the total number of variables minus the number given in terms of others. so the total number of variables minus the number of equations. so generally speaking, if you have n equations and n unknowns, there'll be no free variables left and you'll have a unique solution. however if you have n+1 equations and n unknowns, the first n of your equation solves for the unique solution and then the extra equation probably is something that isn't satisfied by that solution. so generally if you've got too many equations, there are no solutions to the system. however, if you have n- 1 and n unknowns, you generally solve those and you'll still have one pre-variable left, so generally speaking, you'll have a full dimension one subspace. you'll have a line as your solution instead of just a point. okay, so in summary, we can solve systems of linear equations using the method of substitution. and generally speaking, and this isn't always the case. but generally, each equation reduces the number of degrees of freedom by one. now if all you want to do is solve systems of linear equations, you could basically stop here. but we want to do more than that. so next time what we're going to do is we're going to talk about how to systematize this whole thing, and simplify the notation sum to make this into an honest algorithm that we're going to discuss. and so, when you want to talk about how to tell your computer to solve systems of linear equations, that is what we're going to talk about in the next lecture. so i'll see you then. 
hello everybody, welcome back to our linear programming. today we're going to talk about gaussian elimination. so the basic idea is that last time we talked about how to solve linear systems by substitution. today, we're going to make it into an algorithm. so remember last time we could solve systems of linear equations using this method of substitution. and to begin with we'd like to simplify the notation a little bit because the way we did it you had to write down these full equations x+y = 5 2x+4y- 12 these have variables and addition signs and equality signs and all this mess. the only thing that really matters are the coefficients of these equations. so what we're going to do is we're going to simplify notation, and just store these coefficients in what's known as an augmented matrix. that's a matrix with this little bar coming down in the middle of the entries. so here, each row is going to correspond with single equation and the entries in that row are going to be the coefficients. so the first row 1, 1, 5 translates to 1 times x plus 1 times y equals 5 x plus y equals 5. the second row 2, 4, 12 means that 2x plus 4y equals 12. and so, this little matrix is sort of just a convenient way of storing that system of linear equations. now, one complication that this method runs into is when we're storing things in this matrix. how do we implement substitution? how do we solve for x? for example, there's a sense in which we can't write a row that corresponds to the equation x = 5- y. every row corresponds to an equation where x and y are on the same side of the equality. so this sort of doesn't work. on the other hand, the row 1, 1, 5 is almost as good, it corresponds to the equation x + y = 5 which is equivalent. the next question we ask ourselves is how do we substitute this into the second equation? because once again, the immediate thing you get when you substitute has xs and ys on sort of. i guess they're still in the same side but it's got constants on the wrong side of the equation now. and so you can't substitute directly but really you can do something almost as good. what you can do is the substitution was just to get rid of the x's in that second equation and you can do that by subtracting. if you subtract twice the equation (2x+y=2) times the equation (x+y=5) from the equation (2x+4y=12). that tells you that (2y=2) which is exactly what you would have gotten from substitution and this corresponds to a very nice operation on the matrix rows. you just subtract twice the first row from the second to get the row corresponding to this guy. okay, so we're given this augmented matrix and what we're going to do is we're going to manipulate it. we're going to use what are called basic row operations. these are ways of transforming your matrix to give you an equivalent system of equations. now the first piece is addition, just what we just saw. you add or subtract a multiple of one row from another. so for example, we subtract twice the first row from the second and 2, 4, 12 becomes 0, 2, 2 which is good. next off though we have 2y = 2, we want to change that to y = 1, so to do that we need to use scaling. we need to multiply or divide a row by a non-zero constant and that's just multiplying the equation by a constant. so we should be good. so if we divide the second row by two instead of getting 0, 2, 2 it becomes 0, 1, 1 y equals 1. now in some sense these two operations are enough but for bookkeeping purposes we might want to reorder the way in which we list the rows. so a final operation is to swap the order of two of the rows. so just list the second row up top and the first row down bottom. this clearly doesn't really effect anything, we're just sort of listing our equations in another order. so these are the three basic row operations. and what we're going to do is we're going to combine them into an algorithm called row reduction, or sometimes gaussian elimination. we're going to use these operations to put our matrix in a simple standard form. and the idea is actually very simple, we're just going to simulate the method of substitution. okay, so let's consider this example. we have a big matrix here, it corresponds to a system of three equations in four unknowns, which i'll call x, y, z and w and we'd like to solve them. so method of substitution, what do we do? we use the first equation to solve for the first variable x. and in some sense this first equation 2x + 4y- 2z = 2 already implicitly solves for x in terms of the other variables. but for simplicity we'd like to rescale it so that the x entry is 1, so we divided the row by two. and now it says x + 2y- z = 1 or equivalently, x = 1- 2y + z. we now want to substitute that value into the other equations, which we do by adding this row to others, to clear out the x entries. so we add the first row to the second and subtract twice the first row from the third. and now there are no other entries in that column in any of our other rows. okay, so now we're done with x we want to solve for the next variable y using the two other equations. we actually can't use the second to solve for y but there's no y in that equation the entry is 0. but we can use the third equation so what we're going to do for book keeping purposes we're going to swap the second and third rows just so that the, row we're using to solve for y is up top. we now want to solve for the second variable by which we mean rescale so that entry is 1, so we divide by -2. and now, we want to substitute this value into the other equations. so we subtract twice the second row from the first and actually the third row is okay. now the thing to note is we can't actually solve for z. the last equation doesn't have z. the first two equations have z terms in them, but we've already used those equations to solve for x and y. so actually, what we're going to do here, is we're going to skip z and move on to w. we can solve for w in terms of this variable, so we divide the last row by minus 2. we get the equation w = 0 and we substitute that into the other equation. so we subtract twice the third row from the first and then add the third row to the second. and now we're actually done, this is basically as simple as our matrix is going to get. but now that we have this, it's actually very easy to read off the solutions. we have this matrix, it corresponds to the equations x + z = -1, y- z = 1, and w = 0. and here you can basically read off the solution. for any value of z, we have a solution x = 1- z, y = 1 + z and w = 0. and that's the general solution. great, so how do we rowreduce a matrix in general? we're going to do is we're just going to take the leftmost non-zero entry. this is the thing we're going to solve for. we swap that row to the very top of the matrix just for bookkeeping purposes. that leftmost entry we're going to call pivot, we're going to use it to clear out the other entries in it's column. we rescale the row to make that entry one because we want to actually solve for that variable. we then subtract that row from all the others to make the other entries in that column 0. then, we just substitute that into the other equations and we're going to repeat this well, with slight modifications. we want the leftmost non-zero entry not already in a row with the pivot, we then swap that row to be the top of the non-pivot rows. we make a new entry a pivot, we rescale, we clear out the other columns and we keep repeating until all of the non-zero entries are in pivot rows and then we're done. once you have that it's actually pretty easy to read off the answer. each row is going to have one pivot entry and maybe a few other non-pivot entries. this will give an equation that writes the pivot variable in terms of a bunch of non-pivot variables. now, there's a special case if there's a pivot in the units column. that means that we have the equation 0 = 1, and if that happens we have a contradiction and there are no solutions. but otherwise, the non-pivot variables, the variables corresponding to columns with no pivot in them, these are actually free variables. we can set those to whatever we want and then once we've done that each of our rows tells us what the pivot variables should be in terms of the non-pivot variables and that's it. one final thing to discuss is the runtime of this operation. if you have n equations and n variables, there are a minimum of n and m pivots. whenever you find a pivot, you need to subtract a multiple of that row from each other row. now each row has n entries that you need to deal with this subtraction for and there are m rows so that takes o of m, n time. and so the total run time is o of n times m times the minimum of n and m. and this is pretty good, it's polynomial n and m. you could maybe expected you a little bit better and in fact there are sophisticated algorithms that do that. but for practical purposes, this is actually a pretty good runtime and it's a very usable algorithm. so that's basically all for a sort of linear algebra sidebar. on next lecture, we're going to go back to talk about linear program, the systems of linear inequalities and how to deal with them. so until next time. 
hello everybody. welcome back to our linear programming unit. today, we're going to talk about convex polytope. in particular, we're going to try to understand what the solution set to the system of linear inequalities that we need to deal with actually looks like. so remember, in a linear program we're trying to optimize a linear function subject to a bunch of linear inequality constraints. today, we're going to ask the question, what does the region of points defined by these inequalities actually look like? for example, this factory example that we looked at way back at the beginning. if you look at the set of solutions to these five inequalities, you got this nice trapezoid here. so the question is, what did things look like in general? well, another example, if you look at the system were x, y and z and three dimensions were all between zero and one. you've got the unit cube. and in general, you get much more complicated looking regions. but you'll always get what's called convex polytope. and dont worry will unrule these meaning as we go. so the first thing to know is what is a single linear equation? well if you look at the linear equality, it defines a hyperplane, infinite flat surface. now, if instead you want an inequality it gives you what you call a halfspace. it gives you a hyperplane and everything on one side of that hyperplane. so if we want the solutions to a system of linear inequalities, we have a thing defined by a bunch of halfspaces, we want the intersection of all these of halfspaces. we want everything that's inside of all them. we want to solve all of the equations. and so, we sort of get a thing that's defined by these hyperplanes. in fact, what we'll always get is a polytope, that's a region in rn that's bounded by finitely many flat surfaces. but, in fact, polytopes have a little bit more structure than that. if you think about the cube, not only do we have the six faces but these faces intersect at edges and those intersect at vertices. and so, a polytope in general will have these, surfaces may intersect at lower dimensional facets like edges but perhaps some other dimensions, with the zero dimensional facets are called vertices. but it turns out that not every polytope is actually possible as a solution, a set of solutions to such a system of linear inequalities. for example, the donut pictured here is a polytope. but it's not a system solution to one of these systems. because if you look at some of these inward pointing faces. well, these faces lie in a hyperplane. but you've got portions of your region on both sides of that hyperplane. whereas, if you have a polytope defined by one of these systems of linear inequalities. each bounding hyperplane is actually coming from one of those linear inequalities. and you can only have points on one side of that hyperplane or the other. so you have this extra condition that everything must be on only one side of each face. and that leads us to the condition of convexity. a region c and rn is called convex, if for each pair points x and y and c. the line segment connecting x and y is entirely contained in their regions. so the lemma is that any intersection of halfspaces or site of solution to this systems is convex. and the proof is not that hard. our system is defined by ax at least b. we need to show that if two points, x and y, are in this set, then everything on the segment contained between them is also in the set. well, the lines that way you can parameterize is points of the form (tx + (1- t)y) where t is a real number between 0 and 1. so the fast of that point is in our set, we take a (tx + (1- t) y) = tax + (1- t) ay, since x and y are in the set, that's at least tb + (1- t) b which is b. and so, every point in the line segment is in our set so the set is convex, great. so the theorem is the region defined by a system of linear inequalities is always a convex polytope which is nice. so to reveal, we've got three pictures here. which of these three regions, a, b, c is a convex polytope? well, it turns out only b is. so a is not convex because we have these line segments here with the end points are in a but some of the points the middle arc. c is not a convex polytope because there's this region of the boundary here that sort of a curved region whereas if were a polytope, all that bound your regions would have to be straight lines. b on the other hand, is actually a convex polytope. okay, so to conclude this lecture, we're going to actually prove a couple of important lemmas about convex polytopes. so the first one is separation. if you have c to be, in fact, any convex region, and x is a point not in c. then it turns out there's always a hyperplane h that separates x from c, where x is on one side and c is on the other. now, if c is given by a system of liner inequalities. this is actually easy to prove because if x isn't in c violates one of these defining inequalities, and that inequality gives you a hyperplane where c is on one side and x is on the other. but in general, you can prove this as well, you start with x, we're going to let y be the closest point in c to x. and it turns out you can just take the perpendicular bisector of xy or the hyperplane of points equidistant between x and y. now, this is clearly a hyperplane to show that it separates x from c. well, suppose that there were some point z and c that were on the wrong side of this hyperplane. well, z and y are both in c, so everything on line segment between z and y is also in c. but you can show that there's actually always a point on this segment zy, that's closer to x than y was. and this is a contradiction because by assumption y was the closest point in z to x, and we just found a closer one. so that completes it. okay, so the other lemma is about polytopes. suppose, that you have a polytope and there's a linear function on this that you're trying to minimize or maximize. the claim is that it takes its minimum or maximum values on vertices. this is clearly relevant to our linear program because we're exactly trying to minimize and maximize linear functions on this convex polytopes. so, we saw this in it's original factor example with the maximum was at this vertex and turns out that happens in general. now, to maybe get some intuition for why this is true. we've got our polytope and it's, this polytope is sort of spanned by the corners, it's got corners and like things in between these corners. but because we have linear functions like the things in between the corners are never as good as the extreme points and so the optimum must be at the corners. now, to actually prove this the thing to note is that you have a linear function to find on a line segment. it always takes extreme values at the two end points. and we're going to use to sort of push our points toward the corners and let the values get bigger and bigger. so we start at any point in our polytope and what you do is you draw a line through it. and you'll note, that the biggest value that our linear function takes on this line comes all the way over to that line hits an end of the polytope. so it takes an extreme point at the endpoint of that line which is on the face of your polytope. now, once you're on the face or some facet, you can repeat this. you draw a line through that point and what you know is that the extreme values will be at the end points of this line and that lets you push it to a lower dimensional facet. and you keep doing this until you end up at a vertex. and so, we start at any point and we kept going until we hit a vertex. and that vertex has at least as large a value as the point we started. and so, the maximum values must be attained at some vertex. so in summary, the region defined by a linear program is always convex. the optimum of this linear program is always attained at a vertex. and finally, if you have a point that's not in the region, you can always separate it from points on the inside by an appropriate hyperplane. so these are some basic facts about linear programs and their solution sets. come back next time, we'll talk about another interesting property of linear programs called duality. 
hello everybody. welcome back to our linear programming unit. today we're going to talk about an interesting phenomenon in linear programs called duality. so let's recall the first example that we looked at here. we wanted to maximize 200m + 100w subject to this bunch of equations. now it turns out we had a very clever way of proving that optimum was correct once we found it. so the best you could do is 60000, and there was a great way of proving it. we took one constraint, we multiplied by a hundred, we took another constraint and we multiplied it by half. we added those together and we got a new constraint that, if we satisfied our original constraints, it had to be the case. that 200m plus 100w, the thing we were trying to optimize, was at most 60,000. and this is a very interesting and general technique that if you want to bound your objective. you can try and do this by combining the constraints that you have together to prove a bound. so let's see what happens in general. you have a linear program, you say you want to minimize v1x1 plus v2x2 plus. all the way up to vnxn. subject to a bunch of these linear inequality constraints a11x1 plus a12x2 plus dot dot dot is at least b1 and thenetc etc. so how can we try and do this? well if you give me any constant ci bigger than 0, you can take the first constraint and multiply it by c1 and the second constraint multiplied by c2 and so on and so forth, and add those all up. and what you'll get is a new linear inequality, w1x1 plus w2x2 plus dot dot dot is at least t. here the w i are some combination of the cs, w i is the sum of c j a j i and t is the sum of c j b j and this is a new inequality. now, if it's the case that w i is equal to vi for all i, we have is that v1x1 plus v2x2 plus dot, dot, dot that thing we were trying to minimize is at least t. and so, if we can arrange for wi to the vi for all i, we've proven a lower bound on the thing that we're trying to minimize. so, we'd like to find there's a bunch of ci's that are all non-negative such that vi is the sum of j = 1 to m of cj aji for all i. and so that subject to this constraints t the sum of cjbj is as large as possible. so we like the biggest lower bound we can. now the very interesting thing about this, is that this system we just wrote down is actually just another linear program. we want to find the c in rm such that cjbj the sum of that is as large as possible subject to a bunch of linear inequalities. ci bigger than or equal to 0 and a few linear equalities. vi is the sum of cj times aji. and so, to put this formally, given any linear program we can call the primal very often.. say minimize v.x subject to ax at least b. there's a dual linear problem, which is the linear problem that we want to maximize. y.b subject to y transpose a equals v and that's just another way of rewriting our inequality constraints. and y at least 0. and it should be noted that even if your linear program wasn't exactly in this form, you can still write a dual program, it's with a linear program of trying to find a combination of the constraints. to bound the thing that you're trying to optimize. and so it's not hard to show that a solution to the dual program bounds the optimum for the primal. suppose that you have a solution for the dual, you've got a y bigger than or equal to 0, such that y transpose a is equal to v. then for any x where ax is at least b, well, x dot v is equal to y transpose ax. that's at least y transpose b, which is y dot b. and so y dot b, the solution to the dual, it's a lower bound to the solution to the prime. now the surprising thing is that not just is this a way that you can get lower bounds, that these two linear programs they actually have the same solution. if you find the best solution for the dual program, it actually always gives you a tight lower bound for the primal. and the theorem here is a linear programming duality that says a linear program and its dual have the same numerical answer. and this is incredibly useful. on the one hand it says if you have a linear program and want to prove that your answer is optimal you could try and solve the dual to provide a matching upper band or lower band. it also means that if all you care about is the numerical answer you can try to solve with dual program rather than the primal. and often, dual program is easier to solve, and so this makes things more convenient. and even if the dual program isn't easier, often looking at the dual gives you some insight into the solution to the primal. ok so that's the new programming duality. let's look at some examples. for example, let's look at the max flow problem. the size of your flow is the total flow going out of a source minus total flow going into a source. now, we have a bunch of these conservation of flow equations, and we can add any multiples of those that we like. and the objective stays the same. so when we do that, the thing we're trying to maximize is the same as the sum over all vertices v of some constant c sub v times the total flow out of vertex v minus the total flow into vertex v. here c sub s needs to be 1 if s is a source and c sub t needs to be zero if t is a sign but for any other vertex v we can take c sub v to be anything we like. [cough] okay, so we have this expression, what do we get when we write this down? and this is sum over edges from v to w of the flow along the edge times c sub v minus c sub w. we can now try to bound this above using our capacity constraints. and so the best we can do here, it's not hard to show is the sum over edges v to w of the capacity of the edge e times the maximum of either c sub v minus c sub w or zero. okay so this gives us an upper bound and we want this upper bound to be a small as possible. it's not hard to show that we should pick our c sub v to always be either zero or one. now if we do that, let c, be the set of vertices where c sub v equals one. the bound that we prove then reduces to the sum over edges v w, where v is in script c and w isnt of the capacity h. but you'll note, c is just cut and this bound that we proved is just the size of the cut. and so, this dual program in some sense is just trying to find the minimum cut. hence, linear programming in duality, in this special case, just gives us the max flow equals min cut. okay, let's look at this other problem for example. the diet problem. here we want to minimize the total cost of foods you need to buy, subject to constraints. it needs to meet your various daily requirements for various nutrients, and you need to get a non-negative amount of each type of food. so you've got this system, what's the dual program? well okay, so for each nutrients n we have some multiple c sub n of the equation for that nutrient. and then we can add on positive multiples of the constraints even in non-negative amount of each type of food. okay, so when we combine all of those together we're suppose to get a lower bound on the cost of our diet. and so, if you compare coefficients well, the coefficient we need to end up with for a food f is the cost of that food. and this should be equal to the sum of our nutrients n of c sub of n times the amount of that nutrients in the food f. plus some positive amount that we got by adding whatever multiple we had on the constraint that we got a non-negative amount of that food. so what this says is for each food f, the cost of food f, should be at least the sum over nutrients n, times the amount of that nutrient showing up in this food. but theres a nice way now of interpreting this c sub n. we can interpret it as a cost in order to buy a unit of nutrients n. and so if there was a market where you could just buy calories at the cost of cn and you could buy protein at the cost of whatever. what the above equations are saying. is that for each food you can't cheat the system by buying out food. you can't get nutrients more cheaply than you could by buying the nutrients individually. and the cheapest way to get nutrients is buying them individually it's pretty clear the total cost of a balanced diet is at least just the sum over nutrients at the cost of that nutrient times the amount of nutrient that you're required to have in your diet. and so, what this linear program tries to do, is it tries to find non-negative costs for the various nutrients that satisfy this, no food allows you to cheat inequalities. such that the total cost of your diet is large as possible. now, there's one interesting observation about the solution, supposed that we're actually trying to exactly achieved this lower bound. that would mean that you could never afford to buy overpriced food. you can never afford to buy foods where the cost of that food was strictly bigger than the total cost of all the nutrients that make up that food. you could only buy foods where the cost of the foods is exactly the cost of the nutrients in that food. and this gives us as an example of a general phenomena called complementary slackness where basically what this says is that if you look at the solutions the dual, you should look at which equations you needed to use in the dual program. that tells you about which equations in the primal program need to be so in particular, complementary slackness is the following theorem. if you give me a primal in your program, minimize v. x subject to ax at least b. and it's dual. then if you take solutions to these two, if you use a positive multiple of an equation in the dual, if yi is strictly bigger than 0, in the dual. this happens only if the ith equation in the solution to the primal is actually tight. the ith inequality is actually an equality in the optimal solution. so let's reveal what this means. let's suppose that we have a linear program to find by these five linear inequalities labelled 1, 2, 3, 4, 5 and the diagram below. we have allowed what regions this gray region and the red point located is the optimal. now, suppose that we're looking for solutions of the dual program. which of these five equations might actually be used as sort of positive multiple of those equations, in the solutions to the dual program? well, it turns out the only equations that you could actually use are two and four because complementary slackness says that the only equations that get used in the solution to the dual are ones with that equation is tight in the primal. and in this case, two and four are the only lines of this solution to the primal actually lies on. and its those are the only equations that could actually be used in the solution. so in summary everything in your program has a dual in your program. the solution to dual actually bounds to the solutions to the primal. and surprisingly the lp na dit's dual has the same answer. and this means that the solution do dual actually is tight bound to the solution to the primal an. in addition, we have this complementary slackness where knowing the solutions to the dual tells you a lot about where the solutions to the primal lies. in fact, it tells you which equations in the solutions the primal needs to be tied. so that's basically everything we have for this lecture. next lecture we're going to talk about proofs for these things so that material is not necessarily required, but if you'd like to see it, it's informative. 
hello everybody, welcome back to the linear programming unit. today we're going to talk with proofs from the duality lecture. so remember last time we showed that each linear program, we could associate a dual program. which is basically attempting to find a non-negative combination of our constraints that put a bound on the objective function. so in particular, we have the duality theorem, which says a linear program and its dual always have the same numerical answer. today we're going to prove that. so before we get into the proof, let's talk a little bit about intuition. so one way to think like this is the region defined by linear constraints is sort of a well. it's bounded by these linear walls. and you need to find, minimize say a linear constraint. say you want to minimize the height of a point inside this well. now, if you understand some physics, one way to do this is you just take a ball that's pulled down by gravity, you drop it into the well and gravity will pull it down to the lowest point. now when it reaches this point the ball is at rest. and that means the forces acting upon it need to balance. so the force of gravity pulling it down needs to be balanced by the normal forces from the walls pushing it back up. now when you write this down, you have a linear combination of these normal vectors pointing orthogonally to these walls, has to equal the downwards pointing vector from gravity. and if you worked out what this means in terms of the equations, the downward pointing vector, that's the direction in terms of your objective that points in the direction of your objective, and the normal pointing vectors, those are showed by the vectors that correspond to the equation to finding the walls later on. and so, if you sort of work out exactly what this means and put it in terms, it exactly is linear programming. you actually have a solution in a dual program that matches your primal. and if you'll even note which walls you use, only the walls that the ball is actually touching, that actually gives you a taste of why complementary slackness might be true. you only get to use the walls that the ball's actually touching, which are the ones for which your equations are tight. in any case, let's look at an actual proof. so the first thing that we're going to do is, instead of looking at an optimization problem, we're going to look at a solvability problem. we'd like to say when is there a solution to ax at least b and x.v at most t. so, we claim there's a solution to the system unless some combination of these constraints yields a contradiction, yields the equation 0 bigger than 1. of course, if you yield that from a combination of your constraints, it's clear your constraints cannot all simultaneously be satisfied. but it turns out that this is sort of if and only if. and if we can prove this, that will actually be enough to give us the original problem. because basically, the only way we can do that is if combining the constraints not including the last one, we can conclude in its constraints that x.v needs to be strictly bigger than zero. okay, so let's see how that works. now suppose that we have a bunch of constraints and now we want to look at all possible combinations of constraints. so c1e1 plus c2e2 plus dot dot dot plus cmem, where the ci are some non-negative constants. now the thing to note is that the set of inequalities that can be derived in this way, the set c, is actually a convex set. now what happens if the equation zero bigger than 1 is not in this set c? well, you've got a point that's not in the convex set. that means that there has to be a separating hyperplane. now, what this separating hyperplane is is a little bit weird. it's a sort of a hyperplane in the set of linear inequalities but it turns out that from this hyperplane, you can extract solution to the original system of equations. so this is a little bit abstract, so let's look at something slightly more concrete. we have a system of linear inequalities, all the form something x plus something y is at least 1, and we want to know is there a solution to this system? well, we're going to plot these equations. here we've got a bunch of equation in the from ax plus by is at least 1. and we've plotted all the values of a. now, it's clear that in none of these equations is the contradiction 0 bigger than 1. now would be the point of the origin. we also have considered linear combinations of these. it turns out the linear combinations of these equations you can get are exactly the gray region here. now it's still the case that 0 bigger than 1 is not in this region, but it's a convex region so there has to be a separating hyperplane. here we have the separate a plus b is bigger than 0. so what does this mean? well it means that all of our equations were of the from ax plus by at least 1 with a plus b being at least zero. now what this means though is if we take x equals y equals the same big number, ax plus by is equal to a plus b times this big number. since a plus b is positive and this other number is big, that's actually more than 1. and so we actually have a solution. if x equals y is a big number, and in particular x equals y equals 1 gives us the solution. and so we're able to do here is, we said, well 0 bigger than 1 is not a linear combination. we have a separating hyperplane and by looking at the form of this hyperplane, it allowed us to find an actual solution to our system. and this is sort of how it works in general. okay so that's the proof of duality, we should also look at complementary slackness. remember what this said is we have our linear program and it's dual. and if you have the solution to the dual, where yi is strictly bigger than zero. this can happen only if the ith equation in the primal is tight. to prove this is really actually not that hard. you take a solution x to the primal and a matching solution y to the dual. so the best you can do in the primal is x.v equals t. but by duality, you get a matching lower bounds in the dual. so there's a combination of these linear inequalities, ei. so the sum of yi ei, yields the inequality x.v is at least t. now each of these equations ei, those were true equations for that value of x, and the final inequality that we get is actually tight. so we have the sum of a bunch of inequalities, it gives us a tight inequality. the only way that that can happen is if the inequalities that we used are tight. and so that means for each i, either the inequality ei is tight, or yi = 0. and that proves complementary slackness. okay, so that's all we have for this lecture. come back next time and we will go back and start talking about some formulations and different ways of looking at linear programming problems. 
hello everybody, welcome back to our unit on linear program. today, we're going to talk about some sort of different types of linear programming problems. it should have all related but not quite the same. so the point is there's actually several different types of problems that go into the heading of linear programming. now, the one that we've been talking about so far, we might call the full optimization version. minimize or maximize a linear function subject to a system of linear inequality constraints. or maybe say the constraints have no solution if they don't. now, it turns out there are a number sort of related problems dealing with sort of systems of linear inequalities that you might want to solve, that are maybe a little bit easier. and will actually be important, we start coming up with algorithms in the next couple of lectures that will actually be solving algorithm's other formulations. so the first one is optimization from a starting point. given the system of linear inequalities, and the vertex of the polytope they define, optimize a linear function with respect to these constraints, so you're given a place to start. another version is one they call solution finding. given the system of linear inequalities, no objective whatsoever, find some solution to these systems, assuming it exists, this is also somewhat easier. and finally, we can satisfiability. given the system of linear inequalities, determine whether or not there is a solution. so it turns out that these problems are actually equivalent. if you can solve any one of these, you can solve the others. which is very convenient, because the algorithms we'll be looking at will each only solve one of these versions. first off, it's clear that the full optimization of the problem is the strongest. using that you can solve anything else you want. if you have optimization from a starting point, you can just ignore the starting point and solve the problem from scratch. if you're trying to find a solution while the optimal solution is a solution and if you merely want to know if there is a solution. well you try and run a full optimization and it outputs a solution, you have a solution, great. but the nice thing is that you can go the other direction. if you can only solve optimization from a starting point, you can actually do the full optimization. and the problem here is, how do you find the starting point? if you had the starting point you could run the algorithm and you'd be done. but somehow you need a solution to this system and there's actually a clever way to do this, you add equations one at a time. so now we have a solution to the first seven of your equations. we now need to add to make it a solution to the first eight. for that, we need to say, well, maybe your solution, it doesn't satisfy this eight inequality. well what you can do is you can optimize. so not only do we want to satisfy these seven, we want to make this eighth inequality, maybe, as true as possible. and that one will give you a solution set that satisfies all of them. so to see how this works let's look at an example. we start with this rectangle in a nice corner of it. we know where to add an inequality that chops our rectangle at this line. so what we're going to do is we're going to say, well, we want our sort of point to be as much below this line as possible, that's just a linear optimization question. so we can solve that using our optimization from starting point algorithm. we get that vertex, and what do you know? it's a solution to this bigger system. next, we want to add this thing as an inequality. so again, we solve our optimization from starting point, we find a solution, we can now add the additional inequality. we add another one, find a solution, and then finally we've got all of our equations in the mix. we now need to do for our optimization and we can do that. so this is basically how you do it, you act one equation at a time. there is a technical point to keep into account. things are a bit messier if some of these intermediate systems that you're trying to solve don't have optima. they might need one of these unbounded systems where things can get as large as you like. now, to fix this, i mean it's not hard, you just need to play around a bit. first you want to start with n constraints. that means you actually have a single vertex to start out your system and then we you are trying to add the constraint v.x at least t, you don't just maximize v.x. that would be sort of a problem because that might be unbound. so what you'll do is, you'll add the additional constraint that v.x is at most t. and this guarantees that v.x will actually have a maximum at t and once you find it, that'll be good. okay, so that was that, let's talk about solution finding. how do we go from being able to find a solution to find the best one? we somehow need to guarantee that the solution we found was the best solution. but there's actually a good way to do that, we can use duality. the point is that duality gives you a good way to verify that your solution's optimal by solving the dual program and providing matching upper bound. so what you want to do is you want to find both a solution to the original program and a matching dual solution. so if you want to minimize v.x subject to ax at least b, what you can do is you can instead solve this bigger system. ax at least b, y at least 0, y transpose a at least equal to v. that says x is solution to the primal and y is solution to the dual and then we also want x.v = y.b. so, now we have a matching solution to the primal and the dual. the fact that we have this solution to the dual means that you can't do any better in the primal. and so this actually any solution to this system, if you look at x, that will give an optimal solution to the original problem. finally, let's talk what's satisfiable. how do you just know whether or not there is a solution? how is this going to help you find actual solutions? well, we know there's always a solution at a vertex of your problems. and that means that any of these equations that we have are actually tight. now all we need to do is figure out which equations are tight. and then you can solve for the intersection of those equations using gaussian elimination. so how does this work? we have a bunch of linear equations, here are the lines where we've got the equalities. and the solution to that system is that hidden triangle here. we want to find a point on this track, so what are we going to do? we are going to pick some equation here and say is there is a solution to not only this linear system, but this linear system where that equation is actually an equality. and for this guy the answer is no, because there is no point in the triangle that also lies on that line. that means that we can actually throw out that line. there are no solutions, that line actually doesn't help us at all. next we try another one, this guy doesn't work, we throw him out. this guy, yes, there are solutions. they're both inside the triangle and on this line, so we keep that equation around as an equality. and then try another one, what about this guy? is there a solution to this system where we're on both of those lines? no, the intersection of those lines is not a solution so we keep going, what about these lines? yeah, if you look at the intersection of those lines it gives you that point, which is a solution. but now we know there's a solution at the intersection at those lines, we solve for that intersection with gaussian elimination, and we're done. and so by just solving a whole bunch of satisfiability questions, we're able to actually resolve this question and actually find one that satisfies us. so, to make sure that we understand this, if we want to find the solution to a linear program with m equations and n variables. how many times would we have to call a satisfiability algorithm in order for this to work? well, the answer is we need to do it m times. you need to test each equation once. when you find equations that work, you keep them around. but, i mean, you don't need to test them again. those are equalities, and you just keep checking the other ones. each equation is to be tested once, that's a total of m times. but you run this satisfiability algorithm m times and it gives you a solution to actually find a point. so those are the different formulations, next time we're actually going to come back and start looking at honest algorithms for solving the linear problems. 
hello everybody, welcome back to our unit on linear programming. today, we're finally going to get to an actual algorithm to solve linear programs. in particular we're going to talk about the simplex method, which is basically the oldest algorithm for solving linear programs. and as it turns out, it's still one of the most efficient. now unfortunately, as we'll see, the runtime of this algorithm isn't quite as good as we would like, but it's still pretty reasonable for many contexts. so first off, remembering in our lecture last time, where actually this is going to solve a specific formulation of linear programming. and in particular, it's going to solve optimization from a starting point. so if you want to use this method to solve an actual full optimization problem, you'll have to remember how you do that based on the last lecture. so, what's the idea here? we start at some vertex of our polytope, and we know that the optimum is at another vertex. so what we're going to do is, we're going to just find path between vertices that sort of as we go, that our objective will get better and better and better until we reach the optimum. so, how do vertices work? well, you get a vertex of your polytope when you look at the intersection of n of the defining equations. and you take n of your defining equations, you make them all tight, they intersect at a vertex, and you can solve for that vertex using gaussian elimination. now if you relax one of these equations, then instead of having a zero dimensional thing, you have a one dimensional thing. and you end up with an edge, you get a set of points with the form p + tw where t is non-negative. the constraint that you relaxed requires the t now be non-negative. so this gives you an edge and it continues, t being zero all the way up to the whatever, until you violate some other constraint in your linear program. and then you get another vertex at the other end of the edge. now if v dot w is bigger than 0, if you follow this edge, you get a larger value of the objective at the new vertex. so here's the pseudocode for the simplex algorithm, it's actually pretty easy. you start at a vertex p and you repeat the following. you look over each equation, passing through p and you relax that equation to get an edge. if, when you travel along that edge, it improves your objective, you replace p by the vertex you find at the other end of that edge. and then you break, you go back to for each equation running through p. if, however, there was no improvement, you tried every edge going out of the p, none of them did any better, you actually know that you're at the optimal vertex and you return p. now to look at what does it mean to go to the other end of this edge, it's basically what we said before. your vertex p was defined by n equations. you relax one equation and now the general solution of these n- 1 equations is a point at the form p + tw over real numbers t, you solve for this using gaussian elimination. the inequality that you relaxed requires, say, that t be bigger than or equal to 0. and each other inequality in the system might put other bounds on which t are valid to have solutions to your equation. now some of them might put upper bounds on t. they might say t is at most 7, or at most 10, or whatever. and of those upper bounds you take the smallest of them and call that t0. then p + t0 is the vertex that you will get at the other end of that edge. now, to show that this is correct, we need to prove the following theorem. if p is a vertex that is not optimal, then there's actually some vertex adjacent to p that does better. and this means that we will keep finding better and better vertices until we find the best one, the optimal, and then we can return that. now the proof of this isn't so hard, you've got a vertex p. it's the intersection of n equations, e1 through en. now what you'd like to do to prove this is optimal, is you want to use sort of the dual program. you want to find a positive linear combination of e1 through en to find an upper bound, x dot v is at most whatever. now you, of course, can't always do this, but it's actually going to be the case that you can basically always write x dot v less than or equal to something as some linear combination of these constraints. possibly using negative coefficients. now if all the coefficients were positive, of course, we have an action solution that the dual program that p is optimum. however, if some of the coefficients were negative, you could actually show that if you relaxed the equation with a negative coefficient, then that actually will give you an edge where if you move along that edge your optimum gets better. and so that proves the theorem. if we're not at the best point ever, then we can find an edge to follow that does better. okay, so how long does the simplex algorithm take? well, basically, taking one step on this path isn't so bad. it's a nice polynomial time thing involving trying a bunch of edges and doing some gaussian elimination. but we do have to take this path that goes from wherever we started to the optimum. and how long the algorithm takes will depend on how long that path is. and unfortunately, the path might be somewhat long. so we'll suppose that we have the sort of almost cube like thing as sort of the polytope defined by this linear system. we're trying to optimize the height, we're trying to go up as far as possible. we start at the marked vertex and we're going to use the simplex method to travel to other vertices, increasing the height as we go. now the question, of course, is what's the longest path that we might need to take? what's the longest number of steps that we might need to take in order for the simplex algorithm to complete on this example? well, unfortunately, it might take as many as seven steps to find the optimum. because it's possible that we can take this path as shown that actually passes through every single vertex in the polytope. now some paths are better, some other paths could take as few as three steps but seven is big. and, in fact, if you do an n dimensional version of this, you might have only two inequalities that take actually two to the n steps to get where you're going. and this really isn't so good. and so it turns out the runtime of simplex is proportional to the path length. now the path length in practice is very often very reasonable. however, you can always find there are some unusual examples where the path length is actually exponential. and this means that simplex algorithms sometimes takes quite a long time to finish. now there's one other technical problem, this is degeneracy. so in this analysis, we assumed that only n hyperplanes intersect at a vertex, and that's not always the case. for example, in the picture that we have here, we've got a pyramid and the vertex of that pyramid is on four of the defining hyperplanes. even though you're in only three dimensions. if you have some of these degenerate vertices, it's actually a little bit hard to solve the system because we don't know which equation we're supposed to relax in order to follow an action. we'd actually have to relax two of our equations to get to an edge and we wouldn't know which ones to use. so there's actually a fix for this, which is not very hard. which is, if you take all of your equations and tweak them by just a tiny, tiny bit, this basically doesn't change your solution at all. but it avoids having any of these degenerate intersections. now, in fact, if you're willing to be a little bit more sophisticated, you can run a version of this fix that doesn't involve actually changing things at all. you can sort of make infinitesimal changes, changes that are sort of only formally there. so you number your constraints 1, 2, 3, 4, 5, etc. you strengthen the first constraint by epsilon and the next by epsilon squared and the next by epsilon cubed. where epsilon is just some incredibly tiny number, sort of so tiny that it jsut doesn't matter for anything. then in practice, when you want to solve the system, you don't actually need to change any of your equations. if you're at a degenerate point, you need to keep track of which n equations you are really on, which are really the n equations defining this point. and then when you're travelling along an edge that hits a degenerate point, you need to figure out which is the new equation you're actually at. and, for this, you should always add the lowest numbered constraint at the new corner. so if you hit a new corner that actually has three hyperplanes passing through it, you pick the lowest numbered of them to be the real one. now when you do this, you in fact, might have some edges that pass from a degenerate corner to itself. and this is fine as long as you keep track of which n hyperplanes you're actually on at any given time. and you make sure you only actually use that edge if it improves your objective. if the edge that you followed, which should have had zero length, was in a direction at least that made things better for you. but when you do this, we have a nice algorithm called the simplex method. it solves linear programs by moving between adjacent vertices trying to hit an optimum. it actually works pretty well in practice but potentially is exponential time. and if you're really worried about that, come see the next optional lecture that we're doing on the ellipsoid algorithm, which is a much more modern technique for solving linear programs. 
hello, everybody, welcome back to linear programming unit. today, we're going to talk about one more algorithm for solving linear programs, namely the ellipsoid algorithm. so, remember last time we had the simplex algorithm, this solves linear programs. it works pretty well in most cases, but in some of the time, it's actually exponential which is a problem. today we're going to talk about the ellipsoid algorithm, this again solves linear programs. it's actually polynomial time in all cases, but it turns out in practice is often not as good as the simplex method. so, to begin with, the ellipsoid algorithm solves a particular formulation, it solves the satisfiability version of a linear program. given the set of constraints, will just tell you whether or not there is a solution. so, here's how the algorithm works. the first thing you do is you take all the equations and relax them by a tiny, tiny bit, you make them a tiny bit more lenient. and if there weren't solutions before, there still won't be solutions. but if there were solutions before, even if beforehand there was only a single point that was a solution, there's now going to be some small positive volume in your set of solutions. the next thing you do is you bound the set of solutions with a large ball. you find a large ball that either contains all of your solutions or maybe just contains a large fraction of, some notable fraction of your solutions, assuming that they exist. and this isn't too hard to do by just taking it really, really, really big. then what you do, is you have a ball, or in general an ellipsoid, that contains all of your solutions. what you do is, you look at the center of this ellipsoid and say, is that a solution to your equations? on the one hand, it might be a solution to your system, in which case you've found a solution, your system is satisfiable, and you're done. on the other hand, it might not be a solution. if it's not a solution, we have a point that is not in a convex region. so you can find a separating hyperplane that separates this center from your region. what this means is that your entire convex region is on that side of the hyperplane. so instead of being contained in your ellipsoid it's actually contained in a half-ellipsoid. however, when you have a half-ellipsoid you actually can find a new ellipsoid whose volume is smaller than the one you started with that contains this entire half-ellipsoid. and thus, we now have a smaller ellipsoid that also contains your set of solutions. so now what we're going to do is we're going to iterate this. we keep finding smaller and smaller ellipsoids that contain our solution set. and eventually one of two things happens. either eventually we find that the center of our ellipsoid is actually contained in our solution set, in which case we're done. or eventually, we end up with ellipsoids that are really, really tiny and yet still guaranteed to contain our entire set of solutions. but from step one, when we did this relaxation, we knew that if there were solutions, our set of solutions has some small positive volume. and eventually we'll find ellipsoids that are smaller than that. and therefore, they're too small to contain our solution set if they existed. and then we will know that there must be no solutions. so, what's the runtime of this? well, you have to figure out how many iterations it takes, it's a little bit of a mess. but the runtime of ellipsoid algorithm is something like o((m + n squared) n to the fifth log(nu)). where here, n is the dimension of the space that you're working in, m is the number of equations that you're trying to solve, and u is the numerical size of the coefficients. so things to notice about this. one, it's polynomial, hooray. we have a polynomial time algorithm for solving linear programs. and this is pretty great. however, it's a bad polynomial, it runs in something like n to the seven time. n to the seven's really not that great. and there are a lots of circumstances where i'd rather taking an exponential algorithm, or a mildly exponential algorithm at least, over an n to the seven algorithm. finally, we'll note that the runtime actually depends, albeit logarithmically, on the size of the coefficients. and this might be a problem if you have really, really complicated coefficients in your linear program. this affects how much you can relax your equations and how big the ball needs to be. and if you have these sorts of problems, ellipsoid will run very slowly. whereas if you're running the simplex method, no matter how big your coefficients are, at least the number of algebraic operations that you need to perform doesn't depend on the size of the coefficients. now there's one final thing to note about the ellipsoid algorithm. we don't really need that much information about our system of equations or inequalities. all we really need is what's called a separation oracle. that is, given a point, x, we need to either be able to tell is x in our system, or if not, we need to find some hyperplane that separates x from our set of solutions. and there are actually some circumstances where you don't have explicit sets of equations, or defining your system, but can produce a separation oracle. and in these cases you can actually use the ellipsoid algorithm to solve linear programs even though you don't have an explicit list of finitely minute equations. and this is really useful in some cases. so in summary, the ellipsoid algorithm is another way to solve linear programs. it has better worst-case performance than simplex. however, it's usually going to be slower. in practice, it's generally not as good. however, on the plus side, you can run the ellipsoid algorithm, only with access to a separation oracle, which is nice. and there are definitely a few contexts where being able to do this is quite useful. in any case, that wraps up our unit on linear programs. i hope you enjoyed it. come back next time and sasha will start talking about complexity theory. and in particular, we'll be talking about various aspects of np-complete problems. so, i hope you come back for that, and i'll see you then. 
hello and welcome to the next module of the advanced algorithms and complexity class. in this module, we are going to meet problems that combinations very hard. in all previous modules, we considered many efficient algorithms for various combinational problems. by saying efficient, we usually mean polynomial algorithm and this is why. consider for algorithms shown here on the slide, because a running time of the felt algorithm is just n is usual by n with the other size of the input. the running time of the second algorithm is n squared, so it is for the right timing algorithm and the third one is a cubic time algorithm and the last one is running time 2 to the n. so here, we have polynomial time algorithms, three polynomial time algorithms and the last one is exponential time. the second draw here in the table shows the maximum value of n for which the total number of steps performed by the corresponding algorithm stays below than 10 to the 9. why 10 to the 9? well, just because this is roughly the estimate for the number of permutations performed by modern computers in one second. so, we're interested in the maximum value of n for which the running time of the corresponding algorithm stays below the one second. it is not difficult to compute these values. for the first algorithm, this is 10 to 9, of course. for the second algorithm, this is 10 to 4.5 and for the third one, it is 10 to the 3. so polynomial time algorithms are able to handle instances of size roughly thousand, so even millions. while for exponential time algorithms, the maximum value for which it performs less than 10 to the operations is roughly 30. so, it allows us to process only very small instances. recall that any exponential time function grows faster than any polynomial time function. so for this reason, exponential time algorithms are usually considered as impractical. note, however, the following theme. usually, for many computational problems, the corresponding set of all candidate solutions is exponential. let me illustrate this with a few examples. i assume that way given n objects since our goal is to find an optimal permutation. an optimal in some sense permutation of this object. a nice way to do this would be to go through all possible, such permutations and to select an optimal one. the running time the corresponding algorithm, however is going to be at least ten factorial, because there are n factorial different permutations of n given objects. and n factorial grows even faster than any exponential function, 2 to the n, for example, which means that the corresponding algorithm is going to be extremeless low. another example is the following. assume that we're given objects and we need to split them into two sets. for example, we need to partition a set of vertices of a graph in the two sets to find a cut. then again, a nice way to do this would be to go through all possible partitions into two sets and to select an optimal one. however, there are 2 to the n ways to split the n given objects into two sets. and if we do this the running time of the algorithm is going to be at least 2 to the n and we know that this is very slow. this only allows us to handle instances of size roughly 30 in less than 1 second. another example is assume we need to find a minimum spanning tree in a complete graph, that is in a graph where we have an edge between every pair of vertices. a naive way to do this would be to go through all possible minimum spanning trees and to select one with minimum weight. however, the total number of spanning trees in a graph on n vertices is n to the n-2. again, this grows even faster than 2 the n and this makes the corresponding algorithm completely impractical. so once again, in many cases, an efficient polynomial algorithm is called efficient in particular, because it avoids going through all possible candidate solutions, which usually has exponential size. in the rest of this module, we will learn that there are many computational problems that arise frequently in practice actually for which we don't know an efficient that is polynomial time algorithm. for such problem, roughly the best we can do is to go naively through all possible candidate solutions and to select the best one. it will turn out also surprisingly that all these seemingly different problems, well, millions of problems they are related to each other. namely if you construct an efficient, if you design an efficient algorithm, a polynomial time algorithm for at least one of them, this will automatically give you a polynomial time algorithm just for all these problems. at the same time, constructing such an algorithm turns out to be an extremely difficult task. in particular, there is a one million dollar prize for constructing such an algorithm or proving that there is no such algorithm. 
we will now give a formal definition of a search problem. and we will do this by considering the famous boolean satisfiability problem. the input for this problem is a formula in conjunctive normal form, which is usually abbreviated just as cnf. so a formal and conjunctive number form is just a set of clauses. in this case, in this example, we have five clauses. this is the first one. this is the second one. the third one, the fourth one, and the last one. each clause is a logical, is a logical or, or a disjunction of a few literals. for example, the first one is a disjunction of literals x, y, and z. the second one is the disjunction of x and the negation of y. the third one is a disjunction of y and not z, or a negation of z, and so on. so x, y and z are boolean variables. these are variables that take boolean values. the boolean values are true and false and we will usually use 1 instead of true and 0 instead of false. so what this formula tells us is the first clause actually constrains the values of x, y, and z to be so that either x = 1, or y = 1, or z = 1, right? so this is just x, or y, or z. the second clause tells use that either x must be true, or the negation of y must be true. that is, either x = 1, or y = 0 and so on. for example, the last clause tells us that either x = 0 or y = 0, or z = 0. then, the boolean satisfiability problem, or just satisfiability problem, which is also abbreviated as sat usually, is stated as follows. given a formula in conjunctive normal form, we would like to check whether it is satisfiable or not. that is, whether it is possible to assign boolean values to all variables so that all clauses are satisfied. if it is possible, we need to output a satisfying assignment. if it is not possible, we need to report that no such assignment exists. now we give a few examples. in the first example, we're given a formula over two variables, x and y. it contains three clauses, and it is satisfiable. to satisfy it, we can assign the value 1 to x and the value 0 to y. let's check that it indeed satisfies all three clauses. well, in the first clause, x is satisfied. in the second clause, not y is satisfied. and, in the last clause x is satisfied. in the second example, we illustrate that a formula may have more than just one satisfying assignment. for example, for this formula there is a satisfying assignment which assigns the value 1 to x, y, and z and there is another satisfying assignment which is shown here. okay, for the last formula, the last formula is unsatisfiable. and probably the most straightforward way to check this is just to list all possible truth assignments to x, y and z. so there are eight such assignments. let me list them all. then for each of these assignments, we need to check that each of them falsifies at least one clause. for example, the first one falsifies the first clause. when x, and y, and z = 0, the first clause is falsified, right? for the second one, it falsifies the clause y or not z, right? the third one falsifies the clause x or naught y and so on. so it can be checked that each of these eight assignments falsifies at least one clause. so another way of showing that this formula is satisfiable is the following. let's first try to assign the value zero to x. then let's take a look at the following clause. so it is x or not y. x is already assigned zero. so the only way to satisfy this clause is to assign the value 0 to y. so setting x to 0 forces us to set y to 0 also. now let's take a look at this clause. it forces us to set the values 0 to z, also. but then we see that this clause is already falsified, right, which tells us that our initial move, i mean to assign 0 to x was a wrong move. that we need to assign the value 1 to x. let's try to do this. if x = 1, let's take a look at the following clause. not x is already falsified in this clause, so we need to assign the value 1 to z. now let's take at this clause. not z is already falsified here so we need to assign the value 1 to y. but then this clause is falsified. so no matter how we assign x, it forces us to some other assignments and in the end, we falsify some clause which justifies that this formula is unsatisfiable. that is a canonical hard problem. it has applications in various branches of computer science. in particular because many hard combinatorial problems are reduced very easily to the satisfiability problems. i mean, many hard combinatorial problems can be stated very naturally in terms of sat and then when a problem is stated in terms of sat, we can use a so-called sat solver, which is a program that solves the satisfiability problem. there are many such programs and there is even a [inaudible] competition of sat solvers. sat is also a classical example of a so-called search problem. in a search problem, we're given an instance site, or just and and our goal is to final a solution for this instance. a solution s or to report that there is not such solution. for example, in case of the sat problem, an instance i is a formula in conjunctive normal form and s is a satisfied assignment. for this formula, we need to check whether there is a satisfying assignment and to return one if it exists. or to report that this formula is unsatisfiable. this is, that there is no satisfying assignment. a natural property to require from a such problem is that we can quickly check with a given solution s is indeed a solution for i. for example, in case of sat, it is easy. if we are given a truth assignment of values to all the variables, we can quickly check whether it satisfies all the clauses. namely, we just count all the clauses from left to right and for each clause, we check whether it contains literal that satisfies this clause. another natural property is that we require the length of s to be bounded by polynomial in the lengths of i. right, so we want s to be not very large. we do not want s to have, for example, exponential size in the length of i. in this case, it would require us an exponential time just to write down a solution for the instance i. so once again, the natural property of a search problem is the following. we have an algorithm which checks whether the given solution s is indeed a solution for an instance i in time, which is bounded by polynomial in the lengths of i only. this also forces s, the length of s, to be bounded by a polynomial of i. in fact, it is convenient to define a search problem through such a very fine algorithm. namely we say that this search problem is defined by an algorithm c that takes two parameter as an input. an instance i and a candidate solution s. it should run in time polynomial i, in the length of i, and we say that s is a solution for the instance i if c of i, s returns true. for example, sat is clearly a search problem. in this case, once again, i is a cnf formula and s is a truth assignment of boolean values to variables. and this algorithm, c, just scans all the clauses and checks whether each clause contains a literal that is satisfied by the given assignment, s. of course, it's surrounding time is polynomial in the lengths of the formula. great. in the next part, we will see a few examples of search problems that arise frequently in factors for which we still don't know polynomial time algorithms. 
how i feel is hard problem is the traveling salesman problem. in this case, making it a graph with vertices that we know the distance between a, two vertices. together with this graph, we are given a budget, b, and our goal is to find a cycle in this graph that visits each vertex exactly once and has total lengths at most b. finding a short cycle visiting all the given points is a usual task solved by delivery companies. for example, this is how an optimal cycle looks like if we need to deliver something into 15 biggest cities in germany. and those application is drilling holes in circuit boards. assumes that we have a machine that needs to visit some specific places in a circuit board to drill in these places. of course we would like our machine to visit all these places as fast as possible. and for this we need to find a cycle that visits all those places whose length is as short as possible. note the following subtlety. the travelling salesman problem is of course an optimization problem. usually we are given just the graph and our goal is to find the optimal cycle that visits each vertex exactly once. that is a cycle of minimum total weight, of minimum total lengths. at the same time, in our statement of this problem, we also have a budget b. and our goal is to check whether there is a cycle that visits every vertex exactly once, and that has total lengths at most b. we did it to ensure that this is a search problem. indeed, it is very easy to check whether given a solution, it is indeed a solution. for this, we need to check that what is given to us is a sequence of vertices that forms a cycle, that means it's each vertex exactly once, and has total lengths it must be. it is easy to do, we just trace the cycle and check that it's lengths is at most b, right? however, it is not so clear for an optimization version. if you are given a cycle, how are you going to check whether it is optimal or not. once again, we stated the decision version of the travelling salesman problem to ensure that this is a search problem. at the same time, in terms of algorithms, these two versions of this problem. i mean an optimization version where we need to find an optimal cycle and a decision version where we need to check whether there is a cycle of total length at most b. these two problems are hardly different. namely, if have an algorithm that solves optimization problem, we can of course use it to solve the decision version. if we have an algorithm that finds an optimal cycle, we can of course use it to check whether there is a cycle of links that must be or not, and vice versa. if we have an algorithm that for every b checks whether there is a cycle of lengths that must be, we can use it to find that optimal value of b. by using binary research. namely, we first for example check whether there is an optimal cycle of lengths 100. if yes, we check whether there is an optimal cycle of lengths 50. if there is no such cycle we then check whether there is an optimal, whether there is a cycle of lengths at most 75 and so on. so it might, eventually we will find the value of b such that there is a cycle of lengths b but there is no cycle of smaller lengths. at this point, we find, we have found the optimal length of a cycle that visits each vertex exactly once. and this is done by calling our algorithm a logarithmicn umber of times. and the only way to solve the traveling salesman problem is to check all possible n factorial permutations of other vertices. this will give an algorithm whose running time is roughly n factorial. and this is where we quickly draw in function. for example, already for n equal to 15 and factorial is about 10 to 12. which means that this algorithm is completely impractical. there is a better algorithm because running time is still exponential. it is based on dynamic programming and we will see it later in our class. it's running time is n squared times 2 to the n, where n is the number of vertices. so it is still exponential but it is much better than n factorial. in fact, we have no better algorithm for this problem, unfortunately. so this is the best upper bound that we can prove. in particular, we have no algorithm that solves this problem in time, for example 1.99 to the n. at the same time, there are algorithms that solve this problem in practice quite well. namely, even when n is equal to several thousands. it is usually sold by heuristic algorithms. so such algorithms solve practical instances quite well. in practice, however, we have no guarantee on the running time of such algorithms. and also, there are approximation algorithms for this problem. for such algorithms, we have a guarantee, guarantee on the running time. at the same time, what they return is not an optimal solution, but the solution which is not much worse than optimal. for example, in the approximation algorithms that we will study later can find in polynomial time the cycle which may not be optimal but it is guaranteed to be at most two times longer than an optimal one. it is instructive to compare the traveling salesman problem with the minimum spanning tree problem. recall that in the minimum spanning three problem, we are given a graph, or just a set of cities, and our goal is to connect all the cities to each other by adding n minus 1 edges of minimum possible total lengths. for example, the minimum spanning tree for this set of six cities might look like as follows. so added five edges to connect all these six cities. now we can see the travelling salesman was for the same set of cities. so for a moment, assume that in the traveling salesman problem, we need to find not a cycle but a path, okay? then in this case, the optimal path for this set of six cities might look like this. note that in this case, this path, what we're looking for in optimal paths is also a 3, right? so this is a 3 with 5 edges that spans all the vertices. this means that the travel and salesman problem is a problem that we get from the minimum spanning tree problem by posing an additional restriction that the tree that we're looking for should be actually a path, right? so this is emphasized here. and by posing this additional restriction to the minimum spanning tree problem. we get a problem for which we know no polynomial time algorithm. so once again, for this problem. for the minimum spanning tree problem, we have an algorithm whose running time is almost linear. for this problem, we have no polynomial time algorithm. we have no algorithm whose running time is quadratic or cubic or even something like n to the one to 1,000. this is a very difficult problem, resulting from the minimum spending tree problem, by posing some additional small restriction. 
our next search problem is a hamiltonian cycle problem. the input of this problem is a graph directed on, directed without weights and edges and the goal is just to check whether there is a cycle that visits every vertex of this graph exactly once. for example, for this graph, the research cycle. it is shown here on this slide. it is not difficult to check that it indeed, visits every vertex exactly once. and for this reason, in general, this problem is a search problem. given some sequence of vertices, it is easy to check that each vertex appears in this sequence exactly once and that there is a match between any two consequent vertices in this sequence. the eulerian cycle problem looks very similar to the hamiltonian cycle problem. in this problem, we're given a graph again and our goal is to find a cycle that visits every edge exactly once. so in the hamiltonian cycle problem, we need a cycle that visits every vortex exactly once. in that eulerian cycle problem, we're looking for a cycle that visits every edge exactly once. it turns out that the eulerian cycle problem can be solved very efficiently. namely, there is a very simple check whether the input graph is eulerian or not. that is whether it contains a eulerian cycle or not. this is given by the following theorem. so, it deals with undirected graph. assumes that way given an undirected graph and it contains a eulerian cycle if and only if, it is connected and the degrees of all its vertices is even. we now give two toy examples that in particular will shut some light on how to prove the just mentioned serum. our first example is a graph in which there is no eulerian cycle, that is a non-eulerian graph. there is no way eulerian cycle in this graph in particular, because the degree of this vertex is equal to 3. let's prove that it is a degree of a vertex in a graph is equal to 3 and this graph does not have a non-eulerian cital for sure. first, assume as such in cycle existed and assume that it visited this vertex, which is denoted by me. for example, exactly once. so, this is a cycle that visits every edge of our graph exactly once and goes through v exactly once. but in this case, v would have our degree exactly two. there are two edges. we used one of them to go out of v and we used another of them to come back to this galaxy. but in our case, the degree of v is equal to 3. now, assume that there is an eulerian cycle that visits the galaxy at least two times. so, let's start our cycle from the vertex v. so, we walk through our graph. we get back to v and then we walk again, and then we get back to v again. but since in our cycle we visit each edge exactly once, in this case, we see that there are at least four edges adjacent to v. so, which means that either the our degree of v is equal to 2 or it is at least 4. and in general, it is not difficult to see that if we have an eulerian cycle, then the degree of each vertex is must be even. because each time when we come in to some vertex, we need to have a match in edge, which we use to go out from this vertex. i'm going to show an example of an eulerian cycle in a graph and i'll also show how to define it quickly. this is the same graph we stood as a set of three. and in this graph, we can see that the degrees of all vertices are even. namely, the degree of this vertex is 2, the degree of this vertex is 4. this is also 4, this is 6, this is 2, this is 4. so all of them are even and this graph is connected, which means that this graph contains an eulerian cycle. let's try to find it. well, for concreteness, let's start from this vertex and let's just walk through this graph. we first traverse this edge, then we traverse this edge, then we get back to this, to this vertex. at this point, we return to the vertex for which there are no more unused edges. however, there are still some unused edges in our graph. let's just take and let me also mark this cycle, as the first one. so we constructed some cycle, but still there are some unused edges. let's just start traversing the unused edges from some vertex. for example, a couple from this one was in might might traverse this edge and then again get back to the initial vertex and this is cycle number two. in case there are still some unused cycles, some unused edges, so let's extend, for example, let's start from the vertex and let's traverse another cycle. so, this is a set site we're currently constructing the third cycle. so we go here, then here, then we get back here and then we get back here. so at this point, we used all the edges. however, what we have is not just one single cycle, but a bunch of cycles. but the nice property is that if we have several cycles, it is easily to glue them together into a single cycle. schematically, it can be shown as follows. so assume that we have some cycle and we have some other cycle and then in some other point, we have other cycle, then what we can do is to traverse these cycles as follows. so we first go here, then we go this way, then we go this way. and finally, we go this way. let me illustrate how to do this on our example graph. we first go on this edge, then we use this cycle, then we continue in our first cycle, then we triggers this cycle, the third cycle. and finally, we get back to the initial vertex. and this is how we, in general, an eulerian cycle can be constructed. we just work in the graph and when we turn back to vertex which has no unused edges, we just start traversing another cycle from some vertex. the fact that the initial graph is connected ensures that then all the constructed cycles are connected to each other and then we can glue them easily to construct a single cycle, visiting each edge exactly once. let's now summarize. we have two similarly looking problems. in the first one in the eulerian cycle problem, we need to find a cycle that visits every edge of a given graph exactly once. this problem can be solved efficiently in linear time in the size of the input graph. in the second problem, in the hamiltonian cycle problem, we are looking for a cycle that visits every vertex of our graph exactly once. for this problem, we have no polynomial time algorithm. 
our next problem is also about paths in graphs. it is called the longest path problem. here we are given a graph, a weighted graph, and two vertices, s and t, together with a budget b, which is just a number. and our goal is to find a simple path whose total length is at least b. to give a specific example, consider this graph again, and consider the following two vertices. the shortest path, assume also that all the lengths of the shown edges are equal to one. then the shortest path between the following two vertices uses just one edge, and its length is equal to one. on the other hand, there is a very long simple path here in the graph, it visits all other vertices before going from one vertex to the other one, right? so it turns out that the longest path problem is very difficult. it also looks very similar to the shortest path problem, in which we need to find the shortest path. or if we stay at its decision version, then we're given a graph and the budget b, and our goal is to find a path whose lengths is at most b. we know that this problem can be solved very efficiently. for example, if all the weights are equal to one, that is the given graph is unweighted, then it can be solved with for search just in linear time. in the longest path problem, we need to find a path of length at least b, and for this problem we know no polynomial time algorithm. 
our next hard stage problem deals with integers and linear inequalities. namely, the problem is called integer linear programming. the input to this problem is a set, or a collection, or a system of linear inequalities, which we present here in metrics form. and our goal is to find integer values for all the variables that satisfy all the inequalities. to give it our example, consider the following three inequalities. the first one says that x1 should be at least one-half. the second one says that minus x1 plus 8x2 should be non negative. and the last one says that minus x1 minus 8 times x2 should be at least minus 8. as usual, we can represent a set of all solutions, all feasible points to this system of linear inequalities as a convex polygon as follows. we first draw a half space that contains all points which satisfy the first inequality, that's shown here in green. so once again, in the green half space, all the pairs of points (x1, x2) satisfies the inequality x1 is at least half. the second, the blue one, half space, contains all the points that satisfy the second inequality. finally, the red half space shown here, contains all the points that satisfies the last inequality. in particular, the intersection of these three half spaces, this triangle, contains all the points that satisfies our three inequalities. recall, however, that what we need to find is an integer solution. that is, we would like x1 and x2 to have integer values. and so this intersection is non empty, it contains no integer points. so the integer points are here, the closest ones. but none of them is inside this region. right? so it turns out that this additional restriction, namely the restriction that the solution should be integer, gives us a very hard problem. in particular, if we just have a system of linear inequalities and we would like to check whether there is a point that satisfies whether there is a solution to them, then we can use, for example, simplex method to solve it in practice. the running time of simplex method is not bounded by polynomial, so on some pathological cases, it can have exponential running time. but there are other methods like ellipsoid method or interior point method that have polynomial upper bounds in the running time. so in any case, we can solve systems of linear inequalities efficiently in practice. but if we additionally require that we need the solution to be integer, then we get a very difficult problem for which we have no polynomial algorithm at the moment. 
our last hard set problem that we mentioned here, deals with graphs again. it is called an independent set problem. here, we're given a graph and the budget b. and our goal is to select at least b vertices such that there is no edge between any pair of selected vertices. for example, in this graph that we've seen before in this lecture, is there is an independent set of size seven? so the selected vertices are shown here in red. and it is not difficult to check that there is no edge between a pair of red vertices. and the particularization implies that an independent set is indeed a search problem. it is easy to check whether a given set of vertices is an independent set, and that it has size at least b. it is interesting to note, that the problem can be easily solved if the given graph is a three. namely, it can be solved by the following simple greedy strategy, given a tree if you want to find even just the independence side of maximum size we can do the following. the first thing to do is let's just take all the leaves into a solution. then, lets remove all the leaves from the three together with all its parents. and then, lets just continue this process. to prove that this algorithm produces and optimal solution, we need to show the take in all the leaves and our solution is a safe move, that it is consistent with an optimal solution. this is usually done as follows. assume that there is some optimal solution in which not all the leafs are taken. assume that, just for concreteness, assume that this is suboptimal solution. not all the leaves are taken here because, well, we have this leaf, this leaf, and this leaf, which is not in the solution. then we show that it can be transformed into another solution which, without decreasing its size, such that it contains all the leaves. indeed, let's just take all these market leaves into a solution. this will probably require us to discard from a solution all it's parents, but it will not decrease the size of the solution. so what we get is another solution whose size is, in this case, actually the same. but it contains all the leaves. which proves that there always exists an optimal solution which contains all the leaves. and this in turn means that it is safe to take all the leaves. we will see the details of this algorithm later in this class. but in general, once again, if we are given a tree then we can find an independent set of maximum size in this tree. very efficiently in linear time. but at the moment, we have no polynomial algorithm that finds, that even checks where there's a reason and independent set of size b in the given graph in polynomial time. 
now when have a formal definition of the search problem and when we've seen a few example of search problems, we are ready to state the most important open problem in computer science. the problem about classes p and np. so recall once again that the search problem is defined by an algorithm c that takes an instance i and a candidate solution s, and checks in time polynomial in i where the s is indeed a solution for i. in other words, we say that s is a solution for i if and only if the corresponding algorithm c of i and s returns to, then the class np is defined just as the class of all search problems. the name of this class stands for non-deterministic polynomial time. this essentially means that we can guess a solution and then check its correctness in polynomial time. that is a solution for a search problem can be verified in polynomial time. the class p on the other hand, contains all search problems that can be solved in polynomial time. that is all such problems for which we can find a solution in polynomial time. so to summarize, once again the class p contains all search problems whose solution can be found efficiently. this class contains in particulars, as minimum spending 3 problems. the shortest path problem, the linear programming problem, the independent set on trees problem. the class np contains all the problems whose solution can be verified efficiently that is given an instantiation solution for this instance solution, we can check in polynomial time. in the size of this instance, whether this is indeed a solution for. this class contains such problems as a problem, the longest path problem, problem and independent set on general graphs. the main open problem in computer science asks whether these two clauses are equal, namely whether the clause p is equal to the clause np. this is also known as the p versus np question. the problem is open, namely we do not know whether these two clauses are equal and this problem turns out to be very difficult. this is a so-called millenium prize problem. there is a $1 million prize from clay mathematics institute for resolving this problem. note that if p is equal to np, then all search problems, i mean all the problems for which we can efficiently verify a solution, they can be solved in polynomial time. in other words, for all the problems for which we can efficiently verify a solution, we can also efficiently find a solution. on that hand, if p is not equal to np, then there are search problems for which there are no efficient algorithms. so there are problems like, for example, problem for which we can quickly check whether a given candidate solution is indeed a solution, but there is no polynomial time algorithm for finding such a solution efficiently. at this point, we do not know whether p is equals to np or not, i mean, where they are such problems for which there are no polynomial time algorithms. in the next part, at the same time we will show that all the problems that we mentioned in this lecture namely the problem, the longest path problem, the traveling salesman problem, the inter linear programming problem are in some sense, the most difficult search problems in the class np. 
hello and welcome to the next module of the advanced algorithms and complexity class. this module is devoted to reductions. reductions allow us to say that one search problem is at least as hard as another search problem. intuitively, the fact that a search problem a reduces to a search problem b just means that we can use an efficient named the polynomial time algorithm for the problem b to solve the problem a, also in polynomial time. and we can use it just as a black box. pictorially, the fact that the search problem a reduces to search problem b means that we have the following pipeline. assume that we have an instance, i, of a problem, a. now we are going to design an algorithm that solves the instance i using an algorithm, a polynomial-time algorithm for b as a black box. for this reason, it is shown here in a black box. okay, the first thing to do is that we need to transform an instance i into an instance i of the problem a, we must enter an instance of the problem b. we do this by calling an algorithm f. so we plug, we feed the instance i of the problem a into the algorithm f, and it gives us the instance f(i) of the problem b. we then use the algorithm for b as a black box to solve it efficiently, and it gives us one of two outputs. either there is no solution for this instance f(i). in this case, we report that there is no solution also for i, for the instance i of the problem a. otherwise, it gives us a solution s for instance f(i). in this case, we need to transform it back to a solution of i. we do this by using the second algorithm h. and it transforms in solution f(i) into solution h(s) of the initial instance i. we can now state it formally. given two search problems, a and b, we say that a reduces to b and write a to b, if there is a pair of two polynomial time algorithms f and g. the algorithm f transforms any instance of a into any instance f(a) of the problem b, such that the following holds. if there is no solution for the instance f(i) of the problem b, then there is no solution for the instance i of the problem a. otherwise, if there is a solution s for the instance f(i), then by applying the algorithm h to this solution s, we get a solution h(s) of the initial instance i. now when we have an option of reduction, we can imagine a huge, huge graph containing all search problems. so this graph can respond to the class np of all search problems. in this graph, there is a vertex for each search problem and to put an edge between the search problem a and the search problem b a direct approach, if a search problem a reduces to search problem b, okay? then by definition, we say that this search problem is np-complete if all other search problems reduce to it. pictorially, it looks as follows. so the red vertex here corresponds to an np-complete search problem. so in some sense, this problem attracts all other search problems, all other search problems reduce to it. but it otherwise an algorithm for an np-complete problem can be used as a polynomial time algorithm for an np-complete can be used as a black box to solve just all other search problems also in polynomial time. it is not at all clear that such np-complete problems exist in our graph of all such problems, but we will show that they do exist. and we will show, actually, that all the search problems that we've seen in the previous modules, namely satisfiability problem, travel analysis problem, the maximum independence set problem, longest pass problem, integer linear programming problem, they are all np-complete. namely, if you design a polynomial time algorithm for any of them, you will solve just all search problems in polynomial time. 
in this part, we will discuss a general way of proving that a particular search problem is np complete. well, as you know already, we are going to prove np-completeness via reductions. and there are two equivalent ways of using reductions, namely we say when we know that a reduces to b. we know that if the problem b is easy, namely if there is an efficient that is polynomial time algorithm for b, then the results are an efficient algorithm for a. this is just by definition, right? we know that a reduces to b, that is an efficient algorithm for b can be used as a black box for solving a. so if b is easy, if they research an algorithm, then we can use it to solve a in polynomial time. the second way of using reductions is just a counter positive of the first item here. if a reduces to b and we know that a is hard, meaning that there is no polynomial time algorithm for this problem then also there is no polynomial time algorithm for b. then b is also hard. indeed if b was easy then a was easy, right, as shown in the first item. so b cannot be easy in this case. there cannot be a polynomial time algorithm solving b. when proving mp completeness of certain such problems, we will use the following basic fact about reductions. namely, that reductions do compose. more formally, if a search problem a reduces to search problem b, and in turn search problem b reduces to search problem then a reduces to c. let's prove this formally. the fact that a reduces to b means that we have a pair of polynomial time algorithms f sub ab and h sub ab. namely f transforms any instance of a problem a into any instance of a problem b. and hab transforms any solution to problem b and back through a solution of problem a. and similarly for the reduction from b to c. okay, and then to construct such a pair of algorithms to reduce problem a to problem c, we just compose the corresponding function. so, assume that we have an instance ia of the problem a, and we need to get an instance of a problem c. for this, we first apply the algorithm fab to the instance ia, this gives us an instance of the problem b. we then apply the algorithm fbc to the resulting instance of the problem b. this gives us an instance of the problem c. the important thing to note here is that the resulting composed algorithm has polynomial time. to see this note that, assume for example that the running time of the algorithm fab is for example p(n) and its running time is n cubed for example. and the running time of the algorithm f sub ab is q of n and it is n to the 5. then the running time of the composed algorithm is p(q(n). this is p(n) to the 5, and this is in turn n to the 5 cubed, which gives us n to the 15. namely, when you compose two polynomial functions, you get also polynomial function. which justifies that the running time of this algorithm is also polynomial. to get an algorithm which transforms the solution to c back through the solution of a, you just apply the combination of the correspondent to each algorithm. namely, when you get a solution s sub c for the problem c, you first apply the algorithm hbc. this gives you a solution back to the problem b. then you apply the algorithm h of ab to transform it back to a solution of the problem a. pictorially, this composition of reductions means the following. whenever we have these two pair of edges in our graph of all problems of the class np. we also have the following edge, right? so once again if a reduces to b and b reduces to c, then a reduces to c. in other words, if there is an edge from a to b and from b to c, then there is an edge from a to c. yeah, so simple. this allows us to prove np-completeness as follows. if we know that a reduces to b, and we know that a is np-complete, then so is the problem b, then b is also np-complete. let me prove this pictorially, so this is, again, our graph, we have two problems, a and b. and we know that a reduces to b. we also know that a is np-complete. meaning that a attracts all other problems. in other words, there is an edge from any other problem to a. now, since there is an edge from any other problem to a, and that at the same time, there is an edge from a to b, through the composition of reductions, we know that there is an edge from any other problem to b, right. so this is the picture that we have. b also attracts all as a problems, so by definition it is also np-complete. our plan for the rest of this module is the following. we will first show that the independent set problem can be reduced to a vortex cover problem. we will then show that the 3-satisfiability problem reduces to independent set problem. we will then show that satisfiability problems, a general version of satisfiability problems, reduces to its special case, namely to this re-satisfiability problem. and finally we will show that all other search problems reduce to sat, in particular the 3 problems shown here above the satisfiability problem. 
how a fast reduction is from the independent set problem to the vertex cover have a problem? we call that, in the independent set problem, we're given a simple and direct graph and weight it together with the budget b. and our goal is to find at least b vertices in this graph such that, there are no edges between any pair of them. the vertex cover is formulated as follows, the vertex cover problem. we are again given a graph, a simple, unweighted, undirected graph, together with the budget b. and our goal is to find at most b vertices that cover all edges of our graph. that is, any edge of this graph has at least one end point in the selected set of at most b vertices. let's consider for example, in this graph, we have eight vertices, and one example of an independent set of this graph is e and c. it is interesting to know that in this case, we cannot add any other vertex to this independent set. for example, if we add the vertex b, then we will violate the properties that there is no edge between two vertices because there is an edge between b and c. we cannot add f because it is connected to e. we cannot add a because it is connected to e. cannot add h, cannot [inaudible] d because it is connected to c and so on. so we cannot extend the independent set. you see in this case, still there is a larger independent set in this case. it includes vertices a, c, f, and h. it has size four. it is not difficult to check that it is indeed an independent set, that no two red vertices are joined by an edge in this graph. concerning vertex cover, one vertex covering this in this graph had size six. it includes all vertices except for e and c. it is indeed not difficult to check that for any edge in this graph, at least one of its endpoints is green. for example, for this edge that this endpoint, there's the vertex g has selected for this edge, both of its endpoints are included into the vertex cover. recall that we're interested in vertex covers of small size, and in this case, there is a smaller vertex cover, it is a set b,e,g, and d. and again, it is not difficult to check that these being vertices touch all the edges of our graph. by reviewing this example, you probably already noticed the following simple property of independent sets of a graph and its vertex covers. i is an independent set of the graph g even though v it's complimentary set of vertices is a vertex cover in this graph. let's prove this formula. so i assume that i is an independent set, this essentially means that there are no edges in both end points in the set i. this in turn means each edges at least one end point in the complimentary settings of e minus i. and this in turn means three minus i is a vertex cover of a graph, right? for the reverse direction, i assume that v minus i is a vertex cover of the graph. this means that v minus i touches every edge. this again means that for each edge, at least one of its endpoints lies in v minus i. and this means that for no edge, two of its endpoints lie in i, which in turn gives us that i is an independent set of the graph g. this relation allows us to design a very simple reduction from the independent set problem to the vertex cover problem. namely, to check whether the given graph g contains an independent set of size at least b. we just check whether the same graph g contains the vertex cover of size at most v minus b. we already know that there is an independent set of size at least b, if and only if there is a vertex cover of size at most b. so formally, our reduction is specified by two very simple and clearly polynomial time algorithms. algorithm f needs to transform an instance of the problem independent set to the instance of a vertex cover problem. so we take an g and the budget b, and we'll leave the graph g untouched, and we'll replace the budget by v minus b. then we call an algorithm as a black box, an algorithm for the vertex cover problem on the following instance. if it finds out that there is no vertex cover of size at most v minus b in this graph, then we just report immediately that there is no independent set of size at least b in our initial graph. otherwise, it returns as some solution s which is a vertex cover in the graph g of size at most v minus b. by taking the complimentary of this set s, we get a vertex cover of size at most b. 
we now come to a more interesting reduction that connects boolean logic to graphs. namely we are going to reduce this 3-sat [inaudible] problem, there is an independent set problem. recall that in the 3-sat problem, our input is a formula in 3-cnf, that is a collection of clauses. each clause is a disjunction of at most three literals. and our goal is to check whether it is possible to assign boolean values to all the variables of the formula f, so that to satisfy all the given clauses. since we need to reduce this to the independent set problem our goal is to polynomial logarithm that takes the formula f in 3-cnf. and outputs a graph g and an integer b such that the input formula f is satisfiable, if and only if, there is an independent set of size at least b in the graph g. before the sign in, this reduction let's recall what does it mean to satisfy a given formula in conjunctive normal form. this essentially means that we need to assign a boolean value to each variable of the given formula such that each clause contains at least one satisfied literal. the given example, consider the following formula. it is satisfied by an assignment which assigns the value true as value 1 to all the variables. indeed, in the first clause we have just all the literals in the first clause are satisfied. in the second clause, the first literal is satisfied by this truth assignment. and in the last clause, the first literal is also satisfied by this truth assignment. then with another assignment that satisfies the same formula, namely if i assign the value 1 to x and the value 0 to y and z. we satisfy the first literal in the first clause, we satisfy both literals in the second clause, and we satisfy the last literal in the last clause. this point of view gives us an alternative definition of the satisfiability problem. namely to satisfy a given cnf formula, we need to select from each clause of a given formula one literal, such as the resulting said of selected. literals is consistent, meaning that it doesn't contain a literal l, together with its negation. let me illustrate this on a toy example, as usual. assume that we have a formula consistent of three clauses over variables x, y, and z. well by consistent set of literals in this case, we mean for example x, x and not z. so this is one, so from first clause we select the literal x, from the second one we also select the literal x, and from the third clause we select the literal not z. note that for these consistent set of literals, we can find the corresponding satisfying assignment. namely, we assign value 1 to x, and the value 0 to z. namely, we satisfy all the literal in this set. so for this we assign the value 1 to x and value 0 to z. it is easy to see that this is indeed a satisfying assignment. so in the first clause x is satisfied on the second clause, x is satisfied on the last clause not z inside this fight. it was the value of y doesn't actually matter in this case. we can get a the full consistent set of literal just by selecting x and from the first clause x from the second clause, and y from the third clause. and as a possibility is to select x from the first one, not y from the second one and not z from the third one. this will give us the following constant set of literals. once again, we want this set of literals to be consistent so that we are able to assign a specific value for reach literal. so for this, to explain this better let's consider some inconsistent sets of literals. assume that from the first clause we select not y, from the second clause we select the negation of y from the second, from the third clause we select not z. so this gives us the following set of literals, which is inconsistent because it contains was y and a negation of y. in this case, we can not assign the value to y to satisfy all the laterals in this clause. namely if we assigns a value 0 to y, then we falsify this literal y. if we assign the value 1 to y, we falsify this literal in this clause. and this is exactly the reason why we would like our set of selected literals to be consistent. this alternative statement of the satisfiability problem is the main idea, is the main ingredient of our reduction from 3-sat to the independent set problem problem. we illustrate the reduction first on a toy example as usual. given a formula in this case with five clauses, so with three variables, x y and z. the first step is to introduce for each clause two or three new vertices labeled with the literals of this clause. in this case, it gives us the following picture. so x, y and z here correspond to this clause. x and not y here respond to this clause the last three vertices to respond to this clause and so on. the next thing to do is to join all the vertices, all the pairs of vertices inside each block. so for those three clauses, we have two triangles and for those two clauses, we have just three edges. once again, each three clause corresponds to a triangle, each two clause, that is clause of length two corresponds to a single edge. so all these edges actually, what am i going to do, is to find a large independent set in this graph. and all the newly added edges force us to select into our independent set at most one vertex from each block. for example we cannot select two vertices from this block you aren't doing independent set because any two of them are joined by an edge. the same is for this block, for example. we can only select just one vertex from this block into an independent set, because they are connected by en edge. so selecting a vertex from each of these blocks into an independent set, corresponds to selecting a literal from the corresponding clause, that is going to be satisfied in this clause. we call that in each clause we need to select a literal which is going to satisfy this clause. so we mimic this by selecting the corresponding vertex from a block. however, recall that we need to select a consistent set of literals. that is we do not want to select for example, was x from some clause and not x from some other clause. to force our set of selected vertices to be labeled with consistent set of literals, we just add all possible edges between all pairs of complementary literals. namely, whenever we have a vertex labeled with x like this one, and not x kike this one, we join them by an edge. so we have x here, we have not x here, we join them by an edge. another pair of vertices x here and not x here. we also join them by an edge, then there is another vertex labeled by x, we'll also join them by an edge, x and not x and z this not x. and we do this for all of the pairs of complementary literals. now, the claim is that this graph contains an independent set of sides. exactly 5 or at least 5 if and only if the initial formula is satisfiable. and it is already clear, so if the formula is satisfiable, this means that for each clause we can select one literal such that the resulting set of literals is consistent. but this also means that from each of these five block, so this is the first block, corresponding to the first block, this is the second, this is the third one, this is the fourth one, this is the fifth one. we can select exactly the same vertex that corresponds to the selected literal in this formula. so if the formula is satisfiable, then in each block we can select on vertex. and we know that they are not going to be joined by edges, because vertices from different blocks are only joined if they are complementary. and we know that our set of literals does not contain complimentary literals. so if the formula is satisfiable, then this graph contains an independent set of size 5. and vice versa uses graph contents an independent set of size 5 and the formula is satisfied both. why is that? well just because any independent set of size 5 has exactly 1 various of nature of these blocks. and we know that the labels of these vertices give us a set of literals which is consistent, which doesn't contain a literal together with it's complementary literal. so it gives us a set of literals such that it intersects every clause of our initial formula. which means that if the response, they are satisfying the assignment of the initial formula. to summarize, let's state this reduction formally. the reduction works as follows, given formula f in 3-cnf, that is a collection of clauses, each of length at most three, what is the following? we scan the set of the clauses from left to right and for each clause, we introduce three or two, or just one dependent on the number of literals in this clause. three or two, or one vertices, and we'll label them with literals of this clause and we join every two of them by an edge. so we are now constructing a graph g, and this forces every independent set in this graph to contain at most one vertex from each block to responding to a single clause. then we also need to force each independent set to be labeled with a set of literals which is consist. for this we do the following, we join every pair of vertices in our graph which are labeled with complementary literals. that is if i got axis labeled with a literal l, we add new addresses that connect it to all vertices, which are labeled with a negation of l. then what we've just proved on it, our example is the following. the resulting graph g contains an independent set of size b, where b is the number of clauses of the initial formula even though with the initial formula is satisfiable. it remains to note that the running time of the resulting transformation is polynomial. why is that? well just because to construct this graph we first just scan the formula which takes linear time on the size of the formula. it is just big o of the number of clauses, to make it more formal let's denote by m the number of clauses and in our formula then this step takes time o(m). we just count the list of clauses and during this scan we introduce 3m vertices. and then we need to check every pair of these vertices and to join them by intersect if they're labeled by a complimentary pair of literals. so the second step clearly takes time at most m squared. in any case, this is polynomial so the wall transformation takes polynomial time. formally, we need also to describe an algorithm that transforms any solution to the maximum independent set problem for the resulting graph back to a solution to the three satisfiability problem. and we also need to ensure that if there is no solution for the resulting instance of the maximum independent set problems, then there is no solution for the initial problem. but this is not difficult, and we did this already. namely, if there is a solution for the resulting instance of the maximum independent set problem and there is a maximum, there is an independent set of size equal to the number of clauses. that means that from each block we can select a vertex, such that all the labels of these vertices is consistent. if the consistent set of literals, but this also means that this gives the satisfying assignment of the initial formula. if on the other hand, there is no solution for the resulting instance of the independent set problem, then we know that the initial formula is unsatisfiable. indeed if it were satisfiable, if there were a satisfying assignment for the initial formula, then this would give us an independent set in the graph of size equal to the number of clauses. which would contradict to the fact that there is no such independent set in the resulting graph. 
our next reduction is from satisfiability problem to 3-satisfiability problem. that is, from a general version of a problem to its special case. to construct such a reduction, we need to design a polynomial time algorithm that takes as input a formula in conjunctive normal form, that is, a collection of clauses, and produces an equisatisfiable formula in 3-cnf, that is, a formula in which each clause has at most three literals. by saying equisatisfiable, we mean that the resulting formula is satisfiable if and only if the initial formula is satisfiable. once again, if the initial formula is f and the resulting formula is f prime, then by saying equisatisfiable, we mean that f is satisfiable if and only if f prime is satisfiable. to construct such a reduction, we need to somehow get rid of all long clauses in our formula. by saying long, we mean clauses that contain more that three literals. right. to do this, consider such a long clause and denote by l1 and l2 some two literals of this clause and denote by a the rest of this clause. so once again, l1 and l2 are two literal of the clause c, which we consider at the moment. and a is the set of all other literals. now we are going to do the following, introduce a new, fresh variable y and replace the current clause c with the following two clauses. so the first clause contains literals l1, l2, and y, so it is a disjunction of l1, l2, and y. the second clause contains a negation of y and all the other literals which we denote by a set a, okay? then we first of all note an important property. the clause, the first of the two new clauses has length 3. so we are okay with it. we are happy with it. we do not need to get rid of it. at the same time, this clause is shorter than the following one. exactly by one literal. right? so we are going to repeat this procedure while there is at least one long clause. and for sure, we will stop at some point, because at each step, we reduce some long clause with a shorter one. we now need to prove that the constructed transformation is correct, that the constructed reduction is correct, and that it takes polynomial time. concerning time, consider the running time, it is clear because at each iteration we'll replace a clause with a shorter one. this means that the total number of iterations is bounded from above by the total number of literals in all the clauses. this is clearly polynomial in the length of an input formula. to prove that the constructed reduction is correct, we're going to show that the initial formula f with a long clause is satisfiable if and only if the resulting formula where we replaced a long clause with a 3-clause and a shorter clause is also satisfiable. to do this, consider these two formulas. so this is f. it contains a long clause. and this is f prime that results from f by replacing this long clause with the following two clauses, l1, l2 and y, where y is a fresh variable and a clause not y or a. i assume now that the formula f is satisfiable and take a satisfying assignment. we are now going to extend it so that it also satisfies the formula f prime. recall that the only difference between the sets of variables of formulas f and f prime is the variable y. so our goal is to set y so that the resulting assignment satisfies all the clauses of the formula f prime. first of all, all the remaining clauses of f prime are satisfied by exactly the same assignment that satisfies the formula f, so our goal is to set y so that both the first two clauses are satisfied. i mean, both the first two clauses of the formula f prime. okay, so to set the variable y, we just check whether the current satisfying assignment of the formula f satisfies one of the literals l1 or l2 or not. so if it satisfies these two literals, we set y to 0. in this case, the first clause of the formula f prime is satisfied by one of l1 or l2. the second clause, on the other hand, is satisfied by the variable y, right, because we've just assigned the value 0 to y. on the other hand, if l1 and l2 are falsified by the current satisfying assignment, then at least one of the literals from the set i should be satisfied. in this case, we just set the value of y to 1, right. in this case, the first clause is satisfied by the value of y while the second clause is satisfied by one of the literals from the set a. okay, for the reverse direction, note that if we have a satisfying assignment that satisfies the formula f prime, then we can just discard the value of the variable y from this assignment, and then what we get is a satisfying assignment for the formula f. why is that? well, for a simple reason. what we need to show is that this clause is satisfied in a formula f, but it must be satisfied, because at least one of l1, l2, and all the literals of a should be satisfied. assume for the sake of contradiction that in the current satisfying assignment for f prime, l1 is set to 0, l2 is set to 0, and all the literals from the set a are also set to 0. then there is just no possibility to satisfy these two clauses because no matter how we assign the value to y either the first clause or the second clause is going to be unsatisfied. 
we now proceed to the most interesting reduction in this model. namely we are going to show that every search problem, every problem from the class np reduces to sat. so for this actually we're going to show, we're going to introduce an intermediate problem called circuit sat which is also an important computational problem. and we're going to show that any search problem reduces the circuit sat and in turn the circuit sat problem reduces through the sat problem. so, what is a boolean circuit? in some sense it is just a generalization of the boolean formula. so, it is a computational model for computing a boolean function. an example of a circuit is shown here on the slide. so it is in effect, a graph, and there with some input notes. so here we have input nodes marked with variables. and this input node is marked with a boolean constant, 1. and all other nodes are marked by boolean operations, namely with conjunctions. here, these junctions here and negations. so, when x, y and z are assigned boolean values, for example, 0 1, 0, we can compute what is an output of this circuit. to do this we just apply these boolean operations to the inputs of each node. the nodes are called gates. for example, the value of this gate is equal to 0, because this is a logical and of 0 and 1. the value of this gate is equal to 1, because this is a logical or of 1 and 0. then, the value of this gate is 1, because this is a negation of 0. is the value of this gate, is equal to 1, because this is a logical or of 1, and 1. the value of this gate, is also 1 because this is an or of 1 and 1. and finally the value of this gate is also equal to 1. so when x is equal to 0, y is equal to 1 and z is equal to 0, the output of this circuit is equal to 1, okay. more formally, a circuit is a directed acyclic graph such that all of its vertices have in-degree either 0, 1, or 2. the nodes of in-degree 0 are called inputs. and they are marked by boolean variables or by boolean constants. nodes of in-degree 1 are labeled with negations, and nodes of in-degree 2 are labeled with dejunctions or conjunctions or with logical ors and ands. also, there is one node which is labelled as an output. this is a gate of our degree 0. so a circuit is a computational model. such a circuit computes in a natural way a function which depends on input boolean variables, okay. so, we can compute this function exactly because the underlying graph contains no cycles. we can just topologically sort all the gates and compute the value for each gate in typological ordering. i mean, when the input variables are assigned values. now, the correspondent search problem is very natural. we are given a circuit and we would like to check whether it is possible to assign values to its variables so that the circuit outputs 1. this is a generalization of the problem where instead of a circuit, we are given just the formula. note, however, that a cnf formula can also be represented as a circuit in a very natural way. just to give an example. if our example considers the following formula in conjunctive normal form. here, we have two clauses. x or y or not z, and y or not x. we can represent it as a circuit as follows. so first, we need to compute the value of the first clause. we do this as follows. so we compute x or y. this is computed at this gate. and then we compute x or y or z. okay, so this can respond to the first clause. then we need to compute the value of the second clause. for this we first compute the negation of x. it is computed at this gate and then we can compute the logical or of the mitigation effects and y. and finally, so this gate computes the value of the second clause. finally, we take the logical and of these two clauses. so clearly, this circuit computes exactly the same as this boolean formula in conjunctive normal form. what we need to do now is to reduce circuit set to its special case. namely circuit-sat to sat. for this we need to do the following. we need to design a polynomial time algorithm which takes a circuit and transforms it in a polynomial time into a formula in cnf which is satisfiable if and only if the initial circuit is satisfiable. to do this, we are going to do the following. for each gate of the initial circuit, we introduce a new variable. so this variable is going to be assigned to this gate. so, in total, we're going to have variables that correspond to both of these circuit and also, one boolean variable for each gate of the initial circuit. then, we're going to write down a bunch of clauses. for each gate of our initial circuit, we're going to write a few clauses that will somehow en code, the relationship, because between the input of this gate with the value computed at this gate. we'll now explain how to write down these clauses. we start with the case when the current gate g for which we are going to write down these clauses is labeled with a negation sign. so consider such a gate, so it is labelled with a negation sign and the corresponding variable is g. assume that its predecessor is marked with label h. then, we'll write down the following two clauses of lengths 2. so what does these two clauses mean? well, what is essentially says a full, if h is = to 1 then g must be equal to 0. right, so this is forced by the second clause. indeed, if h is equal to 1. then in the second clause, this literal is already falsified, which means that the only way to satisfy the second clause is to set g to be equal to 0, right? and vice versa. if h is equal to 0, then g must be equal to 1. this is forced by the first close. so if h=0, then in the first clause, the value of h is already falsified. so the only way to satisfy the first clause is to set g to be equal to 1. which means that in any satisfying assignment or for formulas that contains the following two clauses, h and g always have the opposite values which in turn means that g always equal to h negated. and this is exactly what we need to encode in our cnf formula. now considering and gate, assume that g is an and gate, and its two direct predecessors are h1 and h2. we then write down the following three clauses. let's again see what do they encode. the first clause says that if h1=0, then the only way to satisfy the first clause is to set g to 0. so if h1=0, then g=0. this is exactly what we need. if h1=0 here, then the resulting gate g also computes 0, right? because this is a conjunction illogical nth of h1 and h2. okay, is the second clause tells us that if h2= 0, then also g=0, right. finally, the last clause says that if h=1 and h2=1, in this case, these two literals in the last clause are already falsified. so the only way to satisfy the last clause is to set g=1. once again this just means that if we have a formula in which we have these three clauses, then in many satisfying assignment to this formula the value of g is equal to the value of h1 and h2. and this is exactly what we need to encode in our cnf formula. or gates are handled similarly, namely if g is an or gate, we write down the following three clauses. the first one says that if h1 = 1, then this is already falsified, then g must be equal to 1. if h=1, then g=1. this corresponds to the second clause. finally, if h1=0, and h2=0, then g=0. again in any satisfying, for a formula that contains the following three clauses, g=h1 or h2. finally for an output gate, for the output gate there is a single such gate. in a circuit we write down a clause containing just one variable or just one literal corresponding to this gate. this corresponds to the fact that we would like this gate to be equal to 1. we would like our circuit to output the value 1. the reduction is now complete. we constructed a cnf formula out of a given circuit, with the following property. in any satisfying assignment of the result in cnf formula. the value of the variable g corresponds. it just equals to the value of the gate g on the corresponding assignment of the inputs to this circuit. this in turn means that the resulting formula is satisfiable if and only if the initial circuit is satisfiable. in particular given a satisfying an assignment for the cnf formula, just by reading the values of the inputs of the variables that corresponds to the inputs of the circuit, we get a satisfying assignment for the initial circuit. it is also clear that the running time of this reduction is polynomial. because we just consider all the gates of the initial circuit. and for each such gate, we write down at most three clauses. 
now our goal is to reduce every search problem. every problem from the class. and b, there's a circut-sat problem. for this consider a particular search problem a. note that we don't even know whether the input, whether the instances of this problem are graphs or boolean formulas, or boolean circuits, or just numbers or systems of linear inequalities. the only thing that we know is that a is a search problem. so this is the only thing that we can use to construct this reduction. by definition this means that there is an algorithm c which for any instance i of the problem a and any candidate solution s in time polynomial in there. so if i check whether s is indeed a solution for i, in particular we require that the lengths of the candidate solution s must be upper bounded by the, by a polynomial of the length of i. now a simple but crucial observation is that a computer is also a circuit. a boolean circuit which on low level operates with beads with boolean values. moreover each particular computer is actually a circuit of fixed constant size, right? now let's run our algorithm c on the inputs i and s on some computer. we know that this computer is going to perform the number of steps, which is polynomial in the size of i. now let's replace each of these steps but as a correspondent circuit. in this case, we have a sequence of circuits such that each circuit, each of the circuits in the sequence uses the values computed by the previous circuits in this, in the secrets and compute sum values. assume that this is a circuit responding to our computer so it uses some bits respondent to input. to instance i and some bits to respond in to the candidate's solution s. then it computes also some values and we plug it into the second circuit and so on. we have polynomial in many such circuit. such circuits, polynomially in the size of i and in the end, our algorithm outputs one, if and only if s is a solution is indeed a solution for i, so while we circuit outputs 1, if s is a solution for i. so once again what we get is a circuit, which has polynomial remaining in the size, in the length of i, polynomial remaining gates, which outputs 1 if and only if s is a solution for i. now, when we construct it as a circuit, we can use it as an input to our black box algorithm that told the circuits had problem in polynomial time, namely we do the following. consider the circuit that we just constructed. recall that this is a circuit cause input variables in code instances i of our such problem a and also solutions, candidate solutions. to out search problem a. and this circuit outputs one. if and only if the given candidate solution s is indeed a solution for an instance i. what i am going to do is the following. for a particular instance i, let's plug the constants for this input bits that encode this solution. what is left is a circuit, whose input variables encode different kinds of data solutions for this input instance i, and this circuit outputs one if, and only if, there is a solution for this instance i. now we can just use our black box algorithms that solves this circuitsat problem to find such a solution. great. which means that we finally reduced our search problem a to the circuitsat problem. once again, if we have an algorithm which finds a satisfying assignment for any given circuits that we can use it to solve any search problem a also in polynomial time. so the reduction is complete. to summarize, what we did is the following. we first reduced the independent set problem to the vertex cover problem. we then reduced the 3-sat problem to the independent set problem. then we reduced the satisfiability problem to the 3-satisfiability problem, and then we show that any, that every problem from the class mb reduces the sat. we did this actually via the circuitsat problem. this ensures that all these problems here on the picture are, in fact, can be complete. that any search problem is reduced to these four problems shown here on the slide. 
we conclude with showing how to use sat-solvers in practice. namely we give a toy, example a toy application we're going to use sat-solvers to implement a very simple sudoku solver. recall that there is a huge gap between theory and practice of sat solving. namely, currently we did not know how to prove that sat can be solved in less than two the n steps. namely, we have now example of an algorithm with the running time, polynomial in the size of the formula times 1.99, for example, to the n, where n is the number of variables of the input formula. it is on one hand. on the other hand we have state of the art sat-solvers that can handle instances that arise in practice that contain thousands and even millions of variables in less than one second. this suggests the following way of solving hard combinatorial problems in practice. first you might want to reduce your problem to a satisfiability problem. the structure of the satisfiability problem allows to do this very naturally for many problems arising in practice. we will see this on an example of solving sudoku puzzle. then just use one of the efficient programs, efficient sat solvers that are already implemented. there are many of them, and as i said they are very efficient. let me now recall you what a sudoku puzzle is. so in this case we are given a 9 x 9 grid which is partially filled by digits from 1 to 9. and our goal is to fill all the vacant places with digits so that each row and each column and each of nine 3 x 3 blocks contain all the digits from 1 to 9. this is an example of such a puzzle and this is a solution that we're looking for in this case. so now that for example in this row we have all digits from 1 to 9 and also in this column and also in all of these nine 3 x 3 blocks. so given a sudoku puzzle we are going to construct a satisfiable cnf formula such that from its satisfying assignment we can construct the solution for the initial puzzle. for this we are going to use the following boolean variables. there will be 9 x 9 x 9 of them. namely for each possibly values of i, j, and k, where i, j, and k are at least 1 and at most 9, the variable xijk = 1 or is equal to true, if the cell [i,j] contains the digit k. okay? so these are boolean variables. there are 9 x 9 x 9 of them, which is roughly 700. in our reduction, we will need a few times the several technical thing. assume that we have a few literals and we need to state a formula which expresses the fact that exactly one of this literals is true. we show here how to do this for three literals but it generalizes easily for arbitrary number of literals. so assume that we have three literals and we want to express the fact in a cnf formula that exactly one of them is true. so first we write down a clause that says then at least one of them is true, this says l1 or l2 or l3. then we need to say also the constraints, the value of these variables to satisfy the following property. at most one of them is true, namely, we need to forbid any two of them to have value 1 simultaneously. we do this by adding a bunch of clauses of length two, namely for any two different literals from our set, we add two clause containing negations of these two literals. for example, this one the first clause says that it is not possible to assign the value 1 to l1 and l2 simulationously. the second one says that we should not assign the value 1 to the first and the third literal. and, finally, the last one says that we should not assign the value 1 to the second and the third literal.. so, finally, when we write down such a formula, then in any satisfying assignment to this formula exactly one of l1, and l2 and l3 is true. and also, it generalizes easily to any number of literals. so we first right down the long clause containing all the literals and then so that will contain k literals. and then we write down roughly k squared literals namely k choose 2 clauses that contain all pairs of negated literals. we now write down the corresponding constraints. first of all, for every values of i and j from 1 to 9, we state that in the cell [i, j], there is exactly one digit, so it looks as follows. so, this is a cell [i, j] in our grid and we say that exactly one of the variables xij1, xij2 and so on, xij9 should be assigned the value true. so, if xij, for example, if xij4 is equal to true, this means that we are going to put the digit 4 into this cell. then we need to state that for each values of k and i, the row i contains exactly one occurrence of k. this is done as follows. we state that for each i and k, exactly one of the following variables is true. so it is either xi1k or xi2k. so this corresponds to the fact that either the first column in the ith row contains k's or the second column or the last column. namely, if this is the ith row either this cell contains k or this cell contains k and so on, the last cell contains k. then we do the same for column j and namely for each values of k and j we state the fact that the column j contains exactly one occurrence of k. we then need also to state that each of nine 3 x 3 blocks contains one occurrence of the digit k. again, we use the corresponding nine variables to say that exactly one of them is equal to true. finally the last type of constraint is that when some cell is already filled. if we have an instance where the cell [i, j] already contains the digit k, we then we need just one clause stating that the corresponding variable must be equal to true. we know that it is already true, so we just add a single unit clause. so the resulting formula contains about 2 to the 700 variables, so the search space of all possible candidate solutions in this case contains roughly ten to the two hundred. so this is a huge search space. it is certainly not possible for modern computers to go through all possible candidates for a solution in this case. at the same time, modern sat-solvers solve the results in formula in blink of an eye. we will see now. we are going to use a mini sat-solver for solving our sudoku puzzle. before writing down the reduction let me show you the input format for this sat-solver. so the formula is given in the following symbol format. the first line here shows that our formula has 2 variables and 3 clauses. then each of the following three lines define a clause. the first clause contains two variables, x1 and x2. the second clause contains the variable x1 and the negation of the variable x2, so minus x2 means the negation of the second variable. finally as the last clause contains two variables, the negation of x1 and the negation of x2 okay? so now let's call minisat on this formula, so it reports immediately that it is satisfiable. now let's make it unsatisfiable, so for this we add the false clause. -1, 2, 0, okay. let's call minisat again. yeah, in this case, the formula is unsatisfiable of course. now finally let me show you the code of an actual reduction that produces the cnf formulas and then calls in minisat solver and then reads the solution for a given sudoku puzzle from a satisfying assignment to these formula. so this is a simple code in python, this gives the initial sudoku puzzle and then we create a list of clauses. then we just use variable digits which contains all the digits from 1 to 9. the following method just returns a unique integer number for every variable. we need this in order to give a formula to a minisat solver. then we have a method which, given a list of literals writes down clauses that express the fact that exactly one of these literals is equal to true. namely, we first add a clause that contains all the literals in the list literals, then for all pairs of literals we add a clause containing two negated literals from this list. as we've discussed already this expresses the fact that exactly one of them is equal to true. we then start writing down clauses expressing that each cell contains exactly one digit. namely, for all pairs of digits we write down a clause stating that at least, for all pairs of i and j, we write down the clause saying that the cell [i, j] contains exactly one digit. so the corresponding variable xijk is equal to 1 for exactly one value of k. then we say that k appears exactly once in row i. for this we consider all possible values of i and k and state that exactly one of the variables i, j, k is equal to true for all possible values of j. then we say the same for columns, and then we say the same for blocks. namely, we can see that all possible starting position of blocks 1, 4, and 7. and then for each such block and for each k we state that each such block contains exactly one occurrence of k. then we read the input puzzle. namely, we read all the cells and if the corresponding cell is specified, then we just add a unit clause containing exactly one corresponding variable. finally, we write down all the clauses. namely, we open a file, tmp.cnf, in the first line we write, p cnf, followed by the number of variables. in our case it is roughly 1,000 and then the number of clauses. then for each clause which is currently in our list of clauses we first append 0 to satisfy the format which is accepted by the mini-sat, and then we just write down this clause. then we call the minisat sat-solver, give it the formula tmp.cnf and also indicate the file tmp.sat, where it will write down the satisfying assignment, okay? then we open the satisfying assignment file and read the first line. if it is unsat, we just report that the formula is unsatisfiable. if the first line is sat, we know that the formula is satisfiable and we know that then the second line will be followed by a satisfying assignment. at this line we read the corresponding satisfying assignment and then we parse it. we just see that if the corresponding variable in this assignment we print it, okay? so and this is basically all, this completes all reduction. so let's see what happens. if we call this python script it will immediately find an answer. so this is an answer for our initial puzzle. let me also make a small sanity check. let me tweak a little bit this puzzle. let me put 5 here. then the corresponding puzzle will be for sure unsatisfiable just because now the first line contains two occurrences of 5 so it is not possible to complete this puzzle. okay, now let me call it once again so in this case, the corresponding formula is unsatisfiable and our scripts reports that there is no solution. as you can see, it works in just the blink of an eye. and you can feed your favorite sudoku puzzle into this script to see how fastly the mini sat-solver will solve this puzzle. 
hello and welcome to the next module in which we are going to consider various ways of solving np-complete problems. before we proceed to actual algorithms, imagine the following situation. at your job, you are given a task to implement an efficient program that solves a certain search problem. well, with some probability, your problem can be solved with some of the known techniques, like dynamic programming or linear programming or graph algorithms. but even if this is the case, usually it's not easy to realize this and at least it takes time. and unfortunately, this happens very rarely. so in about two weeks of trying to apply some of the known techniques to your search problem, you can always go to your boss and say, i can't find an efficient algorithm for this problem, i guess i'm just too dumb. one of the reasons why you cannot design an efficient algorithm for your problem might be just that there is no efficient algorithm for your problem. this would allow you to say to your boss, i can't find an efficient algorithm, because no such algorithm is possible. note, however, that currently, we do not know how to prove that any search problem, but we do not have any example of a search problem for which we can prove that it cannot be solved by polynomial algorithm. in particular, if we had such a proof, we would resolve the p versus np question. what we can do, on the other hand is to show that some search problems are the hardest search problems. so what you do is you show that your problem is the hardest one, that is you show that your problem is np-complete. this in turn allows you to say to your boss, i can't find an efficient algorithm, but neither can all these famous people. you have proved that your problem is np-complete. this in particular means that it is very difficult to design a polynomial time algorithm for it. but at this point, unfortunately, the problem doesn't go away. your boss still wants you to solve it. so on one hand, you need to design an efficient algorithm for your problem. on the other hand, you know that designing such an algorithm is as hard as resolving the famous millennium prize problem, p versus np. should you give up at this point? no, not at all. in this module, we will learn a few techniques that allow you to solve np-complete problems efficiently in practice. this is an outline for the rest of this module. we know already that if p is not equal to np, then for any np-complete problem, there is no polynomial time algorithm that finds an optimal solution in all cases. note that there are three requirements on an algorithm in this statement. so first, we want that algorithm to have polynomial running time. second, we wanted to return an optimal solution. in short, we wanted to do so in all cases. to solve an np-complete problem in practice, you might want to relax one of these conditions. for example, the zara algorithm that solves an np-complete program in practice, but not in polynomial time, but not in all cases, but only in some cases. such algorithms are called algorithms for special cases. so, they have polynomial times. they produce optimal solution for a problem but not in all cases, but just in some cases. also, it is possible sometimes to design an algorithm that works in polynomial time. and in all cases, it returns a solution which might not be optimal, but it is guaranteed to be close to optimal. we will see examples of such algorithms. and finally, we can design an algorithm, which always finds an optimal solution in all cases, but we cannot prove an polynomial upper bound on its running time. but still, it works well in practice. for all these three types of algorithms, we will see examples in the next part. 
in this lesson, we will consider all of the reasons that solve np-complete problems in special cases. the main message of this lesson is the following. the fact that your problem is np-complete does not exclude a possibility of a very efficient algorithm that solves your problem in some special restrictive cases. so once again, the fact that the problem is np-complete means that it is difficult to design an efficient algorithm that works for all possible cases. but it might be the case that for some special cases there exists a much more efficient algorithm. our first example of such a problem is 2-satisfiability problem. for this problem, we will see a surprising connection between boolean formulas in 2-cnf and graphs. namely, we will apply the depth-first search algorithm to solve a special case of the satisfiability problem just in linear time. the input of the 2-satisfiability problem, which is also abbreviated as just 2-sat, is a set of clauses, each of which contains at most two literals. recall that the literal is a boolean variable or a negation of a boolean variable. and what we need to find is to check whether we can assign boolean values to the variables of this formula to satisfy all these clauses. so what we need to return is as i said a satisfying assignment or we need to report that there is no such satisfying assignment. let me give you a few two examples. so this is a formula in 2-cnf. so all clauses contain at most two literals and it can be satisfied with the following assignment. let's check it. if x = 0, y = 1 and z = 0, so that means of course, the first clause is satisfied by the literal y which has value 1. the second clause is satisfied by the negation of z, so z has value 0, so negation of z has value 1. and finally, the last clause is satisfied by the negation of x, okay? this is another formula in 2-cnf and it is unsatisfiable. to see this, know that we have to closest of links one here, so this is the first such clause and it forces us to assign the value 0. so z must be equal to 0 if we would like to satisfy this formula. similarly we need to assign the value 0 to y because of the last clause, right? then what is left is these two clauses. y and z, the literals y and z are already falsified in these two clauses. so essentially what is left in this clause is in the first clause we have x, in the second clause, we have not x. and of course, it is not possible to suggest [inaudible] these clauses by assigning x. okay, so this formula is unsatisfiable. the last example is also an unsatisfiable formula where we have two variables and it is not difficult to see that this is also unsatisfiable. just because these four clauses actually forbid all four possible ways of assigning the values x and y. for example, if we try to assign the value 1 to x and the value 0 to y. then we get the following clause, so which one? this one, which is unsatisfied. and no matter how we assign two values for these two variables, we always get an unsatisfied clause. consider two clause of our input formula. say it contains letters l1 and l2. basically what it says is that we should not assign similar [inaudible] value 0 to both l1 and l2. because in this case, this clause will be clearly falsified. in other words, if we assign the value 0 to l1, we must assign the value 1 to l2. because this is the only way to satisfy this clause when l1 is already equal to zero and vice-versa. if we assign the value zero to l2, we must assign the value 1 to l1. so this is the kind of implication and in fact, implication is a well-known binary operation. it is defined by the following truth table. so it is equal to 1 in all cases except for the following one. when x is equal to 1 and y is equal to 0, the implication from x to y is also equal to 0. in all other cases it is equal to 1. what it does with y is, it says the following, that falsity implies everything. it is shown here. if x if false, then it implies both the truth and the falsity. however, if x is true, it only implies the truth. the implication from x to y when x is equal to 1 is only equal to 1 when y is equal to 1 too. this observation brings us to the following so-called implication graph. it is defined as follows, given a 2-cnf formula, for each variable of this formula, we introduced two vertices. one labeled with this variable and one labeled with its negation. then for each two clause that contains two literals, say l1 and l2, we introduce two directed edges. the first one goes from the negation of l1 to l2 and the second one goes from the negation of l2 to l1, okay? and finally for each clause of length one, which contains just some literal l. we introduce an edge, a directed edge again, that goes from the negation of l to the l itself. okay, so in a sense this graph contains all implications that i imposed by our input formula. to give an example, consider the following 2-cnf formula. consisting of three variables, x, y, z and four clauses of length two. to construct each implication graph, we first introduce six vertices, two vertices for eachiinput variable. this gives us the following six vertices, x nought x, y nought y, and z nought z. then we start introduce edges. the first clause gives us the following two edges, namely an edge from x to y, this edge and the next from nought y to nought x. once again, it corresponds to the following clause. let's understand once again why these two edges correspond to this clause. well, what essentially is this clause says is, is that if not x = 0, which means that x is equal to 1, then y must be equal to 1. this is essentially what is given to us by this implication, right? it says that if x is 1, then this also must be 1 to satisfy this implication. the other edge says that if this edge, if not y = 1, this actually means that y = 0, then not x must be equal to 1. and this is indeed true, if y = 0, then this literal is falsified in this clause. which means that the only way to satisfy it is to set not x to be equal to 1, right? then we can continue in the same manner. for the second clause, we introduce the following two edges. for the set clause, we introduce the following two edges. and finally for the last clause, for the first one, we introduce the following two edges. now we have an implication graph. now we can see that some specific assignment of boolean values to the variables of this formula. for example, when x is equal to 1, y is equal to 1 and z is equal to 1, all the clauses are satisfied. it can be easily seen just by observing that each clause in our input formula contains a positive literal. by saying positive i mean a variable with no negation. so for example here, y satisfies this clause here, z satisfies this clause here, x satisfies this clause and the last clause is satisfied by z and y. also we can see that each edge in this graph is also in a sense satisfied. now let's consider and as our assignment, that actually falsifies the input formula. if we assign the value 0 to x, y and z, then the first three clauses are satisfied. the first one is satisfied by x, the second one by not y, the third one by not z, but in the last clause, both the literals are equal to 0. okay, and we see also that the corresponding two edges in the graph are also falsified. by saying falsified i mean that they are the beginning of this edge has value 1. so z is equal to 0, so not z is equal to 1. while the end of this edge is equal to 0, right? so, one does does not imply 0. so truth does not imply falsity. and the same for this edge. in this case, y = 0, so this vertex has value 1. while the end of this edge has the value 0. and this is exactly the situation when the implication is falsified, when it says that the truth implies falsity. now our goal then, given this graph, our goal is to assign the values to all the variables. so if our input formula, so that all the edges are satisfied. and that's we discussed already. an edge is satisfied if and only if, well, it is easier to say what it means for an edge to be falsified. an edge is falsified, when its beginning is labeled with 1 and its end is labeled with 0. so we need an assignment to all the variables of our input formula such that no edge is falsified. 
before we proceed to an algorithm, let's state a few important properties of the implication graph. the first one says that the graph is skew-symmetric. formally, it means that if there is an edge from a vertex labelled by a literal l1 to a vertex labelled by a literal l2, then there is also an edge from the negation of l2 to the negation of l1. this is just by definition of this graph. so if we introduced an edge from l1 to l2, then we, at the same time, we introduced an edge from denegation of l2 to denegation of l1 and this also generalizes to paths in graph. if there is a directed path from l1 to l2, then there is a direct path from the negation of l2 to the negation of l1. this follows directly from the previous property about edges. to see this, let's consider the path from l1 to l2. so it contains several edges, it starts with l1 and ends with l2. now consider the last edge, denote the variable, the beginning of this edge, for example. so we know that there is an edge from v to l2, which means that there is also an edge from the addition of l2. there's an additional fee, then consider second to last edge, assume that it is from u to v. the fact that there is an edge from u to v means that there is an edge i'm sorry, there should be a negation of v. there is an edge from the negation of v to the negation of u and so on by continuing in the same manner, we will finally reach the negation of the one. so once again, if there is a pass from l1 to l2, then there is also a pass from the negation of l2 to the negation of l1. the second property in a sense says that the implication is transitive. namely, if a implies b and b implies c then a also implies c. more formally, the lemma says the following. assume that there is a path from l1 to l2 in our graph. assume also that we have a truth assignment that satisfies all the edges of our implication graph. then l1 in this assignment, the value of l1 must imply the value of l2. that is, it cannot be the case that the value of l1 is 1, and the value of l2 is 0. to prove it, assume for the sake of contradiction that in our graph and in our assignment. l1 is assigned 1, and l2 is assigned 0. we know that there is a path and all edges are satisfied. so all the edges of this path are also satisfied. so we have a path where all the edges are satisfied and its beginning is labeled with one, and its end is labeled with 0. this means that actually there is a transition on this path where we go from 1 to 0. so there is some edge where which subject is beginning, is labeled with 1 and its end is labeled with 0. but this means that this edge is not satisfied, it is falsified. again, recall that 1 does not imply 0, which leads us to a contradiction. the transitivity properties that we've just proved implies the following additional property. if we have two literals that stay in the same strongly connected component of the implication graph then they must be assigned the same value in many satisfying assignment. why is that? well consider two literals, l1 and l2. assume that they stay in the same strongly connected component. because this is a strongly connected component of the implication graph. assume for the sake of contradictions that there is a satisfying assignment which assigns the value of 1 to 1 of them and the value 0 to the other of them. for example, assume that l1 is assigned the value 1 and l2 is assigned the value 0. the fact that they lie in the same strongly connected component means that there is a pause from l1 to l2, but this contradicts to the fact that one must imply l2. so l1 and l2 if they stay in the same strong connective component must be assigned the same way. this in turn implies that if in the implication graph there is a strongly connected components that contain some literal together with it's negation, then this formula is answered justifiable. because we cannot assign the same value to a literal and its negation. so if a strongly connected component of the implication graph contains a literal together with it's negation, we stop immediately and return that the formula is unsatisfiable. it turns out that this is the only case, when the input formula is unsatisfiable and we will prove it in a minute. as we have just discussed, the input formula is satisfiable if and only if, it's implication graph, i'm sorry, does not contain a strongly connective component, which contains a literal together with its negation. we still need to prove this property but using this property, we already ready to write down an algorithm. so the algorithm proceeds as follows. given a 2-cnf formula f it first constructs the implication graph g of this formula. we then find the strongly connected components of this graph. then we check whether some of the strongly connected components contains a variable together with its negation. if there is such a strongly connected component, we return immediately that the formula is unsatisfiable. this is because we know already in unstaisfiable assignment, all the literals lying in the same strongly connected component must be assigned the same value. and we cannot assign the same value to x and to its negation in the remaining case, we know that the formula is satisfiable. so the remaining part of the algorithm just constructs the corresponding satisfying assignment. for this we first find a topological ordering of all strongly connected components. recall that the so-called meta graph or condensation or graph of strongly connected components is always a directed acyclic graph. which means that we can find some topological ordering of all these strongly connected components. then we process the strongly connected components in reverse topological agreeing. so we first place them on a line such that each edge goes from 1 going from 1 strongly connecting components to some other goals from left to right. and then we proceed these strongly connected components from right to left. for each strongly connected component, we consider the literals of the vertices staying in this strongly connected component. if they are not assigned yet, we assign them the value 1 and we assign the value 0 to all the negations of these literals. this way, we assign all the variables and from there we just return the correspondence of this final assignment. we still need to prove that this algorithm is correct, but we already know that the running time of this algorithm is linear in the size of the input formula, why is that? well because we can assume that m is the number of clauses of the formula, clauses and m is the number of variables. then the first step clearly takes running time n + n, because well we first introduced 2m vertices and then for each, for each of m, for each of m clauses we introduce three edges. so what we get is a graph on 2m vertices and 2m edges. the second step is also linear, we know that the strongly connected components of a graph can be found by just two calls to the [inaudible] search in linear time. this is also a linear step. so we just go through all the variables until each variable we checked wether variable and its negation lie in the same strongly connected component. the topological ordering of a graph can also be found in linear time. finally, here we just process all the strongly connected components and at this step we can see that also all vertices of the initial graph shows is the step also takes linear time. so overall the running time of this algorithm is linear. we now need to prove that the y algorithm is correct. for this we need to show that our way of assigning values to all the variables satisfies all the edges. recall that an edge is not satisfied in the only possible case when its beginning is assigned the value 1 and its end is assigned the value 0. so we want to prove that our algorithm avoids such cases. well intuitively, it happens to avoid such cases, when there's the following. we process all our edges going from right to left, namely in the reverse topological ordering. and we always assign 1s to the right most available vertex. so we process the vertices, namely the strongly connected components containing our vertices from right to left nad we always assign the last available vertex the value one. this way we guarantee that basically we have zero's on the left and one's on the right, well very kind of wavy. more formally, we prove that when a literal is set to 1, that all the literals that are reachable from it, will name it all the literals that are reachable from it stay on the right. they are all ready assigned the value 1, and similarly when a literal is assigned the value 0, all the literals that this literal is reachable from have already been assigned the value 0. well to be more formal, let me draw a little bit. so we would like to show that our algorithm, when assigned the values to all the variables, our algorithm avoids the following thing. that this variable is assigned zero, this literal is assigned zero, this literal is assigned one, let me denote them by u and v. so consider the situation when v was assigned 0 by a point of time when it was assigned 0 by algorithm. this means that at that point not v was assigned 1. then we were processing all the strongly connected components from right to left and we've assigned 1 to the literal not v at this point. and this forces us to assign the 0 to v, but now this symmetry of the graph implies. that the results are a match from v through not u in our graph. and the fact that we processed all the edges from left to right means that u, that not u was assigned 1 also. but this means that you cannot date value 1, it was assigned the value 0 in fact. this is a contradiction, so this completes the analysis of this algorithm. once again, we've just proved that our way of assigning the values to all the variables of the input formula satisfies all the edges and hence satisfies the input formula. 
the next complicated problem that we're going to solve is called independent set. and the corresponding special case for which we are going to design a linear time algorithm is when an input graph is actually a tree. a real life application of this problem is the following, assume that you are organizing a company party. and you would like to invite to this party as many people as possible with a single constraint. you know that no person is going to enjoy your party if he or she is invited together with his or her direct boss. the mathematical model of this problem is the following. you are given a tree, and you would like to find in this tree a subset of vertices of maximum possible size, such that no two of them are adjacent. okay, such that there is no edge between any two of them. so to give an example, consider the following tree. in this tree, there is an independent set of sides five shown here on the slide. there is also another independent set of size six. and an independent set of maximum size is shown here, and it has size seven. so our goal is to find efficiently, an independent set of maximum possible size, in any tree. as we've discussed before, this problem can be solved with a simple algorithm. the corresponding safe move in this case is the following. take into a solution all the leaves. to prove that this algorithm is correct, we need to show that this step is safe, mainly that they always exist an optimal solution consistent with this step. in other words, that there exists a solution which contains all the leaves. to prove this, we usually argue as follows. we can take any solution, and we will transform it without decreasing its size so as to get a solution which contains all the leaves. in particular, if we start with an optimal solution, i mean, with the solution of maximum size. we will transform it into a solution whose size is at least the size of the initial solution, which means that this new solution is optimal, and it is also a valid solution. this will prove that the statement is correct. well to explain how to transform it we, as usual, consider a toy example. so consider such a solution. so the blue vertices here are selected into an independent head. so we see that there are three leaves that are not included into our solution. so let's just take them into our solution. this will force us, also, to exclude all their parents but the crucial observation is that this does not decrease the size of the current solution. and the current solution is still a valid solution because for each of these leaves we excluded their parents. so this way, we get a solution of the same size, that includes all the leaves. and this proves that taking all the leaves in the solution is a safe move. the corresponding algorithm is quite simple. so, given that tree, you repeat the following while the three is not empty. take all the leaves in the solution, then you remove from the tree all the leaves together with all its parents, and then you return the constructed solution. the running time for the current algorithm is if it is carefully implemented. in particular, you might want to do the following. for each vertex, you keep track of the number of its children. you actually do not remove anything from the tree itself. but you keep track of the current number of children for each vertex. so when you remove a vertex, you decrease by one the number of children of each parent. and you also maintain a queue that contains the vertexes of the do not have any children currently. so each iteration you know all the leaves of the current. this is are the exactly the vertexes with no children. now let's generalize the problem. assumed that you are still organizing a company party, but now instead of maximizing the number of people at the party, you are going to maximize the total fun factor of your party. namely, each person is assigned some non-negative fun factor, and you would like to maximize the total fun factor of all the invited people. the mathematical model now is the following, you are given tree with weights on vertices, and you would like to find an independent set in this tree whose total weight is as large as possible, okay? to give an example, again, consider the following tree. one independent set in this tree is shown here in the slide. its total weight is 17. and an optimal one has total weight of 18. as you see, it doesn't include all the leaves. so this problem cannot be solved by the same algorithm. we need to design a new algorithm for this problem. the new problem can be solved with a dynamic programming technique. when we have a problem on trees, it is very natural to try to define sub-problems for subtrees of this tree, and this is what we are going to do. for vertex v, denote by d(v), the maximum weight of an independent set in a tree rooted at the vertex v. then there are basically two cases. either we include v into, into our solution, or we do not include it. if we include it contributes the weight of this vertex to the total factor. and we also cannot take any of its children in the solution because otherwise it will not be an independent set. however, we can take anything from the subtree's root at its grandchildren. and particular, we are maximizing the total fun factor, so we would like to solve the problem optimally for all these subtrees. so, if we included what we can get, the maximum total fun factor that we can get is the weight of the vertex v plus the sum of all the optimal solutions for overall grandchildren w of v. on the other hand, if we do not include v in the solution, then we can solve the same problem independently for all its children. so the second case corresponds to the situation when we do not include v into our solution. and in this case, so this is our vertex v and this is, we do not include that into your solution. then we can just solve it optimally for all its subtrees. and we can then take the sum of all these vertices. so since they state in the independent subtrees, the result in union of all the vertices. is that we get from our subproblems is also an independent set into wall tree. we're going to use dynamic programming to implement the corresponding algorithms. which uses direct relation that we've just discovered. namely, the function funparty, for the vertex v, is going to compute the optimal answer for the subtree rooted at v, when we only go in to compute it if we haven't computed the corresponding value before. this is checked in the first if d(v) is still equal to infinity. so, at this point we assume that the niche earlier, d(v), is equal to infinity to all vertices. so, if we haven't computed this value before we start computing it. then we check whether the vertex v is a leaf. if it is a leaf, then the answer is obvious. then the obvious routine is to take this vertex into a solution. so the optimal answer is just the weight of this vertex. otherwise, we need to compute its value recursively. we do this through grandchildren and children of the current vertex. the first case is when we take the vertex v into a solution. so first we initialize the variable m1 with the weight of the current, of the current vertex. and then derive through all grandchildren of the vertex v, this is done in these two loops. and we add to m1 the optimal answer for each grandchildren. we then proceed to the second case when we do not include v into solution. so m0 is initialized to 0, and then for all children u of vertex v. we need to add the optimal value for the corresponding children. this is down here. okay, then we just select the maximum of the result into values and assign this value to d(v). and finally, we return d(v). the running time of this algorithm is also the goal of society of the tree and this is why. for each vertex there is just one serious call to funparty. by serious, i mean a call which actually is inside this. which gives rise to some computation, because after the first time when we are inside this loop for the vertex v, we will store the value in d(v) and then for any further call for funparty(v) we just return the value immediately. okay we just return it here. note also that the value of d(v) is important for us only when we compute, only when we solve for sub problem can respond in to its parent and to its grandparent. meaning that we will have only a constant number of calls to funparty of v for each vertex v. we now show the result of applying this algorithm to our previous example. so in this tree we start to fill it in from the leaves to the root. so for each leaf, the value is computed in an obvious way. for example, for this subtree the answer is 1. for this subtree the answer is 2. and for this subtree the answer is also 1. for this subtree we need to make a decision. either we take this vertex in which we gain 7, or we don't take it, in which case we can get 1 from here, 2 from here and 1 from here, which gives us 4. 7 is better. so an optimal independent set in this case, has total weight 7. for this vertex, its optimal value is 2 because it is a lift, and for this vertex, again, we need to select whether to take this vertex into a solution or not. if we can take it, then we cannot take the vertexes 7 and 2 it's children. so the value of the solution will be 6 plus the values of its grandchildren which are these. so the values of its grand children are 1, 2 and 1. so 6 + 1 + 2 + 1 = 10. so this corresponds to the case when we takes the root of the current subtree into solution. on the other hand, if we don't take it into solutions than the maximum that we can achieve is 7 in this subtree, and 2 in this subtree, which gives us 9. 10 is better than 9, so 10 is the optimal answer for this subtree. in a similar fashion, we get 5 here and we get 3 here. now for this vertex, again we need to select whether is better to include it into the solution or it is better to have waited. if we included the itinerary solutions and the maximums that we can we get a 3 plus, in this case if we included into solution we can not use it's children, but we can use anything in the subtree rooted by it's grandchildren. so we can get 2+3+7+2. so this gives us 3+2+3+7+2, which is equal to 17. right, if on the other hand we do not include it, then we can get anything we want from the subtrees through it's children. this will give us 5+3+10. 5+3+10=18 which is better so we conclude that 18 is the best we can do for this toy example. so this concludes the lecture for the special cases of complete problems. let me remind you the main idea once again. the fact that your problem is incomplete does not exclude the possibilities that. some special cases that arise in practice of this problem can be efficiently solved, and we've just seen two such examples. despite of the fact that the satisfiability problem is difficult, in the general case its special case is, namely, if all the clauses of a formula contain at most two literals can be easily solved in minimal time. the second example was about independent set. this problem is hard in general, so given a graph, it is difficult to implement an algorithm which always finds an optimum size independent set of a graph. but, if you know that all of your graphs that are right in your application are trees, then it is easier to implement a linear time algorithm that will find an optimal answer quickly. 
welcome to the next lesson of coping with np-completeness model. in which we are going to design exact algorithms for np-complete problems. exact algorithms are also called sometimes intelligent exhaustive search. as opposed to brute force search algorithm. which to find an optimal solution, just enumerates all possible candidate solutions, and selects the best one. instead, we are going to find an optimal solution without actually going through all possible candidate solutions. the first problem for which we illustrate such an algorithm is 3-satisfiability problem. recall that the input of this problem consists of a cnf formula, and it is 3-satisfiability each clause contains at most three literals. so this formula is in three conjuctive normal form, in 3-cnf. and our goal is to find out whether it is possible to assign boolean values to all the variables of this formula so, as to satisfy all the clauses. let me quickly show you once again, two for example. so the first formula here is satisfiable, because we can set for example, the value 1 to x, y, and z. or the value 1 to x, and the value 0 to y and z. and the second formula is unsatisfiable. we cannot set the values of the variables x, y, and z to satisfy all these five clauses. and i used brute force search algorithm for these three satisfiability problem, does the following. it just goes through all possible candidate solutions that is through assignments, through all the variables of a formula and checks whether any of them satisfies the input formula. if n is the number of variables of an input formula f, then the running time of this algorithm is proportional to the length of f times 2 to the n. because we have 2 to the n assignments, and for all of them we need this kind of formula to check whether all of the is fired or not. in particular if your formula is unsatisfiable then your algorithm will be forced to check all these assignments. so it's running time will be 2 to the n. so our goal for this module is to avoid going through all possible 2 to the n assignments and still to check whether the input formula is satisfiable or not. the first technique that we are going to use. this is called backtracking. so this is a well known technique which proceeds as follows. you construct your solution piece by piece and then at each step when you realize that your current solution cannot be extended. your current partial solution cannot be extended to a valid solution you backtrack, you go back. this idea is used in many situations, from many combinatorial optimization problems and for many puzzles, and games. so imagine, for example, that you are playing a game and you are staying in some room, and you are in a maze, and you are looking for an exit from a maze. and in the room that you are currently staying there are three doors leading out of this room. so you use one of these rooms and you enter another room for which you see that there are no other doors. so you are in a deadlock. so you then go back, you backtrack, and you decide to use some other room. so this is way of solving a combinatorial problem, it is called backtracking once again. we will now show an example as usual how this technique named backtracking is used to check the stability of boolean formulas. so consider the following formula, we need to find a satisfying assignment and we don't know what to do so let's try to assign zero to h1. what happens in this case is that x1 disappears from this clause because it is already falsified. on the other hand this clause is already satisfied so this clause disappears entirely. x1 also disappears from this clause and it becomes just x2 or negation of x3. then x1 disappears also from this clause, and this clause is left untouched. we then proceed as follows. now let's try to assign the value 0 to x2. what happens is that, well this clause is satisfied, x2 disappears from this clause because it is falsified already from this clause and from this clause. so what we get is x3 or x4, and then the negation of x3, and the negation of x4. then we try to assign the value 0 to x3. what is left, so this clause disappears, x3 disappears from this clause. so what is left is one clause containing x4, and one clause containing the negation of x4. we then try to assign the value to x4. if we assign the value 0 to x4, then we falsify this clause. so this clause disappears, and this clause becomes empty. meaning that there is no satisfied literal in this clause, that there is no way to satisfy this clause anymore, right. the same happens if we assign the value 1 to x4. in this case, we satisfy this clause but at the same time, we falsify this clause. once again, this clause becomes empty, which means that there is not way at this point to satisfy this clause. so at this point we backtrack farther. we just realized that it was a wrong move to assign the value 0 to x3. it leads to an unsatisfiable formula. so we now try to assign the value 1 to x3. so now we returned back here, and we are trying to assign the value 1 to x3. but in this case we falsify this clause. this clause becomes 17 which means that the roll this formula is unsatisfiable. which means that it was a wrong move to assign the value zero to x2, and now we need to try to assign the value 1 to x2. we do this, but we immediately falsify the following clause, right? so in all other clauses we have x2 so all of them are satisfied but what is left is an empty clause which cannot be satisfied. so at this point we backtrack back to the initial formula and at this point we know already that this we must assign the value one if we want to satisfy this formula. so we do this and see again there is an empty clause. so at this point we conclude that the initial formula is unsatisfiable. note the following important fact. so we realized that is unsatisfiable without considering all possible 16 through assignments. to our four input variables, right? this is how the of the corresponding algorithm looks like. so we first check whether f is empty or not. so if f has no clauses and we have nothing to satisfy, then we just return that the formula is satisfiable. we then check if f is not empty, we check if f is an empty clause. as we've discussed before, an empty clause means a clause which cannot be satisfied. so all the literals in this clause have already been falsified. in this case we return that the formula is unsatisfiable. otherwise we take some variable which is still present in the formula f. so x is some unassigned variable of f. and then we try two cases. we first try to assign the value 0 to x. so this notation means that we assign the value zero to x. when assigning the value zero to x we remove all the clauses which contain the negation effects. all such clauses already satisfied, and we remove all occurrences of x without negation from all other clauses. because x is all ready falsified. if the formula turns out to be satisfiable, we return immediately that the real formula is satisfied. otherwise, we backtrack. we try to assign the value one to x. again, so with f assigns the value one to x, meaning that we remove all the clauses that contain x from the formula. and we remove the x, negate it from all other clauses. if the recursive call returns that the formula is satisfiable we return that the initial formula is also satisfiable. in the remaining case we just report that the formula is unsatisfiable. because no matter how we assign the value to x the resulting formula is unsatisfiable. meaning that there is no way of assigning the value to x to get a satisfiable formula. so as we've seen in our toy example such a strategy arose to find a satisfying assignment to conclude that the formula is unsatisfiable without actually checking all possible 2 to the n assignments, 2 to the n candidate solutions. i'm sorry. so we do this as follows. we build each solution piece by piece. we try to extend each partial solution and whenever we see that this is a deadend, that the current partial solution cannot be extended to a valid solution we backtrack immediately. we cut the correspondent branch of the tree and we do not extend it. so this is a technique which is used in many, many modern and state of the art sat-solvers. we did not prove any upper bound on the running time of the algorithm. well as you expect the running time of the algorithm might be exponential. because if we prove that it is polynomial, we would show that could be solved in polynomial time. this would resolve the p versus np question. however, this technique is very useful in practice. however, in practice sat-solvers also use complicated heuristics for simplifying formula and for selecting the next variable for branching and then in selecting the next value for branching. so in practice this idea improved by various heuristics of simplifying a formula chosen as the next variable and chosen as value for the next variable lead through very efficient algorithms that are able to solve formulas with thousands of variables. and other commonly used technique is called local search, and this is a technique that we will consider in the next part. 
the next technique that we are going to consider is called local search. this is also widely used in practise technique. it is used both for search problems and for optimization problems. it's main idea is roughly the following. you start with some initial solution, then you iteratively do the following. if your current solution is not good enough, you try to change it little bit somewhere to get a new solution which is, in some sense, better, or which fixes something. so pictorially it looks as follows, you go from one solution to a solution which is close to it, which results from the current solution by some small change. then to some other solution, then to some other, until you get a solution which is good enough for you. so this is a very very vague explanation of this technique. we will now give a more concrete explanation of the local search algorithm. so this is how a local search algorithm might look like for the three variability problem. consider a formula in 3-cnf and denote, it's variables by x1, x2 and so on xn. so it has n boolean variables. so a candidate solution for such a formula is just a truth assignment for all these variables. so in other words, this is just a vector in {0, 1} to the n, right? since we are going to apply local search, that is, we need to change each candidate solution a little bit, to go to another solution in its neighborhood. we need to define the notion of distance between two different candidate solutions. so let alpha and beta be two such candidate solutions, that is vectors in 0, 1 to the n. we define the distance between them as the number of bits where they differ. so, more formally the distance we just, we just called distance. the distance between alpha and beta is a number of such indices i where alphai is not equal to betai. consequently by hamming ball with a center at an assignment at alpha and with radius, r, we denote the set of all assignments that are a distance, at most, r from alpha. to give a concrete example, for example, the hamming ball with radius. and this boolean assignment, we center this boolean assignment and write your 0. so this is alpha, this is r contains just exactly the same boolean assignment. the hamming ball was radius one and center at the same boolean assignment contains five different assignments. so in this case, by the way, n is equal to 4. so first of all, it contains the center of this ball. it also contains four boolean assignments that resolved from our initial assignment by changing just one beat. waits on. the first one we change the first bit here. we change the second bit here. we change the third bit here. we change the fourth bit. finally, the hayman ball with the same center and with radius 2 contains the following assignment. so this is an assignment at distance one. these are four assignments at distance one. i'm sorry, so this is an assignment at distance zero. these are four assignments at distance one. and this six assignments at distance exactly two. this is a crucial lemma for our loca search algorithm, which checks satisfiability of a given formula. assume that we are given a formula f. and together with this formula f we are given a hamming ball. we use center alpha and radius r for which we know that is contains a satisfying assignment of a formula f. then we can find a satisfying assignment in this ball in times 3r times the length of the formula. this final assignment that we are going to find might coincide with be or might be a different one. but in any case, if we know that this cannon ball contains a satisfying assignment, then we will find some satisfying assignment in this running time. this is how to do this. we first check whether the assignment alpha, namely the center of our current ball, satisfies the formula f. if it satisfies we just return alpha immediately. otherwise, there is an unsatisfied clause which contains at most three variables. assume that this clause looks as follows, it is xi or negation of xj or xk. we know for sure that alpha assigns the value 0 to xi, the value 1 to xj and the value of 0 for xk right? this is just because this clause in unsatisfied under the assignment alpha. let's then do the following. let's consider three different assignments: alpha i, alpha j, and alpha k. alpha i differs from alpha only in the i speed. alpha j differs from alpha only in the j speed. finally, alpha k results from alpha by flipping the k speed. what we know about these three assignments is that at least one of them is closer to beta, right? so we don't know which one, but we know for sure that at least one of them is closer to beta then alpha, why is that? because beta satisfies the currently considerate clause. so in beta, either x i is equal to 1, or x j is equal to 0, or x k is equal to 1. so in at least alpha i, or alpha j, or alpha k, we fix at the right bit. the ones that have the same value in beta. so at least one of them is closer to beta. probably two of them, or probably three of them, but it doesn't matter at this point. what is important is that at least one of them is closer. so we start with alpha and then we consider three different assignments. alpha i, alpha j, alpha k. for each of them we make the same step. for each of them we also consider three different assignments. and we always know that, at least in one of the branches we get closer to beta. since we know that the distance between alpha and beta is at most r, just because beta lies in the hamming ball. we center alpha we conclude that in the corresponding algorithms, there are at most 3 to the r recursive calls. this how it looks pictorially. so we have with center alpha and radius r. so the radius of this ball is at most r. we know that somewhere in this ball, there is a satisfying assignment beta. we don't know which one. of course we're only looking for a satisfying assignment but i assume that there is a satisfying assignment, beta in this case. then at each step we do the following. we select an unsatisfied clause and from alpha we move to one of, we try all these three satisfying assignments that differ from alpha in just one bit. we don't know in which direction we need to go but we know that in at least one of these directions we get closer to beta. then for all these three assignments we repeat this procedure and in turn for all of them we again repeat this procedure. so eventually, if we make r steps, we will find an assignment beta, or we will stop before this if we find another satisfying assignment. the crucial observation is that to eventually find beta, we need at most three to the r recursive calls. this is just because the distance between alpha and beta is at most r, and at each iteration in at least one of these three branches we get closer to beta. this is of the resulting algorithm. it is given a formula f, also an assignment alpha. and in r. and then it checks whether it tries to find a satisfying assignment in a ball with center alpha and radius r. we first check whether alpha satisfies the formula f. if it does, we return alpha immediately. then we check whether the is equal to zero. if it is equal to zero we return that ball. ball does not contain a satisfying assignment. so we return not found. so recall that if r is equal to zero then the responding ball contains the assignment alpha itself. then we take any unsatisfied clause and we denote by xi, xj, and xk, the variables from this clause. then we construct three different satisfying assignments. alpha i, alpha j, or alpha k. they all differ from the assignment alpha only in i's, j's, or k's positions respectively. we then make three recursive calls. more precisely we first make the first recursive call. we with parameters f, alpha i and r minus 1. if it finds a satisfying assignment we immediately return. if not, we make a second recursive call for f, alpha j and r minus one. if it finds a satisfying assignment, we return immediately. if not, we make a short recursive call. so if none of them find a satisfying assignment we return not found.], okay? now we need to show how to use this procedure which checks whether a bowl containing a satisfying assignment to check whether the given formula is satisfied. well, assume that our initial formula, our formula f, is satisfiable, and the reason it's satisfying assignment beta well, there is a simple observation. assume that there are more 1's than 0's in the assignment beta, then i claim that it's distance from all 1's assignment is, at most, n over 2. well, this is clear. if there are more ones than zeros. then the number of ones in this assignment is at least n over 2. which means that the distance to the all-1's assignment is at most n/2. otherwise, the distance of this assignment to the all zero assignment, that is an assignment which assigns zeros to all the variables is also at most n/2. this means that to check the satisfiability of the formula f, it is enough to check whether any of two huge balls contain a satisfying assignment. once again, if there is a satisfying assignment beta of a formula f, and it is contain it is contained either in a ball of radius n/2 with center in all-1's assignment or in a ball with radius n/2 whose center is in all-0's assignment. so if none of these recursive calls finds a satisfying assignment, then the initial formula is unsatisfiable for sure. the running time of this algorithm is o of the size of the formula times 3 to the n/2. this is just because we will make two calls to check for two balls of radius n/2 and each of these calls takes time roughly 3 to the n/2. so this is roughly 1.733 to the n so this is still exponential, still not acceptable in practice. however, know that the same time that this has exponentially faster than a brute force search algorithm. because a brute force search algorithm runs in time 2 to the n and algorithm runs in time 1.733 to the n this is roughly 1.15 to the n. so it is exponentially faster. already for n = 100, for example, our algorithm is roughly 1 million times faster than the brute force algorithm. so this algorithm works well in theory. and local search heuristics and local search ideas but slightly different form are also widely used in modern set holders. 
the next problem we're going to design efficient algorithms for is the traveling salesmen problem. using this problem, we are going to show the main ideas of the dynamic programming technique and the branch-and-bound technique. well, recall that the input in the traveling salesmen problem is a complete graph with weights on edges. we usually assume that the edges, that the weights are not negative, together with that budget b. and our goal is to find a cycle that visits at each vertex exactly once and have total lengths at most b. it will be convenient for us to assume that the vertices of our graph are numbered with 1, 2, and so on, n. and that the start of the cycle, and also the end of our cycle, is at vertex 1. to give a third example, as usual consider the following graph with five vertices. already for this small graph it is not so easy to find an optimal cycle. there is, for example, a cycle going from 1 to 2 to 3 to 4 to 5, and going back to 1. its length, its total length is 15. there is another cycle whose length is 11 and the optimal cycle in this case has length 9. it is shown here on the slide. so know that a brute force search algorithm just goes through all possible (n-1)! cycles. why (n-1)!? well, because initially we start at vertex 1. from vertex 1, there are (n-1) choices where to go next. for all of this (n-1) vertices, there are (n-2) choices where to go next, right? (n-2) because we cannot go back to 1 and so on. so it is (n-1) times (n-2) times (n-3) and so on. this gives us (n-1) factorial and this is the running time of an algorithm which is (n-1) factorial, or roughly n factorial, is extremely bad. it is an extremely small, it is even worse than 2 to the n, even slower. so already for n equal to 10 it is already unfeasible, not to say about n equal to 100, for example. so our goal in this lecture is to design an algorithm whose running time is roughly n squared * 2 to the n. it is, of course, an exponential algorithm. but it is at least much better than going through all possible candidate solutions. it is at least better than n factorial. in particular, if we can see the following fraction, n factorial, divided by 2 to the n times n squared. then already for n = 100 this fraction is about 10 to the 120. so which means, that already for n equal to 100, the algorithm that we're going to design is going to be 10 to the 100 roughly times faster than a brute force search solution. as we've mentioned already, we're going to apply the dynamic programming technique to design a faster than a brute force algorithm for this problem. since we are going to use dynamic programming, this means that instead of solving one large problem, we are going to solve a collection of smaller problems. usually subproblems that we are going to solve represent just some partial solutions, right? and what is a good partial solution in case we are looking for cycle that visits each vertex exactly once? well, this is some initial part of a cycle, right? so, when we have some initial part we need to try to extend it somehow, and what information do we need to be able to extend it? well, at least we need to know the last vertex of this initial part of this cycle. and we also need to know the set of all series or the set of all vertices that we already visited, right? so we know, we know the last vertex of our path and we know the vertices that were already visited. and then we can consider all possible extensions of this path and select the best one. this way we will eventually construct an optimal cycle. so let's formalize this idea. for a subset of vertices s containing some vertex i and also the vertex 1, we denote by c(s,i) the length of the shortest path that starts at the vertex 1, ends at a vertex i. and visits all vertices from the set s exactly once, so namely c(s,i) is the length. if i show this path that looks like this, it goes from the vertex one to the vertex side and all the vertices that it visits are from the set s. and also it visits all of it's vertices exactly once. in particular, we assign c of the set containing just vertex 1, and vertex 1 = 0, because this is just an empty box, right? and for any other s that contains, that is of size of at least 1, we assign c(s, 1) to be equal to +infinity because a path cannot end in the vertex in the vertex 1, okay? then we need to find a recurrence relation expressing c of si through solutions for smaller problems. for this considers the following thing, so we need to go from vertex 1 to vertex i and visit all the vertices from the set s and we would like to find a shortest possible such path. consider the second to last vertex on this path, so this is about vertex j. so the last step in an optimal path which we're going to form is from j to i. what can we say about the following parse? so first of all of of course, it must also be a shortest possible path from the vertex 1 to vertex j. but also we know exactly the set of vertices that it visits. it visits the vertices s minus the vertex i, right. so this exactly all vertices except for the last one. so this allows us to express that is a solution for the set s and the feature i for solutions for smaller stock problems as follows. we just go through all possible values of j except for j equal to i. and we select the minimum among the following expressions. so it is stated otherwise, an optimal path going through all cities from the set s and ending in the vertex i is some optimal path that goes through all cities except for i and ends at some vertex j. extended with an edge from j to i. we do not know the exact vertex j because we are only looking for this. but we just select the minimum over all size possible values of j. we are almost ready to implement the corresponding dynamic problem, an algorithm so the only technical things is the order in which we are going to solve our sat problems. this order should satisfy the following simple property. when computing a value the value of c(s, i), we already need the values of all the subproblems. it depends on to be computed, right. all such values are of the following form. it is c(s- {i}, j), okay, so in particular s- i always have size less than the set s. so if we just go through all sub sets in order of increase in size then we are safe. this gives us the following algorithm. so given a graph we first initialize c of ({1},1) to be equal to zero. so this just reflects the fact that if we need to find an optimal path, that starts at vertex 1, visits just the vertex 1, and ends at vertex 1, then this is just an empty path, right? we just say at the vertex 1, so it is equal to 0. then, we go through all possible sets of vertices denoted by s, and we do this in order of increasing size. so we gradually increase the parameter small s from 2 to n and for all subsets s of size s we do the following. first of all we assign the value +infinity to c of (s,1). because we need to visit each vertex exactly once. so we just mean that this is an invisible path, it is equal to +infinity. we start at vertex one so we should not end at vertex one, okay? then for all i that lie in the set s, we need to compute, we need to compute the value of c(s,i). for this we can see there's all possible candidates to be the second to last vertex on an optimal pass. so this is done here, the second to last vertex should not be equal to the vertex i itself, so we compute the minimum between the, among the current value and the following value. so this actually says that we first visit all the vertices except for the vertex i and then end in the vertex j. and then we append the edge from j to i. so the total length is increased by the length of this over this last edge. finally we need to return the best such possible path and we do it by just finding the minimum of the following expression. that is an optimal cycle that starts in the vertex i and ends in vertex 1, i'm sorry. and end in the vertex 1. and is the following, it first visits all the vertices in the graph and then ends in the vertex i. and all these values are stored in the following cells in our table, right? so since we don't the last values in this cycle, we just select the minimum and we also end the last edge inside your cycle. so finally, we compute the result using this expression. there is one technical remark left. in the algorithm we need to somehow iterate over all subsets of our n cities. this can be done as follows, there is a natural one-to-one correspondence between integers from 0 to 2 to the n- 1 and subsets of the set containing integers from zero to n minus one. so, when implementing this algorithm, it is more convenient to consider integers from 0 to n minus 1 instead of integers from 1 to m. namely this correspondence is defined as follows. if you have an integer k from the following range namely an integer which is at least zero and at most 2 to the n- 1. then the corresponding set is defined by the positions where this integer has once in binary representation. this is probably easier to show by an example, so consider the following example. here we consider subsets of three elements of 0, 1, and 2. and for this we consider all integers from 0 to 7. so the binary representation of 0 is 000, right? so this corresponds to an empty set. the binary representation of 1 is 001. so we have 1 in the least significant bit, 0 in the, in the next bit. and 0 in the, in the most significant bit. so this corresponds to just subset containing just an element 0. or, for example, 3 is represented in binary as as 011. so it has 1 in the least significant bit, and 1 in the next bit. so this corresponds to the subset {0,1}, right? so this way, instead of using c(s,i), we can use just c(k,i), right? where k is an integer from zero to the n minus 1. the last iterations that we need to do this with is that we need to exclude the element j from the set s. that is, since we are going to represent the set s as an integer, we need to find the corresponding integer. so what does it mean to extract s from the set. this means actually to flip the j-th bit of the corresponding integer from zero to one. and this in turn means, that we are going just to compute the bitwise parity of our current number k, and the number where we have just one in the jth position, right? so we compute the parity of these two numbers, let me probably write it as follows. so assume that this is our number k, and this is the position g where we have 1. and what we need to get is the following, right? so this corresponds to s, and this is s minus the element j. it looks like it follows. so to get this number we just need to sum this number with the following number. just the number, which have 1, only in position number j, right. and this is, in turn, this is 2 to the j, right. so most modern programming languages allow to do this just in one line. namely, you do this as follows. you take a number k, then you compute 2 to the j as follows, and then you just take this bitwise parity. so this gives you exactly the set s without the j element. so now you are completely ready to implement this algorithm. 
so, next general to think was main ideas were going through the stree on a toy example of solving a traveling salesman problem is called the branch-and-bound technique. it is quite similar, actually, to the backtracking technique. but backtracking is usually used for solving decision problems, while the branch-and-bond technique is usually used to solve optimization problems. its main idea is as follows, again we are going to grow a huge tree which in the end is going to represent the search space. that is the space of all candidate solutions. so, we are going to grow these solutions piece by piece, but it's each step. instead of doing this naively in each point in this tree, we check whether the current partial solutions that we have, have any chances to be extended to a solution which is better than the one that we currently found. which is better than the best ones that we've currently seen. if it has no chance, we cut this branch immediately. we do not continue it if we realize that it cannot be extended through a solution which is better than the best one found so far. we illustrate the main idea of the branch-and-bound technique on a toy example. so, consider the following graph consisting of four vertices. the graph is complete, meaning that there is an edge between any pair of vertices. one way to solve this is to consider all possible cycles. for this, we consider the following trees. we start at vertex 1. from vertex 1, we can go to either of the following 3 vertices. go to vertex 2, go to vertex 3 or go to vertex 4, right? from the vertex 2, we are allowed to go right at the vertez 3 or to vertex 4. we are not allowed to go back to vertex 1 because we need a cycle that treated each vertex exactly one. and so, what i am trying to extend each possible partial solution with each possible variant. so, from vertex 3 we actually need to go to vertex 4, and from vertex 4 we have no choice but to return to vertex 1. so, when we see that this is a full cycle visit and each vertex exactly 1, we computer its total length. so, in this case it is 19. and we do this for all possible cases. here we have 7, here we have 18, here we have 7 again, 18 again, and 19 again. so, we have actually pairs of equals numbers here because the corresponding leaves. for example, this leaf and this leaf, they actually correspond to the same cycle, but reversed in two different directions. we can either go this way or this way. this leaf actually is the same cycle, of course, but, with the same total lengths. now, let's do it in a more smart way. let's grow the same three but, let's try to compute the total lengths of total partial, or current partial solution on the fly, instead of computing this at the leaf. namely, initially we stay at the vertex 1. so, the total length is 0, then we go, for example to the vertex 2. at this point the total length is 1. from vertex 1 we, from vertex 2, we try to go vertex 3. this gives us total length safes. so, this is our current as we go from 1 to 2 and then from 2 to 3. the current total length is 6. we then try to go vertex 4, this is only actually possibility, so our total lengths is 9 and then we return back to the vertex 1 because we already visited all other vertices. so, this is the first full cycle that we discovered, so we start the length is 19, so we marked that the best total length that we've seen so far is 19. then we go back, then we backtrack actually to consider as a possibilities. the last vertex on our pass, when there were some possibilities, is the vertex 2. instead of going to vertex 3, we might want to go to vertex 4. this gives us the total of the current length is 3. then we continue on now is the current length is 6. and finally, when we get to the leaf of this tree, we see that the current cycle give us gives us total length 7, so we update our variable which is responsible for the best solution found so far it is 7, okay. then again we backtrack. now the last vertex where there is still a possibility to go to another vertex is the root of this tree. so, we tried to go from 1 to vertex 3 but not to 2. so, the line says the current solution is 1. then we, from 3 we go to 6, from 6 we go to 4. and now we see that the lengths of the total of the current partial solution is always greater. so, 8 is greater than 7. so, out current solution is not going to be extended to some scene which is better than some scenes that we found so far. so, there is no sense to extend the current branch puzzle. so, we just go back, so we return back to this vertex and we try to go from 3 to another vertex, namely to 4. then when we go to 4, we discover another copy of the same cycle, so its length is 7. then it doesn't update our variable, so we just backtrack. we go to the root and we try to visit the vertex 4. but already when we go from 1 to 4 we see that we already traversed the edge of length 10, right. the length of this partial solution is already 10. it is already worse than the solution that we found before of total length 7. so, there is just no sense of extending this branch and we cut it immediately. so, if we do this a little bit smarter, then we do not need to go through all possible candidates solutions. so this is the one small branch that we cut. and this is another small branch that we don't need. so, in general it can save a lot of time. in our example now a toy, example we consider it probably is the most simple lower bound for estimating the size of any extension of a partial solution. so, modern on tsp-solvers that i able to handle graphs with thousands of vertices. use smarter heuristics for lower bounding and optimal solution in a given graph. we provide two examples which are still simple enough and not so smart that's used instead of. so, the first lower bound says that in any graph the total length of any traveling salesman cycle is at least the following expression, one half times the sum over all vertices where for each vertex we compute the sum of two edges of minimum distance adjacent to this vertex. well, why is that? well, if we just consider an optimal cycle. note that if we just consider edges in this cycle, then the total length of this cycle just equals to this expression. where instead of two minimal length edges for each vertex, we used just two edges that are adjacent to it, right? just because in this expression each edge in the sum this edge is counted exactly twice, for example this edge is going to be counted one time, once for this vertex once for this vertex, for this reason we have one one half here. so, if you we just use instead of two adjacent edges in an optimal cycle we use two edges of minimum lengths in the initial graph. this is obviously the lower bound for the lengths of an optimal cycle. and it can already be a good results and practice. and as a lower bound is that in any graph with non negative edge lengths, the total lengths of an optimal cycle, is at least the length the minimum spanning tree. why is that? well, this is just because if you have an optimal, an optimal cycle, and then you remove some edge from it, then what you get is a path in this tree. and this path use a spanning tree in this graph right. so, but it is not the minimum spanning, probably not the minimum spanning trees, so the weight of this part is at least the weight of the minimum spanning tree which means that the length of the cycles is at least the weight of our spanning tree. so, this concludes our module our lesson on exact algorithms. in the next lesson, we are going to consider approximate algorithms. so, these are algorithms that worked in polynomial and written solutions which might not be optimal. but they are guaranteed to be close to optimal. 
hello, and welcome to the next and actually the last lesson in the coping with np-completeness module. in this lesson we're going to design approximation algorithms. such algorithms work in depending on the size of the input and plus their return. is might not be an optimal solution but it is guaranteed to be a solution which is close to optimal. the first problem for which we are going to design an approximation algorithm is called the vertex cover problem. the input of this problem consists of a non-directed graph. and what we'd like to find is a subset of its vertices of minimum size, which touch or covers every edge. to give a specific example, consider the following graph of eight vertices. well for any graph, we can take all the vertices into a cover. this is clearly a vertex cover, it touches every edge. and the other thing is, in this graph, we can also take the following six vertices. it is not difficult to see that each edge is touched by green vertices. and an optimal vertex cover in this case consists of the following three vertices. again, it is not difficult to check that for each edge, at least one of its endpoints is green. for example, for this edge, it is covered with this vertex. for this edge, it's covered with two green vertices, and there are no vertices that was not green, they are connected by a niche. so the green set of vertices indeed touch every edge. the approximation algorithm for this problem that we are going to consider is surprisingly simple. namely it works as follows. initially our vertex cover is empty, so, we initialize c as an empty cell. then we repeat the following while the set of edges of our graph is not empty. we select any edge denoted by u,v of the current set of edges and then we add these vertices to the set c. then we remove from the graph all edges that are already attached by u and v. so we remove all edges that are adjacent to u or v. so we repeat this while there is at least one edge in e, and when e is empty we just return c. let me illustrate this on our previous graph. so first assume that we select this red edge. then we add its two vertices to a solution, and then we remove from the graph all edges adjacent to these green vertices because they are already covered. we did not need to cover them. then we select some other edge, for example this one, we add to its endpoints in the solution, and we remove all edges that are adjacent to these two newly added green vertices. so this leaves just one edge in our graph, so we select it, and we add two vertices into our solution. and we get the following,, the following solution consisting of six vertices. so just to show that it is indeed a solution, let me show you the initial edges of the graph. so we see the green vertices, the six green vertices indeed cover all the edges of our initial graph. we are now going to prove that the present algorithm is 2-approximate. this means that the running time of this algorithm is pretty normal and also that the solution, which is found by this algorithm, is at most twice as large as the optimal solution. well the fact that the running time of our algorithm is polynomial is clear just from its implementation. in fact, its running time is linear. so we proceed to show that it returns a solution, which is at least at most, twice as large as an optimum one. first observe that the set of edges selected by our algorithm is it forms a matching. this means that all the end points of edges selected by our algorithm, the end points of all the edges are just joined. why is that? well, assume that this is the first edge selected by our algorithm. so this is u, v. recall that after selecting this edge, the algorithm, discards all the edges adjacent to u and v, which means that for this algorithm, neither this, neither this and point coincides with u of v, neither this one. this is also, this also means that any vertex cover must take at least one vertex from each of these edges, which in turn means that any vertex cover of our graph must if you have size, it leaves the current analogy, or the size of m, right? now that the same times that we know the size of the vertex covered returned by our algorithm. it equals to just 2 times the size of m. this is just because the size of m is the number of edges in the set m and each edge has two vertices, two end points. so this allows us to conclude that the size of the vertex cover found by our algorithm is equal to 2m precisely. and we know that m is at most the optimum value, the size of the optimum vertex cover. so 2m is at most two times opt which justifies that what our algorithm returns is a vertex cover whose size is at most twice the size of an optimum vertex cover. let me also show you the set of edges selected by our algorithm in the previous example so it selected the following three edges. and you can see that indeed, all these three edges form a matching. so the end points of these three edges are disjoined. and this also means that to cover these three edges, we must take at least three different vertices. so we must take either this vertex or this vertex to cover this, to cover this edge. and this will not help us to cover any of the other red edges right? so we also must take either this vertex or this vertex and then either this vertex or this vertex. in total, at least three vertices. let me emphasize here the following thing. so we have somehow managed to prove that the size of the vertex cover constructed by our algorithm is at most 2 times the optimal value. at the same time, note that we do not know the optimal value. we do not know how to compute it in polynomial time. so our algorithm runs in polynomial time. so we don't know how to compute optimal polynomial time. so our algorithm doesn't compute it. however, what it computes is that we know for sure that it is at most twice times the optimal value. so how it can possibly be that we proved some upper bond on c in terms of opt without knowing the exact value of opt? well, this is because we know some lower bond are not and it is the size of the matching that we constructed. so the size of m gives a good lower bound on opt. more over the matching m can be used to construct some vertex cover. and just by taking all the end points of these m edges we get a vertex cover whose size is twice the size of m so this gives us the final lower bound. so c is equal to two times m, which is, in turn, at most, two times opt. so this allows us to prove an upper bound on c. not in terms of some quantities that we don't know, namely opt but in terms of the size of m which we can compute quickly and which we can use to construct the vertex cover. let me conclude this video with a few remarks. so first of all the bound we've proved that our algorithm is to approximately tied. this means that there are graphs for which our algorithm will return, an answer which is exactly twice as large as an optimum vertex cover. and this can be viewed as follows, lets just consider a full, well a full bipartite graph. or we can just, just consider the following bipartite graph, okay. assume that we have n vertices here and n vertices here. so our algorithm is going to do the following. so with each step, it will select one vertex, one edge, and add the both vertices to the solution. so in the end it is going to return a solution of size 2n, right? at the same time all the edges of this graph can be covered just by one of its part. so there is a solution of size m, but our algorithm returns a solution of size 2m. the last remark is that surprisingly, this algorithm is almost the best one that we know. in particular, we do not know how to find an approximation of this algorithm with the factor 1.99, for example. 
the next problem for which we are going to design an approximation algorithm is the traveling salesman problem. more precisely, it is a special case called metric tsp. so the word metric means that our input is an undirected graph whose edge weights on the negative and they satisfy the following triangle inequality. for any three vertices u, v and w, the distance from u to v was the distance from v to w is at least the distance from u to w. okay, so for this graph, we need to find a cycle that visits each vertex exactly once and has minimum possible total length, okay? as with the case of vertex [inaudible], we are going to design an algorithm which is going to return in polynomial time a cycle which might not be optimal. but it is guaranteed to be at most twice as long as the optimal cycle. so we are going to return cycle c, whose length is at most two times the length of optimal cycle. again, we do not know the value of opt, but we're going to construct something which is at most two times opt. two times something the value of which we do not know. again, we are going to do this by providing a good lower bound. so we're going to show that opt is at least l for some value of l. and then we are going to show is that c is at most two times l. so what is the value l in our case? so we're going to use the minimum spanning tree as a lower bound for the optimal value of the traveling salesman cycle in the graph. so it is stated here in the lemma and the lemma says that for any undirected graph with non-negative traits. so in this case, we don't even need the graph to be metric. so for any such graph, the lengths of the minimum spanning tree is at most the lengths of the traveling salesman problem cycle in this graph. so why is that? this is just because, once again, if we can see the optimal cycle. so we discussed this already, assume that this is an optimal cycle. so the length of this cycle is equal to tsp of g, then let's return any edge from this cycle. let me, just erase it. okay, so we've just removed some edge. so what is left is some parts and it is in particular a spanning tree in this graph, right? it is some spanning tree. so it is a minimum spanning has total lengths, at most, the lengths of these parts. and these parts, since all edges are non-negative, these parts has length at most, the lengths of the traveling salesman cycle in this graph. so we get that the minimum spanning tree of g is at most the lengths of this parts. let me denote it probably by p and p is at most tcp of g. we're now ready to present an approximate algorithm for the metric version of the traveling salesman problem based on the lower bounds that we've just discovered. so the first step is to construct the minimum spanning tree t of the graph g, okay? when it is constructed, we do the following, we double every edge in t and denote the results in graph by d. in the graph d, the degree of each vertex is even. so, there is a eulerian cycle in this graph, so we find one and denote it by c, okay? and then finally, so the cycle c visits every edge of the graph d exactly once. in particular, it visits every vertex of our initial graph, but probably not just once. so we just remove duplicates from the sequence of vertices and return the resulting cycle. in other words, we return the sequence of vertices in order of the first appearance in the cycle c. so let me show you schematically how it works and then we will proceed to some concrete example. so assume that we have for example, just four vertices in our graph. so the first thing to do is to construct a minimum spanning tree in this graph. i assume that it looks as follows. the second step is to double every edge, let's do this. now this graph is eulerian. namely the degree of this vertex is 2, the degree of this vertex is 2, the degree of this vertex is 2, the degree of this vertex is 6. this means that this graph contains an eulerian cycle and we know already that an eulerian cycle can be found in linear time. but in case when we have a tree whose each edge is doubled, this is particularly easy to find. so let me again schematically explain this to you. if we have a tree, let's consider this as a rooted tree. assume that this was our tree. here we have some subtrees and then we double every edge on this tree and also we double everything inside this subtrees. so how to construct in the eulerian cycle in this double tree? well, we can do this just recursively, we started the root, we then go into this subtree. and then we find an eulerian cycle recursively in this subtree and then we get back using this edge. then we go to the second subtree find an eulerian cycle as it traverses all edges inside this subtree and then we will come back using this tree. great, and then we proceed to the zero subtree find in eu;erian cycle and get back. for example, for these three shown here in an eulerian cycle will look like as follows. so let's start from this vertex, then we first traverse this edge. then we traverse this edge. then this edge. then this one. then this one and finally this one. so we've traversed all six edges. this is our cycle c. now we need to return a cycle that visits every vertex in this graph. for this we are going to do the following. let's start in the same vertex. we first use the same edge, then we use the second edge in our cycle c. but then the third edge in our cycle c visits the same vertex that was already visited. instead of doing this, we go directly to vertex four. from this vertex, the fifth's edge of our cycle goes to the vertex which we already visited. so instead of doing this, we go to the initial vertex. so the metric property of our graph implies that instead of edges three and four, we use this just the direct edge from three to four. then we can only decrease the total length of the resulting cycle. okay, we now proceed to a more complete example. in this example, our vertices are just points on a plane. and we assume that the distance between two vertices is just the distance between the corresponding points on a plane. in this case of course, the edge weights or the edge distances satisfy the triangle inequality. so in this case, we're given then points which define implicitly a complete graph on end vertices. so the first thing to do is to construct a minimum spanning tree in this case. it looks as follows. now with double every edge. now it looks as follows and now we find an eulerian cycle in the results in graph. for example, we start from this vertex and then we start traversing it like this. this is first one, the second one, the third one, the fourth one, the fifth one and so on. so 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 and we will count back to the initial vertex. now, this cycle i'm sorry, visits this vertex for example several times and also this vertex several times. so what am i going to do. it's where i'm going to bypass some parts of this cycle. namely, this is the final cycle that was constructed by our algorithm, let me show you how it was constructed. so from here we go to here because this vertex was not visited before. from here we go to here, from here we to go here. and from here we go to this vertex. but now, instead of using the fifth's edge, we go directly to this vertex. because the fifth's edge visits the vertex that was already visited by our cycle. then we go here because this vertex have not been visited before. then we go here. then we go here. and then again, instead of using the edge 10, 11, 12, 13 and so on, we go directly to the initial vertex. and now we visited all the edges. okay, on the next slide we will prove that this algorithm is too approximate. without proof is that the algorithm is too approximate. so this follows from three simple observations. first of all, the total length of the minimum spanning tree t is at most the optimum length of a traveling salesman cycle in our graph. so we proved this already. when we doubled each edge of the tree t, we get a graph whose total weight is at most to opt, right? and then, when we transform this eulerian cycle in this graph, into the hamiltonian cycle. namely into a cycle that visits each vertex exactly once. we can only decrease the total wait of this cycle. and at this point, we use essentially the fact the edge waits of our graph satisfies a triangle in equality. namely when we replaced, if our eulerian cycle goes as follows. and then we go directly from one vertex to the other, then we know for sure that the length of this edge is at most the sum of the lengths of all these edges. because again, the lengths of edges in our graph satisfy triangle inequality. let me conclude this part with a few remarks. first of all, it is known that the same version of the traveling salesman problem namely metric tsp can be approximate with the factor 1.5. the corresponding algorithm is known as christofides' algorithm and it is actually the best known approximation to date. it is also interesting to note that the general version of this problem cannot be approximated within any polynomial time computable function if p is not equivalent to np. in fact, if you design an algorithm that, for example, finds ten approximation for the general version of ten or n squared or even two to the n approximation for the general version. then it can be used to solve the hamiltonian cycle problem in polynomial time. if you design a good approximation algorithm for the traveling salesman problems and you solve in polynomial time also and as np complete problem, which is actually related of course. there's a travelling salesman problem which is called hamiltonian cycle problem. so if p is not equal to np, then no good approximation for the general version is possible. 
we now describe a local search heuristic for the traveling salesman problem. we've already seen the main idea of the local search algorithm, but let me describe it once again. so to solve an optimization problem with a local search heuristic means the following, we usually start with some initial solution s, then we'll repeat the following procedure. for a current solution s, we look into some neighborhood of this solution and we try to find a solution in this neighborhood, if the solution [inaudible] which is better than the current solution s. if there is some solution such a solution s', and we replace s with s' and repeat this and we stop when s is the best one in its neighborhood. so first of all, the first thing to note is that this algorithm might return a sub optimal solution because the only thing which we can guarantee about the return solution it is that it is the local optimum, not the global optimum. so namely it is optimum in its neighborhood. but also, the quality and the running time of this algorithm depends on how we define the neighborhood actually. for example, we might define the neighborhood of some solution as the search of all possible solutions. then this algorithm in one iteration will need to go through all possible candidate solutions and to find the best one. so, this is algorithm will be actually the same as the brute force algorithm. how do we define the neighborhood in case of the traveling salesman problem? well, for this we first need to define a distance between two solutions. we call that, in case of traveling salesman, a candidate solution is a cycle that visits each vertex exactly once. so assume that we have two stage cycles, s and s'. we say that the distance between them is at most d, if we can get s' from s by first deleting the edges, at most, the edges from s, and then adding probably some other d edges to the resulting structure, okay? then we can define a neighborhood, again, as in the case with algorithm, namely, we define a neighborhood of s with radius r as all the cycles that we can obtain from s by changing it's most r edges. to give you a specific example, consider the following graph containing of eight vertices, and, again, we assume that the vertices here are just points on a plane and the distance between any two vertices is just equal to the distance between the corresponding points on a plane. assume that our initial solution looks as follows, it is clearly suboptimal but just changing the two edges in this solution makes it optimal. so instead of using these three edges, we use the following two edges. this gives us an optimal solution. sometimes, however, just changing two edges is not enough to get to improve a solution. for example, consider the following situation. again, it is not difficult to see that this solution is suboptimal. in particular, it is clear that instead of this vertex from this one, instead of using this path, it would be much better to use this part. however, changing just two edges, by changing just two edges we cannot improve this. so, in this case, to improve the solution to find a better solution in its neighborhood, we need to allow changing three edges in particular instead of this three edges, we want to use the following three edges. right? so what we can see, there is a trade off between the quality of solution and the running time of a single iteration. namely, when we increase the size of the bowl, the size of the neighborhood, we increase our chances to find a better solution in a current bowl, but we also need to go over all this solution inside this ball. right? in any case, it is not excluded, the total no of iteration of your algorithm will be exponentially increase in this case. even we use it is not excluded that from this solution we go to that, to that, to that, to that, and so on and we always improve it a little bit. and the total number of steps of iterations is exponential. and at the same time, it is not excluded that the quality of the final solution will be poor. but on the other hand, this heuristic usually works well in practice, but it probably with some additional tricks. for example, an obvious additional trick that is reasonable to use is to allow your algorithm to restart. namely, not to use just one initial solution but to use one solution then do some local search and then to restart from some completely different point and probably to select many such points, probably a trend among other trend, and then return and then the solution which is the best one that you've seen in this process. so to conclude, the role module, let me repeat the main message. when you face an np-complete problem in practice, don't give up. there are several possibilities. first of all, it is not excluded that all the instances of your np-complete problem are some special cases for each, there exist an efficient algorithm. also, it might be the case that you can implement an algorithm which have no theoretical upper bound on its running time. but still, it works well in practice because it intelligently searches for the space of all candidate solutions. and finally, you might want to implement an algorithm, which returns not an optimal solution, but something which is close to an optimal solution. for some applications, it might be good enough. 
hello, my name is michael kapralov. in this lecture i will talk about finding heavy hitters in data streams. so this lecture is about an algorithm in the so-called streaming model of computation. i will start by defining the streaming model of computation for you, and i will define the problem that we want to solve, that is, the heavy hitters problem. i will talk about some of the applications and then proceed to the actual technical content, that is, design a very space efficient algorithm for this problem. well, the streaming model of computation is quite old by now. it was first defined in a foundational paper of alon, matias and szegedy in the 1990s. in this model of computation we think of the algorithm as observing a very long steam of data items. and data items could be anything. it could be ip packets, tweets, search queries, whatnot. so as the algorithm is scanning this very long stream of data items, its task is to maintain the small space summary of the stream seen so far. from this small space summary, we're supposed to be able to query some basic statistical properties of the stream. informally, we think of the algorithm as having a single pass over a stream of data items. we'll denote the data items by i1, i2, through in. capital n will be the length of the stream, and we will assume that the length of the stream is unknown. we typically think of a streaming algorithm as having small, or sub-linear storage capacity. so the space available to the algorithm is substantially smaller than the stream that it's working with. and we typically think of this as n to the alpha for some small constant alpha less than 1. or, which is the setting that we're in for this lecture, as polylogarithmic in the length of the stream. okay, well when i talk about storage capacity, or space availability to the algorithm, what exactly do i measure space or storage capacity in? well, we could use bits, bytes, words to measure storage capacity. in some settings, we could also use points or data items. okay, so a streaming algorithm has to have sublinear space complexity. at the same time, we also want streaming algorithms to be fast. that is, we want to have small processing time per element. it turns out that in many cases and for many problems, it is necessary to use randomization in order to achieve sublinear space complexity. and indeed, this is the case for the problem that we're considering today. so today we will talk about the so-called heavy hitters problem. and, formally, this problem was defined as follows. the algorithm is given a single pass over a sequence of data items, i1, i2, i3, through in. so n is the length of the stream. after the algorithm processes the entire stream, its task is to output the k most frequent items that occur in the stream. so these k dominant most frequent items in the stream are known as k heavy hitters. now of course, we want our algorithm to have small space complexity. it cannot just store the entire stream, and find the top heavy hitters. specifically in this lecture, we will design an algorithm for the heavy hitters problem with space complexity on the order of k times log n, k times the logarithm of the length of the stream. so for example, if the number of heavy hitters that we're looking for is small, say k is equal to 1, then the space complexity of our algorithm is logarithmic in the length of the stream, which is exponentially better than what the trivial solution that just stores the entire stream would have to use. storing the entire stream, of course takes space liner and the length of the stream. let me give you an example for this problem. and here's a representation of the stream that i will use in the rest of the lecture. so, on the slide, at the bottom of the slide, you see the data items arriving one after another in a stream. we'll associate data items with numbers between 1 and some large number n. for my examples on the slides, i will use numbers between 1 and 10. so the bottom line on the slide shows the data items arriving in the stream. in the middle of the slide, you see the frequency histogram for these data items being updated on the fly. so for example, at this point, item 3 arrived twice. so we see frequency 2 in the histogram. and items 4 and 6 appeared only once. so our algorithm is observing this very long stream of data items, and at the end of the stream needs to output the most frequent item that it has seen so far. and in this particular example, the most frequent item is item 3. it occurred six times in the stream, whereas all of the other items occurred at most five times. in the next video, i will talk about some of the applications of this heavy hitters problem. and the applications will be to monitoring network traffic and estimating query statistics. 
so this is the problem that we would like to solve. it is the heavy hitters problem. we're given a single pass over a stream of n data items, i1, i2, through in. we would like to design a small space algorithm that will scan the entire stream and at the end of the stream will tell us the top k most frequent items that occurred in the stream. we also want this algorithm to have very low space complexity, and we're shooting for space complexity on the order of (k log n). okay, so this is definitely a very nice and clean mathematical problem to consider. but are there good applications of this problem? now it turns out that there are quite a few, so i would like to mention two of them in this lecture. the first one that i will talk about is to estimating network traffic. for this application, think about a switch, as pictured on this slide. and think about the traffic, the ip traffic that goes to through that switch. well, this traffic can be quite conveniently visualized using the so-called traffic matrix, pictured on the slide. the rows of this matrix correspond to source ip addresses. the columns correspond to destination ip addresses. and the entry in i,jth element of this matrix is exactly the number of packets that ip address i sent to ip address j that went through our switch over a period of time. now note that the amount of traffic that goes through a typical internet switch is a mess. at the same time, these switches tend to have fairly limited memory. on the other hand, it would be very useful to have statistical estimation parameters that would let us analyze the network traffic through the switch. and one particular question that we would like to ask is whether or not some ip flows that go through that switch actually contribute the bulk of the traffic. the reason for this is that if a certain source ip address talks a lot to a certain destination ip address, and occupies the bulk of this switch, this could be an indication of a denial of service attack or some spamming activity going through that switch. so we definitely would like to design algorithms that are able to operate using the memory available to the switch and detect such situations and find such source destination pairs. these pairs are known as dominant or elephant flows. let me show you an example. so this switch is observing internet traffic going through it, basically packets go from various sources to destinations, and every packet updates an entry of the traffic matrix. so as shown in this example, it could be that at the end of the day, the number of flows, the number of source ip destination pairs that send packets through the switch is immense. but on the other hand, a handful of these flows contribute the bulk of the traffic that goes through the switch. this is exactly the situation shown on the slide. here, we have six flows or seven flows going through the switch, and two of them contribute a bulk of the traffic. one contributes 4 and the other contributes 5. well, we would like to be able to detect such situations and find these dominant flows. on the other hand, if you think about the trivial solution to this problem, this would need a lot of space because the trivial solution would just remember all the source ip destination pairs that ever sent a packet through our switch over a period of time. and the space is linear in the number of such pairs, which is very large. so in this lecture, we will design an algorithm for solving this problem approximately that is for detecting the dominant flows through a network switch using space only logarithmic in the total number of source ip destination pairs that are sending traffic flow through our switch over this period of time. and the improvement from n to log n is exponential, this is great savings. okay, the other application that i would like to talk about, this one was to estimating network traffic, is to estimating query statistics in search engines. and for this application, think of our data steam as a stream of queries on a search engine, think of say google.com, over a period of time. so here's an example stream, three queries. geneva to new york, coffee in geneva, geneva to new york. so it could have been me searching for flights from geneva to new york wanting to go to a conference. and despairing and thinking that this is a very hard process. and going for coffee and then coming back to search for flights. so the question of finding the heavy hitters in this stream of three items is exactly the problem i'm figuring out, which one occurred the most. and if i want to find the one heavy hitter here, the answer is geneva to new york because this query occurred twice and the other query occurred only once. of course, this problem of finding heavy hitters in search logs is more interesting than the example that's shown on the slide. so if you think of solving this problem at scale, that is, think of queries on google.com over a period of time. and this becomes a very hard problem because the length of the stream is much, much longer than three. so again, the trivial solution would amount to just storing all the distinct search queries that we see over this period of time, and the space complexity is linear in the number of search queries. now, the solution that we will see in this lecture, known as the countsketch algorithm, has space complexity logarithmic in the length of the stream. and that's again, an exponential improvement. well, this is great. so i'm claiming exponential improvements from linear space to order log n, but there's big o notation on this slide so the natural question to ask is are the constants actually small? so are these algorithms practical? now in fact, it turns out that a lot of the algorithms that have been developed in the streaming literature actually are, including our algorithm in this lecture. let me give a specific example which relates to a somewhat different problem. this is the so-called distinct elements problem. given a stream of data items, count approximately how many distinct queries did you see. with this problem we have a very theoretically efficient algorithm, and we also have an extremely good implementation known as hyperloglog. to put things in perspective, this algorithm hyperloglog can be used to estimate the number of words in shakespeare's vocabulary using only 128 bits of memory. so the constants in big o notation are actually quite small. okay, so as a consequence of that, a lot of the algorithms that have been developed in the streaming literature are in fact now widely used in practice for scalable data analytics. so we definitely want to study these algorithms, and so enough motivation, so let's get to the actual problem. so in the rest of the lecture, we will design a small space algorithm for our heavy hitters problem. that is, given a single pass over n data items, i1 through in, output at the end of the day, the k most frequent items that occurred in this string, using a small amount of space. 
so we would like to solve the heavy hitters problem in a data stream. so once you design a small space data structure, you can scan a stream of data items, i1, i2 through in. and at the end of the stream, output the k most frequent items that occurred. we also want this data structure to have a small space complexity. we want space requirements at most, order k log n. well formally, we would like to design a small space data structure that solves the following problem. we call it findtop. findtop scans an input stream s of items, and has a parameter k in hand. at the end of the stream, it outputs the top k most frequent items that it saw in this tree. well it turns out to be useful to first design a primitive for the following related problem. we call it pointquery. pointquery is a small space data structure hopefully, that scans the stream s. and at the end of the stream gets a query i. i is a data item. and in response, outputs the frequency fi. that is the number of terms that i occurred in the stream. well for simplicity, we'll assume that the data items are ordered in descending order of their current frequency of occurrence in the stream. so, item one is the most frequent item. item two is somewhat less frequent, etc., etc. what we want to design is a data structure that can solve pointquery, scan the stream, and then answer queries at the end. and we want this data structure to use small space, space on the order of k log n. where k is the number of items we will be ultimately looking for in the stream. well unfortunately this turns out to be impossible in general. and this is because, as stated, we require this pointquery and findtop to output exact answers. well, imagine a stream where every data item occurs with roughly the same probability, or rather roughly the same frequency. it seems very hard to design a small space data structure to figure out which items were exactly the top k. and this can be proved formally that one essentially needs to store the entire stream. so what we definitely need is a notion of approximation if we want to get sublinear space complexity. to that effect, we define the following approximate version of the problem. we call it find approximate top. so this has two parameters k, the number of items that we're looking for, and the precision parameter epsilon. at the end of the stream, this procedure needs to return a set of k elements, such that for every data item i that we report at the end of the stream, the frequency that i occurred within the stream is not much smaller than the frequency of the actual kth most frequent element. so fi should be at least (1- epsilon)fk, where fk is the number of times the kth most frequent element occurred. and now again it turns out to be very useful to design a procedure for the following approximate version of the pointquery problem. so here we want to scan the stream. at the end of the stream, if given a query i, we want to report an approximation at hat i to the number of times item item i occurred in the stream. this approximation should be off by at most an additive epsilon fk term. or again, if k is the number of times, the kth dominant element occurred. okay, so this looks better. at least we have a notion for approximation. now unfortunately, as stated, this is still a hard problem. and indeed, if we still imagine a stream where all the items occur roughly with the same frequency, maybe up to 1 plus minus epsilon, this is still a hard problem to solve. one essentially needs to store most of the stream. so in fact, in this lecture, we will design a small space primitive for both of these problems. that will allow us to find these top k elements, top k most frequent elements in the stream, assuming that they actually contribute a bulk of the stream in a certain sense. and we will see the formal formulation of what this means to contribute the bulk of the stream later in this lecture. 
i have said that if we would like to design an approximate data structure for finding the top key elements in a data stream, it is useful to first design a data structure for approxpointquery. so now let me make this statement precise and give a formal reduction. i will show that if we have the data structure for pointquery, then an exact data structure for find top follows. and, similarly, if we have data structure for approxpointquery, then approximate find top follows as well. and i'll do this in a few steps. and i'll start with the simplest possible case. suppose that we want to find the top one, the most frequent element in the data stream, so k is 1. now we assume that we have a data structure for exact pointquery. we want to find top data structure for k equals 1. look, so we're given a stream s as a sequence of items, i1, i2 through in. we will maintain a data structure for pointquery. besides that, we will maintain two other variables. one is the current maximum, the current most frequent element in the stream so far. the second one is its frequency. we initialize them to null and zero initially, and then we look at every position p between 1 and n on the stream. we'll look at the data item that arrives in position p. we call the pointquery data structure to get the count of this item so far. let's call it f. now, we check. if the current maximum is the data item that we're looking at, then we simply update its count and proceed. if we're not actually storing any of the data items, which only happens at the very beginning of the stream, then we store the item ip, and initialize its frequency to 1. now otherwise, we compare the frequency of the item that we're storing to the frequency of our current item, as reported by pointquery. and if one is smaller than the other one, then we just swap them. okay, so why does this work? well, because in each position in the stream the current maximum, the variable that we're storing is either null or actually contains the current most frequent element so far. good, so this is a very simple reduction. now does it generalize from k equals 1 to k bigger than 1? well yes, it does. and it works as follows. again, we'll assume that we have an exact pointquery data structure, which is a somewhat unrealistic assumption, but it's useful for our development. now we're given a stream of data items. we'll maintain the pointquery data structure, but now instead of just two variables, we'll maintain a heap h of at most k elements by count, by their count so far in the stream. good, so for each position p between 1 and n, we look at the current data item, we ask pointquery to give us the frequency of this data item so far. we'll call it f. if the heap contains our data item already, we'll simply update its count and continue. otherwise, if the heap contains fewer than k elements, we just add our item to the heap with the current count. now if the heap actually contains exactly k elements, then we'll look at the minimum, the least frequent element that we're currently storing in the heap. if that element is less frequent than the one we're holding in our hand, ip, then we evict the least frequent element from the heap and insert our current element with the count as given by pointquery. okay, so why does this work? well, it is not hard to see that following is true. for simplicity, let's assume that the set of top k, in most frequent items is actually unique. the argument is very similar without that, and is left as an exercise. so in this case for every item i, in the top k, let pi denote the last position in the stream where i occurs. we'll notice that then at position pi, our element i will be inserted into the heap if it wasn't there before, and, after that, it will never be evicted. and so the reduction actually works. good, so this is the reduction that we got. if we have an exact data structure for maintaining exact pointquery, then we can find the top k items quite easily. well, the question is why is this useful? because before we argued that exact pointquery is unrealistic. we need to pretty much store the entire stream. well it turns out that the exactly same reduction works for the approximate version of the problem. if we have a small space data structure for approxpointquery, and that's something that actually exists and we will construct it at the end of this lecture, then we can approximately find the top key elements using essentially the same set of code. just whenever we look at an item ip, the current data item arriving in the stream, we ask our approxpointquery to give us an estimate of the count, and use that estimate just as if it was the exact. okay so, the proof of this is very similar. one can check that if we have a procedure data structure approxpointquery, that for every element i at every point in the stream returns an estimate of i's count so far, which is off by at most fk, which is the final frequency of the kth dominant element times epsilon, then we can get a data structure for approximate top k. okay, so now that we have this reduction, in the rest of the lecture, we will concentrate on designing a small space data structure for solving the approxpointquery problem. so we want to design a small space data structure that observes the date items in the stream, and at the end of the stream, can be queried to give an approximate answer to the number of occurrences of any given data item so far. what we will have to specify are the actual parameters of our data structure, so how much space do we use? what is the quality of approximation that we provide? and what is the success probability with which we work? this will be done in the rest of the lecture. 
in this video, we will design a very simple and basic version of our approxpointquery algorithm. the procedure that we design in this part of the lecture will not be strong enough to solve our final problem. nevertheless, it will serve as a building block for our final algorithm. or to simplify our teaching, in this part of the lecture and after that, we'll make the following assumption. we will assume that the elements in our data universe are ordered in descending order of their frequency in string. so we think of element one as being the most frequent in the string, element two being the second most frequent etc, etc. so the stream looks like this. again, at the bottom of the stream, we see data items arriving in the stream one by one. and a frequency histogram is updated on the fly. so this is the stream that our algorithm is processing. and at the end of the stream, we see that the elements are ordered in descending order of their frequency. our final goal in this section is to design a small 15 structure that at the end of the string will let us output the top few most frequent items that we have seen. so, for example, if we were looking for the top four most frequent items, we would like to output these items shown in green on the screen. and we will refer to these items as the head of the stream because they contribute a bulk of the mass. the other items are shown in red, so these are the items that don't occur very frequently, we will refer to those as the tail of the stream. okay, so our goal for now is to design a very basic estimate, very basic version of our approximate point career primitive. this version that we design, in this part of the lecture, will have excellent space requirements. it'll only use a constant amount of space. the problem with it will be analyze precision. nevertheless, later on, we will show how to use it as a building block for the final hour. our basic estimate works like this. well we start by choosing a uniformly random hash function s, that maps our universe of data items. and remember that we associate our data items with numbers, integers, between one of them. our has function maps integers from 1 and m to -1 and +1 independently. now the algorithm our data structure is very simple, it only maintains a single counter that we call c. at the beginning of the stream, we initialized the count to 0 and then the update procedure upon receipt of a new data item i is extremely simple. we basically just increment the counter by the sign of i. so in fact, if the sign of i happens to be plus, the counter's incremented and if it's a minus the counter's decremented. now the processing of the stream now becomes very simple. for every position p in the stream, between 1 and n, we run the update procedure, which simply increments the counter by the sign of the current data item i sub p at the end of the stream. if we're asked to return an estimate for the frequency of item i, what we do is, we take the counter and multiply it by the sin of i and return that as the estimate. so this is a very simple algorithm and indeed it is somewhat hard to believe that this will have the right precision and it will not, but it will have very interesting properties and it will give non-trivial precision that we will later boost into our final algorithm. okay, so we want to analyze the precision of this estimator. this is a randomized estimator so the first question we need to ask is, well, how does one actually argue that a randomized estimate works? so we have a random variable which is the counter times the sine of a fixed data item i that we want to query. we want to prove that this random variable is close to the true answer, which is the frequency of i with high probability. well typically, such proofs go as follows. they proceed in two steps. well first, we'll look at our random variable, the counter times the sin of i, and we want to prove that in expectation this random variable equals the true answer fi. another way of putting this is saying that our estimator is unbiased and it has the right expectation. if we're able to do this, then the next step is to show that our estimator in fact is close to it's mean with high probability. one way to do that is to bound the variants of our estimator. if we're able to show that the variants is appropriately small, we get the result by standard concentration qualities. okay, so let's proceed and we'll proceed to step one. we would like to bound the mean of our randomized estimator. now before we take any expectations with respect to our sine function, i want to look at our estimator which is the counter times the sine of i. and i want to write it in a somewhat more convenient way. okay, well by definition, the counter, times the sine of i, can be written as the summation over all positions in the stream from 1 to n of the sine of the item that arrives in position p times the sine of i. well, we can equivalently write this as a summation over all data items j in existence of the frequency of data item j in the stream times the sine of data item j, times the sine of i. well, since we're estimating fi, it's convenient to extract from destination the contribution of item i itself. so we get the frequency of i times the sine of i squared which is nothing but one. plus some contributions from the other data items. namely, a summation over j and m minus i, data items j other than i. the frequency with which j occurred, times the product of science, s j times s i. okay, and this is already looking great because what we see is that our estimator can be written as the true answer, plus some contribution from the other elements in the universe that, hopefully, we can say looks like noise. and, indeed, it does, as we will see in the rest of this lecture. good, so our first step should be to show that the expectation of this other contribution that we hope to say is noise is zero. so let's just take expectations of both sides of this equality. well to take the expectation of the counter times s(i) with respect to the sine function. of course, the expectation of fi is fi. fi is a deterministic quantity. now, here we can take the expectation inside the sum. and now, because the sine of j and the sine of i are independent random variables when j is distinct from i, we get the product of the expectation of the sine of j, and the expectation of the sine of i. but both of these quantities have mean zero, each data item was mapped to minus one or plus one, with equal probability. so indeed, the contribution of this noise like term in expectation is 0. this means that the mean of our randomized estimate is exactly correct. the mean is fi. so this is great. so our next step, and this is what we will do in the next video, is to estimate the variance of our estimator and show that its small. this will show us that the estimator is actually close to the true answer, fi, with high probability 
well, now that we have been able to show that the expectation of our randomized estimate for fi is exactly fi. we would like to bound the variance and show that our randomized estimate for fi actually is close to fi with high probability. so, one way to show this is to use chebyshev's inequality, which is a quantity aversion of the law of large numbers. chebyshev's inequality says the following. that if we have a random variable x with mean mu and variance sigma squared. then the probability that this random variable deviates from its mean by more than a factor t times its standard deviation sigma is at most, 1/t squared. so, our plan is as follows. we would like to apply chebyshev's inequality to our randomized estimator. so, we want to let x, the random variable, equal the counter times the sine of i, where i is a fixed date item. and to do that, we need to bound the variance, because we know that the mean is fi already. so, let's move onto variance. for that, we want to recall the expressions that we have for our estimator. we know that the counter times s(i) converting as fi plus the contributions from the other data items j distinct from i, f(j) times its sine times the sine of i. we also know that the expectation of the counter times s(i) is exactly fi. so, what we want to bound is the variance of our estimator, which is nothing but the expectation of the sum over j different from i that is contribution from other elements in the universe. of their frequency times their sine times their sine of i in try quantity squared. now it turns out that this is not too hard to compute which we will now do. before we apply the expectation, i want to design a somewhat simpler form that will be easier to apply the expectation to. so the counter times s(i)- f(i) the entire thing squared can do it in estimation over j different from i. j prime, different from i of the products of the frequencies, fj and fj prime times the product of sine times the sine of i squared. now of course the sine of i squared is nothing but 1. so, what we get is a double estimation over a pair of items j and j prime that are distinct from i of the product of their frequencies and the product of their sines. so, now if we take the expectation of this quantity, we'll see that the following happens. if we look at a term in this double summation that corresponds to j different from j prime. then the expectation of the product of the sines is exactly zero by the same token in our mean analysis. so, what we get is that all terms with j distinct from j prime are zero in expectation. and so the variance, is exactly the summation of all data items in the universe other than i of their frequency squared. okay, so we have been able to construct a randomized estimate for the frequency of any fixed item i. and we have been able to balance variants, so we would like to now conclude by chebyshev's inequality our estimate is good. meaning that our estimate is close to its mean the frequency of i with high probability. but in order to do that, we need to check that the square root of the summation over other data items j of their frequency squared is actually small compared to the frequency of our item i. now, this is not generally true. this depends on the actual stream, and on the actual item that we're trying to estimate. so, let's look at some examples, because these examples will show when our estimate works well, and when it does not. this will inform us in using our basic estimate that we just constructed as a building block for the full algorithm in the rest of the lecture. okay, so this is our expression for the variance. and again, think of item i as fixed, this is the item that we're trying to estimate. now suppose for a second that our stream of data items has the following frequency histogram shown on the slide. in this histogram, item number 1 which is the most frequent item in the stream dominates the stream. well, if that is the case, and if we're trying to estimate item one, then our estimate should work fairly well. indeed, our estimate works well exactly when the frequency of item i is larger than roughly the square root of the sum of squares of other frequencies. on the other hand, if this is our stream and instead of estimating item one. we're trying to estimate our item six, our estimate is quite poor because the error in estimating item six will include a contribution from item one. and that item dominates the stream, so that is a problem. so, to summarize, this very, simple basic estimate that we constructed works really well if we're trying to estimate elements that dominate the stream. and works quite poorly when we're trying to estimate elements that don't. especially in the presence of very large elements in the stream. okay, and so now our final algorithm. our full version of approximatepointquery and the final countsketch algorithm will fix these deficiencies of our best basic estimate. the way our algorithm works is as follows. in order to find the top k elements approximately, what we will do is this. we're first given our data string we'll hash the data items in our universe into k buckets. well, this can be viewed as the process of constructing substreams. now we will run our basic estimate that we just designed on every substream that is obtained via these hashing posts. now finally to boost the confidence of our estimates, we'll repeat this procedure a logarithmic number of times. and this will be our algorithm. that the main intuition for why this works is exactly this. this hashing process will allow us to insure and when we're estimating a large item, then most of the substreams that we estimated from will look like the one on the top of the slide. we'll show that our current sketch algorithm will be estimating the large items mostly from substreams that are dominated by these large items. now, if you want to estimate a small item, you should also be able to figure out that this item has a small frequency. the algorithm, as we will show, actually estimates the small items mostly from streams that look like this, that don't have any of the large items contaminating the estimate. and in the next part of the lecture we will see the details of the algorithm and some of the main ideas behind the analysis. 
in the rest of the lecture, we'll put together the ideas that we developed for our basic version of the approximate point query data structure, to get a full float approximate point query. and finally, the count sketch algorithm that gets good results, good approximations to frequency counts with high probability and small space. i will start by providing the intuition for the improvements that we have to make to the original version of approximate point theory. then i will define the algorithm itself and finally give some intuition for the analysis of the parameters. okay, we will recall that our basic version of approxpointquery suffered from two issues. well first, the precision of our estimates depended on the variance of a certain estimator, and that variance sometimes was bad. for example, when several dominant coefficients were present in the same string. to fix this deficiency, what we do for our final algorithm is run our basic estimator that we already designed on subsampled or hashed version of the stream of data items that we get us in. the reason why this works is that this operation as we will show later reduces variance of our estimates. now the second problem with our basic version of approxpointquery was that the success probability was fairly low. it would only succeed with high constant probability. on the other hand, our final algorithm has to provide good estimates for all data items that appearing in streams simultaneously. so to fix this deficiency and make the algorithm run with high probability, we introduce the idea of repeating our estimates independently many, many times and taking medians of answers that they report to boost our confidence. okay, so the main idea of this algorithm, and the first idea that makes this work, is the idea of hashing the input stream into a number of buckets. so again, here's our data items, we associate them with integers between one and some large number, for our examples, we use 1 through 10. what we do is we choose a hash function that maps the data items to a number of buckets. so in this particular case, on the slide you see a random mapping of data items 1 through 10 into 8 hash buckets, b is number of hash buckets, it's equal to 8. the mapping is shown as the arrows, so for example, element number 1 is mapped into bucket 3 and element number 1 also happens to collide with element number 6 in bucket 3. element 10 on the other hand is residing alone in bucket 2, it doesn't collide with any other element. good, now this idea of hashing let's us define the notion of hashed streams. in particular, for every item i, we will say that the hashed stream of item i contains all the other elements that collide with i under hash function h. an example, the subsampled or hashed stream of item 1, under this particular hash function shown on the slide consists of 2 items, item 1 itself and item 6, because they collided in bucket 3. the subsampled or hashed stream of item 5 consists of two items, 5 and 7, because 5 and 7 collided in bucket 5. now it is important to note that we are hashing the universe of data items into a number of buckets, and not positions in the stream. let me give a specific example. so this is the stream that we started with at the beginning of this lecture. so it contains occurrences of items from 1 to 10, and here is the frequency histogram. under the hash function from the previous slide, the subsample stream of item 1, which is the heavy hitter, one of the dominant items, most frequent item on this stream, is shown at the bottom of the slide. it consists of all the occurrences on 1 in the stream and all the occurrences of 6. the frequency histogram for the stream is shown at the bottom of the slide. the crucial observation is that this frequency histogram looks exactly like the good case for our basic estimator from the previous part of the lecture. indeed, if we were to estimate the frequency of item 1 from this particular stream then the arrow would only be contributed by the item number 6. which means that we would actually get a good estimate, because item number 1 dominates the subsampled or hash stream. okay, but item number 1 was a heavy hitter, it was the most frequent item on the string, but see what happens for other items. so for example if you look at the subsampled hashed stream of item number 5, all that consists of two items, 5 and 7. just all occurrences of these items and 5 occurs twice and 7 occurs once, the frequency histogram again is shown at the bottom of the slide. so note that if we were to estimate item number 5 from this particular stream, we would actually get a very good estimate. because this estimate would not be contaminated by occurrences of any of the heavy hitters in the original stream. and this is the basic idea for why our new version of the algorithm will actually work well. 
well, we're now in a position to state the final algorithm for a proximal point query and hence the count sketch algorithm. the algorithm proceeds by first choosing two parameters. we choose the parameter t which is the number of repetitions of the hashing scheme that we'll perform to boost the confidence of our estimates. and one should think of t as logarithmic in the length of the stream. we will set these parameters later in this lecture. the other parameter is b, which is the number of buckets that we will hash our universe of data items into. and b should be thought of as a large constant times k, the number of heavy hitters that we would like to approximately find. so the algorithm starts by selecting t uniformly random hash functions that map our universe of data items into b buckets. and also, t uniformly random sign functions that associate uniformly random signs with every data element in our universe. now this can be conveniently visualized as an array c shown on the slide. this array contains t rows, corresponding to the t independent repetitions, will index the rows by a parameter r, little r. and b columns, every column corresponds to a hash bucket now the pseudo code for the algorithm is as follows. the algorithm will maintain this array c of numbers, of counters and every cell in the array c will correspond to an invocation of the basic approximate point query primitive that we just designed. specifically, the update process looks like this. during a data item c, data item i, we run over all the rows r from 1 to t. and for each row r, we use the rth hash function to figure out the column that our element i has to contribute to. and we increment the corresponding counter, that is, the cell in r throw and h of ith column, by the sign that the sign that the rth sign function assigns to our element i. now the pseudocode is given on the slide and it should be noted that this simply amounts to running for each row r our simple approximate point query primitive that we just designed on the stream of data items that hashed into the same bucket as i itself. okay, so this is the update process. now the estimation process works as follows. at the end of the stream, if we are given an item i whose frequency we want to approximate, what we do is we run over all the r rows in our hash table. and for each row, we take the product off the counter in row r, and the column that i hash to with a sine of r, under the r sine function, which is exactly the estimator that we designed for our basic approximate point query primitive. except that now we have r independent copies of these approximate primitives, and we take the median of the answers at the output as the final result. so for example, the data item 5 hashes into three distinct buckets in every of the three rows of array c shown on the slide. so it will contribute to these three buckets when we perform the update process. and then the answer will be computed as a median of the three values found in these buckets, multiplied by its sine. okay, well, this is the algorithm. now the question is why does this algorithm provide precise estimates for every single item in the stream with high probability. well, the first observation here is that, by our previous analysis of the basic estimation primitive, basic version of approximate point query a few slides back, the mean of every of these t estimators that we use, that we take the median of in our new version, is exactly correct. the mean of every of these estimators, for every r between 1 and t, is exactly the frequency of the i'th item. okay, well, this is reassuring, but as of now this doesn't mean that our algorithm is any better than the basic primitive that we designed before. the distinguishing factor comes in the variance of our estimate. recall that the variance of the basic primitive that we designed before was equal to the summation over all data items j that were different from i of the frequency of the jth item squared. now it is possible to show that the variance of our new estimator is the summation only over those data items j different from i that hashed to the same bucket as i of their frequency squared. now our hash functions map the data items to b buckets, so we should expect the variance to be a factor of one or will be smaller than before. and indeed, it is possible to prove that this is the case. now what this means is that if we choose the number of buckets that we hash into to be sufficiently large, we can drive the standard deviation of every of those t estimates to blow our precision parameter epsilon times the frequency of the kth most frequent item, fk. now this is exactly the additive precision of our estimates that we're shooting for for our approximate pointquery primitive. so choosing the number of buckets large enough fixes the precision issue. now we also have to ensure that our estimates work with high probability for all data items, and this is what we fix by taking medians of algorithmic number of independent estimates. okay, so the formal statement about the position and success probability of our scheme is given by the following lemma. this lemma says that if we choose the number of buckets to hash into to be at least the following quantity, which is the maximum of two numbers, well, first the number of buckets has to be bigger than a large constant times k, which is the number of heavy hitters that we're shooting for approximating. at the same time, it also should be larger than our large constant times the following ratio. this ratio of the l2 squared norm of the frequencies of the tail of the stream, divided by epsilon squared, times the frequency of the kth largest element squared. okay, so if the number of buckets b is chosen to be sufficiently large in the following way, and if we choose the number of repetitions of our hashing scheme to be logarithmic in the length of the stream, then we have that our median estimator is at most epsilon times fk far from the true answer for every data point i and from every position in the stream. okay, now this is exactly the additive precision that we wanted to get, and it remains to understand what our sample complexity is. now recall that the algorithm stores this array c which has t rows and b columns. now the parameter t was chosen to be logarithmic in n, and the parameter b, however, depends in non-trivial ways on the actual structure of the stream. so now the question is, how large is b? this is possible to show that for every stream, b is on the order of k over epsilon squared, for example. this would be very nice. now unfortunately, this is not true. indeed, imagine a stream where every item appears exactly once. then all of the fjs are exactly 1 and we get that we need to set b to be omega of n over epsilon squared. okay, now this is really terrible because in space order n we could have stored the entire stream anyway and solved the problem exactly. however, note that in the importance case where the stream is actually dominated by the top k elements, b turns out to be very small. specifically, suppose that if we sum the frequency squared over all the elements jamming the tail of the signal, or rather of the stream, and divide this by k. so think of summing the squared frequencies over the tail of the stream, and then spreading this equally among the top k elements, the heavy hitters. and if this turns out to be on the order of the frequency squared of the kth largest element, then hashing into b on the order of k over epsilon squared buckets turns out to be sufficient to find the heavy hitters. and this is exactly the sense in which we said that we were going to solve the problem for streams where heavy hitters dominate the stream in a certain sense. and the actual formal sense is the l2 sense shown on this slide. now of course, in practice we don't know what the stream looks like, so we cannot choose b according to this formula. what we can do, though, is choose b as large as possible subject to space constraints because hashing into more buckets only improves the precision guarantees of our l. okay, so now i would like to spend a little bit more time talking about the number of buckets that we have to hash into and showing that actually this number is surprisingly small for some very interesting cases. and for simplicity let's think of k equal to 1. that is, we have a stream and we want to find the most frequent element in the stream. now suppose further that this most frequent element, and by our assumption it's element one, appears square root n times in the stream. and all the other elements appear exactly once. so they're n minus root n elements in the tail of this tree. okay, so now this means that f1 is equal to square of n, and for every i between 2 and n minus root n, fi equals exactly 1. so all of the other elements appear once. now, the lemma shows that if we set b to be the following quantity, the sum of frequency squared over the tail of the signal, divided by epsilon squared, fk squared, then we're able to find our heavy hitter. now all the fjs in the tail of the signal are one, so the summation of their squares is bounded by n. on the other hand, f1 is equal to root n, so f1 squared is n as well. now what this means is that setting b to be a constant divided by epsilon squared suffices to find our heavy hitter, the most frequent element in the stream. and i would like to stress that this a very remarkable result. because if we think about this, the element 1, that is, the heavy hitter, appears only in square root n positions in the stream. so that's a vanishingly small number of positions. if we were to sample a constant number of locations we would never see this element. this means that the algorithm that we have just constructed is quite powerful. okay, well, to summarize, we started this lecture with the goal of designing a small space data structure that can scan a stream of data items arriving one after another, maintain a small amount of space. and at the end of the stream, output an approximation to the top k most frequent coefficient scene in the stream. we showed that this problem can reduce to the problem of designing a different data structure which we'll call approximate point query. this data structure, again, uses a very small amount of space, scans the stream of data items one after another. and at any point, this data structure can be stopped and asked to return a good approximation to the number of times any given data item has been seen so far. now we gave a very powerful primitive for the approximate pointquery data structure. it gives very precise estimates with high probability for all data items. now this primitive also uses a small amount of space on the order of k log n, assuming that the top k elements actually dominate the stream in a certain l2 sense. now putting these two pieces together gives us the countsketch algorithm. 
in this part of the lecture, we will prove our main lemma on the precision guarantees of the final version of our proximal point query primitive. now this lemma is shown on the slide. it says that if the number of hash buckets that we hash into is chosen to be sufficiently large and if the number of repetitions is logarithmic in the length of the stream then our approximate point query primitive gives an approximation to the frequency count of every data item in the universe at every position in the stream that is correct up to an additive epsilon times fk term. where fk is the frequency of the kth most frequent item in the stream. okay, well the failure probability in the statement will be inverse polynomial in the length of the stream. and because of that, instead of proving the lemma for every point p between 1 and n in the stream, it in fact suffices to prove it for the last position in the stream. because after that we can apply the lemma to every prefix of the original stream and get the lemma from the previous slide. okay, so this is the statement that we would like to prove. well, let us start by observing that in fact for every row i of our rec. that is for every of the key hash functions, h r, that we chose by the basic analysis of our basic version of the approximate point query primitive we have that every single one of the estimators that we're taking the median off has the right mean. that is for our data item i, which we consider fixed from now on. for every r between 1 and t, the expectation of the rth estimator is exactly the frequency of the ith item. and now this is very good, it is reassuring, but it doesn't tell us that this estimator is actually better than the basic version of the estimator that we had before. and the distinguishing feature of our new estimator counts in reduced variants. specifically by the analysis of our basic estimator applied to every single one of the little t estimators that were taking the median off we have that for every i. and for every fixed r from 1 to t, the variants of our rth estimator for the frequency f i is equal to the following quantity. it is the summation over all other data items j different from i, that hashed into the same bucket under the r hash function of the frequency, fj squared. and note, that the summation, those only over those j's in the data universe that hash to the same bucket as i. now okay, so our hash functions or hashing the data universe into b buckets. so we should expect the right-hand side to be about a factor of 1 over b smaller in expectation than in our basic analysis. and this is where our gains will come from. okay, so we would like to show that the variance is small. and we would like to show that it at least reduces by roughly a factor of b. in fact, we'll be able to do a bit better than that. now specifically, we consider the variance of our estimator. and we separately consider the contribution of the head element of the stream to this variance and the contribution of the tail elements. so we write the sum over all data items j different from i that hashed under the same bucket, under the r hashing, and we think r is fixed from now on. we write the summation as two separate summations. the first one is a summation over data items j in the head of the stream. think of the heavy hitters that are different from i and again hash into the same buckets of f j squared. so recall that the head of the stream is exactly the top k most frequent elements in the stream. the second term is a summation over the tail of the string, so the less frequent elements that occur in the stream of fj squared. and this is again only over those j that hash to the same bucket as i under the r hush function. now, our plan is as follows. we would like to bound these two terms. what we will do is for each fixed i and for each data item i, we consider both fixed from now on, we will define the following three events. the first event is an event that we will call no collisions of r and i. this event occurs if and only if our data item i does not collide with any of the top k most frequent elements in the stream that is the head of the stream under hashing r. the second event is a small variance event, again it indexed by the hash function index r and i itself. this event happens, if and only if, i does not collide with too many of the tail items under hashing r. so note, that the first event, if it occurs, implies that the first summation above is just 0. there are not other events in the head that i collides with. and the second event will insure that essentially and the second summation is fairly small. after that we will also need a third event which we denote small deviation of r and i. and this is essentially the success event from our basic analysis from the outcome of probabilities inequalities. what we will show in the next few minutes is that in fact for every r and every data item i, all three of these event on the slide hold simultaneously with probabilities strictly better than one half. and after that we'll later show that this implies that if we take a logarithmic number of independent trials, the median estimate will give us a good estimate with high probability. 
so we need to define the no collisions, the small variance, and the small deviation events. let's start with no collisions. we think of i and r as fixed. we let the no collisions of r and i event denote the event that under hash function h r. none of the head elements, j distinct from i, collide with i or has to the same bucket. a note that this definition makes sense for any data item i, whether or not it belongs to the head or the tail in the stream. okay, so why does this event hold with rather high probability? well, recall that our hash function was chosen at random, so for any two distinct elements, i and j. the probability that they collide under the hash function is at most one of the number of buckets] that we hash into. and in fact, it is equal to the number of buckets, but we only need inequality in this instance. good. well by the assumption of dilemma. the number of buckets was chosen to be at least a large constant times k the number of heavy hitters. so now, by union bound over o j on the head of the stream, we have that the no collisions event occurs with reliability. at least 1- k over b, which is at least seven eights by our assumption. great, so let me remind you that we wrote the variance of our estimator as a contribution from the head of the stream. and the contribution from the tail and conditioned on the no collisions event that we just defined. the first contribution's exactly zero, because the summation is over the empty set. so we now know, we now need to handle the second sum. to that effect, we define the small-variance in it. formerly, we say that the small variance of r and i event occurs if the following conditions do. if the summation over elements j and the tail of the signal that are distinct from i and also hash into the same bucket as i. under the rth hash function of their frequency squared is at most eight over b. the number of r hash buckets times the summation of all j in the tail of the frequency of j squared. well, why does this event hold with high probability? well again, recall that our hash function is uniformly random. so for every pair i n j that are distinct, the probability that i n j collide is at most 1 over b. what this means is, that we can take the expectation of the left hand side of this equation and we can write it as follows. the expectation of the left-hand side can be written as the summation over all j. and the tail of the signal that stems from i of fj squared, times the probability of j and i colliding. and that is at most one over b of the entire sum. okay, so now this is the expectation of the left-hand side. why is the left-hand side actually with rather high probability at most eight times higher than its expectation? well, now that's by markov's inequality.. markov's inequality says, that for every non-negative random variable x with mean mu, the probability that x. should overs mean by a factor of k is it most one of a k? so now what we do is, simply apply markov's inequality to the left-hand side of our expression with k equal to eight. good, so we just proved that the expectation of the left-hand side of our inequality is it most one over b times the summation. over all j in the tail of the signal, their frequency squared. by markov's inequality, the probability that this small variance event occurs is at least 1-1/8. that is at least seven over eight, so we wrote the variance of our estimator as the contribution from the head of the stream. and the contribution from the tail of the stream. then, we defined two events. the no-collisions event and the small-variance event. such that conditioned on both of these events. the first contribution is just zero, because the summation is over an empty set. and the second contribution is at most 8 over b times the sum over all data items j and the tail of the stream of their frequency squared. at this point, we need the very last piece. the small deviation event. the small deviation event occurs by definition if the squared deviation of our estimator. mean is at most eight times as variance. how about chebyshev's inequality? this happens with probability at least seven eights. okay, well at this point we actually already have a basic version of our lemma. from which the full version will follow by standard concentration of equalities. now, to state this basic version of the lemma, it is convenient to introduce a parameter of gamma, defined at the top of the slide. we'll let gamma be the square root of 1 over b of the sum of frequency squared all of our frequencies in the tail of the stream. now that we have proved that the three events that we defined occurred simultaneously with probability five over eight at least. we actually have the following claim, stated as the lemma on the slide. if the number of buckets b that we hash into is at least eight times k, then for every data item i, and for every r. between 1 and t, the probability that rth estimate for the frequency fi is 8 gamma close to the true answer, is at least 5 over 8. this can be verified by substituting gamma into our expressions. okay, so this is very good. there are two things that we need to understand. well first, the success probability is only five over eight for a fixed data item i and for a fixed r. whereas we need our estimate to work with high probability for all data items i. we need to fix that and then, we need to relate the precision guaranteed by this lemma to the precision that we want to achieve. that is additive epsilon times f k error term, so let's fix the success probability first. and i'm claiming that using standard concentration equalities. that is chernoff bounds that i will not get into details here. one can show starting from the lemma on the previous slide. that if we take a sufficiently large logarithmic number of independent copies of our estimator then the following will be true. for every data item i with all but say n over 1 to the 4 probability. the median of our estimators will be 8 gamma close to the true answer. now, this follows by standard concentration equalities turn of bounds but i will not give the formal proof here. okay, well, this is very good. because we have just been able to show that our estimators succeeds. that is, gives an answer that is 8 gamma close to the truth. for every fixed data item i with probably at least one minus one over n to the four. so, now we can take the union bound over all data items in the universe, of which there are at most n. those that appear in the stream, and conclude that in fact, our estimator is correct for all data items simultaneously. with probably at least one minus one over n cubed. so we know that for all data items now, with high probability the estimator is 8 gamma close. to get a proof of our final lemma. all we need to understand is how does this 8 gamma relate to the additive error term of epsilon times f k. the frequency of the k-th largest element that we wanted to achieve and this can be verified by direct substitution. recall that the number of buck is b was chosen to be appropriately large. where large depends on the actual statistics of the stream. this was the assumption of our lemma. one can verify it by direct substitution into the expression for gamma. that 8 gamma turns out to be less than epsilon times fk and thus concludes the proof of the lemma. 
hi, and congratulations on finishing all the courses in the specialization. now, the only thing that stands between you and the certificate is me. actually, it's not me. it's the capstone that i will present today. i want to explain that there are some differences between the courses you took before and this capstone. in the courses you took before, we presented you with this perfectly formulated clear algorithmic problems, that defines the range of possible parameters and the specifies the algorithms that you need, to solve this problems. in the capstone, the things will be different, the problems will be loosely defined, and you will need to transform them into exact algorithmic problems. it will not be immediately clear what is the range of the parameters, and will give you only the hint, rather than details of the algorithms that you need. however, you are now algorithmic pros, and i'm confident you will be able to solve all the challenges in this capstone. one of the biggest news in 2011 was european e coli outbreak. it started as food poisoning with bloody diarrhea, that often followed by kidney failure and death. the outbreak quickly spread from germany to many european countries. and in the beginning it was unclear what was the cause of the outbreak. the usual suspects in the case of outbreaks are different vegetables, but which vegetables? cucumbers, carrots, there are a lot of choices. in the beginning of the outbreak, german officials identified cucumbers as the likely source of infection. and thousand of tons of cucumbers, and other vegetables were destroyed all over europe. four years later, it turn out that german officials were wrong, and they were ordered to pay compensation to spanish farmers who lost billions from destroyed cucumbers. in 2011, german house officials identified a restaurant in lubeck where 40% of all visitors developed bloody diarrhea. after interviewing patrons of this restaurant who developed bloody diarrhea, they figure out that almost all of them ate bean sprout. the owner of this restaurants was outraged and publicly offered to eat all sprouts in his restaurants. but it turned out that the cause of the outbreak was actually a huge lot of bean sprouts that was sent from egypt to europe, and was now a time bomb sitting in hundreds of stores and restaurants all over the continent. in may 2011, a girl from hamburg developed bloody diarrhea after eating sprouts, and doctors suspected that it was a common pathogenic e coli strain. but the blood sample from this girl did not pass the test for known e coli strain. at this point, it became clear that a new pathogen has emerged, and the goal was to sequence the genome of this pathogen, and to figure out how it has become pathogenic. and our goal in this capstone will be to figure out what is the genome of this mysterious e coli x from the girl admitted to a hospital room in hamburg? to answer this question, you have to develop your own assembler and apply it for assembler grades obtained from a girl from hamburg who developed bloody diarrhea. developing genome assemblers is not for fainthearted. to learn more about how it is done, you may attend our bioinformatics specialization on coursera or read the book bioinformatics algorithms, an active learning approach. assembling e coli x bacterium is a rather complex algorithmic challenge. and to make it easier for you, we broke it into three simpler challenges. we start from assembling phi x174 virus, which is a small little over 5,000 nucleotide virus with just 11 genes. after wards they will assemble the smallest bacteria genome know, called n deltocephalinicola, with it's just 110 thousand nucleotides and approximately 140 genes. and finally, they will assemble 50 times 100 e coli x bacteria, which has roughly 5 million nucleotides and approximately 5,000 genes. our first task will be to assemble a small phi x174 phage. phages are bacterial viruses and they cannot replicate on their own and must infect bacteria to do so. and phage x174 is almost like a cult organism in genomics, because it was the first sequenced genome completed by noble prize winner fred sanger in 1977. you will follow in the footsteps of fred sanger to assemble the phi x174 genome. our next task will be to assemble the smallest bacterial genome. this bacterial lives inside leafhoppers. and it's sheltered life allow it to reduce the genome to only about 110,000 nucleotides, 50 times smaller that e coli x genome. and only approximately 140 genes. it lacks some genes necessary for survival, but products of these genes are supplied by its bacterial host. and biologists believe that this bacterial genome is losing its bacterial identity and turning into part of the insect genomes. just like mitochondria in human cells. and your goal will be to follow in the footsteps of biologists who sequenced this genome in 2013. and finally, after your sequence the phage and the smallest bacterial genomes, you will assemble e coli x. 
you will start from assembling phage genomes and we will give you 1,000 simulated error-free reads, randomly sampled from the phage genome. and i want to tell you that the phi x174 genome is a circle genome. each read will be 100 nucleotides long. to add suspense, all genomes in this capstone will contain a tag, a 10-nucleotide-long insertion, that we added to the genome to make your life a little bit more difficult. and your goal will be to figure out the sequence of the tag. you are now facing the genome sequencing problem, which is to reconstruct a genome from reads. input to the genome is a collection of strings, reads and output is string genome reconstructed from reads. is the problem clear? this is the first example of the problem in this specialization that has not been well formulated. this is actually not a computational problem, because i have not described precisely what does it mean to assemble a genome from reads? in fact, it will be your task to figure out how to formulate algorithmic statement for this biological problem, what does it mean to assemble a genome? you will need to formulate a rigorous algorithmic problem that adequately models genome assembly. and it will be more like in real life, because in real life you won't be presented with perfect algorithmic formulations. you will presented with real life problems and you goal will be first to transform this real life problems into computational problems. and later on to solve the problem using the algorithms that you learned in this course. and your programming challenge will be after you formulate this problem. your programming challenge will be to assemble mutated phi genome from simulated error-free reads and find out the inserted tag. after you assemble phage genome from error-free reads, your more complex task will be to assemble this same genome from error-prone reads. and in this case, each read will have errors only substitutions of nucleotide, no insertion and deletions with it's probability 1% at each position. this is actually similar to the errors in real sequencing reads. and first you have to formulate this question, which means formulate the rigorous algorithmic problems that adequately model genome assembly from error prone reads. and finally solves the programming challenge, assembling mutated phage genome from simulated error-prone reads and of course, finding the inserted tag. in fact, this programming challenge is very similar to the challenge that fred sanger, the inventor of modern dna sequencing, faced 30 years ago when he assembled the same phage genome. however, at that time, sequencing was very expensive and extending his sequencing method to human genome at that time would cost hundreds of billions of dollars and would be impractical. and that's why many scientists try to find out what would be a better experimental technology to sequence a genome. and that's how they came up with the idea of genechip that we will discuss in the next section. [music] 
we will now talk about one of the alternative technologies for dna sequencing called dna chips or dna arrays. when sanger assembled the phi genome from 500-nucleotide long reads in 1977, scaling this to the human genome would be extremely expensive and would likely fail due to unresolved algorithmic challenges. nevertheless, us government in 1984 started to plan the human genome project, that another 16 years later, in 2000, resulted in the draft sequence of the human genome. but at the same time, three scientists, at three different countries, thought about an alternative technology for dna sequencing and they invented so-called dna chips. here's the main difference between dna chips, or dna arrays technology, and sanger sequencing. in sanger technology, biologists generate some, but not all, long reads sample from the genome. and the read length is approximately 500 nucleotides, that was the read length that sanger used. in the dna chip technology, biologists generate all short k-mers from a genome. but k is much smaller in this case. instead of 500, it may be, in the original dna chips paper it was proposed that k equal to 10. ideally, a dna chip would generate a k-mer composition of a strand, a multiset of k-mers that are present in the strand like shown here. please note that some k-mers in this multiset appear multiple times, for example, atg appears three times. our goal is to reconstruct the original string from this k-mer composition. and please note that also i ordered this 3-mer in the order they appear in the string. in reality, i don't know the order. let's try, nevertheless, to assemble this 3-mer into the string, and we will represent every 3-mer as a vertex in the graph. and this is a graph that corresponds to the string. and the question i want to pose is, can you construct this genome path, so path that spells the genome, without knowing the genome? only from its composition. well, if we simply connect two k-mers, if suffix of the first k-mer is equal to the prefix of the second k-mer, then we will get this path. for example we connect taa, we will use aat. because taa ends in aa and aat starts in aa. however, if we introduce all edges based on this principle, then of course, we will construct this genome path. but in addition, we will also have to connect some other vertices with each other. in fact, many vertices, and many more. and as a result, we will get a graph like this. where is the genome path? well, it is still the same horizontal path, but remember the reality is that the order of these 3-mers in the genome is unknown, and therefore if we order them lexicographically, this is the graph that we get. where is the correct path in this graph? let's try to reconstruct this string. i will even give you hint, let's say this string start with taa. then we look at the vertex taa, and we find a vertex that is connected to this taa by any actions, in this case it will be aat. then we'll continue and continue and continue and continue, like this. what are we trying to do? so we are trying to extend the path, but what is the problem that we are trying to solve while doing this? and please note that from every vertex i visit, there are often multiple choices of the next vertex. well, if we continue further, then you will see that the problem we are trying to solve is finding a hamiltonian path in the graph. or a path that visits each vertex exactly once. [music] 
we will now discuss a problem of assembling a genome from its k-mer composition. this is a puzzle with just 16 pieces, however it is very complex, harder than you think. it may set you back for many hours because it is a highly repetitive puzzle. every frog in this puzzle is repeated multiplied times. when you try to assemble the phi genome from its k-mers, there is a sino publication because if k is small, some k-mers will be repeated, as in example we saw before. as a result, there may be multiple solution of the problems that complicate our task. and the exercise break that i recommend you to think about is to answer the following question. what is the minimum value of k for which the phi genome can be uniquely reconstructed from it k-mer composition? this is another example of a rather complex puzzle reveals repeated thesis. in fact there was two million dollar prize announced for this puzzle, for solution of this puzzle. but the puzzle remains unsolved til this day. please note that what you see at this slide is actually not the solution because there are seven empty pieces that nobody was able to place yet, and it is called eternity ii puzzle. we do not ask you to solve the eternity ii puzzle, it's extremely complex, but we ask you to solve a simpler puzzle assembly problem when you have to assemble a smaller version of this puzzle that requires placing 25 square pieces into a five by five grid. as opposed to the eternity puzzle where you have to face 256 pieces into 16 by 16 grid. 70 years ago, a dutch mathematician nicolaas de bruijn thought about solving a different puzzle, finding the string containing each binary k-mer exactly once. he called the strings universal strings. for example, these are all eight binary c-mers. and we can construct the graph, overlap graph, for these eight c-mers and find a hamiltonian path in this graph. in this case, it will be 0001110100. but de bruijn wanted to solve this problem for any k, and you can imagine that for k equal, let's say to 20, the overlap graph will contain million of vertices, and it will be very difficult to figure out whether there is a hamiltonian path in this graph. and that's why de bruijn wanted to implement a different idea based on construction of a different graph. he wanted to construct a graph in which every k-mer correspond to an edge rather than a vertex. and where each k-universal string corresponds to an eulerian path, or a path that visit every edge exactly once, rather than hamiltonian paths that visits every vertex exactly once. and we will now face the eulerian path problem, construct an eulerian path in a directed graph. where input is a directed graph and output is a path visiting every edge exactly once. and now you may be puzzled or even confused. why in the world we would want to change one problem, hamiltonian path problem, into another problem, eurlerian path problem, that looks almost identical to the hamiltonian path problem. you will learn why in the next section. [music] 
i will now describe de bruijn graphs. it's de bruijn invented for solving the universal string problem. recall that before we were labeling vertices by k-mers and we're looking for hamiltonian path ends the result in graph. now, we will label edges by k-mers as shown here. but how would we label vertices of this graph? well, we will label, given an edge, label by 3-mers, we will label its initial vertex by prefix of the 3-mer and it's final vertex by a suffix of this 3-mer. for example, if you have a 3-mer taa, initial vertex will be labeled by ta and final vertex will be labeled by aa. as a result, we will have the following labeling 3-mers represent edges and 2-mers represent vertices. and after de brujin constructed the path, label it in this way, he started to do something strange, even counter-intuitive. so given this path, let's glue together identically label vertices in this pass. for example, there are multiply vertices labeled at. let's start gluing them together here, here and we glue them in a single vertex. our pass has been transformed integral but that's not it. we also have tg, tg repeated vertices vertices glues them together like this. continue further, where these vertices glue them together. and this is something that is called the de bruijn graph of the string. the interesting thing is we actually don't need to know the string to construct the de bruijn graph. you can construct it from its 3-mers only. indeed, de bruijn graph of the set of k-mers pattern is constructed in the following way. vertices of the graph are all unique (k-1)-mers occurring as a prefix or suffix of k-mers in the set patterns. and edges in this graph represents each k-mer in patterns. it corresponds to a directed edge that connects its prefix vertex to its suffix vertex. and the next problem you will have to solve is constructing de brujin graph from a set of k-mers. input a set of k-mers pattern output a graph de brujin of patterns. remember, this started from a genomic path that spells the genome. and in the top reveals transformation of the genomic parts into the de bruijn graph. but where is the genome hiding in this graph? well, it was there in the beginning, we were just gluing some back there. so it must be in the de bruijn graph somewhere and here it is. so if we fold the edges of the graph, by the way, what are we trying to do when we go through those edges? we are trying to find an eulerian path that visits each edge exactly once. and in the next segment, you will know why i prefer to solve eulerian path problem over the hamiltonian path problem. [music] 
we will now make an abrupt turn from universal strings and talk about another classical problem in combinatorics called bridges of knigsberg. citizen of knigsberg, and this is 300 years old map of knigsberg, were interested in the following problem. knigsberg consisted of four sectors shown by colored circles here. and these sectors were connected together by seven bridges. so the citizens of knigsberg asked the following question. can i leave from my home, walk through each bridge exactly once, and return back to my home? and this puzzle is now known as the bridges of knigsberg problem. and this puzzle, of course, can be transformed into a graph theoretical problem if we connect all four nodes representing four sectors of the city by edges. every edge corresponds to a single bridge in this city. so if bridge connects sector a with this sector b, then we will connect node a with this node b in the corresponding graph. to solve the bridges of knigsberg problem, what problem do we need to solve in this graph? and of course, you realize that we will need to solve the eulerian path problem in this graph, or a path visiting every edge in the graph exactly once. and this is a problem that was solved 300 years ago by a great mathematician, leonhard euler. if you look at two problems, eulerian cycle problem and hamiltonian cycle problem. and we will be, instead of talking about paths, we would prefer to talk about cycles, simply because bacterial genome are usually cyclic genome, circular genome, and we are interested in interesting bacterial genome. can you find a difference between these two problems? the only difference is that the eulerian cycle problem talks about visiting every edge, where hamiltonian cycle problem talks about visiting every vertex. so why did we introduce the eulerian cycle problem instead of hamiltonian cycle problem if they are so similar? it will be become clear in a second. let's return back to the universal string problem, but let's now talk about universal circular string problem, because once again, we are interested in circular bacterial genome. and this problem is to find a circular string containing each binary k-mer exactly once. for example, for 3-mers, for these 3-mers, this is a universal circular string containing them. for example, 101 is encoded here on this circle. we already saw how to solve the universal string problem using the overlap graph. but de bruijn was not satisfied with this approach. and his idea was to construct the de bruijn graph of the same string. and that is how the de bruijn graph of this eight-string looks like. and if we want to construct a universal circular string, we simply need to take an eulerian cycle in this graph that we are building right now by visiting all edges of this graph. we succeeded building the de bruijn graph for all 3-mer. and this is a more complex de bruijn graph for four-universal strings. does it have an eulerian cycle? how would we answer this question, particularly if we are interested in a de bruijn graph for 20-universal strings with over a million vertices? [music] 
to answer the question i ask in the previous section we will have to prove euler's theorem. remember we ask the question whether the reason eulerian cycle ends as graph. and, i will first ask, is the graph for 4-universal strings balanced? and by balanced graph, i mean the in degree of every vertex is equal to the out degree of every vertex. and you can check that this specific graph is balanced. and of course, every eulerian graph is balanced. because if you can find a walk, visiting every edge exactly once, and this walk and per se the ending axis let's say k times, then it has to leave the same vertex the same number of times. so which means that the number of incoming edges in every vertex is equal to the number of outgoing edges from every vertex. so this is very simple. or what is less trivial is, as euler's proof, every balanced graph is eulerian. which means as soon as we prove that the graph is balanced, there must be an eulerian cycle in the corresponding graph. to prove euler's theorem, we will need to recruit an ant. and let it randomly walk through the graph. of course, the ant cannot use the same edge twice, because we want the ant to generate an eulerian cycle in the graph. if ant was a genius, he would simply start from an arbitrary vertex in a graph and start walking along the graph. and chances are that by the end, the ant would generate eulerian cycle right here. and ant can go home, because we have an eulerian cycle. but a less intelligent ant would start walking, and may get stuck at some vertex. but in what vertex an ant can start? you have to prove that the only vertex the ant can start is the vertex where the ant started, which means this red vertex. well, what should we do next? because we have not visited all edges of the graph yet. so the ant has completed a cycle but it's not a eulerian. can we somehow enlarge this green constructed cycle? well, please note that maybe we should start at a different vertex of this green cycle. which one? probably the vertex where there are still some unexplored edges of the graph. so let's try to start in this vertex. and we have chosen this one because there are edges that have not been traversed by the ant. we will have to give the ant different instruction. the ant now doesn't just start walking randomly in the graph because it may end up in the same incomplete cycle. instead, we first have ask the ant to traverse the same green cycle starting from the new vertex. so it will go here, here, here, here and then return back to the red vertex. however, the ant's work is not over yet because now he can continue. and he continues working, working, working and finally returns back to the same red vertex. so we successfully enlarge our green cycle and it's now green-blue cycle. but what do we do next? we are stuck again but let's repeat the same procedure. let's find a node where there are still unexplored edges. here is a vertex with still unexplored edges. and let's once again force our ant to traverse previously constructed green-blue cycle. so let's go, continue, continue, continue, continue, continue. we return back to the red vertex, but now there is a possibility to walk further. we continue enlarging the green-blue cycle. continue. continue and finally, the ant proved euler's theorem. it constructed the eulerian cycle. and this is an example of constructive proof. when the proof of the theorem immediately implies an algorithm for constructing color and cycle. however i have to warn you, this is not the most efficient algorithm for constructing color in cycle. and we will ask you to construct a linear-time algorithm for building an eulerian cycle in the graph. and after implementing the linear-time algorithm for constructing eulerian cycle, you will be able to solve the universal string problem for any reasonable k even let's say for k = 20. the only question left is that in these graphs there are actually many universal strings. and that's fine because any universal string would solve the universal string problem. but in general assembly, there are also often many possible eulerian cycle but only one of them corresponds to real genome. and in the next segment, we will learn what biologists do to sequence the genomes today. and to address this challenge. [music] 
in fact, many genomes assemble today thousands of genomes each year. are not assembled into single circle of strength as in the case of bacterial genome but instead as split into multiply contigs. let me explain why it is happening. the previously described how to move from reads to de burijn graph of genome and finally to genome as a pass, eulerian path in this graph. but in reality, even in this simple graph, there are multiply eulerian paths. for example, here's one eulerian path, and here's another eulerian path. of each of this multiply eulerian paths corresponds to a real journey and often biologists don't have a possibility of using modern sickness and technologies to answer this question, at least to answer it on limited budget. and that's why they break genome into contigs, they note that there's known branching pathogens in this graph, correspond segments of each possible eulerian path, and as a result they break the paragraph into the following context. corresponding there's a following strain and this is often, output, as the solution of genome sickness and problem. it is an imperfect solution, but that's what we want you to do. to output contigs that belong to all possible, eulerian path in the constructed de bruijn graph. and contig generation problem will be given a set of k-mer patterns, generate all contigs in the graph, devoid of patterns [music] 
in the previous section of this presentation, we gave you a somewhat idealized view of genome assembly. now let's go to a more realistic description of how biologists sequence the genome. the reality is that these days they don't generate individual reads. they generate pairs of reads. to achieve this goal, they randomly cut genome into large, equally or roughly equally sized fragments of size in short lengths. and then afterwards, they read just the prefix and suffix of each size segment. for example, the modern most populated technology today reads 250 nucleotides from the beginning and 250 nucleotides from the end of size segment. but the additional piece of information you get from this experiment is this. two reads within a read-pair are separated by a certain length which is called insertlength. so an important update on modern sequencing technology is that instead of generating individual reads, biologists generate read-pairs. and our goal is to take advantage of additional information that read-pairs generate which is the distance between them. in other words, if you have a genome and i showed you read 1, which is tca shown in red, and read 2, which is tcc shown in blue. in addition to this rates we also know the distance between this read. and a paired k-mer is a pair of k-mer at a fixed distance d apart in the genome. for example tca, the red one and tcc is the blue segment at distance d equal 11 apart. disclaimer, biologists actually cannot measure the exact distance. they only measure approximate distance. but for a start, we will assume that the distance is exact. to model generation of paired-reads, we will consider pairedcomposition of a string instead of composition that we considered before. and pairedcomposition of the string will consist, let's say, from a paired 3-mer taa, then unknown nucleotide, and then gcc here. and the pairedcomposition will represent all such paired streamers as shown here. and the problem we will be facing is string reconstruction from read-pairs. input, a set of paired k-mer, and output, a string text such that pairedcomposition of text coincide with the set of paired k-mers. and of course it assumes that the distance between paired k-mers based on a read-pair is not. how would de bruijn assemble paired k-mers? if you understood the idea behind the de bruijn graph approach here's a hint. consider a paired de bruijn graph. [music] 
we'll now discuss some challenges that you will face while working with real sequencing data. in the past, we made some unrealistic assumptions about our sequencing data. we assume the perfect coverage of genome by reads, which means every k-mer from the genome is represented by read. for most of the problems that we looked at. we assumes that reads are error-free. we also assumes that multiplicities of k-mers in the genome are known. and we assumes that distances between reads within read-pairs are exact. in reality, we have imperfect coverage of genome by reads. reads do not start at each position of a genome. in reality, reads are error prone. multiplicities of k-mers are unknown and distances between reads within read-pairs are inexact. and this makes the problem of genome assembly more difficult than the problem being considered before. let's consider the first unrealistic assumption that reads have perfect coverage, or in other words they start, at every position of a genome. in reality they start at some position in the genome. for example 250 nucleotide reads generated by illumina, the leading sequencing company today, capture only a small fraction of 250-mers from the genome. thus, while making the key assumption of the de bruijn graphs. what should we do to generate a perfect coverage from existing coverage limited coverage barriers? there is a simple solution, lets break reads into shorter k-mers. and when you've done it for 40 on the left, the result is perfect coverage by shorter fibers on the right. and thus we can apply the de brujin graph idea to this set of reads broken into k-mers. second unrealistic assumption, with most problems in this [inaudible] so far, we consider error free reads. imagine what happens if the ad for error free reads that we considered before the fifth read, that has one substitution error. and it will result after breaking these reads into fk-mers, into many erroneous k-mers generated from these reads. how will it affect our de brujin graph? if we would construct de brujin graph of this segment from error free reads, we would see a perfect pass. but when we add erroneous k-mers, there is additional alternative pass added to this structure. and this pass forms erroneous paths forms a so-called bubble with this correct path. and bubble detection problem is design an algorithm for finding bubbles in a directed graph. outputs the number of bubbles in the de bruijn graph constructed from the k-mers occurring in 1000 error-prone reads from a mutated phi genome. we define a bubble in this case as two short alternative passes between the same vertices in the de bruijn graph, and it will be up to you to define parameters for selecting bubbles, and looking at data that we will provide to solve these bubble detection problem. and when you move to larger brujin graph for example the brujin graph for bacterial green genome, you will see an explosion of bubbles. there will be a huge number of bubbles of de brujin graph of real data sets and your goal will be to find out nevertheless, in this ocean of bubble, correct edges inside de bruijn graph. and this will result in the problem of reconstructing the phi genome from error-prone reads using de bruijn graph. the third unrealistic assumption they assumes that multiplicity of k-mers in the de brujin graph are known. for example here you have three mers atg connecting the same vertices in the de brujin graph. in reality, we often don't know multiplicities, so in de bruijn graph, instead of three address, you may only see one. and, i want you to think about the problem of inferring multiplicity of k-mers in de bruijn graph. and, in fact, this problem can be formulated as one of the problem you have already studied in this specialization before. which one? after you consider all this complications, you will be ready for solving the last challenging problem in this capstone assembling e. coli x genome from real reads. and after you solve this problem, we have the most challenging problem in this course for you. assembling e. coli x genome from real read-pairs. good luck. it was a real pleasure working with you in this specialization. [music] 